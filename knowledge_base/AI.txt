Token 1:
Artiﬁcial Intelligence A Modern Approach Third Edition

Token 2:


Token 3:
PRENTICE HALL SERIES IN ARTIFICIAL INTELLIGENCE Stuart Russell and Peter Norvig, Editors FORSYTH &P ONCE Computer Vision: A Modern Approach GRAHAM ANSI Common Lisp JURAFSKY &M ARTIN Speech and Language Processing, 2nd ed.

Token 4:
NEAPOLITAN Learning Bayesian Networks RUSSELL &N ORVIG Artiﬁcial Intelligence: A Modern Approach, 3rd ed.

Token 5:


Token 6:
Artiﬁcial Intelligence A Modern Approach Third Edition Stuart J. Russell and Peter Norvig Contributing writers : Ernest Davis Douglas D. Edwards David Forsyth Nicholas J. Hay Jitendra M. Malik Vibhu Mittal Mehran Sahami Sebastian Thrun Upper Saddle River Boston Columbus San Francisco New York Indianapolis London Toronto Sydney Singapore Tokyo Montreal Dubai Madrid Hong Kong Mexico City Munich Paris Amsterdam Cape Town

Token 7:


Token 8:
Vice President and Editorial Director, ECS: Marcia J. Horton Editor-in-Chief: Michael Hirsch Executive Editor: Tracy Dunkelberger Assistant Editor: Melinda Haggerty Editorial Assistant: Allison Michael Vice President, Production: Vince O’BrienSenior Managing Editor: Scott Disanno Production Editor: Jane Bonnell Senior Operations Supervisor: Alan FischerOperations Specialist: Lisa McDowell Marketing Manager: Erin Davis Marketing Assistant: Mack Patterson Cover Designers: Kirsten Sims and Geoffrey Cassar Cover Images: Stan Honda/Getty, Library of Congress, NASA, National Museum of Rome, Peter Norvig, Ian Parker, Shutterstock, Time Life/Getty Interior Designers: Stuart Russell and Peter Norvig Copy Editor: Mary Lou NohrArt Editor: Greg Dulles Media Editor: Daniel Sandin Media Project Manager: Danielle Leone Copyright c/circlecopyrt2010, 2003, 1995 by Pearson Education, Inc., Upper Saddle River, New Jersey 07458.

Token 9:
All rights reserved. Manufactured in the United States of America.

Token 10:
This publication is protected byCopyright and permissions should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise.

Token 11:
To obtain permission(s) to use materials from this work, please submit a written request to Pearson Higher Education, Permissions Department, 1 Lake Street, Upper Saddle River, NJ 07458.

Token 12:
The author and publisher of this book have used their best efforts in preparing this book.

Token 13:
These efforts include the development, research, and tes ting of the theories and programs to determine their effectiveness.

Token 14:
The author and publisher make no warranty of any kind, expressed or implied, with regard to these programs or the documentation contained in this book.

Token 15:
The author and publisher shall not be liable in any event for incidental or consequential damages in connection with, or arising out of, the furnishing, performance, or use of these programs.

Token 16:
Library of Congress Cataloging-in-Publication Data on File 1 0987654321 ISBN-13: 978-0-13-604259-4ISBN-10: 0-13-604259-7

Token 17:
For Loy, Gordon, Lucy, George, and Isaac — S.J.R. For Kris, Isabella, and Juliet —P . N .

Token 18:
This page intentionally left blank

Token 19:
Preface Artiﬁcial Intelligence (AI) is a big ﬁeld, and this is a big book.

Token 20:
We have tried to explore the full breadth of the ﬁeld, which encompasses logic, probability, and continuous mathematics;perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers.

Token 21:
The book is also big because we go into some depth.

Token 22:
The subtitle of this book is “A Modern Approach.” The intended meaning of this rather empty phrase is that we have tried to synthesize what is now known into a common frame-work, rather than trying to explain each subﬁeld of AI in its own historical context.

Token 23:
Weapologize to those whose subﬁelds are, as a result, less recognizable.

Token 24:
New to this edition This edition captures the changes in AI that have taken place since the last edition in 2003.There have been important applications of AI technology, such as the widespread deploy-ment of practical speech recognition, machine translation, autonomous vehicles, and house-hold robotics.

Token 25:
There have been algorithmic landmarks, such as the solution of the game ofcheckers.

Token 26:
And there has been a great deal of theoretical progress, particularly in areas such as probabilistic reasoning, machine learning, and computer vision.

Token 27:
Most important from our point of view is the continued evolution in how we think about the ﬁeld, and thus how weorganize the book.

Token 28:
The major changes are as follows: •We place more emphasis on partially observable and nondeterministic environments, especially in the nonprobabilistic settings of search and planning.

Token 29:
The concepts ofbelief state (a set of possible worlds) and state estimation (maintaining the belief state) are introduced in these settings; later in the book, we add probabilities.

Token 30:
•In addition to discussing the types of environments and types of agents, we now cover in more depth the types of representations that an agent can use.

Token 31:
We distinguish among atomic representations (in which each state of the world is treated as a black box), factored representations (in which a state is a set of attribute/value pairs), and structured representations (in which the world consists of objects and relations between them).

Token 32:
•Our coverage of planning goes into more depth on contingent planning in partially observable environments and includes a new approach to hierarchical planning.

Token 33:
•We have added new material on ﬁrst-order probabilistic models, including open-universe models for cases where there is uncertainty as to what objects exist.

Token 34:
•We have completely rewritten the introductory machine-learning chapter, stressing a wider variety of more modern learning algorithms and placing them on a ﬁrmer theo-retical footing.

Token 35:
•We have expanded coverage of Web search and information extraction, and of tech- niques for learning from very large data sets.

Token 36:
•20% of the citations in this edition are to works published after 2003. •We estimate that about 20% of the material is brand new.

Token 37:
The remaining 80% reﬂects older work but has been largely rewritten to present a more uniﬁed picture of the ﬁeld. vii

Token 38:
viii Preface Overview of the book The main unifying theme is the idea of an intelligent agent .

Token 39:
We deﬁne AI as the study of agents that receive percepts from the environment and perform actions.

Token 40:
Each such agent im- plements a function that maps percept sequences to actions, and we cover different ways torepresent these functions, such as reactive agents, real-time planners, and decision-theoreticsystems.

Token 41:
We explain the role of learning as extending the reach of the designer into unknownenvironments, and we show how that role constrains agent design, favoring explicit knowl-edge representation and reasoning.

Token 42:
We treat robotics and vision not as independently deﬁnedproblems, but as occurring in the service of achieving goals.

Token 43:
We stress the importance of thetask environment in determining the appropriate agent design.

Token 44:
Our primary aim is to convey the ideas that have emerged over the past ﬁfty years of AI research and the past two millennia of related work.

Token 45:
We have tried to avoid excessive formal-ity in the presentation of these ideas while retaining precision.

Token 46:
We have included pseudocode algorithms to make the key ideas concrete; our pseudocode is described in Appendix B.

Token 47:
This book is primarily intended for use in an undergraduate course or course sequence.

Token 48:
The book has 27 chapters, each requiring about a week’s worth of lectures, so workingthrough the whole book requires a two-semester sequence.

Token 49:
A one-semester course can useselected chapters to suit the interests of the instructor and students.

Token 50:
The book can also beused in a graduate-level course (perhaps with the addition of some of the primary sourcessuggested in the bibliographical notes).

Token 51:
Sample syllabi are available at the book’s Web site,aima.cs.berkeley.edu .

Token 52:
The only prerequisite is familiarity with basic concepts of computer science (algorithms, data structures, complexity) at a sophomore level.

Token 53:
Freshman calculus and linear algebra are useful for some of the topics; the required mathematical back-ground is supplied in Appendix A.

Token 54:
Exercises are given at the end of each chapter. Exercises requiring signiﬁcant pro- gramming are marked with a keyboard icon.

Token 55:
These exercises can best be solved by taking advantage of the code repository at aima.cs.berkeley.edu .

Token 56:
Some of them are large enough to be considered term projects.

Token 57:
A number of exercises require some investigation of the literature; these are marked with a book icon.

Token 58:
Throughout the book, important points are marked with a pointing icon.

Token 59:
We have in- cluded an extensive index of around 6,000 items to make it easy to ﬁnd things in the book.

Token 60:
Wherever a new term is ﬁrst deﬁned, it is also marked in the margin.

Token 61:
NEW TERM About the Web site aima.cs.berkeley.edu , the Web site for the book, contains •implementations of the algorithms in the book in several programming languages, •a list of over 1000 schools that have used the book, many with links to online course materials and syllabi, •an annotated list of over 800 links to sites around the Web with useful AI content, •a chapter-by-chapter list of supplementary material and links, •instructions on how to join a discussion group for the book,

Token 62:


Token 63:
Preface ix •instructions on how to contact the authors with questions or comments, •instructions on how to report errors in the book, in the likely event that some exist, and •slides and other materials for instructors.

Token 64:
About the cover The cover depicts the ﬁnal position from the decisive game 6 of the 1997 match between chess champion Garry Kasparov and program D EEPBLUE.

Token 65:
Kasparov, playing Black, was forced to resign, making this the ﬁrst time a computer had beaten a world champion in achess match.

Token 66:
Kasparov is shown at the top.

Token 67:
To his left is the Asimo humanoid robot andto his right is Thomas Bayes (1702–1761), whose ideas about probability as a measure ofbelief underlie much of modern AI technology.

Token 68:
Below that we see a Mars Exploration Rover,a robot that landed on Mars in 2004 and has been exploring the planet ever since.

Token 69:
To theright is Alan Turing (1912–1954), whose fundamental work deﬁned the ﬁelds of computerscience in general and artiﬁcial intelligence in particular.

Token 70:
At the bottom is Shakey (1966–1972), the ﬁrst robot to combine perception, world-modeling, planning, and learning.

Token 71:
WithShakey is project leader Charles Rosen (1917–2002). At the bottom right is Aristotle (384 B.C.–322 B.C.

Token 72:
), who pioneered the study of logic; his work was state of the art until the 19th century (copy of a bust by Lysippos).

Token 73:
At the bottom left, lightly screened behind the authors’names, is a planning algorithm by Aristotle from De Motu Animalium in the original Greek.

Token 74:
Behind the title is a portion of the CPSC Bayesian network for medical diagnosis (Pradhanet al., 1994).

Token 75:
Behind the chess board is part of a Bayesian logic model for detecting nuclear explosions from seismic signals.

Token 76:
Credits: Stan Honda/Getty (Kasparaov), Library of Congress (Bayes), NASA (Mars rover), National Museum of Rome (Aristotle), Peter Norvig (book), Ian Parker (Berkeleyskyline), Shutterstock (Asimo, Chess pieces), Time Life/Getty (Shakey, Turing).

Token 77:
Acknowledgments This book would not have been possible without the many contributors whose names did notmake it to the cover.

Token 78:
Jitendra Malik and David Forsyth wrote Chapter 24 (computer vision)and Sebastian Thrun wrote Chapter 25 (robotics).

Token 79:
Vibhu Mittal wrote part of Chapter 22(natural language).

Token 80:
Nick Hay, Mehran Sahami, and Ernest Davis wrote some of the exercises.Zoran Duric (George Mason), Thomas C. Henderson (Utah), Leon Reznik (RIT), MichaelGourley (Central Oklahoma) and Ernest Davis (NYU) reviewed the manuscript and madehelpful suggestions.

Token 81:
We thank Ernie Davis in particular for his tireless ability to read multipledrafts and help improve the book.

Token 82:
Nick Hay whipped the bibliography into shape and ondeadline stayed up to 5:30 AM writing code to make the book better.

Token 83:
Jon Barron formattedand improved the diagrams in this edition, while Tim Huang, Mark Paskin, and CynthiaBruyns helped with diagrams and algorithms in previous editions.

Token 84:
Ravi Mohan and Ciaran O’Reilly wrote and maintain the Java code examples on the Web site.

Token 85:
John Canny wrote the robotics chapter for the ﬁrst edition and Douglas Edwards researched the historical notes.Tracy Dunkelberger, Allison Michael, Scott Disanno, and Jane Bonnell at Pearson tried theirbest to keep us on schedule and made many helpful suggestions.

Token 86:
Most helpful of all has

Token 87:
x Preface been Julie Sussman, P.P.A., who read every chapter and provided extensive improvements.

Token 88:
In previous editions we had proofreaders who would tell us when we left out a comma and saidwhich when we meant that; Julie told us when we left out a minus sign and said x iwhen we meant xj.

Token 89:
For every typo or confusing explanation that remains in the book, rest assured that Julie has ﬁxed at least ﬁve.

Token 90:
She persevered even when a power failure forced her to work by lantern light rather than LCD glow.

Token 91:
Stuart would like to thank his parents for their support and encouragement and his wife, Loy Sheﬂott, for her endless patience and boundless wisdom.

Token 92:
He hopes that Gordon,Lucy, George, and Isaac will soon be reading this book after they have forgiven him forworking so long on it.

Token 93:
RUGS (Russell’s Unusual Group of Students) have been unusuallyhelpful, as always.

Token 94:
Peter would like to thank his parents (Torsten and Gerda) for getting him started, and his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging andtolerating him through the long hours of writing and longer hours of rewriting.

Token 95:
We both thank the librarians at Berkeley, Stanford, and NASA and the developers of CiteSeer, Wikipedia, and Google, who have revolutionized the way we do research.

Token 96:
We can’tacknowledge all the people who have used the book and made suggestions, but we would like to note the especially helpful comments of Gagan Aggarwal, Eyal Amir, Ion Androutsopou- los, Krzysztof Apt, Warren Haley Armstrong, Ellery Aziel, Jeff Van Baalen, Darius Bacon,Brian Baker, Shumeet Baluja, Don Barker, Tony Barrett, James Newton Bass, Don Beal,Howard Beck, Wolfgang Bibel, John Binder, Larry Bookman, David R. Boxall, Ronen Braf-man, John Bresina, Gerhard Brewka, Selmer Bringsjord, Carla Brodley, Chris Brown, EmmaBrunskill, Wilhelm Burger, Lauren Burka, Carlos Bustamante, Joao Cachopo, Murray Camp-bell, Norman Carver, Emmanuel Castro, Anil Chakravarthy, Dan Chisarick, Berthe Choueiry,Roberto Cipolla, David Cohen, James Coleman, Julie Ann Comparini, Corinna Cortes, GaryCottrell, Ernest Davis, Tom Dean, Rina Dechter, Tom Dietterich, Peter Drake, Chuck Dyer,Doug Edwards, Robert Egginton, Asma’a El-Budrawy, Barbara Engelhardt, Kutluhan Erol,Oren Etzioni, Hana Filip, Douglas Fisher, Jeffrey Forbes, Ken Ford, Eric Fosler-Lussier, John Fosler, Jeremy Frank, Alex Franz, Bob Futrelle, Marek Galecki, Stefan Gerberding, Stuart Gill, Sabine Glesner, Seth Golub, Gosta Grahne, Russ Greiner, Eric Grimson, Bar-bara Grosz, Larry Hall, Steve Hanks, Othar Hansson, Ernst Heinz, Jim Hendler, ChristophHerrmann, Paul Hilﬁnger, Robert Holte, Vasant Honavar, Tim Huang, Seth Hutchinson, JoostJacob, Mark Jelasity, Magnus Johansson, Istvan Jonyer, Dan Jurafsky, Leslie Kaelbling, KeijiKanazawa, Surekha Kasibhatla, Simon Kasif, Henry Kautz, Gernot Kerschbaumer, MaxKhesin, Richard Kirby, Dan Klein, Kevin Knight, Roland Koenig, Sven Koenig, DaphneKoller, Rich Korf, Benjamin Kuipers, James Kurien, John Lafferty, John Laird, Gus Lars-son, John Lazzaro, Jon LeBlanc, Jason Leatherman, Frank Lee, Jon Lehto, Edward Lim,Phil Long, Pierre Louveaux, Don Loveland, Sridhar Mahadevan, Tony Mancill, Jim Martin,Andy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian Milch, Steve Minton, Vibhu Mittal, Mehryar Mohri, Leora Morgenstern, Stephen Muggleton, Kevin Murphy, Ron Musick, Sung Myaeng, Eric Nadeau, Lee Naish, Pandu Nayak, BernhardNebel, Stuart Nelson, XuanLong Nguyen, Nils Nilsson, Illah Nourbakhsh, Ali Nouri, ArthurNunes-Harwitt, Steve Omohundro, David Page, David Palmer, David Parkes, Ron Parr, Mark

Token 97:


Token 98:
Preface xi Paskin, Tony Passera, Amit Patel, Michael Pazzani, Fernando Pereira, Joseph Perla, Wim Pi- jls, Ira Pohl, Martha Pollack, David Poole, Bruce Porter, Malcolm Pradhan, Bill Pringle, Lor-raine Prior, Greg Provan, William Rapaport, Deepak Ravichandran, Ioannis Refanidis, PhilipResnik, Francesca Rossi, Sam Roweis, Richard Russell, Jonathan Schaeffer, Richard Scherl,Hinrich Schuetze, Lars Schuster, Bart Selman, Soheil Shams, Stuart Shapiro, Jude Shav- lik, Yoram Singer, Satinder Singh, Daniel Sleator, David Smith, Bryan So, Robert Sproull, Lynn Stein, Larry Stephens, Andreas Stolcke, Paul Stradling, Devika Subramanian, MarekSuchenek, Rich Sutton, Jonathan Tash, Austin Tate, Bas Terwijn, Olivier Teytaud, MichaelThielscher, William Thompson, Sebastian Thrun, Eric Tiedemann, Mark Torrance, RandallUpham, Paul Utgoff, Peter van Beek, Hal Varian, Paulina Varshavskaya, Sunil Vemuri, VandiVerma, Ubbo Visser, Jim Waldo, Toby Walsh, Bonnie Webber, Dan Weld, Michael Wellman,Kamin Whitehouse, Michael Dean White, Brian Williams, David Wolfe, Jason Wolfe, BillWoods, Alden Wright, Jay Yagnik, Mark Yasuda, Richard Yen, Eliezer Yudkowsky, WeixiongZhang, Ming Zhao, Shlomo Zilberstein, and our esteemed colleague Anonymous Reviewer.

Token 99:
About the Authors Stuart Russell was born in 1962 in Portsmouth, England. He received his B.A.

Token 100:
with ﬁrst- class honours in physics from Oxford University in 1982, and his Ph.D. in computer sciencefrom Stanford in 1986.

Token 101:
He then joined the faculty of the University of California at Berkeley,where he is a professor of computer science, director of the Center for Intelligent Systems,and holder of the Smith–Zadeh Chair in Engineering.

Token 102:
In 1990, he received the PresidentialYoung Investigator Award of the National Science Foundation, and in 1995 he was cowinnerof the Computers and Thought Award.

Token 103:
He was a 1996 Miller Professor of the University of California and was appointed to a Chancellor’s Professorship in 2000.

Token 104:
In 1998, he gave the Forsythe Memorial Lectures at Stanford University.

Token 105:
He is a Fellow and former ExecutiveCouncil member of the American Association for Artiﬁcial Intelligence.

Token 106:
He has publishedover 100 papers on a wide range of topics in artiﬁcial intelligence.

Token 107:
His other books includeThe Use of Knowledge in Analogy and Induction and (with Eric Wefald) Do the Right Thing: Studies in Limited Rationality.

Token 108:
Peter Norvig is currently Director of Research at Google, Inc., and was the director respon- sible for the core Web search algorithms from 2002 to 2005.

Token 109:
He is a Fellow of the American Association for Artiﬁcial Intelligence and the Association for Computing Machinery.

Token 110:
Previ-ously, he was head of the Computational Sciences Division at NASA Ames Research Center,where he oversaw NASA’s research and development in artiﬁcial intelligence and robotics,and chief scientist at Junglee, where he helped develop one of the ﬁrst Internet informationextraction services.

Token 111:
He received a B.S. in applied mathematics from Brown University anda Ph.D. in computer science from the University of California at Berkeley.

Token 112:
He received theDistinguished Alumni and Engineering Innovation awards from Berkeley and the ExceptionalAchievement Medal from NASA.

Token 113:
He has been a professor at the University of Southern Cal-ifornia and a research faculty member at Berkeley.

Token 114:
His other books are Paradigms of AI Programming: Case Studies in Common Lisp andVerbmobil: A Translation System for Face- to-Face Dialog andIntelligent Help Systems for UNIX .

Token 115:
xii

Token 116:
Contents I Artiﬁcial Intelligence 1 Introduction 1 1 . 1 W h a t I s A I ?

Token 117:
................................. 1 1.2 The Foundations of Artiﬁcial Intelligence .................. 5 1.3 The History of Artiﬁcial Intelligence .

Token 118:
................... 1 6 1 . 4 T h e S t a t e o f t h e A r t ............................. 2 81 .

Token 119:
5 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 2 9 2 Intelligent Agents 34 2 .

Token 120:
1 A g e n t s a n d E n v i r o n m e n t s .......................... 3 4 2.2 Good Behavior: The Concept of Rationality . . .............. 3 6 2 .

Token 121:
3 T h e N a t u r e o f E n v i r o n m e n t s ......................... 4 02 .

Token 122:
4 T h e S t r u c t u r e o f A g e n t s........................... 4 62 .

Token 123:
5 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 5 9 II Problem-solving 3 Solving Problems by Searching 64 3 .

Token 124:
1 P r o b l e m - S o l v i n g A g e n t s ........................... 6 43 . 2 E x a m p l e P r o b l e m s.............................. 6 93 .

Token 125:
3 S e a r c h i n g f o r S o l u t i o n s ........................... 7 5 3 .

Token 126:
4 U n i n f o r m e d S e a r c h S t r a t e g i e s........................ 8 1 3 .

Token 127:
5 I n f o r m e d ( H e u r i s t i c ) S e a r c h S t r a t e g i e s ................... 9 23 .

Token 128:
6 H e u r i s t i c F u n c t i o n s ............................. 1 0 23 .

Token 129:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 1 0 8 4 Beyond Classical Search 120 4 .

Token 130:
1 L o c a l S e a r c h A l g o r i t h m s a n d O p t i m i z a t i o n P r o b l e m s ........... 1 2 0 4.2 Local Search in Continuous Spaces . .

Token 131:
................... 1 2 9 4.3 Searching with Nondeterministic Actions .................. 1 3 3 4 .

Token 132:
4 S e a r c h i n g w i t h P a r t i a l O b s e r v a t i o n s ..................... 1 3 84.5 Online Search Agents and Unknown Environments ............ 1 4 7 4 .

Token 133:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 1 5 3 5 Adversarial Search 161 5 .

Token 134:
1 G a m e s .................................... 1 6 1 5 . 2 O p t i m a l D e c i s i o n s i n G a m e s ........................ 1 6 3 5 .

Token 135:
3 A l p h a – B e t a P r u n i n g............................. 1 6 75 .

Token 136:
4 I m p e r f e c t R e a l - T i m e D e c i s i o n s ....................... 1 7 15 .

Token 137:
5 S t o c h a s t i c G a m e s .............................. 1 7 7 xiii

Token 138:
xiv Contents 5 . 6 P a r t i a l l y O b s e r v a b l e G a m e s......................... 1 8 0 5.7 State-of-the-Art Game Programs . . .

Token 139:
................... 1 8 5 5 . 8 A l t e r n a t i v e A p p r o a c h e s ........................... 1 8 75 .

Token 140:
9 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 1 8 9 6 Constraint Satisfaction Problems 202 6 .

Token 141:
1 D e ﬁ n i n g C o n s t r a i n t S a t i s f a c t i o n P r o b l e m s ................. 2 0 26 .

Token 142:
2 C o n s t r a i n t P r o p a g a t i o n : I n f e r e n c e i n C S P s ................. 2 0 86 .

Token 143:
3 B a c k t r a c k i n g S e a r c h f o r C S P s ........................ 2 1 46 .

Token 144:
4 L o c a l S e a r c h f o r C S P s ............................ 2 2 06 .

Token 145:
5 T h e S t r u c t u r e o f P r o b l e m s.......................... 2 2 26 .

Token 146:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 2 2 7 III Knowledge, reasoning, and planning 7 Logical Agents 234 7.1 Knowledge-Based Agents .

Token 147:
......................... 2 3 5 7 . 2 T h e W u m p u s W o r l d............................. 2 3 67 .

Token 148:
3 L o g i c ..................................... 2 4 07.4 Propositional Logic: A Very Simple Logic . . .

Token 149:
.............. 2 4 3 7.5 Propositional Theorem Proving ....................... 2 4 9 7.6 Effective Propositional Model Checking .................. 2 5 9 7.7 Agents Based on Propositional Logic .

Token 150:
................... 2 6 5 7 .

Token 151:
8 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 2 7 4 8 First-Order Logic 285 8.1 Representation Revisited .

Token 152:
......................... 2 8 5 8 . 2 S y n t a x a n d S e m a n t i c s o f F i r s t - O r d e r L o g i c................. 2 9 08 .

Token 153:
3 U s i n g F i r s t - O r d e r L o g i c........................... 3 0 08 .

Token 154:
4 K n o w l e d g e E n g i n e e r i n g i n F i r s t - O r d e r L o g i c ................ 3 0 78 .

Token 155:
5 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 3 1 3 9 Inference in First-Order Logic 322 9.1 Propositional vs. First-Order Inference ................... 3 2 2 9.2 Uniﬁcation and Lifting .

Token 156:
. ......................... 3 2 5 9 . 3 F o r w a r d C h a i n i n g .............................. 3 3 09 .

Token 157:
4 B a c k w a r d C h a i n i n g ............................. 3 3 79 . 5 R e s o l u t i o n .................................. 3 4 59 .

Token 158:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 3 5 7 10 Classical Planning 366 10.1 Deﬁnition of Classical Planning ....................... 3 6 6 1 0 .

Token 159:
2 A l g o r i t h m s f o r P l a n n i n g a s S t a t e - S p a c e S e a r c h............... 3 7 31 0 .

Token 160:
3 P l a n n i n g G r a p h s............................... 3 7 9

Token 161:
Contents xv 10.4 Other Classical Planning Approaches . ................... 3 8 7 1 0 .

Token 162:
5 A n a l y s i s o f P l a n n i n g A p p r o a c h e s...................... 3 9 21 0 .

Token 163:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 3 9 3 11 Planning and Acting in the Real World 401 1 1 .

Token 164:
1 T i m e , S c h e d u l e s , a n d R e s o u r c e s ....................... 4 0 11 1 .

Token 165:
2 H i e r a r c h i c a l P l a n n i n g ............................ 4 0 611.3 Planning and Acting in Nondeterministic Domains ............. 4 1 5 11.4 Multiagent Planning ............................. 4 2 5 1 1 .

Token 166:
5 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 4 3 0 12 Knowledge Representation 437 1 2 .

Token 167:
1 O n t o l o g i c a l E n g i n e e r i n g ........................... 4 3 71 2 .

Token 168:
2 C a t e g o r i e s a n d O b j e c t s ........................... 4 4 0 1 2 . 3 E v e n t s .................................... 4 4 6 1 2 .

Token 169:
4 M e n t a l E v e n t s a n d M e n t a l O b j e c t s ..................... 4 5 01 2 .

Token 170:
5 R e a s o n i n g S y s t e m s f o r C a t e g o r i e s ..................... 4 5 31 2 .

Token 171:
6 R e a s o n i n g w i t h D e f a u l t I n f o r m a t i o n .................... 4 5 812.7 The Internet Shopping World ........................ 4 6 2 1 2 .

Token 172:
8 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 4 6 7 IV Uncertain knowledge and reasoning 13 Quantifying Uncertainty 480 13.1 Acting under Uncertainty .

Token 173:
......................... 4 8 0 13.2 Basic Probability Notation . ......................... 4 8 3 1 3 .

Token 174:
3 I n f e r e n c e U s i n g F u l l J o i n t D i s t r i b u t i o n s ................... 4 9 01 3 .

Token 175:
4 I n d e p e n d e n c e ................................ 4 9 41 3 . 5 B a y e s ’ R u l e a n d I t s U s e........................... 4 9 51 3 .

Token 176:
6 T h e W u m p u s W o r l d R e v i s i t e d........................ 4 9 91 3 .

Token 177:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 5 0 3 14 Probabilistic Reasoning 510 14.1 Representing Knowledge in an Uncertain Domain ............. 5 1 0 1 4 .

Token 178:
2 T h e S e m a n t i c s o f B a y e s i a n N e t w o r k s.................... 5 1 3 14.3 Efﬁcient Representation of Conditional Distributions ............ 5 1 8 1 4 .

Token 179:
4 E x a c t I n f e r e n c e i n B a y e s i a n N e t w o r k s ................... 5 2 21 4 .

Token 180:
5 A p p r o x i m a t e I n f e r e n c e i n B a y e s i a n N e t w o r k s ............... 5 3 0 14.6 Relational and First-Order Probability Models .

Token 181:
.............. 5 3 9 1 4 . 7 O t h e r A p p r o a c h e s t o U n c e r t a i n R e a s o n i n g ................. 5 4 61 4 .

Token 182:
8 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 5 5 1 15 Probabilistic Reasoning over Time 566 1 5 .

Token 183:
1 T i m e a n d U n c e r t a i n t y ............................ 5 6 6

Token 184:
xvi Contents 1 5 . 2 I n f e r e n c e i n T e m p o r a l M o d e l s ........................ 5 7 0 1 5 .

Token 185:
3 H i d d e n M a r k o v M o d e l s ........................... 5 7 8 15.4 Kalman Filters . . .............................. 5 8 4 1 5 .

Token 186:
5 D y n a m i c B a y e s i a n N e t w o r k s ........................ 5 9 01 5 .

Token 187:
6 K e e p i n g T r a c k o f M a n y O b j e c t s ....................... 5 9 9 1 5 .

Token 188:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 6 0 3 16 Making Simple Decisions 610 16.1 Combining Beliefs and Desires under Uncertainty ............. 6 1 0 16.2 The Basis of Utility Theory ......................... 6 1 1 16.3 Utility Functions .

Token 189:
.............................. 6 1 5 16.4 Multiattribute Utility Functions ....................... 6 2 2 1 6 .

Token 190:
5 D e c i s i o n N e t w o r k s .............................. 6 2 61 6 .

Token 191:
6 T h e V a l u e o f I n f o r m a t i o n .......................... 6 2 8 1 6 .

Token 192:
7 D e c i s i o n - T h e o r e t i c E x p e r t S y s t e m s..................... 6 3 3 1 6 .

Token 193:
8 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 6 3 6 17 Making Complex Decisions 645 1 7 .

Token 194:
1 S e q u e n t i a l D e c i s i o n P r o b l e m s........................ 6 4 51 7 .

Token 195:
2 V a l u e I t e r a t i o n................................ 6 5 2 1 7 . 3 P o l i c y I t e r a t i o n ................................ 6 5 6 1 7 .

Token 196:
4 P a r t i a l l y O b s e r v a b l e M D P s ......................... 6 5 817.5 Decisions with Multiple Agents: Game Theory .

Token 197:
.............. 6 6 6 1 7 . 6 M e c h a n i s m D e s i g n ............................. 6 7 9 1 7 .

Token 198:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 6 8 4 VL e a r n i n g 18 Learning from Examples 693 1 8 .

Token 199:
1 F o r m s o f L e a r n i n g.............................. 6 9 3 1 8 .

Token 200:
2 S u p e r v i s e d L e a r n i n g............................. 6 9 5 1 8 .

Token 201:
3 L e a r n i n g D e c i s i o n T r e e s ........................... 6 9 718.4 Evaluating and Choosing the Best Hypothesis .

Token 202:
.............. 7 0 8 1 8 .

Token 203:
5 T h e T h e o r y o f L e a r n i n g........................... 7 1 3 18.6 Regression and Classiﬁcation with Linear Models ............. 7 1 7 1 8 .

Token 204:
7 A r t i ﬁ c i a l N e u r a l N e t w o r k s ......................... 7 2 7 18.8 Nonparametric Models . .

Token 205:
......................... 7 3 7 18.9 Support Vector Machines . ......................... 7 4 4 1 8 .

Token 206:
1 0E n s e m b l e L e a r n i n g ............................. 7 4 8 1 8 .

Token 207:
1 1P r a c t i c a l M a c h i n e L e a r n i n g......................... 7 5 3 1 8 .

Token 208:
1 2S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 7 5 7 19 Knowledge in Learning 768 1 9 .

Token 209:
1 A L o g i c a l F o r m u l a t i o n o f L e a r n i n g..................... 7 6 8

Token 210:
Contents xvii 1 9 .

Token 211:
2 K n o w l e d g e i n L e a r n i n g ........................... 7 7 7 19.3 Explanation-Based Learning ........................ 7 8 0 1 9 .

Token 212:
4 L e a r n i n g U s i n g R e l e v a n c e I n f o r m a t i o n................... 7 8 419.5 Inductive Logic Programming ........................ 7 8 8 1 9 .

Token 213:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 7 9 7 20 Learning Probabilistic Models 802 2 0 .

Token 214:
1 S t a t i s t i c a l L e a r n i n g ............................. 8 0 22 0 .

Token 215:
2 L e a r n i n g w i t h C o m p l e t e D a t a........................ 8 0 62 0 .

Token 216:
3 L e a r n i n g w i t h H i d d e n V a r i a b l e s : T h e E M A l g o r i t h m ............ 8 1 62 0 .

Token 217:
4 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 8 2 5 21 Reinforcement Learning 830 21.1 Introduction .

Token 218:
. . .............................. 8 3 0 21.2 Passive Reinforcement Learning . . . ................... 8 3 2 2 1 .

Token 219:
3 A c t i v e R e i n f o r c e m e n t L e a r n i n g....................... 8 3 92 1 .

Token 220:
4 G e n e r a l i z a t i o n i n R e i n f o r c e m e n t L e a r n i n g ................. 8 4 52 1 .

Token 221:
5 P o l i c y S e a r c h ................................ 8 4 82 1 .

Token 222:
6 A p p l i c a t i o n s o f R e i n f o r c e m e n t L e a r n i n g .................. 8 5 02 1 .

Token 223:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 8 5 3 VI Communicating, perceiving, and acting 22 Natural Language Processing 860 22.1 Language Models .............................. 8 6 0 22.2 Text Classiﬁcation .............................. 8 6 5 2 2 .

Token 224:
3 I n f o r m a t i o n R e t r i e v a l ............................ 8 6 72 2 .

Token 225:
4 I n f o r m a t i o n E x t r a c t i o n ............................ 8 7 32 2 .

Token 226:
5 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 8 8 2 23 Natural Language for Communication 888 2 3 .

Token 227:
1 P h r a s e S t r u c t u r e G r a m m a r s ......................... 8 8 82 3 .

Token 228:
2 S y n t a c t i c A n a l y s i s ( P a r s i n g ) ......................... 8 9 22 3 .

Token 229:
3 A u g m e n t e d G r a m m a r s a n d S e m a n t i c I n t e r p r e t a t i o n ............ 8 9 7 2 3 .

Token 230:
4 M a c h i n e T r a n s l a t i o n............................. 9 0 7 23.5 Speech Recognition ............................. 9 1 2 2 3 .

Token 231:
6 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 9 1 8 24 Perception 928 2 4 .

Token 232:
1 I m a g e F o r m a t i o n ............................... 9 2 924.2 Early Image-Processing Operations . .

Token 233:
................... 9 3 5 24.3 Object Recognition by Appearance . . ................... 9 4 2 2 4 .

Token 234:
4 R e c o n s t r u c t i n g t h e 3 D W o r l d ........................ 9 4 724.5 Object Recognition from Structural Information .............. 9 5 7

Token 235:
xviii Contents 2 4 . 6 U s i n g V i s i o n................................. 9 6 1 2 4 .

Token 236:
7 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 9 6 5 25 Robotics 971 25.1 Introduction .

Token 237:
. . .............................. 9 7 1 25.2 Robot Hardware . .............................. 9 7 3 2 5 .

Token 238:
3 R o b o t i c P e r c e p t i o n .............................. 9 7 82 5 . 4 P l a n n i n g t o M o v e .............................. 9 8 62 5 .

Token 239:
5 P l a n n i n g U n c e r t a i n M o v e m e n t s....................... 9 9 32 5 . 6 M o v i n g .................................... 9 9 72 5 .

Token 240:
7 R o b o t i c S o f t w a r e A r c h i t e c t u r e s....................... 1003 2 5 .

Token 241:
8 A p p l i c a t i o n D o m a i n s ............................ 1 0 0 62 5 .

Token 242:
9 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 1010 VII Conclusions 26 Philosophica l Founda tions 1020 26.1 Weak AI: Can Machines Act Intelligently?

Token 243:
. . . .............. 1020 2 6 . 2 S t r o n g A I : C a n M a c h i n e s R e a l l y T h i n k ?

Token 244:
.................. 1026 26.3 The Ethics and Risks of Developing Artiﬁcial Intelligence . ........ 1034 2 6 .

Token 245:
4 S u m m a r y , B i b l i o g r a p h i c a l a n d H i s t o r i c a l N o t e s , E x e r c i s e s......... 1040 27 AI: The Present and Future 1044 27.1 Agent Components ............................. 1044 2 7 .

Token 246:
2 A g e n t A r c h i t e c t u r e s ............................. 1047 2 7 . 3 A r e W e G o i n g i n t h e R i g h t D i r e c t i o n ?

Token 247:
................... 1049 27.4 What If AI Does Succeed? ......................... 1 0 5 1 A Mathematical background 1053 A .

Token 248:
1 C o m p l e x i t y A n a l y s i s a n d O ( ) N o t a t i o n ................... 1053 A.2 Vectors, Matrices, and Linear Algebra ................... 1055 A.3 Probability Distributions .

Token 249:
.

Token 250:
......................... 1057 B Notes on Languages and Algorithms 1060 B.1 Deﬁning Languages with Backus–Naur Form (BNF) ............ 1060 B.2 Describing Algorithms with Pseudocode .................. 1061 B .

Token 251:
3 O n l i n e H e l p ................................. 1062 Bibliography 1063 Index 1095

Token 252:


Token 253:
1INTRODUCTION In which we try to explain why we consider artiﬁcial intelligence to be a subject most worthy of study, and in which we try to decide what exactly it is, this being a good thing to decide before embarking.

Token 254:
We call ourselves Homo sapiens —man the wise—because our intelligence is so important INTELLIGENCE to us.

Token 255:
For thousands of years, we have tried to understand how we think ; that is, how a mere handful of matter can perceive, understand, predict, and manipulate a world far larger andmore complicated than itself.

Token 256:
The ﬁeld of artiﬁcial intelligence , or AI, goes further still: it ARTIFICIAL INTELLIGENCE attempts not just to understand but also to build intelligent entities.

Token 257:
AI is one of the newest ﬁelds in science and engineering. Work started in earnest soon after World War II, and the name itself was coined in 1956.

Token 258:
Along with molecular biology, AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.A student in physics might reasonably feel that all the good ideas have already been taken byGalileo, Newton, Einstein, and the rest.

Token 259:
AI, on the other hand, still has openings for severalfull-time Einsteins and Edisons.

Token 260:
AI currently encompasses a huge variety of subﬁelds, ranging from the general (learning and perception) to the speciﬁc, such as playing chess, proving mathematical theorems, writingpoetry, driving a car on a crowded street, and diagnosing diseases.

Token 261:
AI is relevant to anyintellectual task; it is truly a universal ﬁeld. 1.1 W HAT ISAI?

Token 262:
We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see eight deﬁnitions of AI, laid out along two dimensions.

Token 263:
The deﬁnitions on top are concernedwith thought processes andreasoning , whereas the ones on the bottom address behavior .T h e deﬁnitions on the left measure success in terms of ﬁdelity to human performance, whereas the ones on the right measure against an ideal performance measure, called rationality .A RATIONALITY system is rational if it does the “right thing,” given what it knows.

Token 264:
Historically, all four approaches to AI have been followed, each by different people with different methods.

Token 265:
A human-centered approach must be in part an empirical science, in- 1

Token 266:
2 Chapter 1.

Token 267:
Introduction Thinking Humanly Thinking Rationally “The exciting new effort to make comput- ers think ...machines with minds ,i nt h e full and literal sense.” (Haugeland, 1985) “The study of mental faculties through theuse of computational models.”(Charniak and McDermott, 1985) “[The automation of] activities that weassociate with human thinking, activitiessuch as decision-making, problem solv-ing, learning ...” (Bellman, 1978) “The study of the computations that make it possible to perceive, reason, and act.”(Winston, 1992) Acting Humanly Acting Rationally “The art of creating machines that per-form functions that require intelligencewhen performed by people.” (Kurzweil,1990) “Computational Intelligence is the studyof the design of intelligent agents.” (Pooleet al.

Token 268:
, 1998) “The study of how to make computers do things at which, at the moment, people are better.” (Rich and Knight, 1991) “AI . . .

Token 269:
is concerned with intelligent be- havior in artifacts.” (Nilsson, 1998) Figure 1.1 Some deﬁnitions of artiﬁcial intelligence, organized into four categories.

Token 270:
volving observations and hypotheses about human behavior. A rationalist1approach involves a combination of mathematics and engineering.

Token 271:
The various group have both disparaged andhelped each other. Let us look at the four approaches in more detail.

Token 272:
1.1.1 Acting humanly: The Turing Test approach The Turing Test , proposed by Alan Turing (1950), was designed to provide a satisfactory TURINGTEST operational deﬁnition of intelligence.

Token 273:
A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a personor from a computer.

Token 274:
Chapter 26 discusses the details of the test and whether a computer wouldreally be intelligent if it passed.

Token 275:
For now, we note that programming a computer to pass arigorously applied test provides plenty to work on.

Token 276:
The computer would need to possess thefollowing capabilities: •natural language processing to enable it to communicate successfully in English; NATURAL LANGUAGE PROCESSING •knowledge representation to store what it knows or hears;KNOWLEDGE REPRESENTATION •automated reasoning to use the stored information to answer questions and to drawAUTOMATED REASONING new conclusions; •machine learning to adapt to new circumstances and to detect and extrapolate patterns.

Token 277:
MACHINE LEARNING 1By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily “irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect: not all chess players are grandmasters; and, unfortunate ly, not everyone gets an A on the exam.

Token 278:
Some systematic errors in human reasoning are cataloged by Kahneman et al. (1982).

Token 279:
Section 1.1. What Is AI?

Token 280:
3 Turing’s test deliberately avoided direct physical interaction between the interrogator and the computer, because physical simulation of a person is unnecessary for intelligence.

Token 281:
However, the so-called total Turing Test includes a video signal so that the interrogator can test the TOTAL TURINGTEST subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical objects “through the hatch.” To pass the total Turing Test, the computer will need •computer vision to perceive objects, and COMPUTER VISION •robotics to manipulate objects and move about.

Token 282:
ROBOTICS These six disciplines compose most of AI, and Turing deserves credit for designing a test that remains relevant 60 years later.

Token 283:
Yet AI researchers have devoted little effort to passingthe Turing Test, believing that it is more important to study the underlying principles of in-telligence than to duplicate an exemplar.

Token 284:
The quest for “artiﬁcial ﬂight” succeeded when the Wright brothers and others stopped imitating birds and started using wind tunnels and learn- ing about aerodynamics.

Token 285:
Aeronautical engineering texts do not deﬁne the goal of their ﬁeldas making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.” 1.1.2 Thinking humanly: The cognitive modeling approach If we are going to say that a given program thinks like a human, we must have some way ofdetermining how humans think.

Token 286:
We need to get inside the actual workings of human minds.

Token 287:
There are three ways to do this: through introspection—trying to catch our own thoughts asthey go by; through psychological experiments—observing a person in action; and throughbrain imaging—observing the brain in action.

Token 288:
Once we have a sufﬁciently precise theory ofthe mind, it becomes possible to express the theory as a computer program.

Token 289:
If the program’sinput–output behavior matches corresponding human behavior, that is evidence that some ofthe program’s mechanisms could also be operating in humans.

Token 290:
For example, Allen Newelland Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,1961), were not content merely to have their program solve problems correctly.

Token 291:
They were more concerned with comparing the trace of its reasoning steps to traces of human subjects solving the same problems.

Token 292:
The interdisciplinary ﬁeld of cognitive science brings together COGNITIVE SCIENCE computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind.

Token 293:
Cognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least one encyclopedia (Wilson and Keil, 1999).

Token 294:
We will occasionally comment on similarities or differences between AI techniques and human cognition.

Token 295:
Real cognitive science, however, is necessarily based on experimental investigation of actual humans or animals.

Token 296:
We will leavethat for other books, as we assume the reader has only a computer for experimentation.

Token 297:
In the early days of AI there was often confusion between the approaches: an author would argue that an algorithm performs well on a task and that it is therefore a good model of human performance, or vice versa.

Token 298:
Modern authors separate the two kinds of claims; this distinction has allowed both AI and cognitive science to develop more rapidly.

Token 299:
The twoﬁelds continue to fertilize each other, most notably in computer vision, which incorporatesneurophysiological evidence into computational models.

Token 300:
4 Chapter 1.

Token 301:
Introduction 1.1.3 Thinking rationally: The “laws of thought” approach The Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that is, irrefutable reasoning processes.

Token 302:
His syllogisms provided patterns for argument structures SYLLOGISM that always yielded correct conclusions when given correct premises—for example, “Socrates is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought weresupposed to govern the operation of the mind; their study initiated the ﬁeld called logic .

Token 303:
LOGIC Logicians in the 19th century developed a precise notation for statements about all kinds of objects in the world and the relations among them.

Token 304:
(Contrast this with ordinary arithmeticnotation, which provides only for statements about numbers .)

Token 305:
By 1965, programs existed that could, in principle, solve anysolvable problem described in logical notation.

Token 306:
(Although if no solution exists, the program might loop forever.)

Token 307:
The so-called logicist tradition within LOGICIST artiﬁcial intelligence hopes to build on such programs to create intelligent systems.

Token 308:
There are two main obstacles to this approach.

Token 309:
First, it is not easy to take informal knowledge and state it in the formal terms required by logical notation, particularly whenthe knowledge is less than 100% certain.

Token 310:
Second, there is a big difference between solvinga problem “in principle” and solving it in practice.

Token 311:
Even problems with just a few hundredfacts can exhaust the computational resources of any computer unless it has some guidanceas to which reasoning steps to try ﬁrst.

Token 312:
Although both of these obstacles apply to anyattempt to build computational reasoning systems, they appeared ﬁrst in the logicist tradition.

Token 313:
1.1.4 Acting rationally: The rational agent approach Anagent is just something that acts ( agent comes from the Latin agere , to do).

Token 314:
Of course, AGENT all computer programs do something, but computer agents are expected to do more: operate autonomously, perceive their environment, persist over a prolonged time period, adapt tochange, and create and pursue goals.

Token 315:
A rational agent is one that acts so as to achieve the RATIONAL AGENT best outcome or, when there is uncertainty, the best expected outcome.

Token 316:
In the “laws of thought” approach to AI, the emphasis was on correct inferences.

Token 317:
Mak- ing correct inferences is sometimes part of being a rational agent, because one way to act rationally is to reason logically to the conclusion that a given action will achieve one’s goalsand then to act on that conclusion.

Token 318:
On the other hand, correct inference is not allof ration- ality; in some situations, there is no provably correct thing to do, but something must still be done.

Token 319:
There are also ways of acting rationally that cannot be said to involve inference.

Token 320:
For example, recoiling from a hot stove is a reﬂex action that is usually more successful than a slower action taken after careful deliberation.

Token 321:
All the skills needed for the Turing Test also allow an agent to act rationally.

Token 322:
Knowledge representation and reasoning enable agents to reach good decisions.

Token 323:
We need to be able togenerate comprehensible sentences in natural language to get by in a complex society.

Token 324:
Weneed learning not only for erudition, but also because it improves our ability to generate effective behavior.

Token 325:
The rational-agent approach has two advantages over the other approaches.

Token 326:
First, it is more general than the “laws of thought” approach because correct inference is just oneof several possible mechanisms for achieving rationality.

Token 327:
Second, it is more amenable to

Token 328:
Section 1.2. The Foundations of Artiﬁcial Intelligence 5 scientiﬁc development than are approaches based on human behavior or human thought.

Token 329:
The standard of rationality is mathematically well deﬁned and completely general, and can be“unpacked” to generate agent designs that provably achieve it.

Token 330:
Human behavior, on the otherhand, is well adapted for one speciﬁc environment and is deﬁned by, well, the sum totalof all the things that humans do.

Token 331:
This book therefore concentrates on general principles of rational agents and on components for constructing them.

Token 332:
We will see that despite the apparent simplicity with which the problem can be stated, an enormous variety of issuescome up when we try to solve it.

Token 333:
Chapter 2 outlines some of these issues in more detail.

Token 334:
One important point to keep in mind: We will see before too long that achieving perfect rationality—always doing the right thing—is not feasible in complicated environments.

Token 335:
Thecomputational demands are just too high.

Token 336:
For most of the book, however, we will adopt theworking hypothesis that perfect rationality is a good starting point for analysis.

Token 337:
It simpliﬁesthe problem and provides the appropriate setting for most of the foundational material inthe ﬁeld.

Token 338:
Chapters 5 and 17 deal explicitly with the issue of limited rationality —acting LIMITED RATIONALITY appropriately when there is not enough time to do all the computations one might like.

Token 339:
1.2 T HEFOUNDATIONS OF ARTIFICIAL INTELLIGENCE In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints, and techniques to AI.

Token 340:
Like any history, this one is forced to concentrate on a small number of people, events, and ideas and to ignore others that also were important.

Token 341:
We organize the history around a series of questions.

Token 342:
We certainly would not wish to give the impression thatthese questions are the only ones the disciplines address or that the disciplines have all beenworking toward AI as their ultimate fruition.

Token 343:
1.2.1 Philosophy •Can formal rules be used to draw valid conclusions? •How does the mind arise from a physical brain? •Where does knowledge come from?

Token 344:
•How does knowledge lead to action? Aristotle (384–322 B.C.

Token 345:
), whose bust appears on the front cover of this book, was the ﬁrst to formulate a precise set of laws governing the rational part of the mind.

Token 346:
He developed aninformal system of syllogisms for proper reasoning, which in principle allowed one to gener-ate conclusions mechanically, given initial premises.

Token 347:
Much later, Ramon Lull (d. 1315) hadthe idea that useful reasoning could actually be carried out by a mechanical artifact.

Token 348:
ThomasHobbes (1588–1679) proposed that reasoning was like numerical computation, that “we addand subtract in our silent thoughts.” The automation of computation itself was already well under way.

Token 349:
Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me- chanical calculator; recent reconstructions have shown the design to be functional.

Token 350:
The ﬁrstknown calculating machine was constructed around 1623 by the German scientist WilhelmSchickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662),

Token 351:
6 Chapter 1. Introduction is more famous.

Token 352:
Pascal wrote that “the arithmetical machine produces effects which appear nearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)built a mechanical device intended to carry out operations on concepts rather than numbers,but its scope was rather limited.

Token 353:
Leibniz did surpass Pascal by building a calculator thatcould add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub- tract.

Token 354:
Some speculated that machines might not just do calculations but actually be able to think and act on their own.

Token 355:
In his 1651 book Leviathan , Thomas Hobbes suggested the idea of an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but somany strings; and the joints, but so many wheels.” It’s one thing to say that the mind operates, at least in part, according to logical rules, and to build physical systems that emulate some of those rules; it’s another to say that the minditself issuch a physical system.

Token 356:
Ren´ e Descartes (1596–1650) gave the ﬁrst clear discussion of the distinction between mind and matter and of the problems that arise.

Token 357:
One problem witha purely physical conception of the mind is that it seems to leave little room for free will:if the mind is governed entirely by physical laws, then it has no more free will than a rock“deciding” to fall toward the center of the earth.

Token 358:
Descartes was a strong advocate of the powerof reasoning in understanding the world, a philosophy now called rationalism , and one that RATIONALISM counts Aristotle and Leibnitz as members.

Token 359:
But Descartes was also a proponent of dualism .

Token 360:
DUALISM He held that there is a part of the human mind (or soul or spirit) that is outside of nature, exempt from physical laws.

Token 361:
Animals, on the other hand, did not possess this dual quality; they could be treated as machines.

Token 362:
An alternative to dualism is materialism , which holds MATERIALISM that the brain’s operation according to the laws of physics constitutes the mind.

Token 363:
Free will is simply the way that the perception of available choices appears to the choosing entity.

Token 364:
Given a physical mind that manipulates knowledge, the next problem is to establish the source of knowledge.

Token 365:
The empiricism movement, starting with Francis Bacon’s (1561– EMPIRICISM 1626) Novum Organum ,2is characterized by a dictum of John Locke (1632–1704): “Nothing is in the understanding, which was not ﬁrst in the senses.” David Hume’s (1711–1776) A Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of induction : that general rules are acquired by exposure to repeated associations between their INDUCTION elements.

Token 366:
Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell (1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed thedoctrine of logical positivism .

Token 367:
This doctrine holds that all knowledge can be characterized by LOGICAL POSITIVISM logical theories connected, ultimately, to observation sentences that correspond to sensoryOBSERVATION SENTENCES inputs; thus logical positivism combines rationalism and empiricism.3Theconﬁrmation the- oryof Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl-CONFIRMATION THEORY edge from experience.

Token 368:
Carnap’s book The Logical Structure of the World (1928) deﬁned an explicit computational procedure for extracting knowledge from elementary experiences.

Token 369:
Itwas probably the ﬁrst theory of mind as a computational process. 2The Novum Organum is an update of Aristotle’s Organon , or instrument of thought.

Token 370:
Thus Aristotle can be seen as both an empiricist and a rationalist.

Token 371:
3In this picture, all meaningful statements can be veriﬁed or falsiﬁed either by experimentation or by analysis of the meaning of the words.

Token 372:
Because this rules out most of metaphysics, as was the intention, logical positivismwas unpopular in some circles.

Token 373:
Section 1.2.

Token 374:
The Foundations of Artiﬁcial Intelligence 7 The ﬁnal element in the philosophical picture of the mind is the connection between knowledge and action.

Token 375:
This question is vital to AI because intelligence requires action as wellas reasoning.

Token 376:
Moreover, only by understanding how actions are justiﬁed can we understandhow to build an agent whose actions are justiﬁable (or rational).

Token 377:
Aristotle argued (in De Motu Animalium ) that actions are justiﬁed by a logical connection between goals and knowledge of the action’s outcome (the last part of this extract also appears on the front cover of this book, in the original Greek): But how does it happen that thinking is sometimes accompanied by action and sometimes not, sometimes by motion, and sometimes not?

Token 378:
It looks as if almost the same thing happens as in the case of reasoning and making inferences about unchanging objects.

Token 379:
But in that case the end is a speculative proposition ...whereas here the conclusion which results from the two premises is an action.

Token 380:
...I need covering; a cloak is a covering. I need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak.

Token 381:
And the conclusion, the “I have to make a cloak,” is an action. In the Nicomachean Ethics (Book III.

Token 382:
3, 1112b), Aristotle further elaborates on this topic, suggesting an algorithm: We deliberate not about ends, but about means.

Token 383:
For a doctor does not deliberate whether he shall heal, nor an orator whether he shall persuade, ...They assume the end and consider how and by what means it is attained, and if it seems easily and best producedthereby; while if it is achieved by one means only they consider how it will be achieved by this and by what means thiswill be achieved, till they come to the ﬁrst cause, ...and what is last in the order of analysis seems to be ﬁrst in the order of becoming.

Token 384:
And if we come on an impossibility, we give up the sear ch, e.g., if we need money and this cannot be got; but if a thing appears possible we try to do it.

Token 385:
Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS program.

Token 386:
We would now call it a regression planning system (see Chapter 10).

Token 387:
Goal-based analysis is useful, but does not say what to do when several actions will achieve the goal or when no action will achieve it completely.

Token 388:
Antoine Arnauld (1612–1694)correctly described a quantitative formula for deciding what action to take in cases like this(see Chapter 16).

Token 389:
John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted the idea of rational decision criteria in all spheres of human activity.

Token 390:
The more formal theory of decisions is discussed in the following section. 1.2.2 Mathematics •What are the formal rules to draw valid conclusions?

Token 391:
•What can be computed? •How do we reason with uncertain information?

Token 392:
Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science required a level of mathematical formalization in three fundamental areas: logic, computa- tion, and probability.

Token 393:
The idea of formal logic can be traced back to the philosophers of ancient Greece, but its mathematical development really began with the work of George Boole (1815–1864), who

Token 394:
8 Chapter 1. Introduction worked out the details of propositional, or Boolean, logic (Boole, 1847).

Token 395:
In 1879, Gottlob Frege (1848–1925) extended Boole’s logic to include objects and relations, creating the ﬁrst-order logic that is used today.

Token 396:
4Alfred Tarski (1902–1983) introduced a theory of reference that shows how to relate the objects in a logic to objects in the real world.

Token 397:
The next step was to determine the limits of what could be done with logic and com- putation.

Token 398:
The ﬁrst nontrivial algorithm is thought to be Euclid’s algorithm for computing ALGORITHM greatest common divisors.

Token 399:
The word algorithm (and the idea of studying them) comes from al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introducedArabic numerals and algebra to Europe.

Token 400:
Boole and others discussed algorithms for logicaldeduction, and, by the late 19th century, efforts were under way to formalize general mathe-matical reasoning as logical deduction.

Token 401:
In 1930, Kurt G¨ odel (1906–1978) showed that there exists an effective procedure to prove any true statement in the ﬁrst-order logic of Frege andRussell, but that ﬁrst-order logic could not capture the principle of mathematical inductionneeded to characterize the natural numbers.

Token 402:
In 1931, G¨ odel showed that limits on deduc- tion do exist.

Token 403:
His incompleteness theorem showed that in any formal theory as strong as INCOMPLETENESS THEOREM Peano arithmetic (the elementary theory of natural numbers), there are true statements that are undecidable in the sense that they have no proof within the theory.

Token 404:
This fundamental result can also be interpreted as showing that some functions on the integers cannot be represented by an algorithm—that is, they cannot be computed.

Token 405:
Thismotivated Alan Turing (1912–1954) to try to characterize exactly which functions arecom- putable —capable of being computed.

Token 406:
This notion is actually slightly problematic because COMPUTABLE the notion of a computation or effective procedure really cannot be given a formal deﬁnition.

Token 407:
However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) iscapable of computing any computable function, is generally accepted as providing a sufﬁcientdeﬁnition.

Token 408:
Turing also showed that there were some functions that no Turing machine cancompute.

Token 409:
For example, no machine can tell in general whether a given program will return an answer on a given input or run forever.

Token 410:
Although decidability and computability are important to an understanding of computa- tion, the notion of tractability has had an even greater impact.

Token 411:
Roughly speaking, a problem TRACTABILITY is called intractable if the time required to solve instances of the problem grows exponentially with the size of the instances.

Token 412:
The distinction between polynomial and exponential growthin complexity was ﬁrst emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965).

Token 413:
It isimportant because exponential growth means that even moderately large instances cannot besolved in any reasonable time.

Token 414:
Therefore, one should strive to divide the overall problem ofgenerating intelligent behavior into tractable subproblems rather than intractable ones.

Token 415:
How can one recognize an intractable problem?

Token 416:
The theory of NP-completeness ,p i o - NP-COMPLETENESS neered by Steven Cook (1971) and Richard Karp (1972), provides a method.

Token 417:
Cook and Karp showed the existence of large classes of canonical combinatorial search and reasoning prob-lems that are NP-complete.

Token 418:
Any problem class to which the class of NP-complete problems can be reduced is likely to be intractable.

Token 419:
(Although it has not been proved that NP-complete 4Frege’s proposed notation for ﬁrst-order logic—an ar cane combination of textual and geometric features— never became popular.

Token 420:
Section 1.2. The Foundations of Artiﬁcial Intelligence 9 problems are necessarily intractable, most theoreticians believe it.)

Token 421:
These results contrast with the optimism with which the popular press greeted the ﬁrst computers—“ElectronicSuper-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,careful use of resources will characterize intelligent systems.

Token 422:
Put crudely, the world is anextremely large problem instance!

Token 423:
Work in AI has helped explain why some instances of NP-complete problems are hard, yet others are easy (Cheeseman et al. , 1991).

Token 424:
Besides logic and computation, the third great contribution of mathematics to AI is the theory of probability .

Token 425:
The Italian Gerolamo Cardano (1501–1576) ﬁrst framed the idea of PROBABILITY probability, describing it in terms of the possible outcomes of gambling events.

Token 426:
In 1654, Blaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre-dict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.Probability quickly became an invaluable part of all the quantitative sciences, helping to dealwith uncertain measurements and incomplete theories.

Token 427:
James Bernoulli (1654–1705), PierreLaplace (1749–1827), and others advanced the theory and introduced new statistical meth-ods.

Token 428:
Thomas Bayes (1702–1761), who appears on the front cover of this book, proposeda rule for updating probabilities in the light of new evidence.

Token 429:
Bayes’ rule underlies mostmodern approaches to uncertain reasoning in AI systems.

Token 430:
1.2.3 Economics •How should we make decisions so as to maximize payoff? •How should we do this when others may not go along?

Token 431:
•How should we do this when the payoff may be far in the future?

Token 432:
The science of economics got its start in 1776, when Scottish philosopher Adam Smith (1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations .

Token 433:
While the ancient Greeks and others had made contributions to economic thought, Smith was the ﬁrst to treat it as a science, using the idea that economies can be thought of as consist- ing of individual agents maximizing their own economic well-being.

Token 434:
Most people think of economics as being about money, but economists will say that they are really studying how people make choices that lead to preferred outcomes.

Token 435:
When McDonald’s offers a hamburger for a dollar, they are asserting that they would prefer the dollar and hoping that customers willprefer the hamburger.

Token 436:
The mathematical treatment of “preferred outcomes” or utility was UTILITY ﬁrst formalized by L´ eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by Frank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their bookThe Theory of Games and Economic Behavior (1944).

Token 437:
Decision theory , which combines probability theory with utility theory, provides a for- DECISION THEORY mal and complete framework for decisions (economic or otherwise) made under uncertainty— that is, in cases where probabilistic descriptions appropriately capture the decision maker’senvironment.

Token 438:
This is suitable for “large” economies where each agent need pay no attention to the actions of other agents as individuals.

Token 439:
For “small” economies, the situation is much more like a game : the actions of one player can signiﬁcantly affect the utility of another (either positively or negatively).

Token 440:
Von Neumann and Morgenstern’s development of game theory (see also Luce and Raiffa, 1957) included the surprising result that, for some games, GAME THEORY

Token 441:
10 Chapter 1. Introduction a rational agent should adopt policies that are (or least appear to be) randomized.

Token 442:
Unlike de- cision theory, game theory does not offer an unambiguous prescription for selecting actions.

Token 443:
For the most part, economists did not address the third question listed above, namely, how to make rational decisions when payoffs from actions are not immediate but instead re-sult from several actions taken in sequence .

Token 444:
This topic was pursued in the ﬁeld of operations research , which emerged in World War II from efforts in Britain to optimize radar installa- OPERATIONS RESEARCH tions, and later found civilian applications in complex management decisions.

Token 445:
The work of Richard Bellman (1957) formalized a class of sequential decision problems called Markov decision processes , which we study in Chapters 17 and 21.

Token 446:
Work in economics and operations research has contributed much to our notion of ra- tional agents, yet for many years AI research developed along entirely separate paths.

Token 447:
Onereason was the apparent complexity of making rational decisions.

Token 448:
The pioneering AI re-searcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his earlywork showing that models based on satisﬁcing —making decisions that are “good enough,” SATISFICING rather than laboriously calculating an optimal decision—gave a better description of actual human behavior (Simon, 1947).

Token 449:
Since the 1990s, there has been a resurgence of interest indecision-theoretic techniques for agent systems (Wellman, 1995).

Token 450:
1.2.4 Neuroscience •How do brains process information? Neuroscience is the study of the nervous system, particularly the brain.

Token 451:
Although the exact NEUROSCIENCE way in which the brain enables thought is one of the great mysteries of science, the fact that it does enable thought has been appreciated for thousands of years because of the evidence that strong blows to the head can lead to mental incapacitation.

Token 452:
It has also long been known thathuman brains are somehow different; in about 335 B.C.

Token 453:
Aristotle wrote, “Of all the animals, man has the largest brain in proportion to his size.”5Still, it was not until the middle of the 18th century that the brain was widely recognized as the seat of consciousness.

Token 454:
Before then, candidate locations included the heart and the spleen.

Token 455:
Paul Broca’s (1824–1880) study of aphasia (speech deﬁcit) in brain-damaged patients in 1861 demonstrated the existence of localized areas of the brain responsible for speciﬁc cognitive functions.

Token 456:
In particular, he showed that speech production was localized to the portion of the left hemisphere now called Broca’s area.6By that time, it was known that the brain consisted of nerve cells, or neurons , but it was not until 1873 that Camillo Golgi NEURON (1843–1926) developed a staining technique allowing the observation of individual neurons in the brain (see Figure 1.2).

Token 457:
This technique was used by Santiago Ramon y Cajal (1852–1934) in his pioneering studies of the brain’s neuronal structures.

Token 458:
7Nicolas Rashevsky (1936, 1938) was the ﬁrst to apply mathematical models to the study of the nervous sytem.

Token 459:
5Since then, it has been discovered that the tree shrew ( Scandentia ) has a higher ratio of brain to body mass.

Token 460:
6Many cite Alexander Hood (1824) as a possible prior source.

Token 461:
7Golgi persisted in his belief that the brain’s functions were carried out primarily in a continuous medium in which neurons were embedded, whereas Cajal propounded t he “neuronal doctrine.” The two shared the Nobel prize in 1906 but gave mutually antagonistic acceptance speeches.

Token 462:
Section 1.2.

Token 463:
The Foundations of Artiﬁcial Intelligence 11 Axon Cell body or SomaNucleusDendrite SynapsesAxonal arborization Axon from another cell Synapse Figure 1.2 The parts of a nerve cell or neuron.

Token 464:
Each neuron consists of a cell body, or soma, that contains a cell nucleus.

Token 465:
Branching out from the cell body are a number of ﬁbers called dendrites and a single long ﬁber called the axon.

Token 466:
The axon stretches out for a long distance, much longer than the scale in this diagram indicates.

Token 467:
Typically, an axon is1 cm long (100 times the diameter of the cell body), but can reach up to 1 meter.

Token 468:
A neuron makes connections with 10 to 100,000 other neur ons at junctions called synapses.

Token 469:
Signals are propagated from neuron to neuron by a complicated electrochemical reaction.

Token 470:
The signalscontrol brain activity in the short term and also enable long-term changes in the connectivity of neurons.

Token 471:
These mechanisms are thought to form the basis for learning in the brain.

Token 472:
Most information processing goes on in the cerebral cortex, the outer layer of the brain.

Token 473:
The basic organizational unit appears to be a column of tissue about 0.5 mm in diameter, containing about 20,000 neurons and extending the full depth of the cortex about 4 mm in humans).

Token 474:
We now have some data on the mapping between areas of the brain and the parts of the body that they control or from which they receive sensory input.

Token 475:
Such mappings are able to change radically over the course of a few weeks, and some animals seem to have multiplemaps.

Token 476:
Moreover, we do not fully understand how other areas can take over functions whenone area is damaged.

Token 477:
There is almost no theory on how an individual memory is stored.

Token 478:
The measurement of intact brain activity began in 1929 with the invention by Hans Berger of the electroencephalograph (EEG).

Token 479:
The recent development of functional magneticresonance imaging (fMRI) (Ogawa et al.

Token 480:
, 1990; Cabeza and Nyberg, 2001) is giving neu- roscientists unprecedentedly detailed images of brain activity, enabling measurements thatcorrespond in interesting ways to ongoing cognitive processes.

Token 481:
These are augmented byadvances in single-cell recording of neuron activity.

Token 482:
Individual neurons can be stimulatedelectrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input– output relationships to be mapped.

Token 483:
Despite these advances, we are still a long way from understanding how cognitive processes actually work.

Token 484:
The truly amazing conclusion is that a collection of simple cells can lead to thought, action, and consciousness or, in the pithy words of John Searle (1992), brains cause minds .

Token 485:
12 Chapter 1.

Token 486:
Introduction Supercomputer Personal Computer Human Brain Computational units 104CPUs, 1012transistors 4C P U s , 109transistors 1011neurons Storage units 1014bits RAM 1011bits RAM 1011neurons 1015bits disk 1013bits disk 1014synapses Cycle time 10−9sec 10−9sec 10−3sec Operations/sec 1015 1010 1017 Memory updates/sec 1014 1010 1014 Figure 1.3 A crude comparison of the raw computational resources available to the IBM BLUE GENE supercomputer, a typical personal computer of 2008, and the human brain.

Token 487:
The brain’s numbers are essentially ﬁxed, whereas the supercomputer’s numbers have been in- creasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with the brain.

Token 488:
The personal computer lags behind on all metrics except cycle time.

Token 489:
The only real alternative theory is mysticism: that minds operate in some mystical realm that is beyond physical science.

Token 490:
Brains and digital computers have somewhat different properties.

Token 491:
Figure 1.3 shows that computers have a cycle time that is a million times faster than a brain.

Token 492:
The brain makes upfor that with far more storage and interconnection than even a high-end personal computer,although the largest supercomputers have a capacity that is similar to the brain’s.

Token 493:
(It shouldbe noted, however, that the brain does not seem to use all of its neurons simultaneously.

Token 494:
)Futurists make much of these numbers, pointing to an approaching singularity at which SINGULARITY computers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the raw comparisons are not especially informative.

Token 495:
Even with a computer of virtually unlimitedcapacity, we still would not know how to achieve the brain’s level of intelligence.

Token 496:
1.2.5 Psychology •How do humans and animals think and act?

Token 497:
The origins of scientiﬁc psychology are usually traced to the work of the German physi- cist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).Helmholtz applied the scientiﬁc method to the study of human vision, and his Handbook of Physiological Optics is even now described as “the single most important treatise on the physics and physiology of human vision” (Nalwa, 1993, p.15).

Token 498:
In 1879, Wundt opened the ﬁrst laboratory of experimental psychology, at the University of Leipzig.

Token 499:
Wundt insisted on carefully controlled experiments in which his workers would perform a perceptual or as-sociative task while introspecting on their thought processes.

Token 500:
The careful controls went along way toward making psychology a science, but the subjective nature of the data madeit unlikely that an experimenter would ever disconﬁrm his or her own theories.

Token 501:
Biologists studying animal behavior, on the other hand, lacked introspective data and developed an ob- jective methodology, as described by H. S. Jennings (1906) in his inﬂuential work Behavior of the Lower Organisms .

Token 502:
Applying this viewpoint to humans, the behaviorism movement, led BEHAVIORISM by John Watson (1878–1958), rejected anytheory involving mental processes on the grounds

Token 503:
Section 1.2. The Foundations of Artiﬁcial Intelligence 13 that introspection could not provide reliable evidence.

Token 504:
Behaviorists insisted on studying only objective measures of the percepts (or stimulus ) given to an animal and its resulting actions (orresponse ).

Token 505:
Behaviorism discovered a lot about rats and pigeons but had less success at understanding humans.

Token 506:
Cognitive psychology , which views the brain as an information-processing device,COGNITIVE PSYCHOLOGY can be traced back at least to the works of William James (1842–1910).

Token 507:
Helmholtz also insisted that perception involved a form of unconscious logical inference.

Token 508:
The cognitiveviewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Ap-plied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling wasable to ﬂourish.

Token 509:
The Nature of Explanation , by Bartlett’s student and successor Kenneth Craik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs andgoals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talkabout gases, despite their being made of molecules that have neither.

Token 510:
Craik speciﬁed thethree key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter-nal representation, (2) the representation is manipulated by cognitive processes to derive newinternal representations, and (3) these are in turn retranslated back into action.

Token 511:
He clearlyexplained why this was a good design for an agent: If the organism carries a “small-scale model” of external reality and of its own possible actions within its head, it is able to try out various alternatives, conclude which is the best of them, react to future situations before they arise, utilize the knowledge of past events in dealing with the present and future, and in every way to react in a much fuller, safer, and more competent manner to the emergencies which face it.

Token 512:
(Craik, 1943) After Craik’s death in a bicycle accident in 1945, his work was continued by Donald Broad- bent, whose book Perception and Communication (1958) was one of the ﬁrst works to model psychological phenomena as information processing.

Token 513:
Meanwhile, in the United States, the development of computer modeling led to the creation of the ﬁeld of cognitive science .T h e ﬁeld can be said to have started at a workshop in September 1956 at MIT.

Token 514:
(We shall see thatthis is just two months after the conference at which AI itself was “born.”) At the workshop,George Miller presented The Magic Number Seven , Noam Chomsky presented Three Models of Language , and Allen Newell and Herbert Simon presented The Logic Theory Machine .

Token 515:
These three inﬂuential papers showed how computer models could be used to address thepsychology of memory, language, and logical thinking, respectively.

Token 516:
It is now a common(although far from universal) view among psychologists that “a cognitive theory should be like a computer program” (Anderson, 1980); that is, it should describe a detailed information- processing mechanism whereby some cognitive function might be implemented.

Token 517:
1.2.6 Computer engineering •How can we build an efﬁcient computer?

Token 518:
For artiﬁcial intelligence to succeed, we need two things: intelligence and an artifact. The computer has been the artifact of choice.

Token 519:
The modern digital electronic computer was in-vented independently and almost simultaneously by scientists in three countries embattled in

Token 520:
14 Chapter 1. Introduction World War II.

Token 521:
The ﬁrst operational computer was the electromechanical Heath Robinson,8 built in 1940 by Alan Turing’s team for a single purpose: deciphering German messages.

Token 522:
In 1943, the same group developed the Colossus, a powerful general-purpose machine basedon vacuum tubes.

Token 523:
9The ﬁrst operational programmable computer was the Z-3, the inven- tion of Konrad Zuse in Germany in 1941.

Token 524:
Zuse also invented ﬂoating-point numbers and the ﬁrst high-level programming language, Plankalk¨ ul.

Token 525:
The ﬁrst electronic computer, the ABC, was assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942at Iowa State University.

Token 526:
Atanasoff’s research received little support or recognition; it wasthe ENIAC, developed as part of a secret military project at the University of Pennsylvaniaby a team including John Mauchly and John Eckert, that proved to be the most inﬂuentialforerunner of modern computers.

Token 527:
Since that time, each generation of computer hardware has brought an increase in speed and capacity and a decrease in price.

Token 528:
Performance doubled every 18 months or so until around2005, when power dissipation problems led manufacturers to start multiplying the number ofCPU cores rather than the clock speed.

Token 529:
Current expectations are that future increases in powerwill come from massive parallelism—a curious convergence with the properties of the brain.

Token 530:
Of course, there were calculating devices before the electronic computer.

Token 531:
The earliest automated machines, dating from the 17th century, were discussed on page 6.

Token 532:
The ﬁrst pro- grammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834), that used punched cards to store instructions for the pattern to be woven.

Token 533:
In the mid-19th century, Charles Babbage (1792–1871) designed two machines, neither of which he com- pleted.

Token 534:
The Difference Engine was intended to compute mathematical tables for engineering and scientiﬁc projects.

Token 535:
It was ﬁnally built and shown to work in 1991 at the Science Museum in London (Swade, 2000).

Token 536:
Babbage’s Analytical Engine was far more ambitious: it includedaddressable memory, stored programs, and conditional jumps and was the ﬁrst artifact capa-ble of universal computation.

Token 537:
Babbage’s colleague Ada Lovelace, daughter of the poet LordByron, was perhaps the world’s ﬁrst programmer.

Token 538:
(The programming language Ada is namedafter her.)

Token 539:
She wrote programs for the unﬁnished Analytical Engine and even speculated that the machine could play chess or compose music.

Token 540:
AI also owes a debt to the software side of computer science, which has supplied the operating systems, programming languages, and tools needed to write modern programs (andpapers about them).

Token 541:
But this is one area where the debt has been repaid: work in AI has pio-neered many ideas that have made their way back to mainstream computer science, includingtime sharing, interactive interpreters, personal computers with windows and mice, rapid de-velopment environments, the linked list data type, automatic storage management, and keyconcepts of symbolic, functional, declarative, and object-oriented programming.

Token 542:
8Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contrap- tions for everyday tasks such as buttering toast.

Token 543:
9In the postwar period, Turing wanted to use these com puters for AI research—for example, one of the ﬁrst chess programs (Turing et al. , 1953).

Token 544:
His efforts were blocked by the British government.

Token 545:
Section 1.2. The Foundations of Artiﬁcial Intelligence 15 1.2.7 Control theory and cybernetics •How can artifacts operate under their own control?

Token 546:
Ktesibios of Alexandria (c. 250 B.C.) built the ﬁrst self-controlling machine: a water clock with a regulator that maintained a constant ﬂow rate.

Token 547:
This invention changed the deﬁnitionof what an artifact could do.

Token 548:
Previously, only living things could modify their behavior inresponse to changes in the environment.

Token 549:
Other examples of self-regulating feedback controlsystems include the steam engine governor, created by James Watt (1736–1819), and thethermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.The mathematical theory of stable feedback systems was developed in the 19th century.

Token 550:
The central ﬁgure in the creation of what is now called control theory was Norbert CONTROL THEORY Wiener (1894–1964).

Token 551:
Wiener was a brilliant mathematician who worked with Bertrand Rus- sell, among others, before developing an interest in biological and mechanical control systemsand their connection to cognition.

Token 552:
Like Craik (who also used control systems as psychologicalmodels), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged thebehaviorist orthodoxy (Rosenblueth et al.

Token 553:
, 1943).

Token 554:
They viewed purposive behavior as aris- ing from a regulatory mechanism trying to minimize “error”—the difference between currentstate and goal state.

Token 555:
In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,and John von Neumann, organized a series of inﬂuential conferences that explored the newmathematical and computational models of cognition.

Token 556:
Wiener’s book Cybernetics (1948) be- CYBERNETICS came a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.

Token 557:
Meanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas.

Token 558:
Ashby, Alan Turing, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas before Wiener’s book appeared.” Ashby’s Design for a Brain (1948, 1952) elaborated on his idea that intelligence could be created by the use of homeostatic devices containing appro- HOMEOSTATIC priate feedback loops to achieve stable adaptive behavior.

Token 559:
Modern control theory, especially the branch known as stochastic optimal control, has as its goal the design of systems that maximize an objective function over time.

Token 560:
This roughlyOBJECTIVE FUNCTION matches our view of AI: designing systems that behave optimally.

Token 561:
Why, then, are AI and control theory two different ﬁelds, despite the close connections among their founders?

Token 562:
Theanswer lies in the close coupling between the mathematical techniques that were familiar tothe participants and the corresponding sets of problems that were encompassed in each worldview.

Token 563:
Calculus and matrix algebra, the tools of control theory, lend themselves to systems that are describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way to escape from the these perceived limitations.

Token 564:
The tools of logical inference and computationallowed AI researchers to consider problems such as language, vision, and planning that fellcompletely outside the control theorist’s purview.

Token 565:
1.2.8 Linguistics •How does language relate to thought? In 1957, B. F. Skinner published Verbal Behavior .

Token 566:
This was a comprehensive, detailed ac- count of the behaviorist approach to language learning, written by the foremost expert in

Token 567:
16 Chapter 1. Introduction the ﬁeld.

Token 568:
But curiously, a review of the book became as well known as the book itself, and served to almost kill off interest in behaviorism.

Token 569:
The author of the review was the linguistNoam Chomsky, who had just published a book on his own theory, Syntactic Structures .

Token 570:
Chomsky pointed out that the behaviorist theory did not address the notion of creativity inlanguage—it did not explain how a child could understand and make up sentences that he or she had never heard before.

Token 571:
Chomsky’s theory—based on syntactic models going back to the Indian linguist Panini (c. 350 B.C.

Token 572:
)—could explain this, and unlike previous theories, it was formal enough that it could in principle be programmed.

Token 573:
Modern linguistics and AI, then, were “born” at about the same time, and grew up together, intersecting in a hybrid ﬁeld called computational linguistics ornatural languageCOMPUTATIONAL LINGUISTICS processing .

Token 574:
The problem of understanding language soon turned out to be considerably more complex than it seemed in 1957.

Token 575:
Understanding language requires an understanding of thesubject matter and context, not just an understanding of the structure of sentences.

Token 576:
This mightseem obvious, but it was not widely appreciated until the 1960s.

Token 577:
Much of the early work inknowledge representation (the study of how to put knowledge into a form that a computer can reason with) was tied to language and informed by research in linguistics, which wasconnected in turn to decades of work on the philosophical analysis of language.

Token 578:
1.3 T HEHISTORY OF ARTIFICIAL INTELLIGENCE With the background material behind us, we are ready to cover the development of AI itself.

Token 579:
1.3.1 The gestation of artiﬁcial intelligence (1943–1955) The ﬁrst work that is now generally recognized as AI was done by Warren McCulloch andWalter Pitts (1943).

Token 580:
They drew on three sources: knowledge of the basic physiology andfunction of neurons in the brain; a formal analysis of propositional logic due to Russell and Whitehead; and Turing’s theory of computation.

Token 581:
They proposed a model of artiﬁcial neurons in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurringin response to stimulation by a sufﬁcient number of neighboring neurons.

Token 582:
The state of aneuron was conceived of as “factually equivalent to a proposition which proposed its adequatestimulus.” They showed, for example, that any computable function could be computed bysome network of connected neurons, and that all the logical connectives (and, or, not, etc.

Token 583:
)could be implemented by simple net structures. McCulloch and Pitts also suggested thatsuitably deﬁned networks could learn.

Token 584:
Donald Hebb (1949) demonstrated a simple updatingrule for modifying the connection strengths between neurons.

Token 585:
His rule, now called Hebbian learning , remains an inﬂuential model to this day.

Token 586:
HEBBIAN LEARNING Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the ﬁrst neural network computer in 1950.

Token 587:
The S NARC , as it was called, used 3000 vacuum tubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of 40 neurons.

Token 588:
Later, at Princeton, Minsky studied universal computation in neural networks.

Token 589:
His Ph.D. committee was skeptical about whether this kind of work should be considered

Token 590:
Section 1.3.

Token 591:
The History of Artiﬁcial Intelligence 17 mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky was later to prove inﬂuential theorems showing the limitations of neural network research.

Token 592:
There were a number of early examples of work that can be characterized as AI, but Alan Turing’s vision was perhaps the most inﬂuential.

Token 593:
He gave lectures on the topic as earlyas 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950 article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test, machine learning, genetic algorithms, and reinforcement learning.

Token 594:
He proposed the Child Programme idea, explaining “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulated the child’s?” 1.3.2 The birth of artiﬁcial intelligence (1956) Princeton was home to another inﬂuential ﬁgure in AI, John McCarthy.

Token 595:
After receiving hisPhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-ford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.

Token 596:
McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring together U.S. researchers interested in automata theory, neural nets, and the study of intel-ligence.

Token 597:
They organized a two-month workshop at Dartmouth in the summer of 1956.

Token 598:
Theproposal states: 10 We propose that a 2 month, 10 man study of artiﬁcial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hamp-shire.

Token 599:
The study is to proceed on the basis of the conjecture that every aspect oflearning or any other feature of intelligence can in principle be so precisely de-scribed that a machine can be made to simulate it.

Token 600:
An attempt will be made to ﬁnd how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.

Token 601:
We think that a signiﬁcant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.

Token 602:
There were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel from IBM, and Ray Solomonoff and Oliver Selfridge from MIT.

Token 603:
Two researchers from Carnegie Tech, 11Allen Newell and Herbert Simon, rather stole the show.

Token 604:
Although the others had ideas and in some cases programs for particular appli- cations such as checkers, Newell and Simon already had a reasoning program, the Logic Theorist (LT), about which Simon claimed, “We have invented a computer program capableof thinking non-numerically, and thereby solved the venerable mind–body problem.” 12Soon after the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus- 10This was the ﬁrst ofﬁcial usage of McCarthy’s term artiﬁcial intelligence .

Token 605:
Perhaps “computational rationality” would have been more precise and less threatening, but “AI” has stuck.

Token 606:
At the 50th anniversary of the Dartmouth conference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert Weiner, who was promoting analog cybernetic devices rather than digital computers.

Token 607:
11Now Carnegie Mellon University (CMU). 12Newell and Simon also invented a list-processing language, IPL, to write LT.

Token 608:
They had no compiler and translated it into machine code by hand.

Token 609:
To avoid errors, they worked in parallel, calling out binary numbers toeach other as they wrote each instruction to make sure they agreed.

Token 610:
18 Chapter 1. Introduction sell and Whitehead’s Principia Mathematica .

Token 611:
Russell was reportedly delighted when Simon showed him that the program had come up with a proof for one theorem that was shorter thanthe one in Principia .

Token 612:
The editors of the Journal of Symbolic Logic were less impressed; they rejected a paper coauthored by Newell, Simon, and Logic Theorist.

Token 613:
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce all the major ﬁgures to each other.

Token 614:
For the next 20 years, the ﬁeld would be dominated by these people and their students and colleagues at MIT, CMU, Stanford, and IBM.

Token 615:
Looking at the proposal for the Dartmouth workshop (McCarthy et al. , 1955), we can see why it was necessary for AI to become a separate ﬁeld.

Token 616:
Why couldn’t all the work donein AI have taken place under the name of control theory or operations research or decisiontheory, which, after all, have objectives similar to those of AI?

Token 617:
Or why isn’t AI a branchof mathematics?

Token 618:
The ﬁrst answer is that AI from the start embraced the idea of duplicatinghuman faculties such as creativity, self-improvement, and language use.

Token 619:
None of the otherﬁelds were addressing these issues. The second answer is methodology.

Token 620:
AI is the only oneof these ﬁelds that is clearly a branch of computer science (although operations research doesshare an emphasis on computer simulations), and AI is the only ﬁeld to attempt to buildmachines that will function autonomously in complex, changing environments.

Token 621:
1.3.3 Early enthusiasm, great expectations (1952–1969) The early years of AI were full of successes—in a limited way.

Token 622:
Given the primitive comput-ers and programming tools of the time and the fact that only a few years earlier computerswere seen as things that could do arithmetic and no more, it was astonishing whenever a com- puter did anything remotely clever.

Token 623:
The intellectual establishment, by and large, preferred tobelieve that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered by Turing.)

Token 624:
AI researchers naturally responded by demonstrating one Xafter another. John McCarthy referred to this period as the “Look, Ma, no hands!” era.

Token 625:
Newell and Simon’s early success was followed up with the General Problem Solver, or GPS.

Token 626:
Unlike Logic Theorist, this program was designed from the start to imitate human problem-solving protocols.

Token 627:
Within the limited class of puzzles it could handle, it turned outthat the order in which the program considered subgoals and possible actions was similar tothat in which humans approached the same problems.

Token 628:
Thus, GPS was probably the ﬁrst pro-gram to embody the “thinking humanly” approach.

Token 629:
The success of GPS and subsequent pro-grams as models of cognition led Newell and Simon (1976) to formulate the famous physical symbol system hypothesis, which states that “a physical symbol system has the necessary and PHYSICAL SYMBOL SYSTEM sufﬁcient means for general intelligent action.” What they meant is that any system (human or machine) exhibiting intelligence must operate by manipulating data structures composedof symbols.

Token 630:
We will see later that this hypothesis has been challenged from many directions.

Token 631:
At IBM, Nathaniel Rochester and his colleagues produced some of the ﬁrst AI pro- grams.

Token 632:
Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was able to prove theorems that many students of mathematics would ﬁnd quite tricky.

Token 633:
Startingin 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventuallylearned to play at a strong amateur level.

Token 634:
Along the way, he disproved the idea that comput-

Token 635:
Section 1.3.

Token 636:
The History of Artiﬁcial Intelligence 19 ers can do only what they are told to: his program quickly learned to play a better game than its creator.

Token 637:
The program was demonstrated on television in February 1956, creating a strongimpression. Like Turing, Samuel had trouble ﬁnding computer time.

Token 638:
Working at night, heused machines that were still on the testing ﬂoor at IBM’s manufacturing plant.

Token 639:
Chapter 5covers game playing, and Chapter 21 explains the learning techniques used by Samuel.

Token 640:
John McCarthy moved from Dartmouth to MIT and there made three crucial contribu- tions in one historic year: 1958. In MIT AI Lab Memo No.

Token 641:
1, McCarthy deﬁned the high-levellanguage Lisp , which was to become the dominant AI programming language for the next 30 LISP years.

Token 642:
With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com- puting resources was also a serious problem.

Token 643:
In response, he and others at MIT invented timesharing.

Token 644:
Also in 1958, McCarthy published a paper entitled Programs with Common Sense , in which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrstcomplete AI system.

Token 645:
Like the Logic Theorist and Geometry Theorem Prover, McCarthy’sprogram was designed to use knowledge to search for solutions to problems.

Token 646:
But unlike theothers, it was to embody general knowledge of the world.

Token 647:
For example, he showed howsome simple axioms would enable the program to generate a plan to drive to the airport.

Token 648:
Theprogram was also designed to accept new axioms in the normal course of operation, thereby allowing it to achieve competence in new areas without being reprogrammed .

Token 649:
The Advice Taker thus embodied the central principles of knowledge representation and reasoning: thatit is useful to have a formal, explicit representation of the world and its workings and to beable to manipulate that representation with deductive processes.

Token 650:
It is remarkable how muchof the 1958 paper remains relevant today. 1958 also marked the year that Marvin Minsky moved to MIT.

Token 651:
His initial collaboration with McCarthy did not last, however.

Token 652:
McCarthy stressed representation and reasoning in for-mal logic, whereas Minsky was more interested in getting programs to work and eventuallydeveloped an anti-logic outlook.

Token 653:
In 1963, McCarthy started the AI lab at Stanford. His planto use logic to build the ultimate Advice Taker was advanced by J.

Token 654:
A. Robinson’s discov-ery in 1965 of the resolution method (a complete theorem-proving algorithm for ﬁrst-order logic; see Chapter 9).

Token 655:
Work at Stanford emphasized general-purpose methods for logical reasoning.

Token 656:
Applications of logic included Cordell Green’s question-answering and planningsystems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute(SRI).

Token 657:
The latter project, discussed further in Chapter 25, was the ﬁrst to demonstrate thecomplete integration of logical reasoning and physical activity.

Token 658:
Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve.

Token 659:
These limited domains became known as microworlds .

Token 660:
James MICROWORLD Slagle’s S AINT program (1963) was able to solve closed-form calculus integration problems typical of ﬁrst-year college courses.

Token 661:
Tom Evans’s A NALOGY program (1968) solved geo- metric analogy problems that appear in IQ tests.

Token 662:
Daniel Bobrow’s S TUDENT program (1967) solved algebra story problems, such as the following: If the number of customers Tom gets is twice the square of 20 percent of the number of advertisements he runs, and the number of advertisements he runs is 45, what is the number of customers Tom gets?

Token 663:
20 Chapter 1. Introduction RedGreenRed GreenGreen BlueBlue Red Figure 1.4 A scene from the blocks world.

Token 664:
S HRDLU (Winograd, 1972) has just completed the command “Find a block which is taller than the one you are holding and put it in the box.” The most famous microworld was the blocks world, which consists of a set of solid blocks placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.A typical task in this world is to rearrange the blocks in a certain way, using a robot handthat can pick up one block at a time.

Token 665:
The blocks world was home to the vision project ofDavid Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),the learning theory of Patrick Winston (1970), the natural-language-understanding programof Terry Winograd (1972), and the planner of Scott Fahlman (1974).

Token 666:
Early work building on the neural networks of McCulloch and Pitts also ﬂourished.

Token 667:
The work of Winograd and Cowan (1963) showed how a large number of elements could collectively represent an individual concept, with a corresponding increase in robustness and parallelism.

Token 668:
Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,1960; Widrow, 1962), who called his networks adalines , and by Frank Rosenblatt (1962) with his perceptrons .T h e perceptron convergence theorem (Block et al.

Token 669:
, 1962) says that the learning algorithm can adjust the connection strengths of a perceptron to match any input data, provided such a match exists.

Token 670:
These topics are covered in Chapter 20.

Token 671:
1.3.4 A dose of reality (1966–1973) From the beginning, AI researchers were not shy about making predictions of their coming successes.

Token 672:
The following statement by Herbert Simon in 1957 is often quoted: It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create.

Token 673:
Moreover,

Token 674:
Section 1.3.

Token 675:
The History of Artiﬁcial Intelligence 21 their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.

Token 676:
Terms such as “visible future” can be interpreted in various ways, but Simon also made more concrete predictions: that within 10 years a computer would be chess champion, and a signiﬁcant mathematical theorem would be proved by machine.

Token 677:
These predictions came true (or approximately true) within 40 years rather than 10.

Token 678:
Simon’s overconﬁdence was dueto the promising performance of early AI systems on simple examples.

Token 679:
In almost all cases,however, these early systems turned out to fail miserably when tried out on wider selectionsof problems and on more difﬁcult problems.

Token 680:
The ﬁrst kind of difﬁculty arose because most early programs knew nothing of their subject matter; they succeeded by means of simple syntactic manipulations.

Token 681:
A typical storyoccurred in early machine translation efforts, which were generously funded by the U.S. Na-tional Research Council in an attempt to speed up the translation of Russian scientiﬁc papersin the wake of the Sputnik launch in 1957.

Token 682:
It was thought initially that simple syntactic trans-formations based on the grammars of Russian and English, and word replacement from an electronic dictionary, would sufﬁce to preserve the exact meanings of sentences.

Token 683:
The fact is that accurate translation requires background knowledge in order to resolve ambiguity andestablish the content of the sentence.

Token 684:
The famous retranslation of “the spirit is willing butthe ﬂesh is weak” as “the vodka is good but the meat is rotten” illustrates the difﬁculties en-countered.

Token 685:
In 1966, a report by an advisory committee found that “there has been no machinetranslation of general scientiﬁc text, and none is in immediate prospect.” All U.S. governmentfunding for academic translation projects was canceled.

Token 686:
Today, machine translation is an im-perfect but widely used tool for technical, commercial, government, and Internet documents.

Token 687:
The second kind of difﬁculty was the intractability of many of the problems that AI was attempting to solve.

Token 688:
Most of the early AI programs solved problems by trying out differentcombinations of steps until the solution was found.

Token 689:
This strategy worked initially because microworlds contained very few objects and hence very few possible actions and very short solution sequences.

Token 690:
Before the theory of computational complexity was developed, it waswidely thought that “scaling up” to larger problems was simply a matter of faster hardwareand larger memories.

Token 691:
The optimism that accompanied the development of resolution theoremproving, for example, was soon dampened when researchers failed to prove theorems involv-ing more than a few dozen facts.

Token 692:
The fact that a program can ﬁnd a solution in principle does not mean that the program contains any of the mechanisms needed to ﬁnd it in practice.

Token 693:
The illusion of unlimited computational power was not conﬁned to problem-solving programs.

Token 694:
Early experiments in machine evolution (now called genetic algorithms )( F r i e d - MACHINEEVOLUTION GENETIC ALGORITHM berg, 1958; Friedberg et al.

Token 695:
, 1959) were based on the undoubtedly correct belief that by making an appropriate series of small mutations to a machine-code program, one can gen- erate a program with good performance for any particular task.

Token 696:
The idea, then, was to try random mutations with a selection process to preserve mutations that seemed useful.

Token 697:
De-spite thousands of hours of CPU time, almost no progress was demonstrated.

Token 698:
Modern geneticalgorithms use better representations and have shown more success.

Token 699:
22 Chapter 1.

Token 700:
Introduction Failure to come to grips with the “combinatorial explosion” was one of the main criti- cisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for thedecision by the British government to end support for AI research in all but two universities.

Token 701:
(Oral tradition paints a somewhat different and more colorful picture, with political ambitionsand personal animosities whose description is beside the point.)

Token 702:
A third difﬁculty arose because of some fundamental limitations on the basic structures being used to generate intelligent behavior.

Token 703:
For example, Minsky and Papert’s book Percep- trons (1969) proved that, although perceptrons (a simple form of neural network) could be shown to learn anything they were capable of representing, they could represent very little.

Token 704:
Inparticular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originallystudied) could not be trained to recognize when its two inputs were different.

Token 705:
Although theirresults did not apply to more complex, multilayer networks, research funding for neural-netresearch soon dwindled to almost nothing.

Token 706:
Ironically, the new back-propagation learning al-gorithms for multilayer networks that were to cause an enormous resurgence in neural-netresearch in the late 1980s were actually discovered ﬁrst in 1969 (Bryson and Ho, 1969).

Token 707:
1.3.5 Knowledge-based systems: The key to power?

Token 708:
(1969–1979) The picture of problem solving that had arisen during the ﬁrst decade of AI research was ofa general-purpose search mechanism trying to string together elementary reasoning steps toﬁnd complete solutions.

Token 709:
Such approaches have been called weak methods because, although WEAK METHOD general, they do not scale up to large or difﬁcult problem instances.

Token 710:
The alternative to weak methods is to use more powerful, domain-speciﬁc knowledge that allows larger reasoningsteps and can more easily handle typically occurring cases in narrow areas of expertise.

Token 711:
One might say that to solve a hard problem, you have to almost know the answer already. The D ENDRAL program (Buchanan et al.

Token 712:
, 1969) was an early example of this approach.

Token 713:
It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobellaureate geneticist) teamed up to solve the problem of inferring molecular structure from theinformation provided by a mass spectrometer.

Token 714:
The input to the program consists of the ele-mentary formula of the molecule (e.g., C 6H13NO2) and the mass spectrum giving the masses of the various fragments of the molecule generated when it is bombarded by an electron beam.For example, the mass spectrum might contain a peak at m=1 5 , corresponding to the mass of a methyl (CH 3)f r a g m e n t .

Token 715:
The naive version of the program generated all possible structures consistent with the formula, and then predicted what mass spectrum would be observed for each, comparing this with the actual spectrum.

Token 716:
As one might expect, this is intractable for even moderate-sizedmolecules.

Token 717:
The D ENDRAL researchers consulted analytical chemists and found that they worked by looking for well-known patterns of peaks in the spectrum that suggested commonsubstructures in the molecule.

Token 718:
For example, the following rule is used to recognize a ketone(C=O) subgroup (which weighs 28): ifthere are two peaks at x1andx2such that (a)x1+x2=M+2 8 (Mis the mass of the whole molecule);

Token 719:
Section 1.3. The History of Artiﬁcial Intelligence 23 (b)x1−28is a high peak; (c)x2−28is a high peak; (d) At least one of x1andx2is high.

Token 720:
then there is a ketone subgroup Recognizing that the molecule contains a particular substructure reduces the number of pos- sible candidates enormously.

Token 721:
D ENDRAL was powerful because All the relevant theoretical knowledge to solve these problems has been mapped over from its general form in the [spectrum prediction component] (“ﬁrst principles”) to efﬁcient special forms (“cookbook recipes”).

Token 722:
(Feigenbaum et al.

Token 723:
, 1971) The signiﬁcance of D ENDRAL was that it was the ﬁrst successful knowledge-intensive sys- tem: its expertise derived from large numbers of special-purpose rules.

Token 724:
Later systems alsoincorporated the main theme of McCarthy’s Advice Taker approach—the clean separation ofthe knowledge (in the form of rules) from the reasoning component.

Token 725:
With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro- gramming Project (HPP) to investigate the extent to which the new methodology of expert systems could be applied to other areas of human expertise.

Token 726:
The next major effort was in EXPERT SYSTEMS the area of medical diagnosis.

Token 727:
Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed MYCIN to diagnose blood infections.

Token 728:
With about 450 rules, M YCIN was able to perform as well as some experts, and considerably better than junior doctors.

Token 729:
It also contained twomajor differences from D ENDRAL .

Token 730:
First, unlike the D ENDRAL rules, no general theoretical model existed from which the M YCIN rules could be deduced.

Token 731:
They had to be acquired from extensive interviewing of experts, who in turn acquired them from textbooks, other experts,and direct experience of cases.

Token 732:
Second, the rules had to reﬂect the uncertainty associated withmedical knowledge.

Token 733:
M YCIN incorporated a calculus of uncertainty called certainty factors CERTAINTY FACTOR (see Chapter 14), which seemed (at the time) to ﬁt well with how doctors assessed the impact of evidence on the diagnosis.

Token 734:
The importance of domain knowledge was also apparent in the area of understanding natural language.

Token 735:
Although Winograd’s S HRDLU system for understanding natural language had engendered a good deal of excitement, its dependence on syntactic analysis caused some of the same problems as occurred in the early machine translation work.

Token 736:
It was able toovercome ambiguity and understand pronoun references, but this was mainly because it wasdesigned speciﬁcally for one area—the blocks world.

Token 737:
Several researchers, including EugeneCharniak, a fellow graduate student of Winograd’s at MIT, suggested that robust languageunderstanding would require general knowledge about the world and a general method forusing that knowledge.

Token 738:
At Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming, “There is no such thing as syntax,” which upset a lot of linguists but did serve to start a usefuldiscussion.

Token 739:
Schank and his students built a series of programs (Schank and Abelson, 1977; Wilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under- standing natural language.

Token 740:
The emphasis, however, was less on language per se and more on the problems of representing and reasoning with the knowledge required for language under-standing.

Token 741:
The problems included representing stereotypical situations (Cullingford, 1981),

Token 742:
24 Chapter 1. Introduction describing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding plans and goals (Wilensky, 1983).

Token 743:
The widespread growth of applications to real-world problems caused a concurrent in- crease in the demands for workable knowledge representation schemes.

Token 744:
A large numberof different representation and reasoning languages were developed.

Token 745:
Some were based on logic—for example, the Prolog language became popular in Europe, and the P LANNER fam- ily in the United States.

Token 746:
Others, following Minsky’s idea of frames (1975), adopted a more FRAMES structured approach, assembling facts about particular object and event types and arranging the types into a large taxonomic hierarchy analogous to a biological taxonomy.

Token 747:
1.3.6 AI becomes an industry (1980–present) The ﬁrst successful commercial expert system, R1, began operation at the Digital EquipmentCorporation (McDermott, 1982).

Token 748:
The program helped conﬁgure orders for new computersystems; by 1986, it was saving the company an estimated $40 million a year.

Token 749:
By 1988,DEC’s AI group had 40 expert systems deployed, with more on the way.

Token 750:
DuPont had 100 inuse and 500 in development, saving an estimated $10 million a year.

Token 751:
Nearly every major U.S.corporation had its own AI group and was either using or investigating expert systems.

Token 752:
In 1981, the Japanese announced the “Fifth Generation” project, a 10-year plan to build intelligent computers running Prolog.

Token 753:
In response, the United States formed the Microelec-tronics and Computer Technology Corporation (MCC) as a research consortium designed to assure national competitiveness.

Token 754:
In both cases, AI was part of a broad effort, including chip design and human-interface research.

Token 755:
In Britain, the Alvey report reinstated the funding thatwas cut by the Lighthill report.

Token 756:
13In all three countries, however, the projects never met their ambitious goals.

Token 757:
Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988, including hundreds of companies building expert systems, vision systems, robots,and software and hardware specialized for these purposes.

Token 758:
Soon after that came a periodcalled the “AI Winter,” in which many companies fell by the wayside as they failed to deliveron extravagant promises.

Token 759:
1.3.7 The return of neural networks (1986–present) In the mid-1980s at least four different groups reinvented the back-propagation learning BACK-PROPAGATION algorithm ﬁrst found in 1969 by Bryson and Ho.

Token 760:
The algorithm was applied to many learn- ing problems in computer science and psychology, and the widespread dissemination of theresults in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986) caused great excitement.

Token 761:
These so-called connectionist models of intelligent systems were seen by some as di- CONNECTIONIST rect competitors both to the symbolic models promoted by Newell and Simon and to the logicist approach of McCarthy and others (Smolensky, 1988).

Token 762:
It might seem obvious thatat some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic 13To save embarrassment, a new ﬁeld called IKBS (Intelligent Knowledge-Based Systems) was invented because Artiﬁcial Intelligence had been ofﬁcially canceled.

Token 763:
Section 1.3.

Token 764:
The History of Artiﬁcial Intelligence 25 Species (1997) suggests that this is the deﬁning characteristic of humans—but the most ar- dent connectionists questioned whether symbol manipulation had any real explanatory role indetailed models of cognition.

Token 765:
This question remains unanswered, but the current view is thatconnectionist and symbolic approaches are complementary, not competing.

Token 766:
As occurred withthe separation of AI and cognitive science, modern neural network research has bifurcated into two ﬁelds, one concerned with creating effective network architectures and algorithms and understanding their mathematical properties, the other concerned with careful modelingof the empirical properties of actual neurons and ensembles of neurons.

Token 767:
1.3.8 AI adopts the scientiﬁc method (1987–present) Recent years have seen a revolution in both the content and the methodology of work inartiﬁcial intelligence.

Token 768:
14It is now more common to build on existing theories than to propose brand-new ones, to base claims on rigorous theorems or hard experimental evidence ratherthan on intuition, and to show relevance to real-world applications rather than toy examples.

Token 769:
AI was founded in part as a rebellion against the limitations of existing ﬁelds like control theory and statistics, but now it is embracing those ﬁelds.

Token 770:
As David McAllester (1998) put it: In the early period of AI it seemed plausible that new forms of symbolic computation, e.g., frames and semantic networks, made much of classical theory obsolete.

Token 771:
This led to a form of isolationism in which AI became largely separated from the rest of computer science.

Token 772:
This isolationism is currently being abandoned.

Token 773:
There is a recognition that machine learning should not be isolated from information theory, that uncertain reasoning should not be isolated from stochastic modeling, that search should not be isolated from classical optimization and control, and that automated reasoning should not be isolatedfrom formal methods and static analysis.

Token 774:
In terms of methodology, AI has ﬁnally come ﬁrmly under the scientiﬁc method.

Token 775:
To be ac- cepted, hypotheses must be subjected to rigorous empirical experiments, and the results mustbe analyzed statistically for their importance (Cohen, 1995).

Token 776:
It is now possible to replicateexperiments by using shared repositories of test data and code. The ﬁeld of speech recognition illustrates the pattern.

Token 777:
In the 1970s, a wide variety of different architectures and approaches were tried.

Token 778:
Many of these were rather ad hoc and fragile, and were demonstrated on only a few specially selected examples.

Token 779:
In recent years,approaches based on hidden Markov models (HMMs) have come to dominate the area. Two HIDDEN MARKOV MODELS aspects of HMMs are relevant.

Token 780:
First, they are based on a rigorous mathematical theory.

Token 781:
This has allowed speech researchers to build on several decades of mathematical results developed in other ﬁelds.

Token 782:
Second, they are generated by a process of training on a large corpus of real speech data.

Token 783:
This ensures that the performance is robust, and in rigorous blind tests the HMMs have been improving their scores steadily.

Token 784:
Speech technology and the related ﬁeld ofhandwritten character recognition are already making the transition to widespread industrial 14Some have characterized this change as a victory of the neats —those who think that AI theories should be grounded in mathematical rigor—over the scrufﬁes —those who would rather try out lots of ideas, write some programs, and then assess what seems to be working.

Token 785:
Both approaches are important. A shift toward neatness implies that the ﬁeld has reached a level of stability and maturity.

Token 786:
Whether that stability will be disrupted by anew scruffy idea is another question.

Token 787:
26 Chapter 1. Introduction and consumer applications.

Token 788:
Note that there is no scientiﬁc claim that humans use HMMs to recognize speech; rather, HMMs provide a mathematical framework for understanding theproblem and support the engineering claim that they work well in practice.

Token 789:
Machine translation follows the same course as speech recognition.

Token 790:
In the 1950s there was initial enthusiasm for an approach based on sequences of words, with models learned according to the principles of information theory.

Token 791:
That approach fell out of favor in the 1960s, but returned in the late 1990s and now dominates the ﬁeld. Neural networks also ﬁt this trend.

Token 792:
Much of the work on neural nets in the 1980s was done in an attempt to scope out what could be done and to learn how neural nets differ from“traditional” techniques.

Token 793:
Using improved methodology and theoretical frameworks, the ﬁeldarrived at an understanding in which neural nets can now be compared with correspondingtechniques from statistics, pattern recognition, and machine learning, and the most promisingtechnique can be applied to each application.

Token 794:
As a result of these developments, so-calleddata mining technology has spawned a vigorous new industry.

Token 795:
DATA MINING Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep- tance of probability and decision theory in AI, following a resurgence of interest epitomizedby Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network BAYESIAN NETWORK formalism was invented to allow efﬁcient representation of, and rigorous reasoning with, uncertain knowledge.

Token 796:
This approach largely overcomes many problems of the probabilisticreasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason-ing and expert systems.

Token 797:
The approach allows for learning from experience, and it combinesthe best of classical AI and neural nets.

Token 798:
Work by Judea Pearl (1982a) and by Eric Horvitz andDavid Heckerman (Horvitz and Heckerman, 1986; Horvitz et al.

Token 799:
, 1986) promoted the idea of normative expert systems: ones that act rationally according to the laws of decision theory and do not try to imitate the thought steps of human experts.

Token 800:
The Windows TMoperating sys- tem includes several normative diagnostic expert systems for correcting problems. Chapters13 to 16 cover this area.

Token 801:
Similar gentle revolutions have occurred in robotics, computer vision, and knowledge representation.

Token 802:
A better understanding of the problems and their complexity properties, com- bined with increased mathematical sophistication, has led to workable research agendas androbust methods.

Token 803:
Although increased formalization and specialization led ﬁelds such as visionand robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend hasreversed in recent years as tools from machine learning in particular have proved effective formany problems.

Token 804:
The process of reintegration is already yielding signiﬁcant beneﬁts 1.3.9 The emergence of intelligent agents (1995–present) Perhaps encouraged by the progress in solving the subproblems of AI, researchers have alsostarted to look at the “whole agent” problem again.

Token 805:
The work of Allen Newell, John Laird, and Paul Rosenbloom on S OAR (Newell, 1990; Laird et al.

Token 806:
, 1987) is the best-known example of a complete agent architecture. One of the most important environments for intelligentagents is the Internet.

Token 807:
AI systems have become so common in Web-based applications thatthe “-bot” sufﬁx has entered everyday language. Moreover, AI technologies underlie many

Token 808:
Section 1.3. The History of Artiﬁcial Intelligence 27 Internet tools, such as search engines, recommender systems, and Web site aggregators.

Token 809:
One consequence of trying to build complete agents is the realization that the previously isolated subﬁelds of AI might need to be reorganized somewhat when their results are to betied together.

Token 810:
In particular, it is now widely appreciated that sensory systems (vision, sonar,speech recognition, etc.)

Token 811:
cannot deliver perfectly reliable information about the environment. Hence, reasoning and planning systems must be able to handle uncertainty.

Token 812:
A second major consequence of the agent perspective is that AI has been drawn into much closer contactwith other ﬁelds, such as control theory and economics, that also deal with agents.

Token 813:
Recentprogress in the control of robotic cars has derived from a mixture of approaches ranging frombetter sensors, control-theoretic integration of sensing, localization and mapping, as well asa degree of high-level planning.

Token 814:
Despite these successes, some inﬂuential founders of AI, including John McCarthy (2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal andWinston, 2009), have expressed discontent with the progress of AI.

Token 815:
They think that AI shouldput less emphasis on creating ever-improved versions of applications that are good at a spe-ciﬁc task, such as driving a car, playing chess, or recognizing speech.

Token 816:
Instead, they believeAI should return to its roots of striving for, in Simon’s words, “machines that think, that learn and that create.” They call the effort human-level AI or HLAI; their ﬁrst symposium was in HUMAN-LEVEL AI 2004 (Minsky et al.

Token 817:
, 2004). The effort will require very large knowledge bases; Hendler et al. (1995) discuss where these knowledge bases might come from.

Token 818:
A related idea is the subﬁeld of Artiﬁcial General Intelligence or AGI (Goertzel andARTIFICIAL GENERAL INTELLIGENCE Pennachin, 2007), which held its ﬁrst conference and organized the Journal of Artiﬁcial Gen- eral Intelligence in 2008.

Token 819:
AGI looks for a universal algorithm for learning and acting in any environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-dees of the original 1956 Dartmouth conference.

Token 820:
Guaranteeing that what we create is reallyFriendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to FRIENDLYAI in Chapter 26.

Token 821:
1.3.10 The availability of very large data sets (2001–present) Throughout the 60-year history of computer science, the emphasis has been on the algorithm as the main subject of study.

Token 822:
But some recent work in AI suggests that for many problems, itmakes more sense to worry about the data and be less picky about what algorithm to apply.

Token 823:
This is true because of the increasing availability of very large data sources: for example,trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,2006); or billions of base pairs of genomic sequences (Collins et al.

Token 824:
, 2003).

Token 825:
One inﬂuential paper in this line was Yarowsky’s (1995) work on word-sense disam- biguation: given the use of the word “plant” in a sentence, does that refer to ﬂora or factory?Previous approaches to the problem had relied on human-labeled examples combined with machine learning algorithms.

Token 826:
Yarowsky showed that the task can be done, with accuracy above 96%, with no labeled examples at all.

Token 827:
Instead, given a very large corpus of unanno-tated text and just the dictionary deﬁnitions of the two senses—“works, industrial plant” and“ﬂora, plant life”—one can label examples in the corpus, and from there bootstrap to learn

Token 828:
28 Chapter 1. Introduction new patterns that help label new examples.

Token 829:
Banko and Brill (2001) show that techniques like this perform even better as the amount of available text goes from a million words to abillion and that the increase in performance from using more data exceeds any difference inalgorithm choice; a mediocre algorithm with 100 million words of unlabeled training dataoutperforms the best known algorithm with 1 million words.

Token 830:
As another example, Hays and Efros (2007) discuss the problem of ﬁlling in holes in a photograph.

Token 831:
Suppose you use Photoshop to mask out an ex-friend from a group photo, butnow you need to ﬁll in the masked area with something that matches the background.

Token 832:
Haysand Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd somethingthat will match.

Token 833:
They found the performance of their algorithm was poor when they useda collection of only ten thousand photos, but crossed a threshold into excellent performancewhen they grew the collection to two million photos.

Token 834:
Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to express all the knowledge that a system needs—may be solved in many applications by learn-ing methods rather than hand-coded knowledge engineering, provided the learning algorithmshave enough data to go on (Halevy et al.

Token 835:
, 2009). Reporters have noticed the surge of new ap- plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein, 2005).

Token 836:
As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply embedded in the infrastructure of every industry.” 1.4 T HESTATE OF THE ART What can AI do today?

Token 837:
A concise answer is difﬁcult because there are so many activities inso many subﬁelds.

Token 838:
Here we sample a few applications; others appear throughout the book.

Token 839:
Robotic vehicles : A driverless robotic car named S TANLEY sped through the rough terrain of the Mojave dessert at 22 mph, ﬁnishing the 132-mile course ﬁrst to win the 2005 DARPA Grand Challenge.

Token 840:
S TANLEY is a Volkswagen Touareg outﬁtted with cameras, radar, and laser rangeﬁnders to sense the environment and onboard software to command the steer-ing, braking, and acceleration (Thrun, 2006).

Token 841:
The following year CMU’s B OSSwon the Ur- ban Challenge, safely driving in trafﬁc through the streets of a closed Air Force base, obeyingtrafﬁc rules and avoiding pedestrians and other vehicles.

Token 842:
Speech recognition : A traveler calling United Airlines to book a ﬂight can have the en- tire conversation guided by an automated speech recognition and dialog management system.

Token 843:
Autonomous planning and scheduling : A hundred million miles from Earth, NASA’s Remote Agent program became the ﬁrst on-board autonomous planning program to controlthe scheduling of operations for a spacecraft (Jonsson et al.

Token 844:
, 2000).

Token 845:
R EMOTE AGENT gen- erated plans from high-level goals speciﬁed from the ground and monitored the execution of those plans—detecting, diagnosing, and recovering from problems as they occurred.

Token 846:
Succes- sor program MAPGEN (Al-Chang et al. , 2004) plans the daily operations for NASA’s Mars Exploration Rovers, and MEXAR2 (Cesta et al.

Token 847:
, 2007) did mission planning—both logistics and science planning—for the European Space Agency’s Mars Express mission in 2008.

Token 848:
Section 1.5.

Token 849:
Summary 29 Game playing :I B M ’ sD EEPBLUE became the ﬁrst computer program to defeat the world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 inan exhibition match (Goodman and Keene, 1997).

Token 850:
Kasparov said that he felt a “new kind ofintelligence” across the board from him.

Token 851:
Newsweek magazine described the match as “The brain’s last stand.” The value of IBM’s stock increased by $18 billion.

Token 852:
Human champions studied Kasparov’s loss and were able to draw a few matches in subsequent years, but the most recent human-computer matches have been won convincingly by the computer.

Token 853:
Spam ﬁghting : Each day, learning algorithms classify over a billion messages as spam, saving the recipient from having to waste time deleting what, for many users, could comprise80% or 90% of all messages, if not classiﬁed away by algorithms.

Token 854:
Because the spammers arecontinually updating their tactics, it is difﬁcult for a static programmed approach to keep up,and learning algorithms work best (Sahami et al.

Token 855:
, 1998; Goodman and Heckerman, 2004).

Token 856:
Logistics planning : During the Persian Gulf crisis of 1991, U.S. forces deployed a Dynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automatedlogistics planning and scheduling for transportation.

Token 857:
This involved up to 50,000 vehicles,cargo, and people at a time, and had to account for starting points, destinations, routes, andconﬂict resolution among all parameters.

Token 858:
The AI planning techniques generated in hours a plan that would have taken weeks with older methods.

Token 859:
The Defense Advanced Research Project Agency (DARPA) stated that this single application more than paid back DARPA’s30-year investment in AI.

Token 860:
Robotics : The iRobot Corporation has sold over two million Roomba robotic vacuum cleaners for home use.

Token 861:
The company also deploys the more rugged PackBot to Iraq and Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify the location of snipers.

Token 862:
Machine Translation : A computer program automatically translates from Arabic to English, allowing an English speaker to see the headline “Ardogan Conﬁrms That TurkeyWould Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses astatistical model built from examples of Arabic-to-English translations and from examples of English text totaling two trillion words (Brants et al.

Token 863:
, 2007). None of the computer scientists on the team speak Arabic, but they do understand statistics and machine learning algorithms.

Token 864:
These are just a few examples of artiﬁcial intelligence systems that exist today.

Token 865:
Not magic or science ﬁction—but rather science, engineering, and mathematics, to which thisbook provides an introduction.

Token 866:
1.5 S UMMARY This chapter deﬁnes AI and establishes the cultural background against which it has devel-oped.

Token 867:
Some of the important points are as follows: •Different people approach AI with different goals in mind.

Token 868:
Two important questions to ask are: Are you concerned with thinking or behavior? Do you want to model humansor work from an ideal standard?

Token 869:
30 Chapter 1. Introduction •In this book, we adopt the view that intelligence is concerned mainly with rational action .

Token 870:
Ideally, an intelligent agent takes the best possible action in a situation.

Token 871:
We study the problem of building agents that are intelligent in this sense. •Philosophers (going back to 400 B.C.)

Token 872:
made AI conceivable by considering the ideas that the mind is in some ways like a machine, that it operates on knowledge encoded in some internal language, and that thought can be used to choose what actions to take.

Token 873:
•Mathematicians provided the tools to manipulate statements of logical certainty as well as uncertain, probabilistic statements.

Token 874:
They also set the groundwork for understandingcomputation and reasoning about algorithms.

Token 875:
•Economists formalized the problem of making decisions that maximize the expected outcome to the decision maker.

Token 876:
•Neuroscientists discovered some facts about how the brain works and the ways in which it is similar to and different from computers.

Token 877:
•Psychologists adopted the idea that humans and animals can be considered information- processing machines.

Token 878:
Linguists showed that language use ﬁts into this model.

Token 879:
•Computer engineers provided the ever-more-powerful machines that make AI applica- tions possible.

Token 880:
•Control theory deals with designing devices that act optimally on the basis of feedback from the environment.

Token 881:
Initially, the mathematical tools of control theory were quitedifferent from AI, but the ﬁelds are coming closer together.

Token 882:
•The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks in enthusiasm and funding.

Token 883:
There have also been cycles of introducing new creativeapproaches and systematically reﬁning the best ones.

Token 884:
•AI has advanced more rapidly in the past decade because of greater use of the scientiﬁc method in experimenting with and comparing approaches.

Token 885:
•Recent progress in understanding the theoretical basis for intelligence has gone hand in hand with improvements in the capabilities of real systems.

Token 886:
The subﬁelds of AI have become more integrated, and AI has found common ground with other disciplines.

Token 887:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The methodological status of artiﬁcial intelligence is investigated in The Sciences of the Artiﬁ- cial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.

Token 888:
It explains how AI can be viewed as both science and mathematics. Cohen (1995) gives anoverview of experimental methodology within AI.

Token 889:
The Turing Test (Turing, 1950) is discussed by Shieber (1994), who severely criticizes the usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes (1995), who argue that the test itself is not helpful for AI.

Token 890:
Bringsjord (2008) gives advice fora Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on the Turing Test.

Token 891:
Artiﬁcial Intelligence: The Very Idea , by John Haugeland (1985), gives a

Token 892:
Exercises 31 readable account of the philosophical and practical problems of AI.

Token 893:
Signiﬁcant early papers in AI are anthologized in the collections by Webber and Nilsson (1981) and by Luger (1995).The Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in AI, as does Wikipedia.

Token 894:
These articles usually provide a good entry point into the researchliterature on each topic.

Token 895:
An insightful and comprehensive history of AI is given by Nils Nillson (2009), one of the early pioneers of the ﬁeld.

Token 896:
The most recent work appears in the proceedings of the major AI conferences: the bi- ennial International Joint Conference on AI (IJCAI), the annual European Conference on AI(ECAI), and the Nationa l Conference on AI, more often kno wn as AAAI, after its sponsoring organization.

Token 897:
The major journals for general AI are Artiﬁcial Intelligence ,Computational Intelligence ,t h e IEEE Transactions on Pattern Analysis and Machine Intelligence ,IEEE In- telligent Systems , and the electronic Journal of Artiﬁcial Intelligence Research .T h e r ea r ea l s o many conferences and journals devoted to speciﬁc areas, which we cover in the appropriatechapters.

Token 898:
The main professional societies for AI are the American Association for ArtiﬁcialIntelligence (AAAI), the ACM Sp ecial Interest Group in Artiﬁcial Intelligence (SIGART), and the Society for Artiﬁcial Intelligence an d Simulation of Behaviour (AISB).

Token 899:
AAAI’s AI Magazine contains many topical and tutorial articles, and its Web site, aaai.org , contains news, tutorials, and background information.

Token 900:
EXERCISES These exercises are intended to stimulate discussion, and some might be set as term projects.

Token 901:
Alternatively, preliminary attempts can be made now, and these attempts can be reviewedafter the completion of the book.

Token 902:
1.1 Deﬁne in your own words: (a) intelligence, (b) artiﬁcial intelligence, (c) agent, (d) rationality, (e) logical reasoning.

Token 903:
1.2 Read Turing’s original paper on AI (Turing, 1950).

Token 904:
In the paper, he discusses several objections to his proposed enterprise and his test for intelligence. Which objections still carry weight?

Token 905:
Are his refutations valid? Can you think of new objections arising from develop-ments since he wrote the paper?

Token 906:
In the paper, he predicts that, by the year 2000, a computerwill have a 30% chance of passing a ﬁve-minute Turing Test with an unskilled interrogator.What chance do you think a computer would have today?

Token 907:
In another 50 years? 1.3 Are reﬂex actions (such as ﬂinching from a hot stove) rational? Are they intelligent?

Token 908:
1.4 Suppose we extend Evans’s A NALOGY program so that it can score 200 on a standard IQ test.

Token 909:
Would we then have a program more intelligent than a human? Explain.

Token 910:
1.5 The neural structure of the sea slug Aplysia has been widely studied (ﬁrst by Nobel Laureate Eric Kandel) because it has only about 20,000 neurons, most of them large and easily manipulated.

Token 911:
Assuming that the cycle time for an Aplysia neuron is roughly the same as for a human neuron, how does the computational power, in terms of memory updates per second, compare with the high-end computer described in Figure 1.3?

Token 912:
32 Chapter 1. Introduction 1.6 How could introspection—reporting on one’s inner thoughts—be inaccurate? Could I be wrong about what I’m thinking?

Token 913:
Discuss. 1.7 To what extent are the following computer systems instances of artiﬁcial intelligence: •Supermarket bar code scanners.

Token 914:
•Web search engines. •Voice-activated telephone menus. •Internet routing algorithms that respond dynamically to the state of the network.

Token 915:
1.8 Many of the computational models of cognitive activities that have been proposed in- volve quite complex mathematical operations, such as convolving an image with a Gaussianor ﬁnding a minimum of the entropy function.

Token 916:
Most humans (and certainly all animals) neverlearn this kind of mathematics at all, almost no one learns it before college, and almost noone can compute the convolution of a function with a Gaussian in their head.

Token 917:
What sensedoes it make to say that the “vision system” is doing this kind of mathematics, whereas theactual person has no idea how to do it?

Token 918:
1.9 Why would evolution tend to result in systems that act rationally? What goals are such systems designed to achieve?

Token 919:
1.10 Is AI a science, or is it engineering? Or neither or both? Explain.

Token 920:
1.11 “Surely computers cannot be intelligent—they can do only what their programmers tell them.” Is the latter statement true, and does it imply the former?

Token 921:
1.12 “Surely animals cannot be intelligent—they can do only what their genes tell them.” Is the latter statement true, and does it imply the former?

Token 922:
1.13 “Surely animals, humans, and computers cannot be intelligent—they can do only what their constituent atoms are told to do by the laws of physics.” Is the latter statement true, and does it imply the former?

Token 923:
1.14 Examine the AI literature to discover whether the following tasks can currently be solved by computers: a.

Token 924:
Playing a decent game of table tennis (Ping-Pong). b. Driving in the center of Cairo, Egypt. c. Driving in Victorville, California.

Token 925:
d. Buying a week’s worth of groceries at the market. e. Buying a week’s worth of groceries on the Web.

Token 926:
f. Playing a decent game of bridge at a competitive level. g. Discovering and proving new mathematical theorems.

Token 927:
h. Writing an intentionally funny story. i. Giving competent legal advice in a specialized area of law.

Token 928:
j. Translating spoken English into spoken Swedish in real time. k. Performing a complex surgical operation.

Token 929:
Exercises 33 For the currently infeasible tasks, try to ﬁnd out what the difﬁculties are and predict when, if ever, they will be overcome.

Token 930:
1.15 Various subﬁelds of AI have held contests by deﬁning a standard task and inviting re- searchers to do their best.

Token 931:
Examples include the DARPA Grand Challenge for robotic cars, The International Planning Competition, the Robocup robotic soccer league, the TREC infor-mation retrieval event, and contests in machine translation, speech recognition.

Token 932:
Investigateﬁve of these contests, and describe the progress made over the years. To what degree have thecontests advanced toe state of the art in AI?

Token 933:
Do what degree do they hurt the ﬁeld by drawingenergy away from new ideas?

Token 934:


Token 935:
2INTELLIGENT AGENTS In which we discuss the nature of agents, perfect or otherwise, the diversity of environments, and the resulting menagerie of agent types.

Token 936:
Chapter 1 identiﬁed the concept of rational agents as central to our approach to artiﬁcial intelligence.

Token 937:
In this chapter, we make this notion more concrete.

Token 938:
We will see that the conceptof rationality can be applied to a wide variety of agents operating in any imaginable environ-ment.

Token 939:
Our plan in this book is to use this concept to develop a small set of design principlesfor building successful agents—systems that can reasonably be called intelligent .

Token 940:
We begin by examining agents, environments, and the coupling between them.

Token 941:
The observation that some agents behave better than others leads naturally to the idea of a rational agent—one that behaves as well as possible.

Token 942:
How well an agent can behave depends on the nature of the environment; some environments are more difﬁcult than others.

Token 943:
We give acrude categorization of environments and show how properties of an environment inﬂuencethe design of suitable agents for that environment.

Token 944:
We describe a number of basic “skeleton”agent designs, which we ﬂesh out in the rest of the book.

Token 945:
2.1 A GENTS AND ENVIRONMENTS Anagent is anything that can be viewed as perceiving its environment through sensors and ENVIRONMENT SENSOR acting upon that environment through actuators .

Token 946:
This simple idea is illustrated in Figure 2.1.

Token 947:
ACTUATOR A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so on for actuators.

Token 948:
A robotic agent might have cameras and infrared range ﬁnders for sensorsand various motors for actuators.

Token 949:
A software agent receives keystrokes, ﬁle contents, andnetwork packets as sensory inputs and acts on the environment by displaying on the screen,writing ﬁles, and sending network packets.

Token 950:
We use the term percept to refer to the agent’s perceptual inputs at any given instant.

Token 951:
An PERCEPT agent’s percept sequence is the complete history of everything the agent has ever perceived.

Token 952:
PERCEPT SEQUENCE In general, an agent’s choice of action at any given instant can depend on the entire percept sequence observed to date, but not on anything it hasn’t perceived.

Token 953:
By specifying the agent’s choice of action for every possible percept sequence, we have said more or less everything 34

Token 954:
Section 2.1. Agents and Environments 35 Agent Sensors ActuatorsEnvironmentPercepts Actions?

Token 955:
Figure 2.1 Agents interact with environments through sensors and actuators. there is to say about the agent.

Token 956:
Mathematically speaking, we say that an agent’s behavior is described by the agent function that maps any given percept sequence to an action.

Token 957:
AGENT FUNCTION We can imagine tabulating the agent function that describes any given agent; for most agents, this would be a very large table—inﬁnite, in fact, unless we place a bound on thelength of percept sequences we want to consider.

Token 958:
Given an agent to experiment with, we can,in principle, construct this table by trying out all possible percept sequences and recordingwhich actions the agent does in response.

Token 959:
1The table is, of course, an external characterization of the agent.

Token 960:
Internally , the agent function for an artiﬁcial agent will be implemented by an agent program . It is important to keep these two ideas distinct.

Token 961:
The agent function is an AGENT PROGRAM abstract mathematical description; the agent program is a concrete implementation, running within some physical system.

Token 962:
To illustrate these ideas, we use a very simple example—the vacuum-cleaner world shown in Figure 2.2.

Token 963:
This world is so simple that we can describe everything that happens;it’s also a made-up world, so we can invent many variations.

Token 964:
This particular world has just twolocations: squares AandB. The vacuum agent perceives which square it is in and whether there is dirt in the square.

Token 965:
It can choose to move left, move right, suck up the dirt, or donothing.

Token 966:
One very simple agent function is the following: if the current square is dirty, thensuck; otherwise, move to the other square.

Token 967:
A partial tabulation of this agent function is shownin Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.

Token 968:
Looking at Figure 2.3, we see that various vacuum-world agents can be deﬁned simply by ﬁlling in the right-hand column in various ways.

Token 969:
The obvious question, then, is this: What is the right way to ﬁll out the table?

Token 970:
In other words, what makes an agent good or bad, intelligent or stupid? We answer these questions in the next section.

Token 971:
1If the agent uses some randomization to choose its actions, then we would have to try each sequence many times to identify the probability of each action.

Token 972:
One might imagine that acting randomly is rather silly, but weshow later in this chapter that it can be very intelligent.

Token 973:
36 Chapter 2. Intelligent Agents AB Figure 2.2 A vacuum-cleaner world with just two locations.

Token 974:
Percept sequence Action [A,Clean ] Right [A,Dirty] Suck [B,Clean ] Left [B,Dirty] Suck [A,Clean ],[A,Clean ] Right [A,Clean ],[A,Dirty] Suck ... ... [A,Clean ],[A,Clean ],[A,Clean ] Right [A,Clean ],[A,Clean ],[A,Dirty] Suck ... ...

Token 975:
Figure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world shown in Figure 2.2.

Token 976:
Before closing this section, we should emphasize that the notion of an agent is meant to be a tool for analyzing systems, not an absolute characterization that divides the world into agents and non-agents.

Token 977:
One could view a hand-held calculator as an agent that chooses the action of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysiswould hardly aid our understanding of the calculator.

Token 978:
In a sense, all areas of engineering canbe seen as designing artifacts that interact with the world; AI operates at (what the authorsconsider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcantcomputational resources and the task environment requires nontrivial decision making.

Token 979:
2.2 G OOD BEHA VIOR :THECONCEPT OF RATIONALITY Arational agent is one that does the right thing—conceptually speaking, every entry in the RATIONAL AGENT table for the agent function is ﬁlled out correctly.

Token 980:
Obviously, doing the right thing is better than doing the wrong thing, but what does it mean to do the right thing?

Token 981:
Section 2.2.

Token 982:
Good Behavior: The Concept of Rationality 37 We answer this age-old question in an age-old way: by considering the consequences of the agent’s behavior.

Token 983:
When an agent is plunked down in an environment, it generates asequence of actions according to the percepts it receives.

Token 984:
This sequence of actions causes theenvironment to go through a sequence of states. If the sequence is desirable, then the agenthas performed well.

Token 985:
This notion of desirability is captured by a performance measure that PERFORMANCE MEASURE evaluates any given sequence of environment states.

Token 986:
Notice that we said environment states, not agent states.

Token 987:
If we deﬁne success in terms of agent’s opinion of its own performance, an agent could achieve perfect rationality simplyby deluding itself that its performance was perfect.

Token 988:
Human agents in particular are notoriousfor “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) afternot getting it.

Token 989:
Obviously, there is not one ﬁxed performance measure for all tasks and agents; typically, a designer will devise one appropriate to the circumstances.

Token 990:
This is not as easy as it sounds.Consider, for example, the vacuum-cleaner agent from the preceding section.

Token 991:
We mightpropose to measure performance by the amount of dirt cleaned up in a single eight-hour shift.With a rational agent, of course, what you ask for is what you get.

Token 992:
A rational agent canmaximize this performance measure by cleaning up the dirt, then dumping it all on the ﬂoor, then cleaning it up again, and so on.

Token 993:
A more suitable performance measure would reward the agent for having a clean ﬂoor.

Token 994:
For example, one point could be awarded for each clean squareat each time step (perhaps with a penalty for electricity consumed and noise generated).

Token 995:
As a general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.

Token 996:
Even when the obvious pitfalls are avoided, there remain some knotty issues to untangle.

Token 997:
For example, the notion of “clean ﬂoor” in the preceding paragraph is based on averagecleanliness over time.

Token 998:
Yet the same average cleanliness can be achieved by two differentagents, one of which does a mediocre job all the time while the other cleans energetically buttakes long breaks.

Token 999:
Which is preferable might seem to be a ﬁne point of janitorial science, butin fact it is a deep philosophical question with far-reaching implications.

Token 1000:
Which is better— a reckless life of highs and lows, or a safe but humdrum existence?

Token 1001:
Which is better—an economy where everyone lives in moderate poverty, or one in which some live in plentywhile others are very poor?

Token 1002:
We leave these questions as an exercise for the diligent reader.

Token 1003:
2.2.1 Rationality What is rational at any given time depends on four things: •The performance measure that deﬁnes the criterion of success.

Token 1004:
•The agent’s prior knowledge of the environment. •The actions that the agent can perform. •The agent’s percept sequence to date.

Token 1005:
This leads to a deﬁnition of a rational agent :DEFINITION OF A RATIONAL AGENT For each possible percept sequence, a rational agent should select an action that is ex- pected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

Token 1006:
38 Chapter 2.

Token 1007:
Intelligent Agents Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the other square if not; this is the agent function tabulated in Figure 2.3.

Token 1008:
Is this a rational agent?That depends!

Token 1009:
First, we need to say what the performance measure is, what is known aboutthe environment, and what sensors and actuators the agent has.

Token 1010:
Let us assume the following: •The performance measure awards one point for each clean square at each time step, over a “lifetime” of 1000 time steps.

Token 1011:
•The “geography” of the environment is known ap r i o r i (Figure 2.2) but the dirt distri- bution and the initial location of the agent are not.

Token 1012:
Clean squares stay clean and suckingcleans the current square.

Token 1013:
The Left andRight actions move the agent left and right except when this would take the agent outside the environment, in which case the agentremains where it is.

Token 1014:
•The only available actions are Left,Right ,a n dSuck . •The agent correctly perceives its location and whether that location contains dirt.

Token 1015:
We claim that under these circumstances the agent is indeed rational; its expected perfor- mance is at least as high as any other agent’s.

Token 1016:
Exercise 2.2 asks you to prove this. One can see easily that the same agent would be irrational under different circum- stances.

Token 1017:
For example, once all the dirt is cleaned up, the agent will oscillate needlessly backand forth; if the performance measure includes a penalty of one point for each movement left or right, the agent will fare poorly.

Token 1018:
A better agent for this case would do nothing once it is sure that all the squares are clean.

Token 1019:
If clean squares can become dirty again, the agent shouldoccasionally check and re-clean them if needed.

Token 1020:
If the geography of the environment is un-known, the agent will need to explore it rather than stick to squares AandB.E x e r c i s e 2 .

Token 1021:
2 asks you to design agents for these cases.

Token 1022:
2.2.2 Omniscience, learning, and autonomy We need to be careful to distinguish between rationality and omniscience .

Token 1023:
An omniscient OMNISCIENCE agent knows the actual outcome of its actions and can act accordingly; but omniscience is impossible in reality.

Token 1024:
Consider the following example: I am walking along the Champs Elys´ees one day and I see an old friend across the street.

Token 1025:
There is no trafﬁc nearby and I’m not otherwise engaged, so, being rational, I start to cross the street.

Token 1026:
Meanwhile, at 33,000 feet, a cargo door falls off a passing airliner,2and before I make it to the other side of the street I am ﬂattened.

Token 1027:
Was I irrational to cross the street?

Token 1028:
It is unlikely that my obituary would read “Idiot attempts to cross street.” This example shows that rationality is not the same as perfection.

Token 1029:
Rationality max- imizes expected performance, while perfection maximizes actual performance.

Token 1030:
Retreating from a requirement of perfection is not just a question of being fair to agents.

Token 1031:
The point isthat if we expect an agent to do what turns out to be the best action after the fact, it will beimpossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performanceof crystal balls or time machines.

Token 1032:
2See N. Henderson, “New door latches urged for Boeing 747 jumbo jets,” Washington Post , August 24, 1989.

Token 1033:
Section 2.2.

Token 1034:
Good Behavior: The Concept of Rationality 39 Our deﬁnition of rationality does not require omniscience, then, because the rational choice depends only on the percept sequence to date .

Token 1035:
We must also ensure that we haven’t inadvertently allowed the agent to engage in decidedly underintelligent activities.

Token 1036:
For exam-ple, if an agent does not look both ways before crossing a busy road, then its percept sequencewill not tell it that there is a large truck approaching at high speed.

Token 1037:
Does our deﬁnition of rationality say that it’s now OK to cross the road? Far from it!

Token 1038:
First, it would not be rational to cross the road given this uninformative percept sequence: the risk of accident from cross-ing without looking is too great.

Token 1039:
Second, a rational agent should choose the “looking” actionbefore stepping into the street, because looking helps maximize the expected performance.Doing actions in order to modify future percepts —sometimes called information gather- ing—is an important part of rationality and is covered in depth in Chapter 16.

Token 1040:
A second INFORMATION GATHERING example of information gathering is provided by the exploration that must be undertaken by EXPLORATION a vacuum-cleaning agent in an initially unknown environment.

Token 1041:
Our deﬁnition requires a rational agent not only to gather information but also to learn LEARNING as much as possible from what it perceives.

Token 1042:
The agent’s initial conﬁguration could reﬂect some prior knowledge of the environment, but as the agent gains experience this may bemodiﬁed and augmented.

Token 1043:
There are extreme cases in which the environment is completely known ap r i o r i .

Token 1044:
In such cases, the agent need not perceive or learn; it simply acts correctly. Of course, such agents are fragile. Consider the lowly dung beetle.

Token 1045:
After digging its nest andlaying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance.

Token 1046:
If the ball ofdung is removed from its grasp en route , the beetle continues its task and pantomimes plug- ging the nest with the nonexistent dung ball, never noticing that it is missing.

Token 1047:
Evolution has built an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior results.

Token 1048:
Slightly more intelligent is the sphex wasp.

Token 1049:
The female sphex will dig a burrow, goout and sting a caterpillar and drag it to the burrow, enter the burrow again to check all iswell, drag the caterpillar inside, and lay its eggs.

Token 1050:
The caterpillar serves as a food source whenthe eggs hatch.

Token 1051:
So far so good, but if an entomologist moves the caterpillar a few inchesaway while the sphex is doing the check, it will revert to the “drag” step of its plan and will continue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.

Token 1052:
The sphex is unable to learn that its innate plan is failing, and thus will not change it.

Token 1053:
To the extent that an agent relies on the prior knowledge of its designer rather than on its own percepts, we say that the agent lacks autonomy .

Token 1054:
A rational agent should be AUTONOMY autonomous—it should learn what it can to compensate for partial or incorrect prior knowl- edge.

Token 1055:
For example, a vacuum-cleaning agent that learns to foresee where and when additionaldirt will appear will do better than one that does not.

Token 1056:
As a practical matter, one seldom re-quires complete autonomy from the start: when the agent has had little or no experience, itwould have to act randomly unless the designer gave some assistance.

Token 1057:
So, just as evolutionprovides animals with enough built-in reﬂexes to survive long enough to learn for themselves,it would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge as well as an ability to learn.

Token 1058:
After sufﬁcient experience of its environment, the behavior of a rational agent can become effectively independent of its prior knowledge.

Token 1059:
Hence, the incorporation of learning allows one to design a single rational agent that will succeed in avast variety of environments.

Token 1060:
40 Chapter 2.

Token 1061:
Intelligent Agents 2.3 T HENATURE OF ENVIRONMENTS Now that we have a deﬁnition of rationality, we are almost ready to think about building rational agents.

Token 1062:
First, however, we must think about task environments , which are essen- TASK ENVIRONMENT tially the “problems” to which rational agents are the “solutions.” We begin by showing how to specify a task environment, illustrating the process with a number of examples.

Token 1063:
We thenshow that task environments come in a variety of ﬂavors.

Token 1064:
The ﬂavor of the task environmentdirectly affects the appropriate design for the agent program.

Token 1065:
2.3.1 Specifying the task environment In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specifythe performance measure, the environment, and the agent’s actuators and sensors.

Token 1066:
We groupall these under the heading of the task environment .

Token 1067:
For the acronymically minded, we call this the PEAS (Performance, Environment, Actuators, Sensors) description.

Token 1068:
In designing an PEAS agent, the ﬁrst step must always be to specify the task environment as fully as possible.

Token 1069:
The vacuum world was a simple example; let us consider a more complex problem: an automated taxi driver.

Token 1070:
We should point out, before the reader becomes alarmed, that a fullyautomated taxi is currently somewhat beyond the capabilities of existing technology.

Token 1071:
(page 28 describes an existing driving robot.)

Token 1072:
The full driving task is extremely open-ended .T h e r e i s no limit to the novel combinations of circumstances that can arise—another reason we choseit as a focus for discussion.

Token 1073:
Figure 2.4 summarizes the PEAS description for the taxi’s taskenvironment. We discuss each element in more detail in the following paragraphs.

Token 1074:
Agent Type Performance Measure Environment Actuators Sensors Taxi driver Safe, fast, legal, comfortable trip, maximize proﬁts Roads, other trafﬁc, pedestrians, customers Steering,accelerator, brake, signal, horn, display Cameras, sonar,speedometer, GPS, odometer, accelerometer, engine sensors, keyboard Figure 2.4 PEAS description of the task environment for an automated taxi.

Token 1075:
First, what is the performance measure to which we would like our automated driver to aspire?

Token 1076:
Desirable qualities include getting to the correct destination; minimizing fuel con-sumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafﬁclaws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz- ing proﬁts.

Token 1077:
Obviously, some of these goals conﬂict, so tradeoffs will be required. Next, what is the driving environment that the taxi will face?

Token 1078:
Any taxi driver must deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.The roads contain other trafﬁc, pedestrians, stray animals, road works, police cars, puddles,

Token 1079:
Section 2.3. The Nature of Environments 41 and potholes. The taxi must also interact with potential and actual passengers.

Token 1080:
There are also some optional choices.

Token 1081:
The taxi might need to operate in Southern California, where snowis seldom a problem, or in Alaska, where it seldom is not.

Token 1082:
It could always be driving on theright, or we might want it to be ﬂexible enough to drive on the left when in Britain or Japan.Obviously, the more restricted the environment, the easier the design problem.

Token 1083:
Theactuators for an automated taxi include those available to a human driver: control over the engine through the accelerator and control over steering and braking.

Token 1084:
In addition, itwill need output to a display screen or voice synthesizer to talk back to the passengers, andperhaps some way to communicate with other vehicles, politely or otherwise.

Token 1085:
The basic sensors for the taxi will include one or more controllable video cameras so that it can see the road; it might augment these with infrared or sonar sensors to detect dis-tances to other cars and obstacles.

Token 1086:
To avoid speeding tickets, the taxi should have a speedome-ter, and to control the vehicle properly, especially on curves, it should have an accelerometer.To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,and electrical system sensors.

Token 1087:
Like many human drivers, it might want a global positioningsystem (GPS) so that it doesn’t get lost.

Token 1088:
Finally, it will need a keyboard or microphone forthe passenger to request a destination.

Token 1089:
In Figure 2.5, we have sketched the basic PEAS elements for a number of additional agent types. Further examples appear in Exercise 2.4.

Token 1090:
It may come as a surprise to some read-ers that our list of agent types includes some programs that operate in the entirely artiﬁcialenvironment deﬁned by keyboard input and character output on a screen.

Token 1091:
“Surely,” one mightsay, “this is not a real environment, is it?” In fact, what matters is not the distinction between“real” and “artiﬁcial” environments, but the complexity of the relationship among the behav-ior of the agent, the percept sequence generated by the environment, and the performancemeasure.

Token 1092:
Some “real” environments are actually quite simple.

Token 1093:
For example, a robot designedto inspect parts as they come by on a conveyor belt can make use of a number of simplifyingassumptions: that the lighting is always just so, that the only thing on the conveyor belt willbe parts of a kind that it knows about, and that only two actions (accept or reject) are possible.

Token 1094:
In contrast, some software agents (or software robots or softbots ) exist in rich, unlim- SOFTWARE AGENT SOFTBOT ited domains.

Token 1095:
Imagine a softbot Web site operator designed to scan Internet news sources and show the interesting items to its users, while selling advertising space to generate revenue.To do well, that operator will need some natural language processing abilities, it will needto learn what each user and advertiser is interested in, and it will need to change its plansdynamically—for example, when the connection for one news source goes down or when anew one comes online.

Token 1096:
The Internet is an environment whose complexity rivals that of thephysical world and whose inhabitants include many artiﬁcial and human agents.

Token 1097:
2.3.2 Properties of task environments The range of task environments that might arise in AI is obviously vast.

Token 1098:
We can, however,identify a fairly small number of dimensions along which task environments can be catego-rized.

Token 1099:
These dimensions determine, to a large extent, the appropriate agent design and theapplicability of each of the principal families of techniques for agent implementation.

Token 1100:
First,

Token 1101:
42 Chapter 2.

Token 1102:
Intelligent Agents Agent Type Performance Measure Environment Actuators Sensors Medical diagnosis system Healthy patient,reduced costs Patient, hospital,staff Display ofquestions, tests, diagnoses,treatments, referrals Keyboard entryof symptoms, ﬁndings, patient’sanswers Satellite imageanalysis system Correct imagecategorization Downlink fromorbiting satellite Display of scenecategorization Color pixelarrays Part-pickingrobot Percentage ofparts in correct bins Conveyor belt with parts; bins Jointed arm andhand Camera, jointangle sensors Reﬁnery controller Purity, yield,safety Reﬁnery,operators Valves, pumps,heaters, displays Temperature,pressure, chemical sensors Interactive English tutor Student’s scoreon test Set of students,testing agency Display ofexercises, suggestions,corrections Keyboard entry Figure 2.5 Examples of agent types and their PEAS descriptions.

Token 1103:
we list the dimensions, then we analyze several task environments to illustrate the ideas.

Token 1104:
The deﬁnitions here are informal; later chapters provide more precise statements and examples ofeach kind of environment.

Token 1105:
Fully observable vs.partially observable : If an agent’s sensors give it access to the FULLY OBSERVABLE PARTIALLY OBSERVABLE complete state of the environment at each point in time, then we say that the task environ- ment is fully observable.

Token 1106:
A task environment is effectively fully observable if the sensorsdetect all aspects that are relevant to the choice of action; relevance, in turn, depends on the performance measure.

Token 1107:
Fully observable environments are convenient because the agent neednot maintain any internal state to keep track of the world.

Token 1108:
An environment might be partiallyobservable because of noisy and inaccurate sensors or because parts of the state are simplymissing from the sensor data—for example, a vacuum agent with only a local dirt sensorcannot tell whether there is dirt in other squares, and an automated taxi cannot see what other drivers are thinking.

Token 1109:
If the agent has no sensors at all then the environment is unobserv- able.

Token 1110:
One might think that in such cases the agent’s plight is hopeless, but, as we discuss in UNOBSERVABLE Chapter 4, the agent’s goals may still be achievable, sometimes with certainty.

Token 1111:
Single agent vs.multiagent : The distinction between single-agent and multiagent en- SINGLE AGENT MULTIAGENT

Token 1112:
Section 2.3. The Nature of Environments 43 vironments may seem simple enough.

Token 1113:
For example, an agent solving a crossword puzzle by itself is clearly in a single-agent environment, whereas an agent playing chess is in a two-agent environment.

Token 1114:
There are, however, some subtle issues.

Token 1115:
First, we have described how anentity may be viewed as an agent, but we have not explained which entities must be viewed as agents.

Token 1116:
Does an agent A(the taxi driver for example) have to treat an object B(another vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of physics, analogous to waves at the beach or leaves blowing in the wind?

Token 1117:
The key distinctionis whether B’s behavior is best described as maximizing a performance measure whose value depends on agent A’s behavior.

Token 1118:
For example, in chess, the opponent entity Bis trying to maximize its performance measure, which, by the rules of chess, minimizes agent A’s per- formance measure.

Token 1119:
Thus, chess is a competitive multiagent environment.

Token 1120:
In the taxi-driving COMPETITIVE environment, on the other hand, avoiding collisions maximizes the performance measure of all agents, so it is a partially cooperative multiagent environment.

Token 1121:
It is also partially com- COOPERATIVE petitive because, for example, only one car can occupy a parking space.

Token 1122:
The agent-design problems in multiagent environments are often quite different from those in single-agent en-vironments; for example, communication often emerges as a rational behavior in multiagent environments; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability.

Token 1123:
Deterministic vs.stochastic .

Token 1124:
If the next state of the environment is completely deter- DETERMINISTIC STOCHASTIC mined by the current state and the action executed by the agent, then we say the environment is deterministic; otherwise, it is stochastic.

Token 1125:
In principle, an agent need not worry about uncer-tainty in a fully observable, deterministic environment.

Token 1126:
(In our deﬁnition, we ignore uncer-tainty that arises purely from the actions of other agents in a multiagent environment; thus,a game can be deterministic even though each agent may be unable to predict the actions ofthe others.)

Token 1127:
If the environment is partially observable, however, then it could appear to be stochastic.

Token 1128:
Most real situations are so complex that it is impossible to keep track of all the unobserved aspects; for practical purposes, they must be treated as stochastic.

Token 1129:
Taxi driving is clearly stochastic in this sense, because one can never predict the behavior of trafﬁc exactly; moreover, one’s tires blow out and one’s engine seizes up without warning.

Token 1130:
The vacuum world as we described it is deterministic, but variations can include stochastic elements such as randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13).

Token 1131:
We say anenvironment is uncertain if it is not fully observable or not deterministic.

Token 1132:
One ﬁnal note: UNCERTAIN our use of the word “stochastic” generally implies that uncertainty about outcomes is quan- tiﬁed in terms of probabilities; a nondeterministic environment is one in which actions are NONDETERMINISTIC characterized by their possible outcomes, but no probabilities are attached to them.

Token 1133:
Nonde- terministic environment descriptions are usually associated with performance measures thatrequire the agent to succeed for all possible outcomes of its actions.

Token 1134:
Episodic vs.sequential : In an episodic task environment, the agent’s experience is EPISODIC SEQUENTIAL divided into atomic episodes.

Token 1135:
In each episode the agent receives a percept and then performs a single action.

Token 1136:
Crucially, the next episode does not depend on the actions taken in previous episodes. Many classiﬁcation tasks are episodic.

Token 1137:
For example, an agent that has to spotdefective parts on an assembly line bases each decision on the current part, regardless ofprevious decisions; moreover, the current decision doesn’t affect whether the next part is

Token 1138:
44 Chapter 2. Intelligent Agents defective.

Token 1139:
In sequential environments, on the other hand, the current decision could affect all future decisions.3Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences.

Token 1140:
Episodic environments are much simpler than sequentialenvironments because the agent does not need to think ahead.

Token 1141:
Static vs.dynamic : If the environment can change while an agent is deliberating, then STATIC DYNAMIC we say the environment is dynamic for that agent; otherwise, it is static.

Token 1142:
Static environments are easy to deal with because the agent need not keep looking at the world while it is decidingon an action, nor need it worry about the passage of time.

Token 1143:
Dynamic environments, on theother hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,that counts as deciding to do nothing.

Token 1144:
If the environment itself does not change with thepassage of time but the agent’s performance score does, then we say the environment issemidynamic .

Token 1145:
Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving SEMIDYNAMIC while the driving algorithm dithers about what to do next.

Token 1146:
Chess, when played with a clock, is semidynamic. Crossword puzzles are static.

Token 1147:
Discrete vs.continuous : The discrete/continuous distinction applies to the state of the DISCRETE CONTINUOUS environment, to the way time is handled, and to the percepts and actions of the agent.

Token 1148:
For example, the chess environment has a ﬁnite number of distinct states (excluding the clock). Chess also has a discrete set of percepts and actions.

Token 1149:
Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweepthrough a range of continuous values and do so smoothly over time.

Token 1150:
Taxi-driving actions arealso continuous (steering angles, etc.).

Token 1151:
Input from digital cameras is discrete, strictly speak-ing, but is typically treated as representing continuously varying intensities and locations.

Token 1152:
Known vs.unknown : Strictly speaking, this distinction refers not to the environment KNOWN UNKNOWN itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment.

Token 1153:
In a known environment, the outcomes (or outcome probabilities if theenvironment is stochastic) for all actions are given.

Token 1154:
Obviously, if the environment is unknown,the agent will have to learn how it works in order to make good decisions.

Token 1155:
Note that thedistinction between known and unknown environments is not the same as the one between fully and partially observable environments.

Token 1156:
It is quite possible for a known environment to be partially observable—for example, in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over.

Token 1157:
Conversely, an unknown environment can be fully observable—in a new video game, the screen may show the entire game state but I still don’t know what the buttons do until I try them.

Token 1158:
As one might expect, the hardest case is partially observable ,multiagent ,stochastic , sequential ,dynamic ,continuous ,a n d unknown .

Token 1159:
Taxi driving is hard in all these senses, except that for the most part the driver’s environment is known.

Token 1160:
Driving a rented car in a new country with unfamiliar geography and trafﬁc laws is a lot more exciting.

Token 1161:
Figure 2.6 lists the properties of a number of familiar environments. Note that the answers are not always cut and dried.

Token 1162:
For example, we describe the part-picking robot asepisodic, because it normally considers each part in isolation.

Token 1163:
But if one day there is a large 3The word “sequential” is also used in computer science as the antonym of “parallel.” The two meanings are largely unrelated.

Token 1164:
Section 2.3.

Token 1165:
The Nature of Environments 45 Task Environment Observable Agents Deterministic Episodic Static Discrete Crossword puzzle Fully Single Deterministic Sequential Static Discrete Chess with a clock Fully Multi Deterministic Sequential Semi Discrete Poker Partially Multi Stochastic Sequential Static Discrete Backgammon Fully Multi Stochastic Sequential Static Discrete Taxi driving Partially Multi Stochastic Sequential Dynamic Continuous Medical diagnosis Partially Single Stochastic Sequential Dynamic Continuous Image analysis Fully Single Deterministic Episodic Semi Continuous Part-picking robot Partially Single Stochastic Episodic Dynamic Continuous Reﬁnery controller Partially Single Stochastic Sequential Dynamic Continuous Interactive English tutor Partially Multi Stochastic Sequential Dynamic Discrete Figure 2.6 Examples of task environments and their characteristics.

Token 1166:
batch of defective parts, the robot should learn from several observations that the distribution of defects has changed, and should modify its behavior for subsequent parts.

Token 1167:
We have not included a “known/unknown” column because, as explained earlier, this is not strictly a prop-erty of the environment.

Token 1168:
For some environments, such as chess and poker, it is quite easy tosupply the agent with full knowledge of the rules, but it is nonetheless interesting to considerhow an agent might learn to play these games without such knowledge.

Token 1169:
Several of the answers in the table depend on how the task environment is deﬁned.

Token 1170:
We have listed the medical-diagnosis task as single-agent because the disease process in a patientis not proﬁtably modeled as an agent; but a medical-diagnosis system might also have todeal with recalcitrant patients and skeptical staff, so the environment could have a multiagentaspect.

Token 1171:
Furthermore, medical diagnosis is episodic if one conceives of the task as selecting adiagnosis given a list of symptoms; the problem is sequential if the task can include proposing a series of tests, evaluating progress over the course of treatment, and so on.

Token 1172:
Also, many environments are episodic at higher levels than the agent’s individual actions.

Token 1173:
For example,a chess tournament consists of a sequence of games; each game is an episode because (byand large) the contribution of the moves in one game to the agent’s overall performance isnot affected by the moves in its previous game.

Token 1174:
On the other hand, decision making within asingle game is certainly sequential.

Token 1175:
The code repository associated with this book (aima.cs.berkeley.edu) includes imple- mentations of a number of environments, together with a general-purpose environment simu-lator that places one or more agents in a simulated environment, observes their behavior overtime, and evaluates them according to a given performance measure.

Token 1176:
Such experiments areoften carried out not for a single environment but for many environments drawn from an en- vironment class .

Token 1177:
For example, to evaluate a taxi driver in simulated trafﬁc, we would want to ENVIRONMENT CLASS run many simulations with different trafﬁc, lighting, and weather conditions.

Token 1178:
If we designed the agent for a single scenario, we might be able to take advantage of speciﬁc propertiesof the particular case but might not identify a good design for driving in general.

Token 1179:
For this

Token 1180:
46 Chapter 2.

Token 1181:
Intelligent Agents reason, the code repository also includes an environment generator for each environmentENVIRONMENT GENERATOR class that selects particular environments (with certain likelihoods) in which to run the agent.

Token 1182:
For example, the vacuum environment generator initializes the dirt pattern and agent locationrandomly.

Token 1183:
We are then interested in the agent’s average performance over the environmentclass.

Token 1184:
A rational agent for a given environment class maximizes this average performance.

Token 1185:
Exercises 2.8 to 2.13 take you through the process of developing an environment class and evaluating various agents therein.

Token 1186:
2.4 T HESTRUCTURE OF AGENTS So far we have talked about agents by describing behavior —the action that is performed after any given sequence of percepts.

Token 1187:
Now we must bite the bullet and talk about how the insideswork.

Token 1188:
The job of AI is to design an agent program that implements the agent function— AGENT PROGRAM the mapping from percepts to actions.

Token 1189:
We assume this program will run on some sort of computing device with physical sensors and actuators—we call this the architecture : ARCHITECTURE agent=architecture +program .

Token 1190:
Obviously, the program we choose has to be one that is appropriate for the architecture.

Token 1191:
If the program is going to recommend actions like Walk , the architecture had better have legs.

Token 1192:
The architecture might be just an ordinary PC, or it might be a robotic car with several onboardcomputers, cameras, and other sensors.

Token 1193:
In general, the architecture makes the percepts fromthe sensors available to the program, runs the program, and feeds the program’s action choicesto the actuators as they are generated.

Token 1194:
Most of this book is about designing agent programs,although Chapters 24 and 25 deal directly with the sensors and actuators.

Token 1195:
2.4.1 Agent programs The agent programs that we design in this book all have the same skeleton: they take thecurrent percept as input from the sensors and return an action to the actuators.

Token 1196:
4Notice the difference between the agent program, which takes the current percept as input, and the agentfunction, which takes the entire percept history.

Token 1197:
The agent program takes just the currentpercept as input because nothing more is available from the environment; if the agent’s actions need to depend on the entire percept sequence, the agent will have to remember the percepts.

Token 1198:
We describe the agent programs in the simple pseudocode language that is deﬁned in Appendix B.

Token 1199:
(The online code repository contains implementations in real programming languages.)

Token 1200:
For example, Figure 2.7 shows a rather trivial agent program that keeps track of the percept sequence and then uses it to index into a table of actions to decide what to do.The table—an example of which is given for the vacuum world in Figure 2.3—representsexplicitly the agent function that the agent program embodies.

Token 1201:
To build a rational agent in 4There are other choices for the agent program skelet on; for example, we could have the agent programs be coroutines that run asynchronously with the environment.

Token 1202:
Each such coroutine has an input and output port and consists of a loop that reads the input port for pe rcepts and writes actions to the output port.

Token 1203:
Section 2.4.

Token 1204:
The Structure of Agents 47 function TABLE -DRIVEN -AGENT (percept )returns an action persistent :percepts , a sequence, initially empty table , a table of actions, indexed by percept sequences, initially fully speciﬁed append percept to the end of percepts action←LOOKUP (percepts ,table ) return action Figure 2.7 The T ABLE -DRIVEN -AGENT program is invoked for each new percept and returns an action each time.

Token 1205:
It retains the complete percept sequence in memory.

Token 1206:
this way, we as designers must construct a table that contains the appropriate action for every possible percept sequence.

Token 1207:
It is instructive to consider why the table-driven approach to agent construction is doomed to failure.

Token 1208:
Let Pbe the set of possible percepts and let Tbe the lifetime of the agent (the total number of percepts it will receive).

Token 1209:
The lookup table will contain/summationtextT t=1|P|t entries.

Token 1210:
Consider the automated taxi: the visual input from a single camera comes in at the rate of roughly 27 megabytes per second (30 frames per second, 640×480pixels with 24 bits of color information).

Token 1211:
This gives a lookup table with over 10250,000,000,000entries for an hour’s driving.

Token 1212:
Even the lookup table for chess—a tiny, well-behaved fragment of the realworld—would have at least 10 150entries.

Token 1213:
The daunting size of these tables (the number of atoms in the observable universe is less than 1080) means that (a) no physical agent in this universe will have the space to store the table, (b) the designer would not have time to create the table, (c) no agent could ever learn all the right table entries from its experience, and (d) even if the environment is simple enough to yield a feasible table size, the designer still hasno guidance about how to ﬁll in the table entries.

Token 1214:
Despite all this, T ABLE -DRIVEN -AGENT does do what we want: it implements the desired agent function.

Token 1215:
The key challenge for AI is to ﬁnd out how to write programs that, to the extent possible, produce rational behavior from a smallish program rather than from a vast table.

Token 1216:
We have many examples showing that this can be done successfully in otherareas: for example, the huge tables of square roots used by engineers and schoolchildren priorto the 1970s have now been replaced by a ﬁve-line program for Newton’s method runningon electronic calculators.

Token 1217:
The question is, can AI do for general intelligent behavior whatNewton did for square roots? We believe the answer is yes.

Token 1218:
In the remainder of this section, we outline four basic kinds of agent programs that embody the principles underlying almost all intelligent systems: •Simple reﬂex agents; •Model-based reﬂex agents; •Goal-based agents; and •Utility-based agents.

Token 1219:
Each kind of agent program combines particular components in particular ways to generate actions.

Token 1220:
Section 2.4.6 explains in general terms how to convert all these agents into learning

Token 1221:
48 Chapter 2.

Token 1222:
Intelligent Agents function REFLEX -VACUUM -AGENT ([location ,status ])returns an action ifstatus =Dirty then return Suck else iflocation =Athen return Right else iflocation =Bthen return Left Figure 2.8 The agent program for a simple reﬂex agent in the two-state vacuum environ- ment.

Token 1223:
This program implements the agent function tabulated in Figure 2.3. agents that can improve the performance of their components so as to generate better actions.

Token 1224:
Finally, Section 2.4.7 describes the variety of ways in which the components themselves can be represented within the agent.

Token 1225:
This variety provides a major organizing principle for the ﬁeld and for the book itself.

Token 1226:
2.4.2 Simple reﬂex agents The simplest kind of agent is the simple reﬂex agent .

Token 1227:
These agents select actions on the basisSIMPLE REFLEX AGENT of the current percept, ignoring the rest of the percept history.

Token 1228:
For example, the vacuum agent whose agent function is tabulated in Figure 2.3 is a simple reﬂex agent, because its decisionis based only on the current location and on whether that location contains dirt.

Token 1229:
An agentprogram for this agent is shown in Figure 2.8.

Token 1230:
Notice that the vacuum agent program is very small indeed compared to the correspond- ing table.

Token 1231:
The most obvious reduction comes from ignoring the percept history, which cuts down the number of possibilities from 4 Tto just 4.

Token 1232:
A further, small reduction comes from the fact that when the current square is dirty, the action does not depend on the location.

Token 1233:
Simple reﬂex behaviors occur even in more complex environments. Imagine yourself as the driver of the automated taxi.

Token 1234:
If the car in front brakes and its brake lights come on, thenyou should notice this and initiate braking.

Token 1235:
In other words, some processing is done on thevisual input to establish the condition we call “The car in front is braking.” Then, this triggerssome established connection in the agent program to the action “initiate braking.” We callsuch a connection a condition–action rule , 5written asCONDITION–ACTION RULE ifcar-in-front-is-braking then initiate-braking .

Token 1236:
Humans also have many such connections, some of which are learned responses (as for driv- ing) and some of which are innate reﬂexes (such as blinking when something approaches theeye).

Token 1237:
In the course of the book, we show several different ways in which such connectionscan be learned and implemented.

Token 1238:
The program in Figure 2.8 is speciﬁc to one particular vacuum environment.

Token 1239:
A more general and ﬂexible approach is ﬁrst to build a general-purpose interpreter for condition–action rules and then to create rule sets for speciﬁc task environments.

Token 1240:
Figure 2.9 gives the structure of this general program in schematic form, showing how the condition–action rules allow the agent to make the connection from percept to action.

Token 1241:
(Do not worry if this seems 5Also called situation–action rules ,productions ,o rif–then rules .

Token 1242:
Section 2.4.

Token 1243:
The Structure of Agents 49 AgentEnvironmentSensors What action I should do nowCondition-action rules ActuatorsWhat the world is like now Figure 2.9 Schematic diagram of a simple reﬂex agent.

Token 1244:
function SIMPLE -REFLEX -AGENT (percept )returns an action persistent :rules , a set of condition–action rules state←INTERPRET -INPUT (percept ) rule←RULE-MATCH (state ,rules ) action←rule.ACTION return action Figure 2.10 A simple reﬂex agent.

Token 1245:
It acts according to a rule whose condition matches the current state, as deﬁned by the percept. trivial; it gets more interesting shortly.)

Token 1246:
We use rectangles to denote the current internal state of the agent’s decision process, and ovals to represent the background information used inthe process.

Token 1247:
The agent program, which is also very simple, is shown in Figure 2.10.

Token 1248:
The I NTERPRET -INPUT function generates an abstracted description of the current state from the percept, and the R ULE-MATCH function returns the ﬁrst rule in the set of rules that matches the given state description.

Token 1249:
Note that the description in terms of “rules” and “matching” ispurely conceptual; actual implementations can be as simple as a collection of logic gatesimplementing a Boolean circuit.

Token 1250:
Simple reﬂex agents have the admirable property of being simple, but they turn out to be of limited intelligence.

Token 1251:
The agent in Figure 2.10 will work only if the correct decision can be made on the basis of only the current percept—that is, only if the environment is fully observ- able.

Token 1252:
Even a little bit of unobservability can cause serious trouble.

Token 1253:
For example, the braking rule given earlier assumes that the condition car-in-front-is-braking can be determined from the current percept—a single frame of video.

Token 1254:
This works if the car in front has a centrallymounted brake light. Unfortunately, older models have different conﬁgurations of taillights,

Token 1255:
50 Chapter 2.

Token 1256:
Intelligent Agents brake lights, and turn-signal lights, and it is not always possible to tell from a single image whether the car is braking.

Token 1257:
A simple reﬂex agent driving behind such a car would either brakecontinuously and unnecessarily, or, worse, never brake at all.

Token 1258:
We can see a similar problem arising in the vacuum world.

Token 1259:
Suppose that a simple reﬂex vacuum agent is deprived of its location sensor and has only a dirt sensor.

Token 1260:
Such an agent has just two possible percepts: [Dirty]and[Clean ].

Token 1261:
It can Suck in response to [Dirty];w h a t should it do in response to [Clean ]?M o v i n g Left fails (forever) if it happens to start in square A, and moving Right fails (forever) if it happens to start in square B. Inﬁnite loops are often unavoidable for simple reﬂex agents operating in partially observable environments.

Token 1262:
Escape from inﬁnite loops is possible if the agent can randomize its actions.

Token 1263:
For ex- RANDOMIZATION ample, if the vacuum agent perceives [Clean ], it might ﬂip a coin to choose between Left and Right .

Token 1264:
It is easy to show that the agent will reach the other square in an average of two steps.

Token 1265:
Then, if that square is dirty, the agent will clean it and the task will be complete.

Token 1266:
Hence, arandomized simple reﬂex agent might outperform a deterministic simple reﬂex agent.

Token 1267:
We mentioned in Section 2.3 that randomized behavior of the right kind can be rational in some multiagent environments.

Token 1268:
In single-agent environments, randomization is usually not rational.

Token 1269:
It is a useful trick that helps a simple reﬂex agent in some situations, but in most cases we can do much better with more sophisticated deterministic agents.

Token 1270:
2.4.3 Model-based reﬂex agents The most effective way to handle partial observability is for the agent to keep track of the part of the world it can’t see now .

Token 1271:
That is, the agent should maintain some sort of internal state that depends on the percept history and thereby reﬂects at least some of the unobserved INTERNAL STATE aspects of the current state.

Token 1272:
For the braking problem, the internal state is not too extensive— just the previous frame from the camera, allowing the agent to detect when two red lights at the edge of the vehicle go on or off simultaneously.

Token 1273:
For other driving tasks such as changing lanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.

Token 1274:
And for any driving to be possible at all, the agent needs to keep track of where its keys are.

Token 1275:
Updating this internal state information as time goes by requires two kinds of knowl- edge to be encoded in the agent program.

Token 1276:
First, we need some information about how theworld evolves independently of the agent—for example, that an overtaking car generally willbe closer behind than it was a moment ago.

Token 1277:
Second, we need some information about howthe agent’s own actions affect the world—for example, that when the agent turns the steeringwheel clockwise, the car turns to the right, or that after driving for ﬁve minutes northboundon the freeway, one is usually about ﬁve miles north of where one was ﬁve minutes ago.

Token 1278:
Thisknowledge about “how the world works”—whether implemented in simple Boolean circuitsor in complete scientiﬁc theories—is called a model of the world.

Token 1279:
An agent that uses such a model is called a model-based agent .

Token 1280:
MODEL-BASED AGENT Figure 2.11 gives the structure of the model-based reﬂex agent with internal state, show- ing how the current percept is combined with the old internal state to generate the updateddescription of the current state, based on the agent’s model of how the world works.

Token 1281:
The agentprogram is shown in Figure 2.12. The interesting part is the function U PDATE -STATE ,w h i c h

Token 1282:
Section 2.4.

Token 1283:
The Structure of Agents 51 AgentEnvironmentSensors State How the world evolves What my actions do Condition-action rules ActuatorsWhat the world is like now What action I should do now Figure 2.11 A model-based reﬂex agent.

Token 1284:
function MODEL -BASED -REFLEX -AGENT (percept )returns an action persistent :state , the agent’s current conception of the world state model , a description of how the next state depends on current state and action rules , a set of condition–action rules action , the most recent action, initially none state←UPDATE -STATE (state ,action ,percept ,model ) rule←RULE-MATCH (state ,rules ) action←rule.ACTION return action Figure 2.12 A model-based reﬂex agent.

Token 1285:
It keeps track of the current state of the world, using an internal model. It then chooses an action in the same way as the reﬂex agent.

Token 1286:
is responsible for creating the new internal state description.

Token 1287:
The details of how models and states are represented vary widely depending on the type of environment and the particulartechnology used in the agent design.

Token 1288:
Detailed examples of models and updating algorithmsappear in Chapters 4, 12, 11, 15, 17, and 25.

Token 1289:
Regardless of the kind of representation used, it is seldom possible for the agent to determine the current state of a partially observable environment exactly .

Token 1290:
Instead, the box labeled “what the world is like now” (Figure 2.11) represents the agent’s “best guess” (orsometimes best guesses).

Token 1291:
For example, an automated taxi may not be able to see around thelarge truck that has stopped in front of it and can only guess about what may be causing the hold-up.

Token 1292:
Thus, uncertainty about the current state may be unavoidable, but the agent still has to make a decision.

Token 1293:
A perhaps less obvious point about the internal “state” maintained by a model-based agent is that it does not have to describe “what the world is like now” in a literal sense.

Token 1294:
For

Token 1295:
52 Chapter 2.

Token 1296:
Intelligent Agents AgentEnvironmentSensors What action I should do nowState How the world evolves What my actions do ActuatorsWhat the world is like now What it will be like if I do action A Goals Figure 2.13 A model-based, goal-based agent.

Token 1297:
It keeps track of the world state as well as a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the achievement of its goals.

Token 1298:
example, the taxi may be driving back home, and it may have a rule telling it to ﬁll up with gas on the way home unless it has at least half a tank.

Token 1299:
Although “driving back home” mayseem to an aspect of the world state, the fact of the taxi’s destination is actually an aspect of the agent’s internal state.

Token 1300:
If you ﬁnd this puzzling, consider that the taxi could be in exactly the same place at the same time, but intending to reach a different destination.

Token 1301:
2.4.4 Goal-based agents Knowing something about the current state of the environment is not always enough to decidewhat to do.

Token 1302:
For example, at a road junction, the taxi can turn left, turn right, or go straight on.

Token 1303:
The correct decision depends on where the taxi is trying to get to.

Token 1304:
In other words, as well as a current state description, the agent needs some sort of goal information that describes GOAL situations that are desirable—for example, being at the passenger’s destination.

Token 1305:
The agent program can combine this with the model (the same information as was used in the model-based reﬂex agent) to choose actions that achieve the goal.

Token 1306:
Figure 2.13 shows the goal-basedagent’s structure.

Token 1307:
Sometimes goal-based action selection is straightforward—for example, when goal sat- isfaction results immediately from a single action.

Token 1308:
Sometimes it will be more tricky—forexample, when the agent has to consider long sequences of twists and turns in order to ﬁnd away to achieve the goal.

Token 1309:
Search (Chapters 3 to 5) and planning (Chapters 10 and 11) are the subﬁelds of AI devoted to ﬁnding action sequences that achieve the agent’s goals.

Token 1310:
Notice that decision making of this kind is fundamentally different from the condition– action rules described earlier, in that it involves consideration of the future—both “What willhappen if I do such-and-such?” and “Will that make me happy?” In the reﬂex agent designs,this information is not explicitly represented, because the built-in rules map directly from

Token 1311:
Section 2.4. The Structure of Agents 53 percepts to actions. The reﬂex agent brakes when it sees brake lights.

Token 1312:
A goal-based agent, in principle, could reason that if the car in front has its brake lights on, it will slow down.

Token 1313:
Giventhe way the world usually evolves, the only action that will achieve the goal of not hittingother cars is to brake.

Token 1314:
Although the goal-based agent appears less efﬁcient, it is more ﬂexible because the knowledge that supports its decisions is represented explicitly and can be modiﬁed.

Token 1315:
If it starts to rain, the agent can update its knowledge of how effectively its brakes will operate; this willautomatically cause all of the relevant behaviors to be altered to suit the new conditions.For the reﬂex agent, on the other hand, we would have to rewrite many condition–actionrules.

Token 1316:
The goal-based agent’s behavior can easily be changed to go to a different destination,simply by specifying that destination as the goal.

Token 1317:
The reﬂex agent’s rules for when to turnand when to go straight will work only for a single destination; they must all be replaced togo somewhere new.

Token 1318:
2.4.5 Utility-based agents Goals alone are not enough to generate high-quality behavior in most environments.

Token 1319:
Forexample, many action sequences will get the taxi to its destination (thereby achieving thegoal) but some are quicker, safer, more reliable, or cheaper than others.

Token 1320:
Goals just provide acrude binary distinction between “happy” and “unhappy” states.

Token 1321:
A more general performancemeasure should allow a comparison of different world states according to exactly how happythey would make the agent.

Token 1322:
Because “happy” does not sound very scientiﬁc, economists andcomputer scientists use the term utility instead.

Token 1323:
6UTILITY We have already seen that a performance measure assigns a score to any given sequence of environment states, so it can easily distinguish between more and less desirable ways of getting to the taxi’s destination.

Token 1324:
An agent’s utility function is essentially an internalization UTILITYFUNCTION of the performance measure.

Token 1325:
If the internal utility function and the external performance measure are in agreement, then an agent that chooses actions to maximize its utility will be rational according to the external performance measure.

Token 1326:
Let us emphasize again that this is not the only way to be rational—we have already seen a rational agent program for the vacuum world (Figure 2.8) that has no idea what itsutility function is—but, like goal-based agents, a utility-based agent has many advantages interms of ﬂexibility and learning.

Token 1327:
Furthermore, in two kinds of cases, goals are inadequate buta utility-based agent can still make rational decisions.

Token 1328:
First, when there are conﬂicting goals,only some of which can be achieved (for example, speed and safety), the utility functionspeciﬁes the appropriate tradeoff.

Token 1329:
Second, when there are several goals that the agent canaim for, none of which can be achieved with certainty, utility provides a way in which thelikelihood of success can be weighed against the importance of the goals.

Token 1330:
Partial observability and stochasticity are ubiquitous in the real world, and so, therefore, is decision making under uncertainty.

Token 1331:
Technically speaking, a rational utility-based agentchooses the action that maximizes the expected utility of the action outcomes—that is, the EXPECTED UTILITY utility the agent expects to derive, on average, given the probabilities and utilities of each 6The word “utility” here refers to “the quality of being useful,” not to the electric company or waterworks.

Token 1332:
54 Chapter 2.

Token 1333:
Intelligent Agents AgentEnvironmentSensors How happy I will be in such a stateState How the world evolves What my actions do Utility ActuatorsWhat action I should do nowWhat it will be like if I do action AWhat the world is like now Figure 2.14 A model-based, utility-based agent.

Token 1334:
It uses a model of the world, along with a utility function that measures its preferences a mong states of the world.

Token 1335:
Then it chooses the action that leads to the best expected utility, where expected utility is computed by averaging over all possible outcome states, weighted by the probability of the outcome.

Token 1336:
outcome. (Appendix A deﬁnes expectation more precisely.)

Token 1337:
In Chapter 16, we show that any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize.

Token 1338:
An agent that possesses an explicit utility function can make rational decisions with a general-purpose algorithm that does not depend on the speciﬁc utility function beingmaximized.

Token 1339:
In this way, the “global” deﬁnition of rationality—designating as rational thoseagent functions that have the highest performance—is turned into a “local” constraint onrational-agent designs that can be expressed in a simple program.

Token 1340:
The utility-based agent structure appears in Figure 2.14.

Token 1341:
Utility-based agent programs appear in Part IV, where we design decision-making agents that must handle the uncertainty inherent in stochastic or partially observable environments.

Token 1342:
At this point, the reader may be wondering, “Is it that simple?

Token 1343:
We just build agents that maximize expected utility, and we’re done?” It’s true that such agents would be intelligent, but it’s not simple.

Token 1344:
A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.

Token 1345:
The results of this research ﬁll many of the chapters of this book.

Token 1346:
Choosingthe utility-maximizing course of action is also a difﬁcult task, requiring ingenious algorithmsthat ﬁll several more chapters.

Token 1347:
Even with these algorithms, perfect rationality is usuallyunachievable in practice because of computational complexity, as we noted in Chapter 1.

Token 1348:
2.4.6 Learning agents We have described agent programs with various methods for selecting actions.

Token 1349:
We havenot, so far, explained how the agent programs come into being .

Token 1350:
In his famous early paper, Turing (1950) considers the idea of actually programming his intelligent machines by hand.

Token 1351:
Section 2.4.

Token 1352:
The Structure of Agents 55 Performance standard AgentEnvironmentSensors Performance elementchanges knowledge learning goals Problem generatorfeedback Learning elementCritic Actuators Figure 2.15 A general learning agent.

Token 1353:
He estimates how much work this might take and concludes “Some more expeditious method seems desirable.” The method he proposes is to build learning machines and then to teachthem.

Token 1354:
In many areas of AI, this is now the preferred method for creating state-of-the-artsystems.

Token 1355:
Learning has another advantage, as we noted earlier: it allows the agent to operatein initially unknown environments and to become more competent than its initial knowledgealone might allow.

Token 1356:
In this section, we brieﬂy introduce the main ideas of learning agents.Throughout the book, we comment on opportunities and methods for learning in particularkinds of agents.

Token 1357:
Part V goes into much more depth on the learning algorithms themselves.

Token 1358:
A learning agent can be divided into four conceptual components, as shown in Fig- ure 2.15.

Token 1359:
The most important distinction is between the learning element ,w h i c hi sr e - LEARNING ELEMENT sponsible for making improvements, and the performance element , which is responsible forPERFORMANCE ELEMENT selecting external actions.

Token 1360:
The performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.

Token 1361:
The learning element usesfeedback from the critic on how the agent is doing and determines how the performance CRITIC element should be modiﬁed to do better in the future.

Token 1362:
The design of the learning element depends very much on the design of the performance element.

Token 1363:
When trying to design an agent that learns a certain capability, the ﬁrst question is not “How am I going to get it to learn this?” but “What kind of performance element will myagent need to do this once it has learned how?” Given an agent design, learning mechanismscan be constructed to improve every part of the agent.

Token 1364:
The critic tells the learning element how well the agent is doing with respect to a ﬁxed performance standard.

Token 1365:
The critic is necessary because the percepts themselves provide no indication of the agent’s success.

Token 1366:
For example, a chess program could receive a perceptindicating that it has checkmated its opponent, but it needs a performance standard to knowthat this is a good thing; the percept itself does not say so.

Token 1367:
It is important that the performance

Token 1368:
56 Chapter 2. Intelligent Agents standard be ﬁxed.

Token 1369:
Conceptually, one should think of it as being outside the agent altogether because the agent must not modify it to ﬁt its own behavior.

Token 1370:
The last component of the learning agent is the problem generator .

Token 1371:
It is responsiblePROBLEM GENERATOR for suggesting actions that will lead to new and informative experiences.

Token 1372:
The point is that if the performance element had its way, it would keep doing the actions that are best, given what it knows.

Token 1373:
But if the agent is willing to explore a little and do some perhaps suboptimal actions in the short run, it might discover much better actions for the long run.

Token 1374:
The problemgenerator’s job is to suggest these exploratory actions. This is what scientists do when theycarry out experiments.

Token 1375:
Galileo did not think that dropping rocks from the top of a tower inPisa was valuable in itself.

Token 1376:
He was not trying to break the rocks or to modify the brains ofunfortunate passers-by.

Token 1377:
His aim was to modify his own brain by identifying a better theoryof the motion of objects.

Token 1378:
To make the overall design more concrete, let us return to the automated taxi example.

Token 1379:
The performance element consists of whatever collection of knowledge and procedures thetaxi has for selecting its driving actions.

Token 1380:
The taxi goes out on the road and drives, usingthis performance element.

Token 1381:
The critic observes the world and passes information along to thelearning element.

Token 1382:
For example, after the taxi makes a quick left turn across three lanes of traf- ﬁc, the critic observes the shocking language used by other drivers.

Token 1383:
From this experience, the learning element is able to formulate a rule saying this was a bad action, and the performanceelement is modiﬁed by installation of the new rule.

Token 1384:
The problem generator might identifycertain areas of behavior in need of improvement and suggest experiments, such as trying outthe brakes on different road surfaces under different conditions.

Token 1385:
The learning element can make changes to any of the “knowledge” components shown in the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14).

Token 1386:
The simplest cases involve learningdirectly from the percept sequence.

Token 1387:
Observation of pairs of successive states of the environ-ment can allow the agent to learn “How the world evolves,” and observation of the results ofits actions can allow the agent to learn “What my actions do.” For example, if the taxi exertsa certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much deceleration is actually achieved.

Token 1388:
Clearly, these two learning tasks are more difﬁcult if the environment is only partially observable.

Token 1389:
The forms of learning in the preceding paragraph do not need to access the external performance standard—in a sense, the standard is the universal one of making predictionsthat agree with experiment.

Token 1390:
The situation is slightly more complex for a utility-based agentthat wishes to learn utility information.

Token 1391:
For example, suppose the taxi-driving agent receivesno tips from passengers who have been thoroughly shaken up during the trip.

Token 1392:
The externalperformance standard must inform the agent that the loss of tips is a negative contribution toits overall performance; then the agent might be able to learn that violent maneuvers do notcontribute to its own utility.

Token 1393:
In a sense, the performance standard distinguishes part of theincoming percept as a reward (orpenalty ) that provides direct feedback on the quality of the agent’s behavior.

Token 1394:
Hard-wired performance standards such as pain and hunger in animals can be understood in this way. This issue is discussed further in Chapter 21.

Token 1395:
In summary, agents have a variety of components, and those components can be repre- sented in many ways within the agent program, so there appears to be great variety among

Token 1396:
Section 2.4. The Structure of Agents 57 learning methods. There is, however, a single unifying theme.

Token 1397:
Learning in intelligent agents can be summarized as a process of modiﬁcation of each component of the agent to bring thecomponents into closer agreement with the available feedback information, thereby improv-ing the overall performance of the agent.

Token 1398:
2.4.7 How the components of agent programs work We have described agent programs (in very high-level terms) as consisting of various compo- nents, whose function it is to answer questions such as: “What is the world like now?” “What action should I do now?” “What do my actions do?” The next question for a student of AIis, “How on earth do these components work?” It takes about a thousand pages to begin toanswer that question properly, but here we want to draw the reader’s attention to some basicdistinctions among the various ways that the components can represent the environment thatthe agent inhabits.

Token 1399:
Roughly speaking, we can place the representations along an axis of increasing com- plexity and expressive power— atomic ,factored ,a n d structured .

Token 1400:
To illustrate these ideas, it helps to consider a particular agent component, such as the one that deals with “What myactions do.” This component describes the changes that might occur in the environment asthe result of taking an action, and Figure 2.16 provides schematic depictions of how those transitions might be represented.

Token 1401:
B C (a) Atomic (b) Factored (b) StructuredBC Figure 2.16 Three ways to represent states and the transitions between them.

Token 1402:
(a) Atomic representation: a state (such as B or C) is a black box with no internal structure; (b) Factored representation: a state consists of a vector of attribute values; values can be Boolean, real-valued, or one of a ﬁxed set of symbols.

Token 1403:
(c) Structured representation: a state includes objects, each of which may have attributes of its own as well as relationships to other objects.

Token 1404:
In an atomic representation each state of the world is indivisible—it has no internalATOMIC REPRESENTATION structure.

Token 1405:
Consider the problem of ﬁnding a driving route from one end of a country to the other via some sequence of cities (we address this problem in Figure 3.2 on page 68).

Token 1406:
For the purposes of solving this problem, it may sufﬁce to reduce the state of world to just the nameof the city we are in—a single atom of knowledge; a “black box” whose only discernibleproperty is that of being identical to or different from another black box.

Token 1407:
The algorithms

Token 1408:
58 Chapter 2.

Token 1409:
Intelligent Agents underlying search andgame-playing (Chapters 3–5), Hidden Markov models (Chapter 15), andMarkov decision processes (Chapter 17) all work with atomic representations—or, at least, they treat representations as if they were atomic.

Token 1410:
Now consider a higher-ﬁdelity description for the same problem, where we need to be concerned with more than just atomic location in one city or another; we might need to pay attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil warning light is working, how much spare change we have for toll crossings, what station ison the radio, and so on.

Token 1411:
A factored representation splits up each state into a ﬁxed set of FACTORED REPRESENTATION variables orattributes , each of which can have a value .

Token 1412:
While two different atomic states VARIABLE ATTRIBUTE VALUEhave nothing in common—they are just different black boxes—two different factored states can share some attributes (such as being at some particular GPS location) and not others (suchas having lots of gas or having no gas); this makes it much easier to work out how to turnone state into another.

Token 1413:
With factored representations, we can also represent uncertainty —for example, ignorance about the amount of gas in the tank can be represented by leaving thatattribute blank.

Token 1414:
Many important areas of AI are based on factored representations, includingconstraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning (Chapters 10 and 11), Bayesian networks (Chapters 13–16), and the machine learning al- gorithms in Chapters 18, 20, and 21.

Token 1415:
For many purposes, we need to understand the world as having things in it that are related to each other, not just variables with values.

Token 1416:
For example, we might notice that a large truck ahead of us is reversing into the driveway of a dairy farm but a cow has got looseand is blocking the truck’s path.

Token 1417:
A factored representation is unlikely to be pre-equippedwith the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with valuetrue orfalse .

Token 1418:
Instead, we would need a structured representation , in which ob- STRUCTURED REPRESENTATION jects such as cows and trucks and their various and varying relationships can be described explicitly.

Token 1419:
(See Figure 2.16(c).)

Token 1420:
Structured representations underlie relational databases andﬁrst-order logic (Chapters 8, 9, and 12), ﬁrst-order probability models (Chapter 14), knowledge-based learning (Chapter 19) and much of natural language understanding (Chapters 22 and 23).

Token 1421:
In fact, almost everything that humans express in natural language concerns objects and their relationships.

Token 1422:
As we mentioned earlier, the axis along which atomic, factored, and structured repre- sentations lie is the axis of increasing expressiveness .

Token 1423:
Roughly speaking, a more expressive EXPRESSIVENESS representation can capture, at least as concisely, everything a less expressive one can capture, plus some more.

Token 1424:
Often, the more expressive language is much more concise; for example, the rules of chess can be written in a page or two of a structured-representation language suchas ﬁrst-order logic but require thousands of pages when written in a factored-representationlanguage such as propositional logic.

Token 1425:
On the other hand, reasoning and learning becomemore complex as the expressive power of the representation increases.

Token 1426:
To gain the beneﬁtsof expressive representations while avoiding their drawbacks, intelligent systems for the real world may need to operate at all points along the axis simultaneously.

Token 1427:
Section 2.5.

Token 1428:
Summary 59 2.5 S UMMARY This chapter has been something of a whirlwind tour of AI, which we have conceived of as the science of agent design.

Token 1429:
The major points to recall are as follows: •Anagent is something that perceives and acts in an environment.

Token 1430:
The agent function for an agent speciﬁes the action taken by the agent in response to any percept sequence.

Token 1431:
•The performance measure evaluates the behavior of the agent in an environment.

Token 1432:
A rational agent acts so as to maximize the expected value of the performance measure, given the percept sequence it has seen so far.

Token 1433:
•Atask environment speciﬁcation includes the performance measure, the external en- vironment, the actuators, and the sensors.

Token 1434:
In designing an agent, the ﬁrst step mustalways be to specify the task environment as fully as possible.

Token 1435:
•Task environments vary along several signiﬁcant dimensions.

Token 1436:
They can be fully or partially observable, single-agent or multiagent, deterministic or stochastic, episodic or sequential, static or dynamic, discrete or continuous, and known or unknown.

Token 1437:
•The agent program implements the agent function.

Token 1438:
There exists a variety of basic agent-program designs reﬂecting the kind of information made explicit and used in thedecision process.

Token 1439:
The designs vary in efﬁciency, compactness, and ﬂexibility. The appropriate design of the agent program depends on the nature of the environment.

Token 1440:
•Simple reﬂex agents respond directly to percepts, whereas model-based reﬂex agents maintain internal state to track aspects of the world that are not evident in the currentpercept.

Token 1441:
Goal-based agents act to achieve their goals, and utility-based agents try to maximize their own expected “happiness.” •All agents can improve their performance through learning .

Token 1442:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The central role of action in intelligence—the notion of practical reasoning—goes back at least as far as Aristotle’s Nicomachean Ethics .

Token 1443:
Practical reasoning was also the subject of McCarthy’s (1958) inﬂuential paper “Programs with Common Sense.” The ﬁelds of roboticsand control theory are, by their very nature, concerned principally with physical agents.

Token 1444:
Theconcept of a controller in control theory is identical to that of an agent in AI.

Token 1445:
Perhaps sur- CONTROLLER prisingly, AI has concentrated for most of its history on isolated components of agents— question-answering systems, theorem-provers, vision systems, and so on—rather than onwhole agents.

Token 1446:
The discussion of agents in the text by Genesereth and Nilsson (1987) was an inﬂuential exception.

Token 1447:
The whole-agent view is now widely accepted and is a central theme in recent texts (Poole et al.

Token 1448:
, 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007). Chapter 1 traced the roots of the concept of rationality in philosophy and economics.

Token 1449:
In AI, the concept was of peripheral interest until the mid-1980s, when it began to suffuse many

Token 1450:
60 Chapter 2. Intelligent Agents discussions about the proper technical foundations of the ﬁeld.

Token 1451:
A paper by Jon Doyle (1983) predicted that rational agent design would come to be seen as the core mission of AI, whileother popular topics would spin off to form new disciplines.

Token 1452:
Careful attention to the properties of the environment and their consequences for ra- tional agent design is most apparent in the control theory tradition—for example, classical control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,2007) handles partially observable, stochastic environments; and hybrid control (Henzingerand Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing bothdiscrete and continuous elements.

Token 1453:
The distinction between fully and partially observable en-vironments is also central in the dynamic programming literature developed in the ﬁeld of operations research (Puterman, 1994), which we discuss in Chapter 17.

Token 1454:
Reﬂex agents were the primary model for psychological behaviorists such as Skinner (1953), who attempted to reduce the psychology of organisms strictly to input/output or stim-ulus/response mappings.

Token 1455:
The advance from behaviorism to functionalism in psychology,which was at least partly driven by the application of the computer metaphor to agents (Put-nam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture.

Token 1456:
Most work in AI views the idea of pure reﬂex agents with state as too simple to provide much leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption(see Chapter 25).

Token 1457:
In recent years, a great deal of work has gone into ﬁnding efﬁcient algo-rithms for keeping track of complex environments (Hamscher et al.

Token 1458:
, 1992; Simon, 2006).

Token 1459:
The Remote Agent program (described on page 28) that controlled the Deep Space One spacecraft is a particularly impressive example (Muscettola et al.

Token 1460:
, 1998; Jonsson et al. , 2000).

Token 1461:
Goal-based agents are presupposed in everything from Aristotle’s view of practical rea- soning to McCarthy’s early papers on logical AI.

Token 1462:
Shakey the Robot (Fikes and Nilsson,1971; Nilsson, 1984) was the ﬁrst robotic embodiment of a logical, goal-based agent.

Token 1463:
Afull logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and agoal-based programming methodology called agent-oriented programming was developed by Shoham (1993).

Token 1464:
The agent-based approach is now extremely popular in software engineer- ing (Ciancarini and Wooldridge, 2001).

Token 1465:
It has also inﬁltrated the area of operating systems,where autonomic computing refers to computer systems and networks that monitor and con- AUTONOMIC COMPUTING trol themselves with a perceive–act loop and machine learning methods (Kephart and Chess, 2003).

Token 1466:
Noting that a collection of agent programs designed to work well together in a truemultiagent environment necessarily exhibits modularity—the programs share no internal stateand communicate with each other only through the environment—it is common within theﬁeld of multiagent systems to design the agent program of a single agent as a collection of MULTIAGENT SYSTEMS autonomous sub-agents.

Token 1467:
In some cases, one can even prove that the resulting system gives the same optimal solutions as a monolithic design.

Token 1468:
The goal-based view of agents also dominates the cognitive psychology tradition in the area of problem solving, beginning with the enormously inﬂuential Human Problem Solv- ing(Newell and Simon, 1972) and running through all of Newell’s later work (Newell, 1990).

Token 1469:
Goals, further analyzed as desires (general) and intentions (currently pursued), are central to the theory of agents developed by Bratman (1987).

Token 1470:
This theory has been inﬂuential both in

Token 1471:
Exercises 61 natural language understanding and multiagent systems. Horvitz et al.

Token 1472:
(1988) speciﬁcally suggest the use of rationality conceived as the maxi- mization of expected utility as a basis for AI.

Token 1473:
The text by Pearl (1988) was the ﬁrst in AI tocover probability and utility theory in depth; its exposition of practical methods for reasoningand decision making under uncertainty was probably the single biggest factor in the rapid shift towards utility-based agents in the 1990s (see Part IV).

Token 1474:
The general design for learning agents portrayed in Figure 2.15 is classic in the machine learning literature (Buchanan et al.

Token 1475:
, 1978; Mitchell, 1997).

Token 1476:
Examples of the design, as em- bodied in programs, go back at least as far as Arthur Samuel’s (1959, 1967) learning programfor playing checkers.

Token 1477:
Learning agents are discussed in depth in Part V. Interest in agents and in agent design has risen rapidly in recent years, partly because of the growth of the Internet and the perceived need for automated and mobile softbot (Etzioni and Weld, 1994).

Token 1478:
Relevant papers are collected in Readings in Agents (Huhns and Singh, 1998) and Foundations of Rational Agency (Wooldridge and Rao, 1999).

Token 1479:
Texts on multiagent systems usually provide a good introduction to many aspects of agent design (Weiss, 2000a;Wooldridge, 2002).

Token 1480:
Several conference series devoted to agents began in the 1990s, includingthe International Workshop on Agent Theories, Architectures, and Languages (ATAL), the International Conference on Autonomous Agents (AGENTS), and the International Confer- ence on Multi-Agent Systems (ICMAS).

Token 1481:
In 2002, these three merged to form the InternationalJoint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS).

Token 1482:
The journalAutonomous Agents and Multi-Agent Systems was founded in 1998.

Token 1483:
Finally, Dung Beetle Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the behavior of dung beetles.

Token 1484:
YouTube features inspiring video recordings of their activities.

Token 1485:
EXERCISES 2.1 Suppose that the performance measure is concerned with just the ﬁrst Ttime steps of the environment and ignores everything thereafter.

Token 1486:
Show that a rational agent’s action maydepend not just on the state of the environment but also on the time step it has reached.

Token 1487:
2.2 Let us examine the rationality of various vacuum-cleaner agent functions. a.

Token 1488:
Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed rational under the assumptions listed on page 38. b.

Token 1489:
Describe a rational agent function for the case in which each movement costs one point. Does the corresponding agent program require internal state?

Token 1490:
c. Discuss possible agent designs for the cases in which clean squares can become dirty and the geography of the environment is unknown.

Token 1491:
Does it make sense for the agent tolearn from its experience in these cases? If so, what should it learn? If not, why not?

Token 1492:
2.3 For each of the following assertions, say whether it is true or false and support your answer with examples or counterexamples where appropriate.

Token 1493:
a. An agent that senses only partial information about the state cannot be perfectly rational.

Token 1494:
62 Chapter 2. Intelligent Agents b. There exist task environments in which no pure reﬂex agent can behave rationally.

Token 1495:
c. There exists a task environment in which every agent is rational. d. The input to an agent program is the same as the input to the agent function.

Token 1496:
e. Every agent function is implementable by some program/machine combination.

Token 1497:
f. Suppose an agent selects its action uniformly at random from the set of possible actions.

Token 1498:
There exists a deterministic task environment in which this agent is rational.

Token 1499:
g. It is possible for a given agent to be perfectly rational in two distinct task environments.

Token 1500:
h. Every agent is rational in an unobservable environment. i. A perfectly rational poker-playing agent never loses.

Token 1501:
2.4 For each of the following activities, give a PEAS description of the task environment and characterize it in terms of the properties listed in Section 2.3.2.

Token 1502:
•Playing soccer. •Exploring the subsurface oceans of Titan. •Shopping for used AI books on the Internet. •Playing a tennis match.

Token 1503:
•Practicing tennis against a wall. •Performing a high jump. •Knitting a sweater. •Bidding on an item at an auction.

Token 1504:
2.5 Deﬁne in your own words the following terms: agent, agent function, agent program, rationality, autonomy, reﬂex agent, model-based agent, goal-based agent, utility-based agent,learning agent.

Token 1505:
2.6 This exercise explores the differences between agent functions and agent programs. a.

Token 1506:
Can there be more than one agent program that implements a given agent function? Give an example, or show why one is not possible. b.

Token 1507:
Are there agent functions that cannot be implemented by any agent program?

Token 1508:
c. Given a ﬁxed machine architecture, does each agent program implement exactly one agent function?

Token 1509:
d. Given an architecture with nbits of storage, how many different possible agent pro- grams are there?

Token 1510:
e. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two. Does that change the agent function?

Token 1511:
2.7 Write pseudocode agent programs for the goal-based and utility-based agents.

Token 1512:
The following exercises all concern the implementation of environments and agents for the vacuum-cleaner world.

Token 1513:
Exercises 63 2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.2 and speciﬁed on page 38.

Token 1514:
Your implementation should be modu-lar so that the sensors, actuators, and environment characteristics (size, shape, dirt placement,etc.)

Token 1515:
can be changed easily.

Token 1516:
( Note: for some choices of programming language and operating system there are already implementations in the online code repository.)

Token 1517:
2.9 Implement a simple reﬂex agent for the vacuum environment in Exercise 2.8.

Token 1518:
Run the environment with this agent for all possible initial dirt conﬁgurations and agent locations.Record the performance score for each conﬁguration and the overall average score.

Token 1519:
2.10 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the agent is penalized one point for each movement. a.

Token 1520:
Can a simple reﬂex agent be perfectly rational for this environment? Explain. b. What about a reﬂex agent with state? Design such an agent.

Token 1521:
c. How do your answers to aandbchange if the agent’s percepts give it the clean/dirty status of every square in the environment?

Token 1522:
2.11 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the geography of the environment—its extent, boundaries, and obstacles—is unknown, as is theinitial dirt conﬁguration.

Token 1523:
(The agent can go UpandDown as well as Left andRight .) a. Can a simple reﬂex agent be perfectly rational for this environment? Explain. b.

Token 1524:
Can a simple reﬂex agent with a randomized agent function outperform a simple reﬂex agent?

Token 1525:
Design such an agent and measure its performance on several environments.

Token 1526:
c. Can you design an environment in which your randomized agent will perform poorly? Show your results.

Token 1527:
d. Can a reﬂex agent with state outperform a simple reﬂex agent? Design such an agent and measure its performance on several environments.

Token 1528:
Can you design a rational agent of this type?

Token 1529:
2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a “bump” sensor that detects the agent’s attempts to move into an obstacle or to cross theboundaries of the environment.

Token 1530:
Suppose the bump sensor stops working; how should theagent behave?

Token 1531:
2.13 The vacuum environments in the preceding exercises have all been deterministic.

Token 1532:
Dis- cuss possible agent programs for each of the following stochastic versions: a. Murphy’s law: twenty-ﬁve percent of the time, the Suck action fails to clean the ﬂoor if it is dirty and deposits dirt onto the ﬂoor if the ﬂoor is clean.

Token 1533:
How is your agent programaffected if the dirt sensor gives the wrong answer 10% of the time? b.

Token 1534:
Small children: At each time step, each clean square has a 10% chance of becoming dirty. Can you come up with a rational agent design for this case?

Token 1535:
3SOLVING PROBLEMS BY SEARCHING In which we see how an agent can ﬁnd a sequence of actions that achieves its goals when no single action will do.

Token 1536:
The simplest agents discussed in Chapter 2 were the reﬂex agents, which base their actions on a direct mapping from states to actions.

Token 1537:
Such agents cannot operate well in environments forwhich this mapping would be too large to store and would take too long to learn.

Token 1538:
Goal-basedagents, on the other hand, consider future actions and the desirability of their outcomes.

Token 1539:
This chapter describes one kind of goal-based agent called a problem-solving agent .

Token 1540:
PROBLEM-SOLVING AGENT Problem-solving agents use atomic representations, as described in Section 2.4.7—that is, states of the world are considered as wholes, with no internal structure visible to the problem- solving algorithms.

Token 1541:
Goal-based agents that use more advanced factored orstructured rep- resentations are usually called planning agents and are discussed in Chapters 7 and 10.

Token 1542:
Our discussion of problem solving begins with precise deﬁnitions of problems and their solutions and give several examples to illustrate these deﬁnitions.

Token 1543:
We then describe several general-purpose search algorithms that can be used to solve these problems.

Token 1544:
We will seeseveral uninformed search algorithms—algorithms that are given no information about the problem other than its deﬁnition.

Token 1545:
Although some of these algorithms can solve any solvableproblem, none of them can do so efﬁciently.

Token 1546:
Informed search algorithms, on the other hand, can do quite well given some guidance on where to look for solutions.

Token 1547:
In this chapter, we limit ourselves to the simplest kind of task environment, for which the solution to a problem is always a ﬁxed sequence of actions.

Token 1548:
The more general case—where the agent’s future actions may vary depending on future percepts—is handled in Chapter 4.

Token 1549:
This chapter uses the concepts of asymptotic complexity (that is, O()notation) and NP-completeness.

Token 1550:
Readers unfamiliar with these concepts should consult Appendix A.

Token 1551:
3.1 P ROBLEM -SOLVING AGENTS Intelligent agents are supposed to maximize their performance measure.

Token 1552:
As we mentionedin Chapter 2, achieving this is sometimes simpliﬁed if the agent can adopt a goal and aim at satisfying it.

Token 1553:
Let us ﬁrst look at why and how an agent might do this. 64

Token 1554:
Section 3.1. Problem-Solving Agents 65 Imagine an agent in the city of Arad, Romania, enjoying a touring holiday.

Token 1555:
The agent’s performance measure contains many factors: it wants to improve its suntan, improve its Ro-manian, take in the sights, enjoy the nightlife (such as it is), avoid hangovers, and so on.

Token 1556:
Thedecision problem is a complex one involving many tradeoffs and careful reading of guide-books.

Token 1557:
Now, suppose the agent has a nonrefundable ticket to ﬂy out of Bucharest the follow- ing day.

Token 1558:
In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.

Token 1559:
Courses of action that don’t reach Bucharest on time can be rejected without further consid-eration and the agent’s decision problem is greatly simpliﬁed.

Token 1560:
Goals help organize behaviorby limiting the objectives that the agent is trying to achieve and hence the actions it needsto consider.

Token 1561:
Goal formulation , based on the current situation and the agent’s performance GOAL FORMULATION measure, is the ﬁrst step in problem solving.

Token 1562:
We will consider a goal to be a set of world states—exactly those states in which the goal is satisﬁed.

Token 1563:
The agent’s task is to ﬁnd out how to act, now and in the future, so that itreaches a goal state.

Token 1564:
Before it can do this, it needs to decide (or we need to decide on itsbehalf) what sorts of actions and states it should consider.

Token 1565:
If it were to consider actions atthe level of “move the left foot forward an inch” or “turn the steering wheel one degree left,”the agent would probably never ﬁnd its way out of the parking lot, let alone to Bucharest, because at that level of detail there is too much uncertainty in the world and there would be too many steps in a solution.

Token 1566:
Problem formulation is the process of deciding what actions PROBLEM FORMULATION and states to consider, given a goal.

Token 1567:
We discuss this process in more detail later.

Token 1568:
For now, let us assume that the agent will consider actions at the level of driving from one major town toanother.

Token 1569:
Each state therefore corresponds to being in a particular town.

Token 1570:
Our agent has now adopted the goal of driving to Bucharest and is considering where to go from Arad.

Token 1571:
Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and oneto Zerind.

Token 1572:
None of these achieves the goal, so unless the agent is familiar with the geographyof Romania, it will not know which road to follow.

Token 1573:
1In other words, the agent will not know which of its possible actions is best, because it does not yet know enough about the state that results from taking each action.

Token 1574:
If the agent has no additional information—i.e., if the environment is unknown in the sense deﬁned in Section 2.3—then it is has no choice but to try one of the actions at random.

Token 1575:
This sad situation is discussed in Chapter 4. But suppose the agent has a map of Romania.

Token 1576:
The point of a map is to provide the agent with information about the states it might get itself into and the actions it can take.

Token 1577:
Theagent can use this information to consider subsequent stages of a hypothetical journey via each of the three towns, trying to ﬁnd a journey that eventually gets to Bucharest.

Token 1578:
Once it hasfound a path on the map from Arad to Bucharest, it can achieve its goal by carrying out thedriving actions that correspond to the legs of the journey.

Token 1579:
In general, an agent with several immediate options of unknown value can decide what to do by ﬁrst examining future actions that eventually lead to states of known value.

Token 1580:
To be more speciﬁc about what we mean by “examining future actions,” we have to be more speciﬁc about properties of the environment, as deﬁned in Section 2.3.

Token 1581:
For now, 1We are assuming that most readers are in the same position and can easily imagine themselves to be as clueless as our agent.

Token 1582:
We apologize to Romanian readers who are unable to take advantage of this pedagogical device.

Token 1583:
66 Chapter 3. Solving Problems by Searching we assume that the environment is observable , so the agent always knows the current state.

Token 1584:
For the agent driving in Romania, it’s reasonable to suppose that each city on the map has asign indicating its presence to arriving drivers.

Token 1585:
We also assume the environment is discrete , so at any given state there are only ﬁnitely many actions to choose from.

Token 1586:
This is true fornavigating in Romania because each city is connected to a small number of other cities.

Token 1587:
We will assume the environment is known , so the agent knows which states are reached by each action.

Token 1588:
(Having an accurate map sufﬁces to meet this condition for navigation problems.

Token 1589:
)Finally, we assume that the environment is deterministic , so each action has exactly one outcome.

Token 1590:
Under ideal conditions, this is true for the agent in Romania—it means that if itchooses to drive from Arad to Sibiu, it does end up in Sibiu.

Token 1591:
Of course, conditions are notalways ideal, as we show in Chapter 4.

Token 1592:
Under these assumptions, the solution to any problem is a ﬁxed sequence of actions.

Token 1593:
“Of course!” one might say, “What else could it be?” Well, in general it could be a branchingstrategy that recommends different actions in the future depending on what percepts arrive.For example, under less than ideal conditions, the agent might plan to drive from Arad toSibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case itarrives by accident in Zerind instead of Sibiu.

Token 1594:
Fortunately, if the agent knows the initial state and the environment is known and deterministic, it knows exactly where it will be after the ﬁrst action and what it will perceive.

Token 1595:
Since only one percept is possible after the ﬁrst action,the solution can specify only one possible second action, and so on.

Token 1596:
The process of looking for a sequence of actions that reaches the goal is called search .

Token 1597:
SEARCH A search algorithm takes a problem as input and returns a solution in the form of an action SOLUTION sequence.

Token 1598:
Once a solution is found, the actions it recommends can be carried out. This is called the execution phase.

Token 1599:
Thus, we have a simple “formulate, search, execute” design EXECUTION for the agent, as shown in Figure 3.1.

Token 1600:
After formulating a goal and a problem to solve, the agent calls a search procedure to solve it.

Token 1601:
It then uses the solution to guide its actions,doing whatever the solution recommends as the next thing to do—typically, the ﬁrst action ofthe sequence—and then removing that step from the sequence.

Token 1602:
Once the solution has been executed, the agent will formulate a new goal.

Token 1603:
Notice that while the agent is executing the solution sequence it ignores its percepts when choosing an action because it knows in advance what they will be.

Token 1604:
An agent thatcarries out its plans with its eyes closed, so to speak, must be quite certain of what is goingon.

Token 1605:
Control theorists call this an open-loop system, because ignoring the percepts breaks the OPEN-LOOP loop between agent and environment.

Token 1606:
We ﬁrst describe the process of problem formulation, and then devote the bulk of the chapter to various algorithms for the S EARCH function.

Token 1607:
We do not discuss the workings of the U PDATE -STATE and F ORMULATE -GOAL functions further in this chapter.

Token 1608:
3.1.1 Well-deﬁned problems and solutions Aproblem can be deﬁned formally by ﬁve components: PROBLEM •The initial state that the agent starts in.

Token 1609:
For example, the initial state for our agent in INITIAL STATE Romania might be described as In(Arad).

Token 1610:
Section 3.1.

Token 1611:
Problem-Solving Agents 67 function SIMPLE -PROBLEM -SOLVING -AGENT (percept )returns an action persistent :seq, an action sequence, initially empty state , some description of the current world state goal, a goal, initially null problem , a problem formulation state←UPDATE -STATE (state ,percept ) ifseqis empty then goal←FORMULATE -GOAL(state ) problem←FORMULATE -PROBLEM (state ,goal) seq←SEARCH (problem ) ifseq=failure then return a null action action←FIRST (seq) seq←REST(seq) return action Figure 3.1 A simple problem-solving agent.

Token 1612:
It ﬁrst formulates a goal and a problem, searches for a sequence of actions that would solve the problem, and then executes the actionsone at a time.

Token 1613:
When this is complete, it formulates another goal and starts over. •A description of the possible actions available to the agent.

Token 1614:
Given a particular state s, ACTIONS ACTIONS (s)returns the set of actions that can be executed in s. We say that each of these actions is applicable ins.

Token 1615:
For example, from the state In(Arad), the applicable APPLICABLE actions are{Go(Sibiu),Go(Timisoara ),Go(Zerind )}.

Token 1616:
•A description of what each action does; the formal name for this is the transition model , speciﬁed by a function R ESULT (s,a) that returns the state that results from TRANSITION MODEL doing action ain state s. We also use the term successor to refer to any state reachable SUCCESSOR from a given state by a single action.2For example, we have RESULT (In(Arad),Go(Zerind )) =In(Zerind ).

Token 1617:
Together, the initial state, actions, and transition model implicitly deﬁne the state space STATE SPACE of the problem—the set of all states reachable from the initial state by any sequence of actions.

Token 1618:
The state space forms a directed network or graph in which the nodes GRAPH are states and the links between nodes are actions.

Token 1619:
(The map of Romania shown in Figure 3.2 can be interpreted as a state-space graph if we view each road as standingfor two driving actions, one in each direction.)

Token 1620:
A path in the state space is a sequence PATH of states connected by a sequence of actions.

Token 1621:
•Thegoal test , which determines whether a given state is a goal state.

Token 1622:
Sometimes there GOAL TEST is an explicit set of possible goal states, and the test simply checks whether the given state is one of them.

Token 1623:
The agent’s goal in Romania is the singleton set {In(Bucharest )}.

Token 1624:
2Many treatments of problem solving, including previous editions of this book, use a successor function ,w h i c h returns the set of all successors, instead of separate A CTIONS and R ESULT functions.

Token 1625:
The successor function makes it difﬁcult to describe an agent that knows what actions it can try but not what they achieve.

Token 1626:
Also, notesome author use R ESULT (a,s) instead of R ESULT (s,a), and some use D Oinstead of R ESULT .

Token 1627:
68 Chapter 3.

Token 1628:
Solving Problems by Searching GiurgiuUrziceniHirsova EforieNeamtOradea Zerind Arad Timisoara Lugoj Mehadia Drobeta CraiovaSibiu Fagaras PitestiVasluiIasi Rimnicu Vilcea Bucharest71 75 118 111 70 75 120151 140 99 80 97 101211 138146 85 90981429287 86 Figure 3.2 A simpliﬁed road map of part of Romania.

Token 1629:
Sometimes the goal is speciﬁed by an abstract property rather than an explicitly enumer- ated set of states.

Token 1630:
For example, in chess, the goal is to reach a state called “checkmate,”where the opponent’s king is under attack and can’t escape.

Token 1631:
•Apath cost function that assigns a numeric cost to each path.

Token 1632:
The problem-solving PATH COST agent chooses a cost function that reﬂects its own performance measure.

Token 1633:
For the agent trying to get to Bucharest, time is of the essence, so the cost of a path might be its lengthin kilometers.

Token 1634:
In this chapter, we assume that the cost of a path can be described as thesumof the costs of the individual actions along the path.

Token 1635:
3Thestep cost of taking action STEP COST ain state sto reach state s/primeis denoted by c(s,a,s/prime).

Token 1636:
The step costs for Romania are shown in Figure 3.2 as route distances.

Token 1637:
We assume that step costs are nonnegative.4 The preceding elements deﬁne a problem and can be gathered into a single data structurethat is given as input to a problem-solving algorithm.

Token 1638:
A solution to a problem is an action sequence that leads from the initial state to a goal state.

Token 1639:
Solution quality is measured by thepath cost function, and an optimal solution has the lowest path cost among all solutions.

Token 1640:
OPTIMAL SOLUTION 3.1.2 Formulating problems In the preceding section we proposed a formulation of the problem of getting to Bucharest in terms of the initial state, actions, transition model, goal test, and path cost.

Token 1641:
This formulationseems reasonable, but it is still a model —an abstract mathematical description—and not the 3This assumption is algorithmically convenient but also theoretically justiﬁable—see page 649 in Chapter 17.

Token 1642:
4The implications of negative costs are explored in Exercise 3.8.

Token 1643:
Section 3.2. Example Problems 69 real thing.

Token 1644:
Compare the simple state description we have chosen, In(Arad) , to an actual cross- country trip, where the state of the world includes so many things: the traveling companions,the current radio program, the scenery out of the window, the proximity of law enforcementofﬁcers, the distance to the next rest stop, the condition of the road, the weather, and so on.All these considerations are left out of our state descriptions because they are irrelevant to the problem of ﬁnding a route to Bucharest.

Token 1645:
The process of removing detail from a representation is called abstraction .

Token 1646:
ABSTRACTION In addition to abstracting the state description, we must abstract the actions themselves. A driving action has many effects.

Token 1647:
Besides changing the location of the vehicle and its oc-cupants, it takes up time, consumes fuel, generates pollution, and changes the agent (as theysay, travel is broadening).

Token 1648:
Our formulation takes into account only the change in location.Also, there are many actions that we omit altogether: turning on the radio, looking out ofthe window, slowing down for law enforcement ofﬁcers, and so on.

Token 1649:
And of course, we don’tspecify actions at the level of “turn steering wheel to the left by one degree.” Can we be more precise about deﬁning the appropriate level of abstraction?

Token 1650:
Think of the abstract states and actions we have chosen as corresponding to large sets of detailed worldstates and detailed action sequences.

Token 1651:
Now consider a solution to the abstract problem: for example, the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest.

Token 1652:
This abstract solution corresponds to a large number of more detailed paths.

Token 1653:
For example, we could drivewith the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest ofthe trip.

Token 1654:
The abstraction is valid if we can expand any abstract solution into a solution in the more detailed world; a sufﬁcient condition is that for every detailed state that is “in Arad,” there is a detailed path to some state that is “in Sibiu,” and so on.

Token 1655:
5The abstraction is useful if carrying out each of the actions in the solution is easier than the original problem; in thiscase they are easy enough that they can be carried out without further search or planning byan average driving agent.

Token 1656:
The choice of a good abstraction thus involves removing as muchdetail as possible while retaining validity and ensuring that the abstract actions are easy tocarry out.

Token 1657:
Were it not for the ability to construct useful abstractions, intelligent agents would be completely swamped by the real world.

Token 1658:
3.2 E XAMPLE PROBLEMS The problem-solving approach has been applied to a vast array of task environments.

Token 1659:
We list some of the best known here, distinguishing between toyand real-world problems.

Token 1660:
A toy problem is intended to illustrate or exercise various problem-solving methods.

Token 1661:
It can be TOY PROBLEM given a concise, exact description and hence is usable by different researchers to compare the performance of algorithms.

Token 1662:
A real-world problem is one whose solutions people actuallyREAL-WORLD PROBLEM care about.

Token 1663:
Such problems tend not to have a single agreed-upon description, but we can give the general ﬂavor of their formulations.

Token 1664:
5See Section 11.2 for a more complete set of deﬁnitions and algorithms.

Token 1665:
70 Chapter 3. Solving Problems by Searching R L S S SSR LR L R LS S SSL LL L R RR R Figure 3.3 The state space for the vacuum world.

Token 1666:
Links denote actions: L = Left,R= Right ,S= Suck. 3.2.1 Toy problems The ﬁrst example we examine is the vacuum world ﬁrst introduced in Chapter 2.

Token 1667:
(See Figure 2.2.) This can be formulated as a problem as follows: •States : The state is determined by both the agent location and the dirt locations.

Token 1668:
The agent is in one of two locations, each of which might or might not contain dirt. Thus,there are 2×2 2=8 possible world states.

Token 1669:
A larger environment with nlocations has n·2nstates. •Initial state : Any state can be designated as the initial state.

Token 1670:
•Actions : In this simple environment, each state has just three actions: Left,Right ,a n d Suck. Larger environments might also include UpandDown .

Token 1671:
•Transition model : The actions have their expected effects, except that moving Leftin the leftmost square, moving Right in the rightmost square, and Sucking in a clean square have no effect.

Token 1672:
The complete state space is shown in Figure 3.3. •Goal test : This checks whether all the squares are clean.

Token 1673:
•Path cost : Each step costs 1, so the path cost is the number of steps in the path.

Token 1674:
Compared with the real world, this toy problem has discrete locations, discrete dirt, reliable cleaning, and it never gets any dirtier.

Token 1675:
Chapter 4 relaxes some of these assumptions.

Token 1676:
The8-puzzle , an instance of which is shown in Figure 3.4, consists of a 3 ×3 board with 8-PUZZLE eight numbered tiles and a blank space.

Token 1677:
A tile adjacent to the blank space can slide into the space.

Token 1678:
The object is to reach a speciﬁed goal state, such as the one shown on the right of the ﬁgure. The standard formulation is as follows:

Token 1679:
Section 3.2. Example Problems 71 2 Start State Goal State1 3 4 6 75 12 34 67 85 8 Figure 3.4 A typical instance of the 8-puzzle.

Token 1680:
•States : A state description speciﬁes the location of each of the eight tiles and the blank in one of the nine squares.

Token 1681:
•Initial state : Any state can be designated as the initial state.

Token 1682:
Note that any given goal can be reached from exactly half of the possible initial states (Exercise 3.4).

Token 1683:
•Actions : The simplest formulation deﬁnes the actions as movements of the blank space Left,Right ,Up,o rDown .

Token 1684:
Different subsets of these are possible depending on where the blank is.

Token 1685:
•Transition model : Given a state and action, this returns the resulting state; for example, if we apply Leftto the start state in Figure 3.4, the resulting state has the 5 and the blank switched.

Token 1686:
•Goal test : This checks whether the state matches the goal conﬁguration shown in Fig- ure 3.4. (Other goal conﬁgurations are possible.)

Token 1687:
•Path cost : Each step costs 1, so the path cost is the number of steps in the path. What abstractions have we included here?

Token 1688:
The actions are abstracted to their beginning and ﬁnal states, ignoring the intermediate locations where the block is sliding.

Token 1689:
We have abstracted away actions such as shaking the board when pieces get stuck and ruled out extracting the pieces with a knife and putting them back again.

Token 1690:
We are left with a description of the rules ofthe puzzle, avoiding all the details of physical manipulations.

Token 1691:
The 8-puzzle belongs to the family of sliding-block puzzles , which are often used as SLIDING-BLOCK PUZZLES test problems for new search algorithms in AI.

Token 1692:
This family is known to be NP-complete, so one does not expect to ﬁnd methods signiﬁcantly better in the worst case than the search algorithms described in this chapter and the next.

Token 1693:
The 8-puzzle has 9!/2= 181 ,440reachable states and is easily solved.

Token 1694:
The 15-puzzle (on a 4×4board) has around 1.3 trillion states, and random instances can be solved optimally in a few milliseconds by the best search algorithms.The 24-puzzle (on a 5×5board) has around 10 25states, and random instances take several hours to solve optimally.

Token 1695:
The goal of the 8-queens problem is to place eight queens on a chessboard such that 8-QUEENS PROBLEM no queen attacks any other.

Token 1696:
(A queen attacks any piece in the same row, column or diago- nal.)

Token 1697:
Figure 3.5 shows an attempted solution that fails: the queen in the rightmost column isattacked by the queen at the top left.

Token 1698:
72 Chapter 3. Solving Problems by Searching Figure 3.5 Almost a solution to the 8-queens problem. (Solution is left as an exercise.)

Token 1699:
Although efﬁcient special-purpose algorithms exist for this problem and for the whole n-queens family, it remains a useful test problem for search algorithms.

Token 1700:
There are two main kinds of formulation.

Token 1701:
An incremental formulation involves operators that augment the stateINCREMENTAL FORMULATION description, starting with an empty state; for the 8-queens problem, this means that each action adds a queen to the state.

Token 1702:
A complete-state formulation starts with all 8 queens onCOMPLETE-STATE FORMULATION the board and moves them around.

Token 1703:
In either case, the path cost is of no interest because only the ﬁnal state counts.

Token 1704:
The ﬁrst incremental formulation one might try is the following: •States : Any arrangement of 0 to 8 queens on the board is a state.

Token 1705:
•Initial state : No queens on the board. •Actions : Add a queen to any empty square.

Token 1706:
•Transition model : Returns the board with a queen added to the speciﬁed square. •Goal test : 8 queens are on the board, none attacked.

Token 1707:
In this formulation, we have 64·63···57≈1.8×1014possible sequences to investigate.

Token 1708:
A better formulation would prohibit placing a queen in any square that is already attacked: •States : All possible arrangements of nqueens (0≤n≤8), one per column in the leftmost ncolumns, with no queen attacking another.

Token 1709:
•Actions : Add a queen to any square in the leftmost empty column such that it is not attacked by any other queen.

Token 1710:
This formulation reduces the 8-queens state space from 1.8×1014to just 2,057, and solutions are easy to ﬁnd.

Token 1711:
On the other hand, for 100 queens the reduction is from roughly 10400states to about 1052states (Exercise 3.5)—a big improvement, but not enough to make the problem tractable.

Token 1712:
Section 4.1 describes the complete-state formulation, and Chapter 6 gives a simple algorithm that solves even the million-queens problem with ease.

Token 1713:
Section 3.2. Example Problems 73 Our ﬁnal toy problem was devised by Donald Knuth (1964) and illustrates how inﬁnite state spaces can arise.

Token 1714:
Knuth conjectured that, starting with the number 4, a sequence of fac-torial, square root, and ﬂoor operations will reach any desired positive integer.

Token 1715:
For example,we can reach 5 from 4 as follows: /floorleftBig/radicaltp/radicalvertex/radicalvertex/radicalbt /radicalBigg /radicalbigg /radicalBig /radicalbig (4!

Token 1716:
)!/floorrightBig =5. The problem deﬁnition is very simple: •States : Positive numbers. •Initial state :4 .

Token 1717:
•Actions : Apply factorial, square root, or ﬂoor operation (factorial for integers only).

Token 1718:
•Transition model : As given by the mathematical deﬁnitions of the operations. •Goal test : State is the desired positive integer.

Token 1719:
To our knowledge there is no bound on how large a number might be constructed in the pro- cess of reaching a given target—for example, the number 620,448,401,733,239,439,360,000is generated in the expression for 5—so the state space for this problem is inﬁnite.

Token 1720:
Suchstate spaces arise frequently in tasks involving the generation of mathematical expressions,circuits, proofs, programs, and other recursively deﬁned objects.

Token 1721:
3.2.2 Real-world problems We have already seen how the route-ﬁnding problem is deﬁned in terms of speciﬁed loca-ROUTE-FINDING PROBLEM tions and transitions along links between them.

Token 1722:
Route-ﬁnding algorithms are used in a variety of applications.

Token 1723:
Some, such as Web sites and in-car systems that provide driving directions,are relatively straightforward extensions of the Romania example.

Token 1724:
Others, such as routingvideo streams in computer networks, military operations planning, and airline travel-planningsystems, involve much more complex speciﬁcations.

Token 1725:
Consider the airline travel problems thatmust be solved by a travel-planning Web site: •States : Each state obviously includes a location (e.g., an airport) and the current time.

Token 1726:
Furthermore, because the cost of an action (a ﬂight segment) may depend on previoussegments, their fare bases, and their status as domestic or international, the state mustrecord extra information about these “historical” aspects.

Token 1727:
•Initial state : This is speciﬁed by the user’s query.

Token 1728:
•Actions : Take any ﬂight from the current location, in any seat class, leaving after the current time, leaving enough time for within-airport transfer if needed.

Token 1729:
•Transition model : The state resulting from taking a ﬂight will have the ﬂight’s desti- nation as the current location and the ﬂight’s arrival time as the current time.

Token 1730:
•Goal test : Are we at the ﬁnal destination speciﬁed by the user?

Token 1731:
•Path cost : This depends on monetary cost, waiting time, ﬂight time, customs and im- migration procedures, seat quality, time of day, type of airplane, frequent-ﬂyer mileageawards, and so on.

Token 1732:
74 Chapter 3.

Token 1733:
Solving Problems by Searching Commercial travel advice systems use a problem formulation of this kind, with many addi- tional complications to handle the byzantine fare structures that airlines impose.

Token 1734:
Any sea-soned traveler knows, however, that not all air travel goes according to plan.

Token 1735:
A really goodsystem should include contingency plans—such as backup reservations on alternate ﬂights—to the extent that these are justiﬁed by the cost and likelihood of failure of the original plan.

Token 1736:
Touring problems are closely related to route-ﬁnding problems, but with an impor- TOURINGPROBLEM tant difference.

Token 1737:
Consider, for example, the problem “Visit every city in Figure 3.2 at least once, starting and ending in Bucharest.” As with route ﬁnding, the actions correspondto trips between adjacent cities.

Token 1738:
The state space, however, is quite different. Each statemust include not just the current location but also the set of cities the agent has visited .

Token 1739:
So the initial state would be In(Bucharest ),Visited ({Bucharest}), a typical intermedi- ate state would be In(Vaslui ),Visited ({Bucharest ,Urziceni ,Vaslui}), and the goal test would check whether the agent is in Bucharest and all 20 cities have been visited.

Token 1740:
The traveling salesperson problem (TSP) is a touring problem in which each city TRAVELING SALESPERSONPROBLEM must be visited exactly once.

Token 1741:
The aim is to ﬁnd the shortest tour.

Token 1742:
The problem is known to be NP-hard, but an enormous amount of effort has been expended to improve the capabilitiesof TSP algorithms.

Token 1743:
In addition to planning trips for traveling salespersons, these algorithms have been used for tasks such as planning movements of automatic circuit-board drills and of stocking machines on shop ﬂoors.

Token 1744:
AVLSI layout problem requires positioning millions of components and connections VLSI LAYOUT on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max- imize manufacturing yield.

Token 1745:
The layout problem comes after the logical design phase and isusually split into two parts: cell layout andchannel routing .

Token 1746:
In cell layout, the primitive components of the circuit are grouped into cells, each of which performs some recognizedfunction.

Token 1747:
Each cell has a ﬁxed footprint (size and shape) and requires a certain number ofconnections to each of the other cells.

Token 1748:
The aim is to place the cells on the chip so that they donot overlap and so that there is room for the connecting wires to be placed between the cells.Channel routing ﬁnds a speciﬁc route for each wire through the gaps between the cells.

Token 1749:
These search problems are extremely complex, but deﬁnitely worth solving. Later in this chapter, we present some algorithms capable of solving them.

Token 1750:
Robot navigation is a generalization of the route-ﬁnding problem described earlier.

Token 1751:
ROBOT NAVIGATION Rather than following a discrete set of routes, a robot can move in a continuous space with (in principle) an inﬁnite set of possible actions and states.

Token 1752:
For a circular robot moving on aﬂat surface, the space is essentially two-dimensional.

Token 1753:
When the robot has arms and legs orwheels that must also be controlled, the search space becomes many-dimensional.

Token 1754:
Advancedtechniques are required just to make the search space ﬁnite. We examine some of thesemethods in Chapter 25.

Token 1755:
In addition to the complexity of the problem, real robots must alsodeal with errors in their sensor readings and motor controls.

Token 1756:
Automatic assembly sequencing of complex objects by a robot was ﬁrst demonstrated AUTOMATIC ASSEMBLY SEQUENCINGby F REDDY (Michie, 1972).

Token 1757:
Progress since then has been slow but sure, to the point where the assembly of intricate objects such as electric motors is economically feasible.

Token 1758:
In assemblyproblems, the aim is to ﬁnd an order in which to assemble the parts of some object.

Token 1759:
If thewrong order is chosen, there will be no way to add some part later in the sequence without

Token 1760:
Section 3.3. Searching for Solutions 75 undoing some of the work already done.

Token 1761:
Checking a step in the sequence for feasibility is a difﬁcult geometrical search problem closely related to robot navigation.

Token 1762:
Thus, the generationof legal actions is the expensive part of assembly sequencing.

Token 1763:
Any practical algorithm mustavoid exploring all but a tiny fraction of the state space.

Token 1764:
Another important assembly problemisprotein design , in which the goal is to ﬁnd a sequence of amino acids that will fold into a PROTEIN DESIGN three-dimensional protein with the right properties to cure some disease.

Token 1765:
3.3 S EARCHING FOR SOLUTIONS Having formulated some problems, we now need to solve them.

Token 1766:
A solution is an action sequence, so search algorithms work by considering various possible action sequences.

Token 1767:
Thepossible action sequences starting at the initial state form a search tree with the initial state SEARCH TREE at the root; the branches are actions and the nodes correspond to states in the state space of NODE the problem.

Token 1768:
Figure 3.6 shows the ﬁrst few steps in growing the search tree for ﬁnding a route from Arad to Bucharest.

Token 1769:
The root node of the tree corresponds to the initial state, In(Arad) . The ﬁrst step is to test whether this is a goal state.

Token 1770:
(Clearly it is not, but it is important to check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then weneed to consider taking various actions.

Token 1771:
We do this by expanding the current state; that is, EXPANDING applying each legal action to the current state, thereby generating a new set of states.

Token 1772:
In GENERATING this case, we add three branches from the parent node In(Arad) leading to three new child PARENT NODE nodes :In(Sibiu), In(Timisoara), and In(Zerind) .

Token 1773:
Now we must choose which of these three CHILDNODE possibilities to consider further.

Token 1774:
This is the essence of search—following up one option now and putting the others aside for later, in case the ﬁrst choice does not lead to a solution.

Token 1775:
Suppose we choose Sibiu ﬁrst.We check to see whether it is a goal state (it is not) and then expand it to get In(Arad) , In(Fagaras) ,In(Oradea) ,a n d In(RimnicuVilcea) .

Token 1776:
We can then choose any of these four or go back and choose Timisoara or Zerind.

Token 1777:
Each of these six nodes is a leaf node , that is, a node LEAF NODE with no children in the tree.

Token 1778:
The set of all leaf nodes available for expansion at any given point is called the frontier .

Token 1779:
(Many authors call it the open list , which is both geographically FRONTIER OPEN LIST less evocative and less accurate, because other data structures are better suited than a list.)

Token 1780:
In Figure 3.6, the frontier of each tree consists of those nodes with bold outlines.

Token 1781:
The process of expanding nodes on the frontier continues until either a solution is found or there are no more states to expand.

Token 1782:
The general T REE-SEARCH algorithm is shown infor- mally in Figure 3.7.

Token 1783:
Search algorithms all share this basic structure; they vary primarilyaccording to how they choose which state to expand next—the so-called search strategy .

Token 1784:
SEARCH STRATEGY The eagle-eyed reader will notice one peculiar thing about the search tree shown in Fig- ure 3.6: it includes the path from Arad to Sibiu and back to Arad again!

Token 1785:
We say that In(Arad) is arepeated state in the search tree, generated in this case by a loopy path .

Token 1786:
Considering REPEATED STATE LOOPY PATH such loopy paths means that the complete search tree for Romania is inﬁnite because there is no limit to how often one can traverse a loop.

Token 1787:
On the other hand, the state space—the map shown in Figure 3.2—has only 20 states. As we discuss in Section 3.4, loops can cause

Token 1788:
76 Chapter 3. Solving Problems by Searching certain algorithms to fail, making otherwise solvable problems unsolvable.

Token 1789:
Fortunately, there is no need to consider loopy paths.

Token 1790:
We can rely on more than intuition for this: because pathcosts are additive and step costs are nonnegative, a loopy path to any given state is neverbetter than the same path with the loop removed.

Token 1791:
Loopy paths are a special case of the more general concept of redundant paths ,w h i c h REDUNDANT PATH exist whenever there is more than one way to get from one state to another.

Token 1792:
Consider the paths Arad–Sibiu (140 km long) and Arad–Zerind–Oradea–Sibiu (297 km long).

Token 1793:
Obviously, thesecond path is redundant—it’s just a worse way to get to the same state.

Token 1794:
If you are concernedabout reaching the goal, there’s never any reason to keep more than one path to any givenstate, because any goal state that is reachable by extending one path is also reachable byextending the other.

Token 1795:
In some cases, it is possible to deﬁne the problem itself so as to eliminate redundant paths.

Token 1796:
For example, if we formulate the 8-queens problem (page 71) so that a queen can beplaced in any column, then each state with nqueens can be reached by n!different paths; but if we reformulate the problem so that each new queen is placed in the leftmost empty column,then each state can be reached only through one path.

Token 1797:
(a) The initial state (b) After expanding Arad (c) After expanding SibiuRimnicu Vilcea Lugoj Arad Fagaras Oradea Arad Arad OradeaRimnicu Vilcea LugojZerind Sibiu Arad Fagaras OradeaTimisoara Arad Arad Oradea Lugoj Arad Arad OradeaZerindArad Sibiu TimisoaraArad Rimnicu VilceaZerindArad Sibiu Arad Fagaras OradeaTimisoara Figure 3.6 Partial search trees for ﬁnding a route from Arad to Bucharest.

Token 1798:
Nodes that have been expanded are shaded; nodes that have been generated but not yet expanded are outlined in bold; nodes that have not yet been generated are shown in faint dashed lines.

Token 1799:
Section 3.3.

Token 1800:
Searching for Solutions 77 function TREE-SEARCH (problem )returns a solution, or failure initialize the frontier using the initial state of problem loop do ifthe frontier is empty then return failure choose a leaf node and remove it from the frontier ifthe node contains a goal state then return the corresponding solution expand the chosen node, adding the resulting nodes to the frontier function GRAPH -SEARCH (problem )returns a solution, or failure initialize the frontier using the initial state of problem initialize the explored set to be empty loop do ifthe frontier is empty then return failure choose a leaf node and remove it from the frontier ifthe node contains a goal state then return the corresponding solution add the node to the explored set expand the chosen node, adding the resulting nodes to the frontier only if not in the frontier or explored set Figure 3.7 An informal description of the general tree-search and graph-search algo- rithms.

Token 1801:
The parts of G RAPH -SEARCH marked in bold italic are the additions needed to handle repeated states. In other cases, redundant paths are unavoidable.

Token 1802:
This includes all problems where the actions are reversible, such as route-ﬁnding problems and sliding-block puzzles.

Token 1803:
Route-ﬁnding on a rectangular grid (like the one used later for Figure 3.9) is a particularly impor- RECTANGULAR GRID tant example in computer games.

Token 1804:
In such a grid, each state has four successors, so a search tree of depth dthat includes repeated states has 4dleaves; but there are only about 2d2distinct states within dsteps of any given state.

Token 1805:
For d=2 0 , this means about a trillion nodes but only about 800 distinct states.

Token 1806:
Thus, following redundant paths can cause a tractable problem tobecome intractable.

Token 1807:
This is true even for algorithms that know how to avoid inﬁnite loops.

Token 1808:
As the saying goes, algorithms that forget their history are doomed to repeat it.

Token 1809:
The way to avoid exploring redundant paths is to remember where one has been.

Token 1810:
To do this, we augment the T REE-SEARCH algorithm with a data structure called the explored set (also EXPLORED SET known as the closed list ), which remembers every expanded node.

Token 1811:
Newly generated nodes CLOSED LIST that match previously generated nodes—ones in the explored set or the frontier—can be dis- carded instead of being added to the frontier.

Token 1812:
The new algorithm, called G RAPH -SEARCH ,i s shown informally in Figure 3.7. The speciﬁc algorithms in this chapter draw on this generaldesign.

Token 1813:
Clearly, the search tree constructed by the G RAPH -SEARCH algorithm contains at most one copy of each state, so we can think of it as growing a tree directly on the state-space graph,as shown in Figure 3.8.

Token 1814:
The algorithm has another nice property: the frontier separates the SEPARATOR state-space graph into the explored region and the unexplored region, so that every path from

Token 1815:
78 Chapter 3. Solving Problems by Searching Figure 3.8 A sequence of search trees generated by a graph search on the Romania prob- lem of Figure 3.2.

Token 1816:
At each stage, we have extended each path by one step.

Token 1817:
Notice that at the third stage, the northernmost city (Oradea) has become a dead end: both of its successors arealready explored via other paths.

Token 1818:
(c) (b) (a) Figure 3.9 The separation property of G RAPH -SEARCH , illustrated on a rectangular-grid problem.

Token 1819:
The frontier (white nodes) always separates the explored region of the state space (black nodes) from the unexplored region (gray nodes).

Token 1820:
In (a), just the root has been ex- panded. In (b), one leaf node has been expanded.

Token 1821:
In (c), the remaining successors of the root have been expanded in clockwise order.

Token 1822:
the initial state to an unexplored state has to pass through a state in the frontier. (If this seems completely obvious, try Exercise 3.13 now.)

Token 1823:
This property is illustrated in Figure 3.9.As every step moves a state from the frontier into the explored region while moving some states from the unexplored region into the frontier, we see that the algorithm is systematically examining the states in the state space, one by one, until it ﬁnds a solution.

Token 1824:
3.3.1 Infrastructure for search algorithms Search algorithms require a data structure to keep track of the search tree that is being con-structed.

Token 1825:
For each node nof the tree, we have a structure that contains four components: •n.S TATE : the state in the state space to which the node corresponds; •n.PARENT : the node in the search tree that generated this node; •n.ACTION : the action that was applied to the parent to generate the node; •n.PATH-COST: the cost, traditionally denoted by g(n), of the path from the initial state to the node, as indicated by the parent pointers.

Token 1826:
Section 3.3.

Token 1827:
Searching for Solutions 79 1 2 34 5 6 78 1 2 34 5 6 78Node STATEPARENT ACTION = Right PATH- COST = 6 Figure 3.10 Nodes are the data structures from which the search tree is constructed.

Token 1828:
Each has a parent, a state, and various bookkeeping ﬁelds. Arrows point from child to parent.

Token 1829:
Given the components for a parent node, it is easy to see how to compute the necessary components for a child node.

Token 1830:
The function C HILD -NODE takes a parent node and an action and returns the resulting child node: function CHILD -NODE(problem ,parent ,action )returns a node return a node with STATE =problem .RESULT (parent .STATE ,action ), PARENT =parent ,ACTION =action , PATH-COST =parent .PATH-COST +problem .STEP-COST(parent .STATE ,action ) The node data structure is depicted in Figure 3.10.

Token 1831:
Notice how the P ARENT pointers string the nodes together into a tree structure.

Token 1832:
These pointers also allow the solution path to beextracted when a goal node is found; we use the S OLUTION function to return the sequence of actions obtained by following parent pointers back to the root.

Token 1833:
Up to now, we have not been very careful to distinguish between nodes and states, but in writing detailed algorithms it’s important to make that distinction.

Token 1834:
A node is a bookkeepingdata structure used to represent the search tree. A state corresponds to a conﬁguration of theworld.

Token 1835:
Thus, nodes are on particular paths, as deﬁned by P ARENT pointers, whereas states are not.

Token 1836:
Furthermore, two different nodes can contain the same world state if that state is generated via two different search paths.

Token 1837:
Now that we have nodes, we need somewhere to put them.

Token 1838:
The frontier needs to be stored in such a way that the search algorithm can easily choose the next node to expandaccording to its preferred strategy.

Token 1839:
The appropriate data structure for this is a queue .T h e QUEUE operations on a queue are as follows: •EMPTY ?

Token 1840:
(queue ) returns true only if there are no more elements in the queue. •POP(queue ) removes the ﬁrst element of the queue and returns it.

Token 1841:
•INSERT (element ,queue ) inserts an element and returns the resulting queue.

Token 1842:
80 Chapter 3. Solving Problems by Searching Queues are characterized by the order in which they store the inserted nodes.

Token 1843:
Three common variants are the ﬁrst-in, ﬁrst-out or FIFO queue , which pops the oldest element of the queue; FIFO QUEUE the last-in, ﬁrst-out or LIFO queue (also known as a stack ), which pops the newest element LIFO QUEUE of the queue; and the priority queue , which pops the element of the queue with the highest PRIORITY QUEUE priority according to some ordering function.

Token 1844:
The explored set can be implemented with a hash table to allow efﬁcient checking for repeated states.

Token 1845:
With a good implementation, insertion and lookup can be done in roughlyconstant time no matter how many states are stored.

Token 1846:
One must take care to implement thehash table with the right notion of equality between states.

Token 1847:
For example, in the travelingsalesperson problem (page 74), the hash table needs to know that the set of visited cities{Bucharest,Urziceni,Vaslui }is the same as{Urziceni,Vaslui,Bucharest }.

Token 1848:
Sometimes this can be achieved most easily by insisting that the data structures for states be in some canonical form ; that is, logically equivalent states should map to the same data structure.

Token 1849:
In the case CANONICAL FORM of states described by sets, for example, a bit-vector representation or a sorted list without repetition would be canonical, whereas an unsorted list would not.

Token 1850:
3.3.2 Measuring problem-solving performance Before we get into the design of speciﬁc search algorithms, we need to consider the criteriathat might be used to choose among them.

Token 1851:
We can evaluate an algorithm’s performance infour ways: •Completeness : Is the algorithm guaranteed to ﬁnd a solution when there is one?

Token 1852:
COMPLETENESS •Optimality : Does the strategy ﬁnd the optimal solution, as deﬁned on page 68?

Token 1853:
OPTIMALITY •Time complexity : How long does it take to ﬁnd a solution?

Token 1854:
TIME COMPLEXITY •Space complexity : How much memory is needed to perform the search?

Token 1855:
SPACE COMPLEXITY Time and space complexity are always considered with respect to some measure of the prob- lem difﬁculty.

Token 1856:
In theoretical computer science, the typical measure is the size of the state space graph,|V|+|E|,w h e r e Vis the set of vertices (nodes) of the graph and Eis the set of edges (links).

Token 1857:
This is appropriate when the graph is an explicit data structure that is input to the search program. (The map of Romania is an example of this.)

Token 1858:
In AI, the graph is often represented implicitly by the initial state, actions, and transition model and is frequently inﬁ- nite.

Token 1859:
For these reasons, complexity is expressed in terms of three quantities: b,t h e branching factor or maximum number of successors of any node; d,t h e depth of the shallowest goal BRANCHINGFACTOR DEPTH node (i.e., the number of steps along the path from the root); and m, the maximum length of any path in the state space.

Token 1860:
Time is often measured in terms of the number of nodes generatedduring the search, and space in terms of the maximum number of nodes stored in memory.For the most part, we describe time and space complexity for search on a tree; for a graph,the answer depends on how “redundant” the paths in the state space are.

Token 1861:
To assess the effectiveness of a search algorithm, we can consider just the search cost — SEARCH COST which typically depends on the time complexity but can also include a term for memory usage—or we can use the total cost , which combines the search cost and the path cost of the TOTAL COST solution found.

Token 1862:
For the problem of ﬁnding a route from Arad to Bucharest, the search cost is the amount of time taken by the search and the solution cost is the total length of the path

Token 1863:
Section 3.4. Uninformed Search Strategies 81 in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.

Token 1864:
There is no “ofﬁcial exchange rate” between the two, but it might be reasonable in this case toconvert kilometers into milliseconds by using an estimate of the car’s average speed (becausetime is what the agent cares about).

Token 1865:
This enables the agent to ﬁnd an optimal tradeoff pointat which further computation to ﬁnd a shorter path becomes counterproductive.

Token 1866:
The more general problem of tradeoffs between different goods is taken up in Chapter 16.

Token 1867:
3.4 U NINFORMED SEARCH STRATEGIES This section covers several search strategies that come under the heading of uninformed search (also called blind search ).

Token 1868:
The term means that the strategies have no additionalUNINFORMED SEARCH BLIND SEARCH information about states beyond that provided in the problem deﬁnition.

Token 1869:
All they can do is generate successors and distinguish a goal state from a non-goal state.

Token 1870:
All search strategiesare distinguished by the order in which nodes are expanded.

Token 1871:
Strategies that know whether one non-goal state is “more promising” than another are called informed search orheuristic INFORMED SEARCH search strategies; they are covered in Section 3.5.

Token 1872:
HEURISTIC SEARCH 3.4.1 Breadth-ﬁrst search Breadth-ﬁrst search is a simple strategy in which the root node is expanded ﬁrst, then all theBREADTH-FIRST SEARCH successors of the root node are expanded next, then their successors, and so on.

Token 1873:
In general, all the nodes are expanded at a given depth in the search tree before any nodes at the nextlevel are expanded.

Token 1874:
Breadth-ﬁrst search is an instance of the general graph-search algorithm (Figure 3.7) in which the shallowest unexpanded node is chosen for expansion.

Token 1875:
This is achieved very simply by using a FIFO queue for the frontier.

Token 1876:
Thus, new nodes (which are always deeper than theirparents) go to the back of the queue, and old nodes, which are shallower than the new nodes, get expanded ﬁrst.

Token 1877:
There is one slight tweak on the general graph-search algorithm, which is that the goal test is applied to each node when it is generated rather than when it is selected for expansion.

Token 1878:
This decision is explained below, where we discuss time complexity.

Token 1879:
Note alsothat the algorithm, following the general template for graph search, discards any new path toa state already in the frontier or explored set; it is easy to see that any such path must be atleast as deep as the one already found.

Token 1880:
Thus, breadth-ﬁrst search always has the shallowestpath to every node on the frontier. Pseudocode is given in Figure 3.11.

Token 1881:
Figure 3.12 shows the progress of the search on a simple binary tree.

Token 1882:
How does breadth-ﬁrst search rate according to the four criteria from the previous sec- tion?

Token 1883:
We can easily see that it is complete —if the shallowest goal node is at some ﬁnite depth d, breadth-ﬁrst search will eventually ﬁnd it after generating all shallower nodes (provided the branching factor bis ﬁnite).

Token 1884:
Note that as soon as a goal node is generated, we know it is the shallowest goal node because all shallower nodes must have been generated alreadyand failed the goal test.

Token 1885:
Now, the shallowest goal node is not necessarily the optimal one;

Token 1886:
82 Chapter 3.

Token 1887:
Solving Problems by Searching function BREADTH -FIRST-SEARCH (problem )returns a solution, or failure node←a node with S TATE =problem .INITIAL -STATE ,PATH-COST =0 ifproblem .GOAL-TEST(node .STATE )then return SOLUTION (node ) frontier←a FIFO queue with node as the only element explored←an empty set loop do ifEMPTY ?

Token 1888:
(frontier )then return failure node←POP(frontier ) /* chooses the shallowest node in frontier */ addnode .STATE toexplored for each action inproblem .ACTIONS (node .STATE )do child←CHILD -NODE(problem ,node ,action ) ifchild .STATE is not in explored orfrontier then ifproblem .GOAL-TEST(child .STATE )then return SOLUTION (child ) frontier←INSERT (child ,frontier ) Figure 3.11 Breadth-ﬁrst search on a graph.

Token 1889:
technically, breadth-ﬁrst search is optimal if the path cost is a nondecreasing function of the depth of the node.

Token 1890:
The most common such scenario is that all actions have the same cost. So far, the news about breadth-ﬁrst search has been good.

Token 1891:
The news about time and space is not so good. Imagine searching a uniform tree where every state has bsuccessors.

Token 1892:
The root of the search tree generates bnodes at the ﬁrst level, each of which generates bmore nodes, for a total of b2at the second level.

Token 1893:
Each of these generates bmore nodes, yielding b3 nodes at the third level, and so on.

Token 1894:
Now suppose that the solution is at depth d.I nt h ew o r s t case, it is the last node generated at that level.

Token 1895:
Then the total number of nodes generated is b+b2+b3+···+bd=O(bd).

Token 1896:
(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than when generated, the whole layer of nodes at depth dwould be expanded before the goal was detected and the time complexity would be O(bd+1).)

Token 1897:
As for space complexity: for any kind of graph search, which stores every expanded node in the explored set, the space complexity is always within a factor of bof the time complexity.

Token 1898:
For breadth-ﬁrst graph search in particular, every node generated remains in memory.

Token 1899:
There will be O(bd−1)nodes in the explored set and O(bd)nodes in the frontier, A B C E F G DA B D E F GCA C D E F GB B C D E F GA Figure 3.12 Breadth-ﬁrst search on a simple binary tree.

Token 1900:
At each stage, the node to be expanded next is indicated by a marker.

Token 1901:
Section 3.4. Uninformed Search Strategies 83 so the space complexity is O(bd), i.e., it is dominated by the size of the frontier.

Token 1902:
Switching to a tree search would not save much space, and in a state space with many redundant paths,switching could cost a great deal of time.

Token 1903:
An exponential complexity bound such as O(b d)is scary. Figure 3.13 shows why.

Token 1904:
It lists, for various values of the solution depth d, the time and memory required for a breadth- ﬁrst search with branching factor b=1 0 .

Token 1905:
The table assumes that 1 million nodes can be generated per second and that a node requires 1000 bytes of storage.

Token 1906:
Many search problemsﬁt roughly within these assumptions (give or take a factor of 100) when run on a modernpersonal computer.

Token 1907:
Depth Nodes Time Memory 2 110 .11 milliseconds 107 kilobytes 4 11,110 11 milliseconds 10.6 megabytes 6 1061.1 seconds 1 gigabyte 8 1082 minutes 103 gigabytes 10 10103 hours 10 terabytes 12 101213 days 1 petabyte 14 10143.5 years 99 petabytes 16 1016350 years 10 exabytes Figure 3.13 Time and memory requirements for breadth-ﬁrst search.

Token 1908:
The numbers shown assume branching factor b=1 0 ; 1 million nodes/sec ond; 1000 bytes/node. Two lessons can be learned from Figure 3.13.

Token 1909:
First, the memory requirements are a bigger problem for breadth-ﬁrst search than is the execution time.

Token 1910:
One might wait 13 days for the solution to an important problem with search depth 12, but no personal computer hasthe petabyte of memory it would take.

Token 1911:
Fortunately, other strategies require less memory. The second lesson is that time is still a major factor.

Token 1912:
If your problem has a solution at depth 16, then (given our assumptions) it will take about 350 years for breadth-ﬁrst search (or indeed any uninformed search) to ﬁnd it.

Token 1913:
In general, exponential-complexity search problems cannot be solved by uninformed methods for any but the smallest instances.

Token 1914:
3.4.2 Uniform-cost search When all step costs are equal, breadth-ﬁrst search is optimal because it always expands the shallowest unexpanded node.

Token 1915:
By a simple extension, we can ﬁnd an algorithm that is optimal with any step-cost function.

Token 1916:
Instead of expanding the shallowest node, uniform-cost searchUNIFORM-COST SEARCH expands the node nwith the lowest path cost g(n).

Token 1917:
This is done by storing the frontier as a priority queue ordered by g. The algorithm is shown in Figure 3.14.

Token 1918:
In addition to the ordering of the queue by path cost, there are two other signiﬁcant differences from breadth-ﬁrst search.

Token 1919:
The ﬁrst is that the goal test is applied to a node whenit is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7) rather than when it is ﬁrst generated.

Token 1920:
The reason is that the ﬁrst goal node that is generated

Token 1921:
84 Chapter 3.

Token 1922:
Solving Problems by Searching function UNIFORM -COST-SEARCH (problem )returns a solution, or failure node←a node with S TATE =problem .INITIAL -STATE ,PATH-COST =0 frontier←a priority queue ordered by P ATH-COST, withnode as the only element explored←an empty set loop do ifEMPTY ?

Token 1923:
(frontier )then return failure node←POP(frontier ) /* chooses the lowest-cost node in frontier */ ifproblem .GOAL-TEST(node .STATE )then return SOLUTION (node ) addnode .STATE toexplored for each action inproblem .ACTIONS (node .STATE )do child←CHILD -NODE(problem ,node ,action ) ifchild .STATE is not in explored orfrontier then frontier←INSERT (child ,frontier ) else ifchild .STATE is infrontier with higher P ATH-COST then replace that frontier node with child Figure 3.14 Uniform-cost search on a graph.

Token 1924:
The algorithm is identical to the general graph search algorithm in Figure 3.7, except for the use of a priority queue and the addition of an extra check in case a shorter path to a frontier state is discovered.

Token 1925:
The data structure for frontier needs to support efﬁcient membership tes ting, so it should com bine the capabilities of a priority queue and a hash table.

Token 1926:
Sibiu Fagaras PitestiRimnicu Vilcea Bucharest99 80 97 101211 Figure 3.15 Part of the Romania state space, selected to illustrate uniform-cost search.

Token 1927:
may be on a suboptimal path. The second difference is that a test is added in case a better path is found to a node currently on the frontier.

Token 1928:
Both of these modiﬁcations come into play in the example shown in Figure 3.15, where the problem is to get from Sibiu to Bucharest.

Token 1929:
The successors of Sibiu are Rimnicu Vilcea and Fagaras, with costs 80 and 99, respectively.

Token 1930:
The least-cost node, Rimnicu Vilcea, is expanded next, adding Pitesti with cost 80 + 97= 177 .

Token 1931:
The least-cost node is now Fagaras, so it is expanded, adding Bucharest with cost 99 + 211= 310 .

Token 1932:
Now a goal node has been generated, but uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path

Token 1933:
Section 3.4. Uninformed Search Strategies 85 to Bucharest with cost 80+97+101= 278 .

Token 1934:
Now the algorithm checks to see if this new path is better than the old one; it is, so the old one is discarded.

Token 1935:
Bucharest, now with g-cost 278, is selected for expansion and the solution is returned.

Token 1936:
It is easy to see that uniform-cost search is optimal in general.

Token 1937:
First, we observe that whenever uniform-cost search selects a node nfor expansion, the optimal path to that node has been found.

Token 1938:
(Were this not the case, there would have to be another frontier node n/primeon the optimal path from the start node to n, by the graph separation property of Figure 3.9; by deﬁnition, n/primewould have lower g-cost than nand would have been selected ﬁrst.)

Token 1939:
Then, because step costs are nonnegative, paths never get shorter as nodes are added.

Token 1940:
These twofacts together imply that uniform-cost search expands nodes in order of their optimal path cost.

Token 1941:
Hence, the ﬁrst goal node selected for expansion must be the optimal solution.

Token 1942:
Uniform-cost search does not care about the number of steps a path has, but only about their total cost.

Token 1943:
Therefore, it will get stuck in an inﬁnite loop if there is a path with an inﬁnitesequence of zero-cost actions—for example, a sequence of NoOp actions.

Token 1944:
6Completeness is guaranteed provided the cost of every step exceeds some small positive constant /epsilon1.

Token 1945:
Uniform-cost search is guided by path costs rather than depths, so its complexity is not easily characterized in terms of bandd.

Token 1946:
Instead, let C∗be the cost of the optimal solution,7 and assume that every action costs at least /epsilon1.

Token 1947:
Then the algorithm’s worst-case time and space complexity is O(b1+⌊C∗//epsilon1⌋), which can be much greater than bd.

Token 1948:
This is because uniform- cost search can explore large trees of small steps before exploring paths involving large and perhaps useful steps.

Token 1949:
When all step costs are equal, b1+⌊C∗//epsilon1⌋is just bd+1.

Token 1950:
When all step costs are the same, uniform-cost search is similar to breadth-ﬁrst search, except that the latterstops as soon as it generates a goal, whereas uniform-cost search examines all the nodes atthe goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly morework by expanding nodes at depth dunnecessarily.

Token 1951:
3.4.3 Depth-ﬁrst search Depth-ﬁrst search always expands the deepest node in the current frontier of the search tree.DEPTH-FIRST SEARCH The progress of the search is illustrated in Figure 3.16.

Token 1952:
The search proceeds immediately to the deepest level of the search tree, where the nodes have no successors.

Token 1953:
As those nodesare expanded, they are dropped from the frontier, so then the search “backs up” to the nextdeepest node that still has unexplored successors.

Token 1954:
The depth-ﬁrst search algorithm is an instance of the graph-search algorithm in Fig- ure 3.7; whereas breadth-ﬁrst-search uses a FIFO queue, depth-ﬁrst search uses a LIFO queue.

Token 1955:
A LIFO queue means that the most recently generated node is chosen for expansion.

Token 1956:
Thismust be the deepest unexpanded node because it is one deeper than its parent—which, in turn,was the deepest unexpanded node when it was selected.

Token 1957:
As an alternative to the G RAPH -SEARCH -style implementation, it is common to im- plement depth-ﬁrst search with a recursive function that calls itself on each of its children inturn.

Token 1958:
(A recursive depth-ﬁrst algorithm incorporating a depth limit is shown in Figure 3.17.)

Token 1959:
6NoOp , or “no operation,” is the name of an assembly language instruction that does nothing.

Token 1960:
7Here, and throughout the book, the “star” in C∗means an optimal value for C.

Token 1961:
86 Chapter 3.

Token 1962:
Solving Problems by Searching A C F G MNOA C F G LMNOA C F G LMNOC F G LMNOA B C E F G KLMNOA C E F G JKLMNOA C E F G JKLMNOA B C D E F G IJKLMNOA B C D E F G H IJKLMNOA B C D E F G H IJKLMNOA B C D E F G H IJKLMNOA B C D E F G H IJKLMNO Figure 3.16 Depth-ﬁrst search on a binary tree.

Token 1963:
The unexplored region is shown in light gray. Explored nodes with no descendants in the frontier are removed from memory.

Token 1964:
Nodes at depth 3 have no successors and Mis the only goal node.

Token 1965:
The properties of depth-ﬁrst search depend strongly on whether the graph-search or tree-search version is used.

Token 1966:
The graph-search version, which avoids repeated states and re-dundant paths, is complete in ﬁnite state spaces because it will eventually expand every node.The tree-search version, on the other hand, is notcomplete—for example, in Figure 3.6 the algorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever.

Token 1967:
Depth-ﬁrst tree search can bemodiﬁed at no extra memory cost so that it checks new states against those on the path fromthe root to the current node; this avoids inﬁnite loops in ﬁnite state spaces but does not avoidthe proliferation of redundant paths.

Token 1968:
In inﬁnite state spaces, both versions fail if an inﬁnitenon-goal path is encountered.

Token 1969:
For example, in Knuth’s 4 problem, depth-ﬁrst search wouldkeep applying the factorial operator forever.

Token 1970:
For similar reasons, both versions are nonoptimal.

Token 1971:
For example, in Figure 3.16, depth- ﬁrst search will explore the entire left subtree even if node Cis a goal node.

Token 1972:
If node Jwere also a goal node, then depth-ﬁrst search would return it as a solution instead of C,w h i c h would be a better solution; hence, depth-ﬁrst search is not optimal.

Token 1973:
Section 3.4.

Token 1974:
Uninformed Search Strategies 87 The time complexity of depth-ﬁrst graph search is bounded by the size of the state space (which may be inﬁnite, of course).

Token 1975:
A depth-ﬁrst tree search, on the other hand, may generatea l lo ft h e O(b m)nodes in the search tree, where mis the maximum depth of any node; this can be much greater than the size of the state space.

Token 1976:
Note that mitself can be much larger thand(the depth of the shallowest solution) and is inﬁnite if the tree is unbounded.

Token 1977:
So far, depth-ﬁrst search seems to have no clear advantage over breadth-ﬁrst search, so why do we include it? The reason is the space complexity.

Token 1978:
For a graph search, there isno advantage, but a depth-ﬁrst tree search needs to store only a single path from the rootto a leaf node, along with the remaining unexpanded sibling nodes for each node on thepath.

Token 1979:
Once a node has been expanded, it can be removed from memory as soon as all itsdescendants have been fully explored. (See Figure 3.16.)

Token 1980:
For a state space with branchingfactor band maximum depth m, depth-ﬁrst search requires storage of only O(bm)nodes.

Token 1981:
Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth asthe goal node have no successors, we ﬁnd that depth-ﬁrst search would require 156 kilobytesinstead of 10 exabytes at depth d=1 6 , a factor of 7 trillion times less space.

Token 1982:
This has led to the adoption of depth-ﬁrst tree search as the basic workhorse of many areas of AI,including constraint satisfaction (Chapter 6), propositional satisﬁability (Chapter 7), and logic programming (Chapter 9).

Token 1983:
For the remainder of this section, we focus primarily on the tree- search version of depth-ﬁrst search.

Token 1984:
A variant of depth-ﬁrst search called backtracking search uses still less memory. (See BACKTRACKING SEARCH Chapter 6 for more details.)

Token 1985:
In backtracking, only one successor is generated at a time rather than all successors; each partially expanded node remembers which successor to generate next.

Token 1986:
In this way, only O(m)memory is needed rather than O(bm).

Token 1987:
Backtracking search facilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc-cessor by modifying the current state description directly rather than copying it ﬁrst.

Token 1988:
This reduces the memory requirements to just one state description and O(m)actions.

Token 1989:
For this to work, we must be able to undo each modiﬁcation when we go back to generate the next suc-cessor.

Token 1990:
For problems with large state descriptions, such as robotic assembly, these techniques are critical to success.

Token 1991:
3.4.4 Depth-limited search The embarrassing failure of depth-ﬁrst search in inﬁnite state spaces can be alleviated by supplying depth-ﬁrst search with a predetermined depth limit /lscript.

Token 1992:
That is, nodes at depth /lscriptare treated as if they have no successors.

Token 1993:
This approach is called depth-limited search .T h eDEPTH-LIMITED SEARCH depth limit solves the inﬁnite-path problem.

Token 1994:
Unfortunately, it also introduces an additional source of incompleteness if we choose /lscript<d , that is, the shallowest goal is beyond the depth limit.

Token 1995:
(This is likely when dis unknown.) Depth-limited search will also be nonoptimal if we choose /lscript>d .

Token 1996:
Its time complexity is O(b/lscript)and its space complexity is O(b/lscript).

Token 1997:
Depth-ﬁrst search can be viewed as a special case of depth-limited search with /lscript=∞.

Token 1998:
Sometimes, depth limits can be based on knowledge of the problem. For example, on the map of Romania there are 20 cities.

Token 1999:
Therefore, we know that if there is a solution, it mustbe of length 19 at the longest, so /lscript=1 9 is a possible choice.

Token 2000:
But in fact if we studied the

Token 2001:
88 Chapter 3.

Token 2002:
Solving Problems by Searching function DEPTH -LIMITED -SEARCH (problem ,limit )returns a solution, or failure/cutoff return RECURSIVE -DLS(M AKE-NODE(problem .INITIAL -STATE ),problem ,limit ) function RECURSIVE -DLS( node ,problem ,limit )returns a solution, or failure/cutoff ifproblem .GOAL-TEST(node .STATE )then return SOLUTION (node ) else iflimit =0then return cutoﬀ else cutoﬀ occurred ?←false for each action inproblem .ACTIONS (node .STATE )do child←CHILD -NODE(problem ,node ,action ) result←RECURSIVE -DLS( child ,problem ,limit−1) ifresult =cutoﬀ thencutoﬀ occurred ?←true else ifresult/negationslash=failure then return result ifcutoﬀ occurred ?then return cutoﬀ else return failure Figure 3.17 A recursive implementation of depth-limited tree search.

Token 2003:
map carefully, we would discover that any city can be reached from any other city in at most 9 steps.

Token 2004:
This number, known as the diameter of the state space, gives us a better depth limit, DIAMETER which leads to a more efﬁcient depth-limited search.

Token 2005:
For most problems, however, we will not know a good depth limit until we have solved the problem.

Token 2006:
Depth-limited search can be implemented as a simple modiﬁcation to the general tree- or graph-search algorithm.

Token 2007:
Alternatively, it can be implemented as a simple recursive al-gorithm as shown in Figure 3.17.

Token 2008:
Notice that depth-limited search can terminate with twokinds of failure: the standard failure value indicates no solution; the cutoﬀ value indicates no solution within the depth limit.

Token 2009:
3.4.5 Iterative deepening depth-ﬁrst search Iterative deepening search (or iterative deepening depth-ﬁrst search) is a general strategy,ITERATIVE DEEPENING SEARCH often used in combination with depth-ﬁrst tree search, that ﬁnds the best depth limit.

Token 2010:
It does this by gradually increasing the limit—ﬁrst 0, then 1, then 2, and so on—until a goal is found.This will occur when the depth limit reaches d, the depth of the shallowest goal node.

Token 2011:
The algorithm is shown in Figure 3.18. Iterative deepening combines the beneﬁts of depth-ﬁrst and breadth-ﬁrst search.

Token 2012:
Like depth-ﬁrst search, its memory requirements are modest: O(bd) to be precise.

Token 2013:
Like breadth-ﬁrst search, it is complete when the branching factor is ﬁnite and optimal when the path cost is a nondecreasing function of the depth of the node.

Token 2014:
Figure 3.19shows four iterations of I TERATIVE -DEEPENING -SEARCH on a binary search tree, where the solution is found on the fourth iteration.

Token 2015:
Iterative deepening search may seem wasteful because states are generated multiple times. It turns out this is not too costly.

Token 2016:
The reason is that in a search tree with the same (or nearly the same) branching factor at each level, most of the nodes are in the bottom level,so it does not matter much that the upper levels are generated multiple times.

Token 2017:
In an iterativedeepening search, the nodes on the bottom level (depth d) are generated once, those on the

Token 2018:
Section 3.4.

Token 2019:
Uninformed Search Strategies 89 function ITERATIVE -DEEPENING -SEARCH (problem )returns a solution, or failure fordepth =0to∞do result←DEPTH -LIMITED -SEARCH (problem ,depth ) ifresult/negationslash=cutoff then return result Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depth- limited search with increasing limits.

Token 2020:
It terminates when a solution is found or if the depth- limited search returns failure , meaning that no solution exists.

Token 2021:
Limit = 3Limit = 2Limit = 1Limit = 0A A A B CA B CA B CA B C A B C D E F GA B C D E F GA B C D E F GA B C D E F G A B C D E F GA B C D E F GA B C D E F GA B C D E F G A B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H I JK L M N OA B C D E F G H JK L M N O IA B C D E F G H I JK L M N O Figure 3.19 Four iterations of iterative deepening search on a binary tree.

Token 2022:
90 Chapter 3.

Token 2023:
Solving Problems by Searching next-to-bottom level are generated twice, and so on, up to the children of the root, which are generated dtimes.

Token 2024:
So the total number of nodes generated in the worst case is N(IDS)=(d)b+(d−1)b2+···+( 1 )bd, which gives a time complexity of O(bd)—asymptotically the same as breadth-ﬁrst search.

Token 2025:
There is some extra cost for generating the upper levels multiple times, but it is not large.

Token 2026:
Forexample, if b=1 0 andd=5, the numbers are N(IDS) = 50 + 400 + 3 ,000 + 20 ,000 + 100 ,000 = 123 ,450 N(BFS) = 10 + 100 + 1 ,000 + 10 ,000 + 100 ,000 = 111 ,110.

Token 2027:
If you are really concerned about repeating the repetition, you can use a hybrid approach that runs breadth-ﬁrst search until almost all the available memory is consumed, and thenruns iterative deepening from all the nodes in the frontier.

Token 2028:
In general, iterative deepening is the preferred uninformed search method when the search space is large and the depth of the solution is not known.

Token 2029:
Iterative deepening search is analogous to breadth-ﬁrst search in that it explores a com- plete layer of new nodes at each iteration before going on to the next layer.

Token 2030:
It would seemworthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo-rithm’s optimality guarantees while avoiding its memory requirements.

Token 2031:
The idea is to useincreasing path-cost limits instead of increasing depth limits.

Token 2032:
The resulting algorithm, callediterative lengthening search , is explored in Exercise 3.17.

Token 2033:
It turns out, unfortunately, that ITERATIVE LENGTHENINGSEARCH iterative lengthening incurs substantial overhead compared to uniform-cost search.

Token 2034:
3.4.6 Bidirectional search The idea behind bidirectional search is to run two simultaneous searches—one forward from the initial state and the other backward from the goal—hoping that the two searches meet inthe middle (Figure 3.20).

Token 2035:
The motivation is that b d/2+bd/2is much less than bd,o ri nt h e ﬁgure, the area of the two small circles is less than the area of one big circle centered on the start and reaching to the goal.

Token 2036:
Bidirectional search is implemented by replacing the goal test with a check to see whether the frontiers of the two searches intersect; if they do, a solution has been found.

Token 2037:
(It is important to realize that the ﬁrst such solution found may not be optimal, even if thetwo searches are both breadth-ﬁrst; some additional search is required to make sure thereisn’t another short-cut across the gap.)

Token 2038:
The check can be done when each node is generatedor selected for expansion and, with a hash table, will take constant time.

Token 2039:
For example, if aproblem has solution depth d=6, and each direction runs breadth-ﬁrst search one node at a time, then in the worst case the two searches meet when they have generated all of the nodesat depth 3.

Token 2040:
For b=1 0 , this means a total of 2,220 node generations, compared with 1,111,110 for a standard breadth-ﬁrst search.

Token 2041:
Thus, the time complexity of bidirectional search using breadth-ﬁrst searches in both directions is O(b d/2). The space complexity is also O(bd/2).

Token 2042:
We can reduce this by roughly half if one of the two searches is done by iterative deepening,but at least one of the frontiers must be kept in memory so that the intersection check can bedone.

Token 2043:
This space requirement is the most signiﬁcant weakness of bidirectional search.

Token 2044:
Section 3.4.

Token 2045:
Uninformed Search Strategies 91 GoalStart Figure 3.20 A schematic view of a bidirectional search that is about to succeed when a branch from the start node meets a branch from the goal node.

Token 2046:
The reduction in time complexity makes bidirectional search attractive, but how do we search backward? This is not as easy as it sounds.

Token 2047:
Let the predecessors of a state xbe all PREDECESSOR those states that have xas a successor.

Token 2048:
Bidirectional search requires a method for computing predecessors.

Token 2049:
When all the actions in the state space are reversible, the predecessors of xare just its successors. Other cases may require substantial ingenuity.

Token 2050:
Consider the question of what we mean by “the goal” in searching “backward from the goal.” For the 8-puzzle and for ﬁnding a route in Romania, there is just one goal state, so thebackward search is very much like the forward search.

Token 2051:
If there are several explicitly listed goal states—for example, the two dirt-free goal states in Figure 3.3—then we can construct anew dummy goal state whose immediate predecessors are all the actual goal states.

Token 2052:
But if the goal is an abstract description, such as the goal that “no queen attacks another queen” in the n-queens problem, then bidirectional search is difﬁcult to use.

Token 2053:
3.4.7 Comparing uninformed search strategies Figure 3.21 compares search strategies in terms of the four evaluation criteria set forth in Section 3.3.2.

Token 2054:
This comparison is for tree-search versions.

Token 2055:
For graph searches, the maindifferences are that depth-ﬁrst search is complete for ﬁnite state spaces and that the space andtime complexities are bounded by the size of the state space.

Token 2056:
Criterion Breadth- Uniform- Depth- Depth- Iterative Bidirectional First Cost First Limited Deepening (if applicable) Complete?

Token 2057:
YesaYesa,bNo No YesaYesa,d Time O(bd) O(b1+⌊C∗//epsilon1⌋)O(bm) O(b/lscript) O(bd) O(bd/2) Space O(bd) O(b1+⌊C∗//epsilon1⌋)O(bm)O(b/lscript) O(bd) O(bd/2) Optimal?

Token 2058:
YescYes No No YescYesc,d Figure 3.21 Evaluation of tree-search strategies.

Token 2059:
bis the branching factor; dis the depth of the shallowest solution; mis the maximum depth of the search tree; lis the depth limit.

Token 2060:
Superscript caveats are as follows:acomplete if bis ﬁnite;bcomplete if step costs ≥/epsilon1for positive /epsilon1;coptimal if step costs are all identical;dif both directions use breadth-ﬁrst search.

Token 2061:
92 Chapter 3.

Token 2062:
Solving Problems by Searching 3.5 I NFORMED (HEURISTIC )SEARCH STRATEGIES This section shows how an informed search strategy—one that uses problem-speciﬁc knowl- INFORMED SEARCH edge beyond the deﬁnition of the problem itself—can ﬁnd solutions more efﬁciently than can an uninformed strategy.

Token 2063:
The general approach we consider is called best-ﬁrst search .

Token 2064:
Best-ﬁrst search is an BEST-FIRST SEARCH instance of the general T REE-SEARCH or G RAPH -SEARCH algorithm in which a node is selected for expansion based on an evaluation function ,f(n).

Token 2065:
The evaluation function isEVALUATION FUNCTION construed as a cost estimate, so the node with the lowest evaluation is expanded ﬁrst.

Token 2066:
The implementation of best-ﬁrst graph search is identical to that for uniform-cost search (Fig- ure 3.14), except for the use of finstead of gto order the priority queue.

Token 2067:
The choice of fdetermines the search strategy.

Token 2068:
(For example, as Exercise 3.21 shows, best-ﬁrst tree search includes depth-ﬁrst search as a special case.)

Token 2069:
Most best-ﬁrst algorithmsinclude as a component of faheuristic function , denoted h(n): HEURISTIC FUNCTION h(n)= estimated cost of the cheapest path from the state at node nto a goal state.

Token 2070:
(Notice that h(n)takes a node as input, but, unlike g(n), it depends only on the state at that node.)

Token 2071:
For example, in Romania, one might estimate the cost of the cheapest path from Arad to Bucharest via the straight-line distance from Arad to Bucharest.

Token 2072:
Heuristic functions are the most common form in which additional knowledge of the problem is imparted to the search algorithm.

Token 2073:
We study heuristics in more depth in Section 3.6.For now, we consider them to be arbitrary, nonnegative, problem-speciﬁc functions, with oneconstraint: if nis a goal node, then h(n)=0 .

Token 2074:
The remainder of this section covers two ways to use heuristic information to guide search.

Token 2075:
3.5.1 Greedy best-ﬁrst search Greedy best-ﬁrst search8tries to expand the node that is closest to the goal, on the groundsGREEDY BEST-FIRST SEARCH that this is likely to lead to a solution quickly.

Token 2076:
Thus, it evaluates nodes by using just the heuristic function; that is, f(n)=h(n).

Token 2077:
Let us see how this works for route-ﬁnding problems in Romania; we use the straight- line distance heuristic, which we will call hSLD.

Token 2078:
If the goal is Bucharest, we need toSTRAIGHT-LINE DISTANCE know the straight-line distances to Bucharest, which are shown in Figure 3.22.

Token 2079:
For exam- ple,hSLD(In(Arad))= 366 . Notice that the values of hSLD cannot be computed from the problem description itself.

Token 2080:
Moreover, it takes a certain amount of experience to know that hSLD is correlated with actual road distances and is, therefore, a useful heuristic.

Token 2081:
Figure 3.23 shows the progress of a greedy best-ﬁrst search using hSLD to ﬁnd a path from Arad to Bucharest.

Token 2082:
The ﬁrst node to be expanded from Arad will be Sibiu because itis closer to Bucharest than either Zerind or Timisoara.

Token 2083:
The next node to be expanded willbe Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal.

Token 2084:
Forthis particular problem, greedy best-ﬁrst search using h SLD ﬁnds a solution without ever 8Our ﬁrst edition called this greedy search ; other authors have called it best-ﬁrst search .

Token 2085:
Our more general usage of the latter term follows Pearl (1984).

Token 2086:
Section 3.5.

Token 2087:
Informed (Heuristic) Search Strategies 93 UrziceniNeamt Oradea ZerindTimisoaraMehadia SibiuPitesti Rimnicu Vilcea VasluiBucharest Giurgiu HirsovaEforieArad LugojDrobetaCraiova Fagaras Iasi 0 160 242161 77 151366 244226176241 253 329 80 199380234 374100 193 Figure 3.22 Values of hSLD—straight-line distances to Bucharest.

Token 2088:
expanding a node that is not on the solution path; hence, its search cost is minimal.

Token 2089:
It is not optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longerthan the path through Rimnicu Vilcea and Pitesti.

Token 2090:
This shows why the algorithm is called“greedy”—at each step it tries to get as close to the goal as it can.

Token 2091:
Greedy best-ﬁrst tree search is also incomplete even in a ﬁnite state space, much like depth-ﬁrst search.

Token 2092:
Consider the problem of getting from Iasi to Fagaras.

Token 2093:
The heuristic sug-gests that Neamt be expanded ﬁrst because it is closest to Fagaras, but it is a dead end.

Token 2094:
Thesolution is to go ﬁrst to Vaslui—a step that is actually farther from the goal according to the heuristic—and then to continue to Urziceni, Bucharest, and Fagaras.

Token 2095:
The algorithm will never ﬁnd this solution, however, because expanding Neamt puts Iasi back into the frontier,Iasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an inﬁ-nite loop.

Token 2096:
(The graph search version iscomplete in ﬁnite spaces, but not in inﬁnite ones.)

Token 2097:
The worst-case time and space complexity for the tree version is O(b m),w h e r e mis the maximum depth of the search space.

Token 2098:
With a good heuristic function, however, the complexity can bereduced substantially.

Token 2099:
The amount of the reduction depends on the particular problem and onthe quality of the heuristic.

Token 2100:
3.5.2 A* search: Minimizing the total estimated solution cost The most widely known form of best-ﬁrst search is called A∗search (pronounced “A-star A∗SEARCH search”).

Token 2101:
It evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost to get from the node to the goal: f(n)=g(n)+h(n).

Token 2102:
Since g(n)gives the path cost from the start node to node n,a n dh(n)is the estimated cost of the cheapest path from nto the goal, we have f(n)= estimated cost of the cheapest solution through n. Thus, if we are trying to ﬁnd the cheapest solution, a reasonable thing to try ﬁrst is the node with the lowest value of g(n)+h(n).

Token 2103:
It turns out that this strategy is more than just reasonable: provided that the heuristic function h(n)satisﬁes certain conditions, A∗search is both complete and optimal.

Token 2104:
The algorithm is identical to U NIFORM -COST-SEARCH except that A∗usesg+hinstead of g.

Token 2105:
94 Chapter 3.

Token 2106:
Solving Problems by Searching Rimnicu VilceaZerindArad Sibiu Arad Fagaras OradeaTimisoara Sibiu Bucharest329 374 366 380 193 253 0Rimnicu VilceaArad Sibiu Arad Fagaras OradeaTimisoara 329Zerind 374 366 176 380 193ZerindArad Sibiu Timisoara 253 329 374Arad 366(a) The initial state (b) After expanding Arad (c) After expanding Sibiu (d) After expanding Fagaras Figure 3.23 Stages in a greedy best-ﬁrst tree search for Bucharest with the straight-line distance heuristic hSLD.

Token 2107:
Nodes are labeled with their h-values.

Token 2108:
Conditions for optimality: Admissibility and consistency The ﬁrst condition we require for optimality is that h(n)be an admissible heuristic .A nADMISSIBLE HEURISTIC admissible heuristic is one that never overestimates the cost to reach the goal.

Token 2109:
Because g(n) is the actual cost to reach nalong the current path, and f(n)=g(n)+h(n),w eh a v ea sa n immediate consequence that f(n)never overestimates the true cost of a solution along the current path through n. Admissible heuristics are by nature optimistic because they think the cost of solving the problem is less than it actually is.

Token 2110:
An obvious example of an admissible heuristic is thestraight-line distance h SLD that we used in getting to Bucharest.

Token 2111:
Straight-line distance is admissible because the shortest path between any two points is a straight line, so the straight

Token 2112:
Section 3.5. Informed (Heuristic) Search Strategies 95 line cannot be an overestimate.

Token 2113:
In Figure 3.24, we show the progress of an A∗tree search for Bucharest.

Token 2114:
The values of gare computed from the step costs in Figure 3.2, and the values of hSLDare given in Figure 3.22.

Token 2115:
Notice in particular that Bucharest ﬁrst appears on the frontier at step (e), but it is not selected for expansion because its f-cost (450) is higher than that of Pitesti (417).

Token 2116:
Another way to say this is that there might be a solution through Pitesti whose cost is as low as 417, so the algorithm will not settle for a solution that costs 450.

Token 2117:
A second, slightly stronger condition called consistency (or sometimes monotonicity ) CONSISTENCY MONOTONICITY is required only for applications of A∗to graph search.9A heuristic h(n)is consistent if, for every node nand every successor n/primeofngenerated by any action a, the estimated cost of reaching the goal from nis no greater than the step cost of getting to n/primeplus the estimated cost of reaching the goal from n/prime: h(n)≤c(n,a,n/prime)+h(n/prime).

Token 2118:
This is a form of the general triangle inequality , which stipulates that each side of a triangleTRIANGLE INEQUALITY cannot be longer than the sum of the other two sides.

Token 2119:
Here, the triangle is formed by n,n/prime, and the goal Gnclosest to n. For an admissible heuristic, the inequality makes perfect sense: if there were a route from ntoGnvian/primethat was cheaper than h(n), that would violate the property that h(n)is a lower bound on the cost to reach Gn.

Token 2120:
It is fairly easy to show (Exercise 3.29) that every consistent heuristic is also admissible.

Token 2121:
Consistency is therefore a stricter requirement than admissibility, but one has to work quitehard to concoct heuristics that are admissible but not consistent.

Token 2122:
All the admissible heuristicswe discuss in this chapter are also consistent. Consider, for example, h SLD.

Token 2123:
We know that the general triangle inequality is satisﬁed when each side is measured by the straight-linedistance and that the straight-line distance between nandn /primeis no greater than c(n,a,n/prime).

Token 2124:
Hence, hSLD is a consistent heuristic.

Token 2125:
Optimality of A* As we mentioned earlier, A∗has the following properties: the tree-search version of A∗is optimal if h(n)is admissible, while the graph-search version is optimal if h(n)is consistent.

Token 2126:
We show the second of these two claims since it is more useful.

Token 2127:
The argument es- sentially mirrors the argument for the optimality of uniform-cost search, with greplaced by f—just as in the A∗algorithm itself.

Token 2128:
The ﬁrst step is to establish the following: ifh(n)is consistent, then the values of f(n)along any path are nondecreasing.

Token 2129:
The proof follows directly from the deﬁnition of consistency.

Token 2130:
Suppose n/primeis a successor of n;t h e n g(n/prime)=g(n)+c(n,a,n/prime)for some action a, and we have f(n/prime)=g(n/prime)+h(n/prime)=g(n)+c(n,a,n/prime)+h(n/prime)≥g(n)+h(n)=f(n).

Token 2131:
The next step is to prove that whenever A∗selects a node nfor expansion, the optimal path to that node has been found.

Token 2132:
Were this not the case, there would have to be another frontier noden/primeon the optimal path from the start node to n, by the graph separation property of 9With an admissible but inconsistent heuristic, A∗requires some extra bookkeeping to ensure optimality.

Token 2133:
96 Chapter 3.

Token 2134:
Solving Problems by Searching (a) The initial state (b) After expanding Arad (c) After expanding SibiuArad Sibiu Timisoara 447=118+329Zerind 449=75+374 393=140+253Arad 366=0+366 (d) After expanding Rimnicu Vilcea (e) After expanding Fagaras (f) After expanding PitestiZerindArad Sibiu AradTimisoara Rimnicu Vilcea Fagaras Oradea447=118+329 449=75+374 646=280+366 413=220+193 415=239+176 671=291+380 ZerindArad Sibiu Timisoara 447=118+329 449=75+374 Rimnicu Vilcea Craiova Pitesti Sibiu 526=366+160 553=300+253 417=317+100 ZerindArad Sibiu AradTimisoara Sibiu BucharestFagaras Oradea Craiova Pitesti Sibiu447=118+329 449=75+374 646=280+366 591=338+253 450=450+0 526=366+160 553=300+253 417=317+100671=291+380 ZerindArad Sibiu AradTimisoara Sibiu BucharestOradea Craiova Pitesti Sibiu Bucharest Craiova Rimnicu Vilcea 418=418+0447=118+329 449=75+374 646=280+366 591=338+253 450=450+0 526=366+160 553=300+253 615=455+160 607=414+193671=291+380Rimnicu Vilcea Fagaras Rimnicu VilceaArad Fagaras Oradea 646=280+366 415=239+176 671=291+380 Figure 3.24 Stages in an A∗search for Bucharest.

Token 2135:
Nodes are labeled with f=g+h.T h e hvalues are the straight-line distances to Bucharest taken from Figure 3.22.

Token 2136:
Section 3.5.

Token 2137:
Informed (Heuristic) Search Strategies 97 O Z A T L M D CRF P GBUH EVIN 380 400 420S Figure 3.25 Map of Romania showing contours at f= 380 ,f= 400 ,a n df= 420 , with Arad as the start state.

Token 2138:
Nodes inside a given contour have f-costs less than or equal to the contour value.

Token 2139:
Figure 3.9; because fis nondecreasing along any path, n/primewould have lower f-cost than n and would have been selected ﬁrst.

Token 2140:
From the two preceding observations, it follows that the sequence of nodes expanded by A∗using G RAPH -SEARCH is in nondecreasing order of f(n).

Token 2141:
Hence, the ﬁrst goal node selected for expansion must be an optimal solution because fis the true cost for goal nodes (which have h=0) and all later goal nodes will be at least as expensive.

Token 2142:
The fact that f-costs are nondecreasing along any path also means that we can draw contours in the state space, just like the contours in a topographic map.

Token 2143:
Figure 3.25 shows CONTOUR an example. Inside the contour labeled 400, all nodes have f(n)less than or equal to 400, and so on.

Token 2144:
Then, because A∗expands the frontier node of lowest f-cost, we can see that an A∗search fans out from the start node, adding nodes in concentric bands of increasing f-cost.

Token 2145:
With uniform-cost search (A∗search using h(n)=0 ), the bands will be “circular” around the start state.

Token 2146:
With more accurate heuristics, the bands will stretch toward the goal state and become more narrowly focused around the optimal path.

Token 2147:
If C∗is the cost of the optimal solution path, then we can say the following: •A∗expands all nodes with f(n)<C∗.

Token 2148:
•A∗might then expand some of the nodes right on the “goal contour” (where f(n)=C∗) before selecting a goal node.

Token 2149:
Completeness requires that there be only ﬁnitely many nodes with cost less than or equal to C∗, a condition that is true if all step costs exceed some ﬁnite /epsilon1and if bis ﬁnite.

Token 2150:
Notice that A∗expands no nodes with f(n)>C∗—for example, Timisoara is not expanded in Figure 3.24 even though it is a child of the root.

Token 2151:
We say that the subtree below

Token 2152:
98 Chapter 3.

Token 2153:
Solving Problems by Searching Timisoara is pruned ; because hSLDis admissible, the algorithm can safely ignore this subtree PRUNING while still guaranteeing optimality.

Token 2154:
The concept of pruning—eliminating possibilities from consideration without having to examine them—is important for many areas of AI.

Token 2155:
One ﬁnal observation is that among optimal algorithms of this type—algorithms that extend search paths from the root and use the same heuristic information—A∗isoptimally efﬁcient for any given consistent heuristic.

Token 2156:
That is, no other optimal algorithm is guaran-OPTIMALLY EFFICIENT teed to expand fewer nodes than A∗(except possibly through tie-breaking among nodes with f(n)=C∗).

Token 2157:
This is because any algorithm that does not expand all nodes with f(n)<C∗ runs the risk of missing the optimal solution.

Token 2158:
That A∗search is complete, optimal, and optimally efﬁcient among all such algorithms is rather satisfying.

Token 2159:
Unfortunately, it does not mean that A∗is the answer to all our searching needs.

Token 2160:
The catch is that, for most problems, the number of states within the goal contoursearch space is still exponential in the length of the solution.

Token 2161:
The details of the analysis arebeyond the scope of this book, but the basic results are as follows.

Token 2162:
For problems with constantstep costs, the growth in run time as a function of the optimal solution depth dis analyzed in terms of the the absolute error or the relative error of the heuristic.

Token 2163:
The absolute error is ABSOLUTE ERROR RELATIVE ERROR deﬁned as Δ≡h∗−h,w h e r e h∗is the actual cost of getting from the root to the goal, and the relative error is deﬁned as /epsilon1≡(h∗−h)/h∗.

Token 2164:
The complexity results depend very strongly on the assumptions made about the state space.

Token 2165:
The simplest model studied is a state space that has a single goal and is essentially atree with reversible actions.

Token 2166:
(The 8-puzzle satisﬁes the ﬁrst and third of these assumptions.

Token 2167:
)In this case, the time complexity of A ∗is exponential in the maximum absolute error, that is, O(bΔ).

Token 2168:
For constant step costs, we can write this as O(b/epsilon1d),w h e r e dis the solution depth.

Token 2169:
For almost all heuristics in practical use, the absolute error is at least proportional to the pathcosth ∗,s o/epsilon1is constant or growing and the time complexity is exponential in d. We can also see the effect of a more accurate heuristic: O(b/epsilon1d)=O((b/epsilon1)d), so the effective branching factor (deﬁned more formally in the next section) is b/epsilon1.

Token 2170:
When the state space has many goal states—particularly near-optimal goal states—the search process can be led astray from the optimal path and there is an extra cost proportional to the number of goals whose cost is within a factor /epsilon1of the optimal cost.

Token 2171:
Finally, in the general case of a graph, the situation is even worse.

Token 2172:
There can be exponentially many stateswithf(n)<C ∗even if the absolute error is bounded by a constant.

Token 2173:
For example, consider a version of the vacuum world where the agent can clean up any square for unit cost withouteven having to visit it: in that case, squares can be cleaned in any order.

Token 2174:
With Ninitially dirty squares, there are 2 Nstates where some subset has been cleaned and all of them are on an optimal solution path—and hence satisfy f(n)<C∗—even if the heuristic has an error of 1.

Token 2175:
The complexity of A∗often makes it impractical to insist on ﬁnding an optimal solution.

Token 2176:
One can use variants of A∗that ﬁnd suboptimal solutions quickly, or one can sometimes design heuristics that are more accurate but not strictly admissible.

Token 2177:
In any case, the use of a good heuristic still provides enormous savings compared to the use of an uninformed search.

Token 2178:
In Section 3.6, we look at the question of designing good heuristics. Computation time is not, however, A∗’s main drawback.

Token 2179:
Because it keeps all generated nodes in memory (as do all G RAPH -SEARCH algorithms), A∗usually runs out of space long

Token 2180:
Section 3.5.

Token 2181:
Informed (Heuristic) Search Strategies 99 function RECURSIVE -BEST-FIRST-SEARCH (problem )returns a solution, or failure return RBFS( problem ,MAKE-NODE(problem .INITIAL -STATE ),∞) function RBFS( problem ,node ,f limit )returns a solution, or failure and a new f-cost limit ifproblem .GOAL-TEST(node .STATE )then return SOLUTION (node ) successors←[] for each action inproblem .ACTIONS (node .STATE )do add C HILD -NODE(problem ,node ,action )i n t osuccessors ifsuccessors is empty then return failure ,∞ for each sinsuccessors do/* update fwith value from previous search, if any */ s.f←max(s.g+s.h,node.f)) loop do best←the lowest f-value node in successors ifbest.f > f limit then return failure ,best.f alternative←the second-lowest f-value among successors result ,best.f←RBFS( problem ,best,min(f limit,alternative )) ifresult/negationslash=failure then return result Figure 3.26 The algorithm for recursive best-ﬁrst search.

Token 2182:
before it runs out of time. For this reason, A∗is not practical for many large-scale prob- lems.

Token 2183:
There are, however, algorithms that overcome the space problem without sacriﬁcingoptimality or completeness, at a small cost in execution time.

Token 2184:
We discuss these next.

Token 2185:
3.5.3 Memory-bounded heuristic search The simplest way to reduce memory requirements for A∗is to adapt the idea of iterative deepening to the heuristic search context, resulting in the iterative-deepening A∗(IDA∗)a l -ITERATIVE- DEEPENING A∗ gorithm.

Token 2186:
The main difference between IDA∗and standard iterative deepening is that the cutoff used is the f-cost (g+h) rather than the depth; at each iteration, the cutoff value is the small- estf-cost of any node that exceeded the cutoff on the previous iteration.

Token 2187:
IDA∗is practical for many problems with unit step costs and avoids the substantial overhead associated withkeeping a sorted queue of nodes.

Token 2188:
Unfortunately, it suffers from the same difﬁculties with real-valued costs as does the iterative version of uniform-cost search described in Exercise 3.17.This section brieﬂy examines two other memory-bounded algorithms, called RBFS and MA ∗.

Token 2189:
Recursive best-ﬁrst search (RBFS) is a simple recursive algorithm that attempts toRECURSIVE BEST-FIRST SEARCH mimic the operation of standard best-ﬁrst search, but using only linear space.

Token 2190:
The algorithm is shown in Figure 3.26.

Token 2191:
Its structure is similar to that of a recursive depth-ﬁrst search, but rather than continuing indeﬁnitely down the current path, it uses the f limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node.

Token 2192:
If the current node exceeds this limit, the recursion unwinds back to the alternative path.

Token 2193:
As the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value —the best f-value of its children.

Token 2194:
In this way, RBFS remembers the BACKED-UP VALUE f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth

Token 2195:
100 Chapter 3.

Token 2196:
Solving Problems by Searching ZerindArad Sibiu Arad Fagaras Oradea Craiova Sibiu Bucharest Craiova Rimnicu VilceaZerindArad Sibiu Arad Sibiu BucharestRimnicu Vilcea OradeaZerindArad Sibiu Arad TimisoaraTimisoaraTimisoara Fagaras Oradea Rimnicu Vilcea Craiova Pitesti Sibiu646 415 671 526 553 646 671 450 591 646 671 526 553 418 615 607447 449447447 449 449366 393 366 393413 413 417 415 366 393 415 450417Rimnicu VilceaFagaras447 415 447447 417(a) After expanding Arad, Sibiu, and Rimnicu Vilcea (c) After switching back to Rimnicu Vilcea and expanding Pitesti(b) After unwinding back to Sibiu and expanding Fagaras 447 447∞ ∞ ∞ 417417 Pitesti Figure 3.27 Stages in an RBFS search for the shortest route to Bucharest.

Token 2197:
The f-limit value for each recursive call is shown on top of each current node, and every node is labeledwith its f-cost.

Token 2198:
(a) The path via Rimnicu Vilcea is followed until the current best leaf (Pitesti) has a value that is worse than the best alternative path (Fagaras).

Token 2199:
(b) The recursion unwinds and the best leaf value of the forgotten subtree (417) is backed up to Rimnicu Vilcea; thenFagaras is expanded, revealing a best leaf value of 450.

Token 2200:
(c) The recursion unwinds and the best leaf value of the forgotten subtree (450) is backed up to Fagaras; then Rimnicu Vilcea is expanded.

Token 2201:
This time, because the best alternative path (through Timisoara) costs at least 447, the expansion continues to Bucharest.

Token 2202:
reexpanding the subtree at some later time. Figure 3.27 shows how RBFS reaches Bucharest.

Token 2203:
RBFS is somewhat more efﬁcient than IDA∗, but still suffers from excessive node re- generation.

Token 2204:
In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then

Token 2205:
Section 3.5. Informed (Heuristic) Search Strategies 101 “changes its mind” and tries Fagaras, and then changes its mind back again.

Token 2206:
These mind changes occur because every time the current best path is extended, its f-value is likely to increase— his usually less optimistic for nodes closer to the goal.

Token 2207:
When this happens, the second-best path might become the best path, so the search has to backtrack to follow it.Each mind change corresponds to an iteration of IDA ∗and could require many reexpansions of forgotten nodes to recreate the best path and extend it one more node.

Token 2208:
Like A∗tree search, RBFS is an optimal algorithm if the heuristic function h(n)is admissible.

Token 2209:
Its space complexity is linear in the depth of the deepest optimal solution, butits time complexity is rather difﬁcult to characterize: it depends both on the accuracy of theheuristic function and on how often the best path changes as nodes are expanded.

Token 2210:
IDA ∗and RBFS suffer from using too little memory. Between iterations, IDA∗retains only a single number: the current f-cost limit.

Token 2211:
RBFS retains more information in memory, but it uses only linear space: even if more memory were available, RBFS has no way to makeuse of it.

Token 2212:
Because they forget most of what they have done, both algorithms may end up reex-panding the same states many times over.

Token 2213:
Furthermore, they suffer the potentially exponentialincrease in complexity associated with redundant paths in graphs (see Section 3.3).

Token 2214:
It seems sensible, therefore, to use all available memory. Two algorithms that do this areMA ∗(memory-bounded A∗)a n d SMA∗(simpliﬁed MA∗).

Token 2215:
SMA∗is—well—simpler, so MA* SMA* we will describe it. SMA∗proceeds just like A∗, expanding the best leaf until memory is full.

Token 2216:
At this point, it cannot add a new node to the search tree without dropping an old one.

Token 2217:
SMA∗ always drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA∗ then backs up the value of the forgotten node to its parent.

Token 2218:
In this way, the ancestor of a forgotten subtree knows the quality of the best path in that subtree.

Token 2219:
With this information,SMA ∗regenerates the subtree only when all other paths have been shown to look worse than the path it has forgotten.

Token 2220:
Another way of saying this is that, if all the descendants of a node n are forgotten, then we will not know which way to go from n, but we will still have an idea of how worthwhile it is to go anywhere from n. The complete algorithm is too complicated to reproduce here,10but there is one subtlety worth mentioning.

Token 2221:
We said that SMA∗expands the best leaf and deletes the worst leaf. What ifallthe leaf nodes have the same f-value?

Token 2222:
To avoid selecting the same node for deletion and expansion, SMA∗expands the newest best leaf and deletes the oldest worst leaf.

Token 2223:
These coincide when there is only one leaf, but in that case, the current search tree must be a singlepath from root to leaf that ﬁlls all of memory.

Token 2224:
If the leaf is not a goal node, then even if it is on an optimal solution path , that solution is not reachable with the available memory.

Token 2225:
Therefore, the node can be discarded exactly as if it had no successors.

Token 2226:
SMA ∗is complete if there is any reachable solution—that is, if d, the depth of the shallowest goal node, is less than the memory size (expressed in nodes).

Token 2227:
It is optimal if anyoptimal solution is reachable; otherwise, it returns the best reachable solution.

Token 2228:
In practicalterms, SMA ∗is a fairly robust choice for ﬁnding optimal solutions, particularly when the state space is a graph, step costs are not uniform, and node generation is expensive compared to the overhead of maintaining the frontier and the explored set.

Token 2229:
10A rough sketch appeared in the ﬁrst edition of this book.

Token 2230:
102 Chapter 3.

Token 2231:
Solving Problems by Searching On very hard problems, however, it will often be the case that SMA∗is forced to switch back and forth continually among many candidate solution paths, only a small subset of whichcan ﬁt in memory.

Token 2232:
(This resembles the problem of thrashing in disk paging systems.)

Token 2233:
Then THRASHING the extra time required for repeated regeneration of the same nodes means that problems that would be practically solvable by A∗, given unlimited memory, become intractable for SMA∗.

Token 2234:
That is to say, memory limitations can make a problem intractable from the point of view of computation time.

Token 2235:
Although no current theory explains the tradeoff between time and memory, it seems that this is an inescapable problem.

Token 2236:
The only way out is to drop theoptimality requirement.

Token 2237:
3.5.4 Learning to search better We have presented several ﬁxed strategies—breadth-ﬁrst, greedy best-ﬁrst, and so on—that have been designed by computer scientists.

Token 2238:
Could an agent learn how to search better?

Token 2239:
The answer is yes, and the method rests on an important concept called the metalevel state space .METALEVEL STATE SPACE Each state in a metalevel state space captures the internal (computational) state of a program that is searching in an object-level state space such as Romania.

Token 2240:
For example, the internalOBJECT-LEVEL STATE SPACE state of the A∗algorithm consists of the current search tree.

Token 2241:
Each action in the metalevel state space is a computation step that alters the internal state; for example, each computation stepin A ∗expands a leaf node and adds its successors to the tree.

Token 2242:
Thus, Figure 3.24, which shows a sequence of larger and larger search trees, can be seen as depicting a path in the metalevelstate space where each state on the path is an object-level search tree.

Token 2243:
Now, the path in Figure 3.24 has ﬁve steps, including one step, the expansion of Fagaras, that is not especially helpful.

Token 2244:
For harder problems, there will be many such missteps, and a metalevel learning algorithm can learn from these experiences to avoid exploring unpromis- METALEVEL LEARNING ing subtrees.

Token 2245:
The techniques used for this kind of learning are described in Chapter 21.

Token 2246:
The goal of learning is to minimize the total cost of problem solving, trading off computational expense and path cost.

Token 2247:
3.6 H EURISTIC FUNCTIONS In this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature ofheuristics in general.

Token 2248:
The 8-puzzle was one of the earliest heuristic search problems.

Token 2249:
As mentioned in Sec- tion 3.2, the object of the puzzle is to slide the tiles horizontally or vertically into the emptyspace until the conﬁguration matches the goal conﬁguration (Figure 3.28).

Token 2250:
The average solution cost for a randomly generated 8-puzzle instance is about 22 steps. The branching factor is about 3.

Token 2251:
(When the empty tile is in the middle, four moves are possible; when it is in a corner, two; and when it is along an edge, three.)

Token 2252:
This means that an exhaustive tree search to depth 22 would look at about 3 22≈3.1×1010states.

Token 2253:
A graph search would cut this down by a factor of about 170,000 because only 9!/2= 181,440distinct states are reachable. (See Exercise 3.4.)

Token 2254:
This is a manageable number, but

Token 2255:
Section 3.6. Heuristic Functions 103 2 Start State Goal State1 3 4 6 75 12 34 67 85 8 Figure 3.28 A typical instance of the 8-puzzle.

Token 2256:
The solution is 26 steps long.

Token 2257:
the corresponding number for the 15-puzzle is roughly 1013, so the next order of business is to ﬁnd a good heuristic function.

Token 2258:
If we want to ﬁnd the shortest solutions by using A∗,w e need a heuristic function that never overestimates the number of steps to the goal.

Token 2259:
There is along history of such heuristics for the 15-puzzle; here are two commonly used candidates: •h 1=the number of misplaced tiles.

Token 2260:
For Figure 3.28, all of the eight tiles are out of position, so the start state would have h1=8.h1is an admissible heuristic because it is clear that any tile that is out of place must be moved at least once.

Token 2261:
•h2=the sum of the distances of the tiles from their goal positions.

Token 2262:
Because tiles cannot move along diagonals, the distance we will count is the sum of the horizontaland vertical distances.

Token 2263:
This is sometimes called the city block distance orManhattan distance .h 2is also admissible because all any move can do is move one tile one stepMANHATTAN DISTANCE closer to the goal.

Token 2264:
Tiles 1 to 8 in the start state give a Manhattan distance of h2=3+1+2+2+2+3+3+2=1 8 .

Token 2265:
As expected, neither of these overestimates the true solution cost, which is 26.

Token 2266:
3.6.1 The effect of heuristic accuracy on performance One way to characterize the quality of a heuristic is the effective branching factor b∗.I ft h eEFFECTIVE BRANCHINGFACTOR total number of nodes generated by A∗for a particular problem is Nand the solution depth is d,t h e n b∗is the branching factor that a uniform tree of depth dw o u l dh a v et oh a v ei no r d e r to contain N+1nodes.

Token 2267:
Thus, N+1=1+ b∗+(b∗)2+···+(b∗)d. For example, if A∗ﬁnds a solution at depth 5 using 52 nodes, then the effective branching factor is 1.92.

Token 2268:
The effective branching factor can vary across problem instances, but usually it is fairly constant for sufﬁciently hard problems.

Token 2269:
(The existence of an effective branching factor follows from the result, mentioned earlier, that the number of nodes expanded by A∗ grows exponentially with solution depth.)

Token 2270:
Therefore, experimental measurements of b∗on a small set of problems can provide a good guide to the heuristic’s overall usefulness.

Token 2271:
A well- designed heuristic would have a value of b∗close to 1, allowing fairly large problems to be solved at reasonable computational cost.

Token 2272:
104 Chapter 3.

Token 2273:
Solving Problems by Searching To test the heuristic functions h1andh2, we generated 1200 random problems with solution lengths from 2 to 24 (100 for each even number) and solved them with iterativedeepening search and with A ∗tree search using both h1andh2.

Token 2274:
Figure 3.29 gives the average number of nodes generated by each strategy and the effective branching factor.

Token 2275:
The resultssuggest that h 2is better than h1, and is far better than using iterative deepening search.

Token 2276:
Even for small problems with d=1 2 ,A∗withh2is 50,000 times more efﬁcient than uninformed iterative deepening search.

Token 2277:
Search Cost (nodes generated) Effective Branching Factor d IDS A∗(h1) A∗(h2) IDS A∗(h1) A∗(h2) 2 10 6 6 2.45 1.79 1.79 4 112 13 12 2.87 1.48 1.45 6 680 20 18 2.73 1.34 1.30 8 6384 39 25 2.80 1.33 1.24 10 47127 93 39 2.79 1.38 1.22 12 3644035 227 73 2.78 1.42 1.24 14 – 539 113 – 1.44 1.23 16 – 1301 211 – 1.45 1.25 18 – 3056 363 – 1.46 1.26 20 – 7276 676 – 1.47 1.27 22 – 18094 1219 – 1.48 1.28 24 – 39135 1641 – 1.48 1.26 Figure 3.29 Comparison of the search costs and effective branching factors for the ITERATIVE -DEEPENING -SEARCH and A∗algorithms with h1,h2.

Token 2278:
Data are averaged over 100 instances of the 8-puzzle for each of various solution lengths d. One might ask whether h2isalways better than h1.

Token 2279:
The answer is “Essentially, yes.” It is easy to see from the deﬁnitions of the two heuristics that, for any node n,h2(n)≥h1(n).

Token 2280:
We thus say that h2dominates h1.

Token 2281:
Domination translates directly into efﬁciency: A∗using DOMINATION h2will never expand more nodes than A∗using h1(except possibly for some nodes with f(n)=C∗).

Token 2282:
The argument is simple. Recall the observation on page 97 that every node withf(n)<C∗will surely be expanded.

Token 2283:
This is the same as saying that every node with h(n)<C∗−g(n)will surely be expanded.

Token 2284:
But because h2is at least as big as h1for all nodes, every node that is surely expanded by A∗search with h2will also surely be expanded withh1,a n dh1might cause other nodes to be expanded as well.

Token 2285:
Hence, it is generally better to use a heuristic function with higher values, provided it is consistent and that thecomputation time for the heuristic is not too long.

Token 2286:
3.6.2 Generating admissible heuristics from relaxed problems We have seen that both h1(misplaced tiles) and h2(Manhattan distance) are fairly good heuristics for the 8-puzzle and that h2is better.

Token 2287:
How might one have come up with h2?I si t possible for a computer to invent such a heuristic mechanically?

Token 2288:
h1andh2are estimates of the remaining path length for the 8-puzzle, but they are also perfectly accurate path lengths for simpliﬁed versions of the puzzle.

Token 2289:
If the rules of the puzzle

Token 2290:
Section 3.6.

Token 2291:
Heuristic Functions 105 were changed so that a tile could move anywhere instead of just to the adjacent empty square, thenh1would give the exact number of steps in the shortest solution.

Token 2292:
Similarly, if a tile could move one square in any direction, even onto an occupied square, then h2would give the exact number of steps in the shortest solution.

Token 2293:
A problem with fewer restrictions on the actions iscalled a relaxed problem .

Token 2294:
The state-space graph of the relaxed problem is a supergraph of RELAXED PROBLEM the original state space because the removal of restrictions creates added edges in the graph.

Token 2295:
Because the relaxed problem adds edges to the state space, any optimal solution in the original problem is, by deﬁnition, also a solution in the relaxed problem; but the relaxedproblem may have better solutions if the added edges provide short cuts.

Token 2296:
Hence, the cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.

Token 2297:
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it mustobey the triangle inequality and is therefore consistent (see page 95).

Token 2298:
If a problem deﬁnition is written down in a formal language, it is possible to construct relaxed problems automatically.

Token 2299:
11For example, if the 8-puzzle actions are described as A tile can move from square A to square B if A is horizontally or vertically adjacent to B andB is blank, we can generate three relaxed problems by removing one or both of the conditions: (a) A tile can move from square A to square B if A is adjacent to B.

Token 2300:
(b) A tile can move from square A to square B if B is blank. (c) A tile can move from square A to square B.

Token 2301:
From (a), we can derive h 2(Manhattan distance). The reasoning is that h2would be the proper score if we moved each tile in turn to its destination.

Token 2302:
The heuristic derived from (b) isdiscussed in Exercise 3.31.

Token 2303:
From (c), we can derive h 1(misplaced tiles) because it would be the proper score if tiles could move to their intended destination in one step.

Token 2304:
Notice that it iscrucial that the relaxed problems generated by this technique can be solved essentially without search , because the relaxed rules allow the problem to be decomposed into eight independent subproblems.

Token 2305:
If the relaxed problem is hard to solve, then the values of the corresponding heuristic will be expensive to obtain.

Token 2306:
12 A program called A BSOLVER can generate heuristics automatically from problem def- initions, using the “relaxed problem” method and various other techniques (Prieditis, 1993).

Token 2307:
ABSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting heuristic and found the ﬁrst useful heuristic for the famous Rubik’s Cube puzzle.

Token 2308:
One problem with generating new heuristic functions is that one often fails to get a single “clearly best” heuristic.

Token 2309:
If a collection of admissible heuristics h1...h mis available for a problem and none of them dominates any of the others, which should we choose?

Token 2310:
As itturns out, we need not make a choice. We can have the best of all worlds, by deﬁning h(n)=m a x{h 1(n),...,h m(n)}.

Token 2311:
11In Chapters 8 and 10, we describe formal languages suitable for this task; with formal descriptions that can be manipulated, the construction of relaxed problems can be automated.

Token 2312:
For now, we use English.

Token 2313:
12Note that a perfect heuristic can be obtained simply by allowing hto run a full breadth-ﬁrst search “on the sly.” Thus, there is a tradeoff between accuracy and computation time for heuristic functions.

Token 2314:
106 Chapter 3.

Token 2315:
Solving Problems by Searching Start State Goal State12 34 6 852 1 3 6 7 854 Figure 3.30 A subproblem of the 8-puzzle instance given in Figure 3.28.

Token 2316:
The task is to get tiles 1, 2, 3, and 4 into their correct positions, without worrying about what happens to the other tiles.

Token 2317:
This composite heuristic uses whichever function is most accurate on the node in question.

Token 2318:
Because the component heuristics are admissible, his admissible; it is also easy to prove that his consistent.

Token 2319:
Furthermore, hdominates all of its component heuristics.

Token 2320:
3.6.3 Generating admissible heuristics from subproblems: Pattern databases Admissible heuristics can also be derived from the solution cost of a subproblem of a given SUBPROBLEM problem.

Token 2321:
For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig- ure 3.28.

Token 2322:
The subproblem involves getting tiles 1, 2, 3, 4 into their correct positions.

Token 2323:
Clearly,the cost of the optimal solution of this subproblem is a lower bound on the cost of the com-plete problem.

Token 2324:
It turns out to be more accurate than Manhattan distance in some cases.

Token 2325:
The idea behind pattern databases is to store these exact solution costs for every pos- PATTERN DATABASE sible subproblem instance—in our example, every possible conﬁguration of the four tiles and the blank.

Token 2326:
(The locations of the other four tiles are irrelevant for the purposes of solv-ing the subproblem, but moves of those tiles do count toward the cost.)

Token 2327:
Then we computean admissible heuristic h DBfor each complete state encountered during a search simply by looking up the corresponding subproblem conﬁguration in the database.

Token 2328:
The database itself is constructed by searching back13from the goal and recording the cost of each new pattern en- countered; the expense of this search is amortized over many subsequent problem instances.

Token 2329:
The choice of 1-2-3-4 is fairly arbitrary; we could also construct databases for 5-6-7-8, for 2-4-6-8, and so on.

Token 2330:
Each database yields an admissible heuristic, and these heuristics canbe combined, as explained earlier, by taking the maximum value.

Token 2331:
A combined heuristic ofthis kind is much more accurate than the Manhattan distance; the number of nodes generatedwhen solving random 15-puzzles can be reduced by a factor of 1000.

Token 2332:
One might wonder whether the heuristics obtained from the 1-2-3-4 database and the 5-6-7-8 could be added , since the two subproblems seem not to overlap.

Token 2333:
Would this still give an admissible heuristic?

Token 2334:
The answer is no, because the solutions of the 1-2-3-4 subproblemand the 5-6-7-8 subproblem for a given state will almost certainly share some moves—it is 13By working backward from the goal, the exact solution cost of every instance encountered is immediately available.

Token 2335:
This is an example of dynamic programming , which we discuss further in Chapter 17.

Token 2336:
Section 3.6. Heuristic Functions 107 unlikely that 1-2-3-4 can be moved into place without touching 5-6-7-8, and vice versa.

Token 2337:
But what if we don’t count those moves?

Token 2338:
That is, we record not the total cost of solving the 1-2-3-4 subproblem, but just the number of moves involving 1-2-3-4.

Token 2339:
Then it is easy to see thatthe sum of the two costs is still a lower bound on the cost of solving the entire problem.

Token 2340:
Thisis the idea behind disjoint pattern databases .

Token 2341:
With such databases, it is possible to solve DISJOINT PATTERN DATABASES random 15-puzzles in a few milliseconds—the number of nodes generated is reduced by a factor of 10,000 compared with the use of Manhattan distance.

Token 2342:
For 24-puzzles, a speedup ofroughly a factor of a million can be obtained.

Token 2343:
Disjoint pattern databases work for sliding-tile puzzles because the problem can be divided up in such a way that each move affects only one subproblem—because only one tileis moved at a time.

Token 2344:
For a problem such as Rubik’s Cube, this kind of subdivision is difﬁcultbecause each move affects 8 or 9 of the 26 cubies.

Token 2345:
More general ways of deﬁning additive,admissible heuristics have been proposed that do apply to Rubik’s cube (Yang et al.

Token 2346:
, 2008), but they have not yielded a heuristic better than the best nonadditive heuristic for the problem.

Token 2347:
3.6.4 Learning heuristics from experience A heuristic function h(n)is supposed to estimate the cost of a solution beginning from the state at node n. How could an agent construct such a function?

Token 2348:
One solution was given in the preceding sections—namely, to devise relaxed problems for which an optimal solutioncan be found easily.

Token 2349:
Another solution is to learn from experience. “Experience” here meanssolving lots of 8-puzzles, for instance.

Token 2350:
Each optimal solution to an 8-puzzle problem providesexamples from which h(n)can be learned.

Token 2351:
Each example consists of a state from the solu- tion path and the actual cost of the solution from that point.

Token 2352:
From these examples, a learning algorithm can be used to construct a function h(n)that can (with luck) predict solution costs for other states that arise during search.

Token 2353:
Techniques for doing just this using neural nets, de- cision trees, and other methods are demonstrated in Chapter 18.

Token 2354:
(The reinforcement learning methods described in Chapter 21 are also applicable.)

Token 2355:
Inductive learning methods work best when supplied with features of a state that are FEATURE relevant to predicting the state’s value, rather than with just the raw state description.

Token 2356:
For example, the feature “number of misplaced tiles” might be helpful in predicting the actualdistance of a state from the goal.

Token 2357:
Let’s call this feature x 1(n). We could take 100 randomly generated 8-puzzle conﬁgurations and gather statistics on their actual solution costs.

Token 2358:
Wemight ﬁnd that when x 1(n)is 5, the average solution cost is around 14, and so on. Given these data, the value of x1can be used to predict h(n).

Token 2359:
Of course, we can use several features.

Token 2360:
A second feature x2(n)might be “number of pairs of adjacent tiles that are not adjacent in the goal state.” How should x1(n)andx2(n)be combined to predict h(n)?

Token 2361:
A common approach is to use a linear combination: h(n)=c1x1(n)+c2x2(n).

Token 2362:
The constants c1andc2are adjusted to give the best ﬁt to the actual data on solution costs.

Token 2363:
One expects both c1andc2to be positive because misplaced tiles and incorrect adjacent pairs make the problem harder to solve.

Token 2364:
Notice that this heuristic does satisfy the condition thath(n)=0 for goal states, but it is not necessarily admissible or consistent.

Token 2365:
108 Chapter 3.

Token 2366:
Solving Problems by Searching 3.7 S UMMARY This chapter has introduced methods that an agent can use to select actions in environments that are deterministic, observable, static, and completely known.

Token 2367:
In such cases, the agent canconstruct sequences of actions that achieve its goals; this process is called search .

Token 2368:
•Before an agent can start searching for solutions, a goal must be identiﬁed and a well- deﬁned problem must be formulated.

Token 2369:
•A problem consists of ﬁve parts: the initial state , a set of actions ,atransition model describing the results of those actions, a goal test function, and a path cost function.

Token 2370:
The environment of the problem is represented by a state space .Apath through the state space from the initial state to a goal state is a solution .

Token 2371:
•Search algorithms treat states and actions as atomic : they do not consider any internal structure they might possess.

Token 2372:
•A general T REE-SEARCH algorithm considers all possible paths to ﬁnd a solution, whereas a G RAPH -SEARCH algorithm avoids consideration of redundant paths.

Token 2373:
•Search algorithms are judged on the basis of completeness ,optimality ,time complex- ity,a n d space complexity .

Token 2374:
Complexity depends on b, the branching factor in the state space, and d, the depth of the shallowest solution.

Token 2375:
•Uninformed search methods have access only to the problem deﬁnition.

Token 2376:
The basic algorithms are as follows: – Breadth-ﬁrst search expands the shallowest nodes ﬁrst; it is complete, optimal for unit step costs, but has exponential space complexity.

Token 2377:
– Uniform-cost search expands the node with lowest path cost, g(n), and is optimal for general step costs.

Token 2378:
– Depth-ﬁrst search expands the deepest unexpanded node ﬁrst. It is neither com- plete nor optimal, but has linear space complexity.

Token 2379:
Depth-limited search adds a depth bound. – Iterative deepening search calls depth-ﬁrst search with increasing depth limits until a goal is found.

Token 2380:
It is complete, optimal for unit step costs, has time complexity comparable to breadth-ﬁrst search, and has linear space complexity.

Token 2381:
– Bidirectional search can enormously reduce time complexity, but it is not always applicable and may require too much space.

Token 2382:
•Informed search methods may have access to a heuristic function h(n)that estimates the cost of a solution from n. –The generic best-ﬁrst search algorithm selects a node for expansion according to anevaluation function .

Token 2383:
– Greedy best-ﬁrst search expands nodes with minimal h(n). It is not optimal but is often efﬁcient.

Token 2384:


Token 2385:
Bibliographical and Historical Notes 109 –A∗search expands nodes with minimal f(n)=g(n)+h(n).A∗is complete and optimal, provided that h(n)is admissible (for T REE-SEARCH ) or consistent (for GRAPH -SEARCH ).

Token 2386:
The space complexity of A∗is still prohibitive.

Token 2387:
–R B F S (recursive best-ﬁrst search) and SMA∗(simpliﬁed memory-bounded A∗) are robust, optimal search algorithms that use limited amounts of memory; givenenough time, they can solve problems that A ∗cannot solve because it runs out of memory.

Token 2388:
•The performance of heuristic search algorithms depends on the quality of the heuristic function.

Token 2389:
One can sometimes construct good heuristics by relaxing the problem deﬁ-nition, by storing precomputed solution costs for subproblems in a pattern database, orby learning from experience with the problem class.

Token 2390:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The topic of state-space search originated in more or less its current form in the early years of AI.

Token 2391:
Newell and Simon’s work on the Logic Theorist (1957) and GPS (1961) led to the estab- lishment of search algorithms as the primary weapons in the armory of 1960s AI researchersand to the establishment of problem solving as the canonical AI task.

Token 2392:
Work in operationsresearch by Richard Bellman (1957) showed the importance of additive path costs in sim-plifying optimization algorithms.

Token 2393:
The text on Automated Problem Solving by Nils Nilsson (1971) established the area on a solid theoretical footing.

Token 2394:
Most of the state-space search problems analyzed in this chapter have a long history in the literature and are less trivial than they might seem.

Token 2395:
The missionaries and cannibalsproblem used in Exercise 3.9 was analyzed in detail by Amarel (1968).

Token 2396:
It had been consid-ered earlier—in AI by Simon and Newell (1961) and in operations research by Bellman andDreyfus (1962).

Token 2397:
The 8-puzzle is a smaller cousin of the 15-puzzle, whose history is recounted at length by Slocum and Sonneveld (2006).

Token 2398:
It was widely believed to have been invented by the fa-mous American game designer Sam Loyd, based on his claims to that effect from 1891 on-ward (Loyd, 1959).

Token 2399:
Actually it was invented by Noyes Chapman, a postmaster in Canastota,New York, in the mid-1870s.

Token 2400:
(Chapman was unable to patent his invention, as a genericpatent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinseyin 1878.)

Token 2401:
It quickly attracted the attention of the public and of mathematicians (Johnson andStory, 1879; Tait, 1880).

Token 2402:
The editors of the American Journal of Mathematics stated, “The ‘15’ puzzle for the last few weeks has been prominently before the American public, and maysafely be said to have engaged the attention of nine out of ten persons of both sexes and allages and conditions of the community.” Ratner and Warmuth (1986) showed that the general n×nversion of the 15-puzzle belongs to the class of NP-complete problems.

Token 2403:
The 8-queens problem was ﬁrst published anonymously in the German chess maga- zine Schach in 1848; it was later attributed to one Max Bezzel.

Token 2404:
It was republished in 1850 and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who

Token 2405:
110 Chapter 3.

Token 2406:
Solving Problems by Searching attempted to enumerate all possible solutions; initially he found only 72, but eventually he found the correct answer of 92, although Nauck published all 92 solutions ﬁrst, in 1850.Netto (1901) generalized the problem to nqueens, and Abramson and Yung (1989) found an O(n)algorithm.

Token 2407:
Each of the real-world search problems listed in the chapter has been the subject of a good deal of research effort.

Token 2408:
Methods for selecting optimal airline ﬂights remain proprietary for the most part, but Carl de Marcken (personal communication) has shown that airline ticketpricing and restrictions have become so convoluted that the problem of selecting an optimalﬂight is formally undecidable .

Token 2409:
The traveling-salesperson problem is a standard combinato- rial problem in theoretical computer science (Lawler et al. , 1992).

Token 2410:
Karp (1972) proved the TSP to be NP-hard, but effective heuristic approximation methods were developed (Lin andKernighan, 1973).

Token 2411:
Arora (1998) devised a fully polynomial approximation scheme for Eu-clidean TSPs.

Token 2412:
VLSI layout methods are surveyed by Shahookar and Mazumder (1991), andmany layout optimization papers appear in VLSI journals.

Token 2413:
Robotic navigation and assemblyproblems are discussed in Chapter 25.

Token 2414:
Uninformed search algorithms for problem solving are a central topic of classical com- puter science (Horowitz and Sahni, 1978) and operations research (Dreyfus, 1969).

Token 2415:
Breadth- ﬁrst search was formulated for solving mazes by Moore (1959).

Token 2416:
The method of dynamic programming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records solutions for all subproblems of increasing lengths, can be seen as a form of breadth-ﬁrst search on graphs.

Token 2417:
The two-point shortest-path algorithm of Dijkstra (1959) is the origin of uniform-cost search.

Token 2418:
These works also introduced the idea of explored and frontier sets (closed and open lists).

Token 2419:
A version of iterative deepening designed to make efﬁcient use of the chess clock was ﬁrst used by Slate and Atkin (1977) in the C HESS 4.5 game-playing program.

Token 2420:
Martelli’s algorithm B (1977) includes an iterative deepening aspect and also dominates A∗’s worst-case performance with admissible but inconsistent heuristics.

Token 2421:
The iterative deepening techniquecame to the fore in work by Korf (1985a).

Token 2422:
Bidirectional search, which was introduced by Pohl (1971), can also be effective in some cases.

Token 2423:
The use of heuristic information in problem solving appears in an early paper by Simon and Newell (1958), but the phrase “heuristic search” and the use of heuristic functions thatestimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).Doran and Michie (1966) conducted extensive experimental studies of heuristic search.

Token 2424:
Al-though they analyzed path length and “penetrance” (the ratio of path length to the total num-ber of nodes examined so far), they appear to have ignored the information provided by thepath cost g(n).T h e A ∗algorithm, incorporating the current path cost into heuristic search, was developed by Hart, Nilsson, and Raphael (1968), with some later corrections (Hart et al.

Token 2425:
, 1972). Dechter and Pearl (1985) demonstrated the optimal efﬁciency of A∗.

Token 2426:
The original A∗paper introduced the consistency condition on heuristic functions.

Token 2427:
The monotone condition was introduced by Pohl (1977) as a simpler replacement, but Pearl (1984) showed that the two were equivalent.

Token 2428:
Pohl (1977) pioneered the study of the relationship between the error in heuristic func- tions and the time complexity of A∗.

Token 2429:
Basic results were obtained for tree search with unit step

Token 2430:
Bibliographical and Historical Notes 111 costs and a single goal node (Pohl, 1977; Gaschnig, 1979; Huyn et al.

Token 2431:
, 1980; Pearl, 1984) and with multiple goal nodes (Dinh et al. , 2007).

Token 2432:
The “effective branching factor” was proposed by Nilsson (1971) as an empirical measure of the efﬁciency; it is equivalent to assuming atime cost of O((b ∗)d).

Token 2433:
For tree search applied to a graph, Korf et al.

Token 2434:
(2001) argue that the time cost is better modeled as O(bd−k),w h e r e kdepends on the heuristic accuracy; this analysis has elicited some controversy, however.

Token 2435:
For graph search, Helmert and R¨ oger (2008) noted that several well-known problems contained exponentially many nodes on optimal solutionpaths, implying exponential time complexity for A ∗even with constant absolute error in h. There are many variations on the A∗algorithm.

Token 2436:
Pohl (1973) proposed the use of dynamic weighting , which uses a weighted sum fw(n)=wgg(n)+whh(n)of the current path length and the heuristic function as an evaluation function, rather than the simple sum f(n)=g(n)+ h(n)used in A∗.

Token 2437:
The weights wgandwhare adjusted dynamically as the search progresses.

Token 2438:
Pohl’s algorithm can be shown to be /epsilon1-admissible—that is, guaranteed to ﬁnd solutions within a factor 1+/epsilon1of the optimal solution, where /epsilon1is a parameter supplied to the algorithm.

Token 2439:
The same property is exhibited by the A∗ /epsilon1algorithm (Pearl, 1984), which can select any node from the frontier provided its f-cost is within a factor 1+/epsilon1of the lowest- f-cost frontier node.

Token 2440:
The selection can be done so as to minimize search cost.

Token 2441:
Bidirectional versions of A∗have been investigated; a combination of bidirectional A∗ and known landmarks was used to efﬁciently ﬁnd driving routes for Microsoft’s online map service (Goldberg et al.

Token 2442:
, 2006).

Token 2443:
After caching a set of paths between landmarks, the algorithm can ﬁnd an optimal path between any pair of points in a 24 million point graph of the UnitedStates, searching less than 0.1% of the graph.

Token 2444:
Others approaches to bidirectional searchinclude a breadth-ﬁrst search backward from the goal up to a ﬁxed depth, followed by aforward IDA ∗search (Dillenburg and Nelson, 1994; Manzini, 1995).

Token 2445:
A∗and other state-space search algorithms are closely related to the branch-and-bound techniques that are widely used in operations research (Lawler and Wood, 1966).

Token 2446:
Therelationships between state-space search and branch-and-bound have been investigated indepth (Kumar and Kanal, 1983; Nau et al.

Token 2447:
, 1984; Kumar et al. , 1988).

Token 2448:
Martelli and Monta- nari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and certain types of state-space search.

Token 2449:
Kumar and Kanal (1988) attempt a “grand uniﬁcation” ofheuristic search, dynamic programming, and branch-and-bound techniques under the nameof CDP—the “composite decision process.” Because computers in the late 1950s and early 1960s had at most a few thousand words of main memory, memory-bounded heuristic search was an early research topic.

Token 2450:
The GraphTraverser (Doran and Michie, 1966), one of the earliest search programs, commits to anoperator after searching best-ﬁrst up to the memory limit.

Token 2451:
IDA ∗(Korf, 1985a, 1985b) was the ﬁrst widely used optimal, memory-bounded heuristic search algorithm, and a large numberof variants have been developed.

Token 2452:
An analysis of the efﬁciency of IDA ∗and of its difﬁculties with real-valued heuristics appears in Patrick et al. (1992).

Token 2453:
RBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown in Figure 3.26, which is closer to an independently developed algorithm called iterative ex- pansion (Russell, 1992).

Token 2454:
RBFS uses a lower bound as well as the upper bound; the two al-ITERATIVE EXPANSION gorithms behave identically with admissible heuristics, but RBFS expands nodes in best-ﬁrst

Token 2455:
112 Chapter 3. Solving Problems by Searching order even with an inadmissible heuristic.

Token 2456:
The idea of keeping track of the best alternative path appeared earlier in Bratko’s (1986) elegant Prolog implementation of A∗and in the DTA∗ algorithm (Russell and Wefald, 1991).

Token 2457:
The latter work also discusses metalevel state spacesand metalevel learning. The MA ∗algorithm appeared in Chakrabarti et al. (1989).

Token 2458:
SMA∗, or Simpliﬁed MA∗, emerged from an attempt to implement MA∗as a comparison algorithm for IE (Russell, 1992).

Token 2459:
Kaindl and Khorsand (1994) have applied SMA∗to produce a bidirectional search algorithm that is substantially faster than previous algorithms.

Token 2460:
Korf and Zhang (2000) describe a divide-and-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded A ∗graph search and a strategy for switching to breadth-ﬁrst search to increase memory-efﬁciency(Zhou and Hansen, 2006).

Token 2461:
Korf (1995) surveys memory-bounded search techniques.

Token 2462:
The idea that admissible heuristics can be derived by problem relaxation appears in the seminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic tosolve the TSP.

Token 2463:
(See Exercise 3.30.)

Token 2464:
The automation of the relaxation process was implemented successfully by Priedi- tis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989).

Token 2465:
Holte andHernadvolgyi (2001) describe more recent steps towards automating the process.

Token 2466:
The use of pattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and Schaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);a similar method using symbolic patterns is due to Edelkamp (2009).

Token 2467:
Felner et al. (2007) show how to compress pattern databases to save space.

Token 2468:
The probabilistic interpretation of heuristics was investigated in depth by Pearl (1984) and Hansson and Mayer (1989).

Token 2469:
By far the most comprehensive source on heuristics and heuristic search algorithms is Pearl’s (1984) Heuristics text.

Token 2470:
This book provides especially good coverage of the wide variety of offshoots and variations of A ∗, including rigorous proofs of their formal properties.

Token 2471:
Kanal and Kumar (1988) present an anthology of important articles on heuristic search, andRayward-Smith et al.

Token 2472:
(1996) cover approaches from Operations Research.

Token 2473:
Papers about new search algorithms—which, remarkably, continue to be discovered—appear in journals such asArtiﬁcial Intelligence andJournal of the ACM .

Token 2474:
The topic of parallel search algorithms was not covered in the chapter, partly because PARALLEL SEARCH it requires a lengthy discussion of parallel computer architectures.

Token 2475:
Parallel search became a popular topic in the 1990s in both AI and theoretical computer science (Mahanti and Daniels,1993; Grama and Kumar, 1995; Crauser et al.

Token 2476:
, 1998) and is making a comeback in the era of new multicore and cluster architectures (Ralphs et al. , 2004; Korf and Schultze, 2005).

Token 2477:
Also of increasing importance are search algorithms for very large graphs that require diskstorage (Korf, 2008).

Token 2478:
EXERCISES 3.1 Explain why problem formulation must follow goal formulation. 3.2 Your goal is to navigate a robot out of a maze.

Token 2479:
The robot starts in the center of the maze

Token 2480:
Exercises 113 facing north. You can turn the robot to face north, east, south, or west.

Token 2481:
You can direct the robot to move forward a certain distance, although it will stop before hitting a wall. a. Formulate this problem.

Token 2482:
How large is the state space? b. In navigating a maze, the only place we need to turn is at the intersection of two or more corridors.

Token 2483:
Reformulate this problem using this observation. How large is the statespace now?

Token 2484:
c. From each point in the maze, we can move in any of the four directions until we reach a turning point, and this is the only action we need to do.

Token 2485:
Reformulate the problem usingthese actions. Do we need to keep track of the robot’s orientation now?

Token 2486:
d. In our initial description of the problem we already abstracted from the real world, restricting actions and removing details.

Token 2487:
List three such simpliﬁcations we made. 3.3 Suppose two friends live in different cities on a map, such as the Romania map shown in Figure 3.2.

Token 2488:
On every turn, we can simultaneously move each friend to a neighboring cityon the map.

Token 2489:
The amount of time needed to move from city ito neighbor jis equal to the road distance d(i,j)between the cities, but on each turn the friend that arrives ﬁrst must wait until the other one arrives (and calls the ﬁrst on his/her cell phone) before the next turn can begin.We want the two friends to meet as quickly as possible.

Token 2490:
a. Write a detailed formulation for this search problem. (You will ﬁnd it helpful to deﬁne some formal notation here.)

Token 2491:
b.L e tD(i,j)be the straight-line distance between cities iandj. Which of the following heuristic functions are admissible?

Token 2492:
(i) D(i,j); (ii)2·D(i,j); (iii)D(i,j)/2. c. Are there completely connected maps for which no solution exists?

Token 2493:
d. Are there maps in which all solutions require one friend to visit the same city twice?

Token 2494:
3.4 Show that the 8-puzzle states are divided into two disjoint sets, such that any state is reachable from any other state in the same set, while no state is reachable from any state inthe other set.

Token 2495:
( Hint: See Berlekamp et al. (1982).)

Token 2496:
Devise a procedure to decide which set a given state is in, and explain why this is useful for generating random states.

Token 2497:
3.5 Consider the n-queens problem using the “efﬁcient” incremental formulation given on page 72.

Token 2498:
Explain why the state space has at least 3√ n!states and estimate the largest nfor which exhaustive exploration is feasible.

Token 2499:
( Hint: Derive a lower bound on the branching factor by considering the maximum number of squares that a queen can attack in any column.)

Token 2500:
3.6 Give a complete problem formulation for each of the following. Choose a formulation that is precise enough to be implemented. a.

Token 2501:
Using only four colors, you have to color a planar map in such a way that no two adjacent regions have the same color. b.

Token 2502:
A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot ceiling. He would like to get the bananas.

Token 2503:
The room contains two stackable, movable,climbable 3-foot-high crates.

Token 2504:
114 Chapter 3. Solving Problems by Searching SG Figure 3.31 A scene with polygonal obstacles. SandGare the start and goal states.

Token 2505:
c. You have a program that outputs the message “illegal input record” when fed a certain ﬁle of input records.

Token 2506:
You know that processing of each record is independent of theother records. You want to discover what record is illegal.

Token 2507:
d. You have three jugs, measuring 12 gallons, 8 gallons, and 3 gallons, and a water faucet.

Token 2508:
You can ﬁll the jugs up or empty them out from one to another or onto the ground. You need to measure out exactly one gallon.

Token 2509:
3.7 Consider the problem of ﬁnding the shortest path between two points on a plane that has convex polygonal obstacles as shown in Figure 3.31.

Token 2510:
This is an idealization of the problem that a robot has to solve to navigate in a crowded environment. a.

Token 2511:
Suppose the state space consists of all positions (x,y)in the plane. How many states are there? How many paths are there to the goal? b.

Token 2512:
Explain brieﬂy why the shortest path from one polygon vertex to any other in the scene must consist of straight-line segments joining some of the vertices of the polygons.Deﬁne a good state space now.

Token 2513:
How large is this state space?

Token 2514:
c. Deﬁne the necessary functions to implement the search problem, including an A CTIONS function that takes a vertex as input and returns a set of vectors, each of which maps the current vertex to one of the vertices that can be reached in a straight line.

Token 2515:
(Do not forget the neighbors on the same polygon.) Use the straight-line distance for the heuristic function.

Token 2516:
d. Apply one or more of the algorithms in this chapter to solve a range of problems in the domain, and comment on their performance.

Token 2517:
3.8 On page 68, we said that we would not consider problems with negative path costs. In this exercise, we explore this decision in more depth. a.

Token 2518:
Suppose that actions can have arbitrarily large negative costs; explain why this possi- bility would force any optimal algorithm to explore the entire state space.

Token 2519:
Exercises 115 b. Does it help if we insist that step costs must be greater than or equal to some negative constant c? Consider both trees and graphs.

Token 2520:
c. Suppose that a set of actions forms a loop in the state space such that executing the set in some order results in no net change to the state.

Token 2521:
If all of these actions have negative cost,what does this imply about the optimal behavior for an agent in such an environment?

Token 2522:
d. One can easily imagine actions with high negative cost, even in domains such as route ﬁnding.

Token 2523:
For example, some stretches of road might have such beautiful scenery as to far outweigh the normal costs in terms of time and fuel.

Token 2524:
Explain, in precise terms, within the context of state-space search, why humans do not drive around scenic loopsindeﬁnitely, and explain how to deﬁne the state space and actions for route ﬁnding sothat artiﬁcial agents can also avoid looping.

Token 2525:
e. Can you think of a real domain in which step costs are such as to cause looping?

Token 2526:
3.9 Themissionaries and cannibals problem is usually stated as follows.

Token 2527:
Three mission- aries and three cannibals are on one side of a river, along with a boat that can hold one or two people.

Token 2528:
Find a way to get everyone to the other side without ever leaving a group of mis- sionaries in one place outnumbered by the cannibals in that place.

Token 2529:
This problem is famous in AI because it was the subject of the ﬁrst paper that approached problem formulation from ananalytical viewpoint (Amarel, 1968).

Token 2530:
a. Formulate the problem precisely, making only those distinctions necessary to ensure a valid solution. Draw a diagram of the complete state space.

Token 2531:
b. Implement and solve the problem optimally using an appropriate search algorithm. Is it a good idea to check for repeated states?

Token 2532:
c. Why do you think people have a hard time solving this puzzle, given that the state space is so simple?

Token 2533:
3.10 Deﬁne in your own words the following terms: state, state space, search tree, search node, goal, action, transition model, and branching factor.

Token 2534:
3.11 What’s the difference between a world state, a state description, and a search node? Why is this distinction useful?

Token 2535:
3.12 An action such as Go(Sibiu) really consists of a long sequence of ﬁner-grained actions: turn on the car, release the brake, accelerate forward, etc.

Token 2536:
Having composite actions of thiskind reduces the number of steps in a solution sequence, thereby reducing the search time.Suppose we take this to the logical extreme, by making super-composite actions out of everypossible sequence of Goactions.

Token 2537:
Then every problem instance is solved by a single super- composite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest) .

Token 2538:
Explain how search would work in this formulation. Is this a practical approach for speeding up problem solving?

Token 2539:
3.13 Prove that G RAPH -SEARCH satisﬁes the graph separation property illustrated in Fig- ure 3.9.

Token 2540:
( Hint: Begin by showing that the property holds at the start, then show that if it holds before an iteration of the algorithm, it holds afterwards.)

Token 2541:
Describe a search algorithm thatviolates the property.

Token 2542:
116 Chapter 3.

Token 2543:
Solving Problems by Searching x 12 x 16x 2 x 2 Figure 3.32 The track pieces in a wooden railway set; each is labeled with the number of copies in the set.

Token 2544:
Note that curved pieces and “ fork” pieces (“switches” or “points”) can be ﬂipped over so they can curve in either direction.

Token 2545:
Each curve subtends 45 degrees. 3.14 Which of the following are true and which are false? Explain your answers.

Token 2546:
a. Depth-ﬁrst search always expands at least as many nodes as A∗search with an admissi- ble heuristic.

Token 2547:
b.h(n)=0 is an admissible heuristic for the 8-puzzle. c.A∗is of no use in robotics because percepts, states, and actions are continuous.

Token 2548:
d. Breadth-ﬁrst search is complete even if zero step costs are allowed.

Token 2549:
e. Assume that a rook can move on a chessboard any number of squares in a straight line, vertically or horizontally, but cannot jump over other pieces.

Token 2550:
Manhattan distance is anadmissible heuristic for the problem of moving the rook from square A to square B inthe smallest number of moves.

Token 2551:
3.15 Consider a state space where the start state is number 1 and each state khas two successors: numbers 2kand2k+1. a.

Token 2552:
Draw the portion of the state space for states 1 to 15. b. Suppose the goal state is 11.

Token 2553:
List the order in which nodes will be visited for breadth- ﬁrst search, depth-limited search with limit 3, and iterative deepening search.

Token 2554:
c. How well would bidirectional search work on this problem? What is the branching factor in each direction of the bidirectional search?

Token 2555:
d. Does the answer to (c) suggest a reformulation of the problem that would allow you to solve the problem of getting from state 1 to a given goal state with almost no search?

Token 2556:
e. Call the action going from kto2kLeft, and the action going to 2k+1Right.

Token 2557:
Can you ﬁnd an algorithm that outputs the solution to this problem without any search at all?

Token 2558:
3.16 A basic wooden railway set contains the pieces shown in Figure 3.32.

Token 2559:
The task is to connect these pieces into a railway that has no overlapping tracks and no loose ends where atrain could run off onto the ﬂoor. a.

Token 2560:
Suppose that the pieces ﬁt together exactly with no slack. Give a precise formulation of the task as a search problem. b.

Token 2561:
Identify a suitable uninformed search algorithm for this task and explain your choice.

Token 2562:
c. Explain why removing any one of the “fork” pieces makes the problem unsolvable.

Token 2563:
Exercises 117 d. Give an upper bound on the total size of the state space deﬁned by your formulation.

Token 2564:
(Hint: think about the maximum branching factor for the construction process and the maximum depth, ignoring the problem of overlapping pieces and loose ends.

Token 2565:
Begin bypretending that every piece is unique.)

Token 2566:
3.17 On page 90, we mentioned iterative lengthening search , an iterative analog of uni- form cost search.

Token 2567:
The idea is to use increasing limits on path cost. If a node is generated whose path cost exceeds the current limit, it is immediately discarded.

Token 2568:
For each new itera-tion, the limit is set to the lowest path cost of any node discarded in the previous iteration. a.

Token 2569:
Show that this algorithm is optimal for general path costs. b. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.

Token 2570:
How many iterations will iterative lengthening require?

Token 2571:
c. Now consider step costs drawn from the continuous range [/epsilon1,1],w h e r e 0</epsilon1< 1.H o w many iterations are required in the worst case?

Token 2572:
d. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales- person problems.

Token 2573:
Compare the algorithm’s performance to that of uniform-cost search,and comment on your results.

Token 2574:
3.18 Describe a state space in which iterative deepening search performs much worse than depth-ﬁrst search (for example, O(n 2)vs.O(n)).

Token 2575:
3.19 Write a program that will take as input two Web page URLs and ﬁnd a path of links from one to the other. What is an appropriate search strategy?

Token 2576:
Is bidirectional search a good idea? Could a search engine be used to implement a predecessor function?

Token 2577:
3.20 Consider the vacuum-world problem deﬁned in Figure 2.2. a. Which of the algorithms deﬁned in this chapter would be appropriate for this problem?

Token 2578:
Should the algorithm use tree search or graph search? b.

Token 2579:
Apply your chosen algorithm to compute an optimal sequence of actions for a 3×3 world whose initial state has dirt in the three top squares and the agent in the center.

Token 2580:
c. Construct a search agent for the vacuum world, and evaluate its performance in a set of 3×3worlds with probability 0.2 of dirt in each square.

Token 2581:
Include the search cost as well as path cost in the performance measure, using a reasonable exchange rate.

Token 2582:
d. Compare your best search agent with a simple randomized reﬂex agent that sucks if there is dirt and otherwise moves randomly.

Token 2583:
e. Consider what would happen if the world were enlarged to n×n. How does the per- formance of the search agent and of the reﬂex agent vary with n?

Token 2584:
3.21 Prove each of the following statements, or give a counterexample: a. Breadth-ﬁrst search is a special case of uniform-cost search.

Token 2585:
b. Depth-ﬁrst search is a special case of best-ﬁrst tree search. c. Uniform-cost search is a special case of A∗search.

Token 2586:
118 Chapter 3.

Token 2587:
Solving Problems by Searching 3.22 Compare the performance of A∗and RBFS on a set of randomly generated problems in the 8-puzzle (with Manhattan distance) and TSP (with MST—see Exercise 3.30) domains.

Token 2588:
Discuss your results. What happens to the performance of RBFS when a small random num-ber is added to the heuristic values in the 8-puzzle domain?

Token 2589:
3.23 Trace the operation of A ∗search applied to the problem of getting to Bucharest from Lugoj using the straight-line distance heuristic.

Token 2590:
That is, show the sequence of nodes that thealgorithm will consider and the f,g,a n dhscore for each node.

Token 2591:
3.24 Devise a state space in which A ∗using G RAPH -SEARCH returns a suboptimal solution with an h(n)function that is admissible but inconsistent.

Token 2592:
3.25 The heuristic path algorithm (Pohl, 1977) is a best-ﬁrst search in which the evalu-HEURISTIC PATH ALGORITHM ation function is f(n)=( 2−w)g(n)+wh(n).

Token 2593:
For what values of wis this complete? For what values is it optimal, assuming that his admissible?

Token 2594:
What kind of search does this perform for w=0,w=1,a n dw=2? 3.26 Consider the unbounded version of the regular 2D grid shown in Figure 3.9.

Token 2595:
The start state is at the origin, (0,0), and the goal state is at (x,y). a. What is the branching factor bin this state space? b.

Token 2596:
How many distinct states are there at depth k(fork>0)? c. What is the maximum number of nodes expanded by breadth-ﬁrst tree search?

Token 2597:
d. What is the maximum number of nodes expanded by breadth-ﬁrst graph search? e.I sh=|u−x|+|v−y|an admissible heuristic for a state at (u,v)? Explain.

Token 2598:
f. How many nodes are expanded by A∗graph search using h? g.D o e s hremain admissible if some links are removed?

Token 2599:
h.D o e s hremain admissible if some links are added between nonadjacent states?

Token 2600:
3.27 nvehicles occupy squares (1,1)through (n,1)(i.e., the bottom row) of an n×ngrid.

Token 2601:
The vehicles must be moved to the top row but in reverse order; so the vehicle ithat starts in (i,1)must end up in (n−i+1,n).

Token 2602:
On each time step, every one of the nvehicles can move one square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacentvehicle (but not more than one) can hop over it.

Token 2603:
Two vehicles cannot occupy the same square. a. Calculate the size of the state space as a function of n. b.

Token 2604:
Calculate the branching factor as a function of n. c. Suppose that vehicle iis at(x i,yi); write a nontrivial admissible heuristic hifor the number of moves it will require to get to its goal location (n−i+1,n), assuming no other vehicles are on the grid.

Token 2605:
d. Which of the following heuristics are admissible for the problem of moving all nvehi- cles to their destinations? Explain.

Token 2606:
(i)/summationtextn i=1hi. (ii)max{h1,...,h n}. (iii)min{h1,...,h n}.

Token 2607:


Token 2608:
Exercises 119 3.28 Invent a heuristic function for the 8-puzzle that sometimes overestimates, and show how it can lead to a suboptimal solution on a particular problem.

Token 2609:
(You can use a computer tohelp if you want.)

Token 2610:
Prove that if hnever overestimates by more than c,A ∗usinghreturns a solution whose cost exceeds that of the optimal solution by no more than c. 3.29 Prove that if a heuristic is consistent, it must be admissible.

Token 2611:
Construct an admissible heuristic that is not consistent.

Token 2612:
3.30 The traveling salesperson problem (TSP) can be solved with the minimum-spanning- tree (MST) heuristic, which estimates the cost of completing a tour, given that a partial tour has already been constructed.

Token 2613:
The MST cost of a set of cities is the smallest sum of the linkcosts of any tree that connects all the cities. a.

Token 2614:
Show how this heuristic can be derived from a relaxed version of the TSP. b. Show that the MST heuristic dominates straight-line distance.

Token 2615:
c. Write a problem generator for instances of the TSP where cities are represented by random points in the unit square.

Token 2616:
d. Find an efﬁcient algorithm in the literature for constructing the MST, and use it with A ∗ graph search to solve instances of the TSP.

Token 2617:
3.31 On page 105, we deﬁned the relaxation of the 8-puzzle in which a tile can move from square A to square B if B is blank.

Token 2618:
The exact solution of this problem deﬁnes Gaschnig’s heuristic (Gaschnig, 1979).

Token 2619:
Explain why Gaschnig’s heuristic is at least as accurate as h1 (misplaced tiles), and show cases where it is more accurate than both h1andh2(Manhattan distance).

Token 2620:
Explain how to calculate Gaschnig’s heuristic efﬁciently. 3.32 We gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced tiles.

Token 2621:
Several heuristics in the literature purport to improve on this—see, for example, Nils- son (1971), Mostow and Prieditis (1989), and Hansson et al.

Token 2622:
(1992). Test these claims by implementing the heuristics and comparing the performance of the resulting algorithms.

Token 2623:
4BEYOND CLASSICAL SEARCH In which we relax the simplifying assumptions of the previous chapter, thereby getting closer to the real world.

Token 2624:
Chapter 3 addressed a single category of problems: observable, deterministic, known envi- ronments where the solution is a sequence of actions.

Token 2625:
In this chapter, we look at what happenswhen these assumptions are relaxed.

Token 2626:
We begin with a fairly simple case: Sections 4.1 and 4.2cover algorithms that perform purely local search in the state space, evaluating and modify- ing one or more current states rather than systematically exploring paths from an initial state.These algorithms are suitable for problems in which all that matters is the solution state, notthe path cost to reach it.

Token 2627:
The family of local search algorithms includes methods inspired by statistical physics ( simulated annealing ) and evolutionary biology ( genetic algorithms ).

Token 2628:
Then, in Sections 4.3–4.4, we examine what happens when we relax the assumptions of determinism and observability.

Token 2629:
The key idea is that if an agent cannot predict exactly whatpercept it will receive, then it will need to consider what to do under each contingency that its percepts may reveal.

Token 2630:
With partial observability, the agent will also need to keep track of the states it might be in.

Token 2631:
Finally, Section 4.5 investigates online search , in which the agent is faced with a state space that is initially unknown and must be explored.

Token 2632:
4.1 L OCAL SEARCH ALGORITHMS AND OPTIMIZATION PROBLEMS The search algorithms that we have seen so far are designed to explore search spaces sys-tematically.

Token 2633:
This systematicity is achieved by keeping one or more paths in memory and byrecording which alternatives have been explored at each point along the path.

Token 2634:
When a goal isfound, the path to that goal also constitutes a solution to the problem.

Token 2635:
In many problems, how- ever, the path to the goal is irrelevant.

Token 2636:
For example, in the 8-queens problem (see page 71), what matters is the ﬁnal conﬁguration of queens, not the order in which they are added.

Token 2637:
The same general property holds for many important applications such as integrated-circuit de-sign, factory-ﬂoor layout, job-shop scheduling, automatic programming, telecommunicationsnetwork optimization, vehicle routing, and portfolio management.

Token 2638:
120

Token 2639:
Section 4.1.

Token 2640:
Local Search Algorithms and Optimization Problems 121 If the path to the goal does not matter, we might consider a different class of algo- rithms, ones that do not worry about paths at all.

Token 2641:
Local search algorithms operate using LOCAL SEARCH a single current node (rather than multiple paths) and generally move only to neighbors CURRENT NODE of that node.

Token 2642:
Typically, the paths followed by the search are not retained.

Token 2643:
Although local search algorithms are not systematic, they have two key advantages: (1) they use very little memory—usually a constant amount; and (2) they can often ﬁnd reasonable solutions in large or inﬁnite (continuous) state spaces for which systematic algorithms are unsuitable.

Token 2644:
In addition to ﬁnding goals, local search algorithms are useful for solving pure op- timization problems , in which the aim is to ﬁnd the best state according to an objectiveOPTIMIZATION PROBLEM function .

Token 2645:
Many optimization problems do not ﬁt the “standard” search model introduced inOBJECTIVE FUNCTION Chapter 3.

Token 2646:
For example, nature provides an objective function—reproductive ﬁtness—that Darwinian evolution could be seen as attempting to optimize, but there is no “goal test” andno “path cost” for this problem.

Token 2647:
To understand local search, we ﬁnd it useful to consider the state-space landscape (as STATE-SPACE LANDSCAPE in Figure 4.1).

Token 2648:
A landscape has both “location” (deﬁned by the state) and “elevation” (deﬁned by the value of the heuristic cost function or objective function).

Token 2649:
If elevation corresponds tocost, then the aim is to ﬁnd the lowest valley—a global minimum ; if elevation corresponds GLOBAL MINIMUM to an objective function, then the aim is to ﬁnd the highest peak—a global maximum .

Token 2650:
( Y o u GLOBAL MAXIMUM can convert from one to the other just by inserting a minus sign.) Local search algorithms explore this landscape.

Token 2651:
A complete local search algorithm always ﬁnds a goal if one exists; anoptimal algorithm always ﬁnds a global minimum/maximum.

Token 2652:
current stateobjective function state spaceglobal maximum local maximum “flat” local maximumshoulder Figure 4.1 A one-dimensional state-space landscape in which elevation corresponds to the objective function.

Token 2653:
The aim is to ﬁnd the global maximum. Hill-climbing search modiﬁes the current state to try to improve it, as shown by the arrow.

Token 2654:
The various topographic features are deﬁned in the text.

Token 2655:
122 Chapter 4.

Token 2656:
Beyond Classical Search function HILL-CLIMBING (problem )returns a state that is a local maximum current←MAKE-NODE(problem .INITIAL -STATE ) loop do neighbor←a highest-valued successor of current ifneighbor.V ALUE≤current.V ALUE then return current .STATE current←neighbor Figure 4.2 The hill-climbing search algorithm, which is the most basic local search tech- nique.

Token 2657:
At each step the current node is replaced by the best neighbor; in this version, thatmeans the neighbor with the highest V ALUE , but if a heuristic cost estimate his used, we would ﬁnd the neighbor with the lowest h. 4.1.1 Hill-climbing search The hill-climbing search algorithm ( steepest-ascent version) is shown in Figure 4.2.

Token 2658:
It is HILL CLIMBING STEEPEST ASCENT simply a loop that continually moves in the direction of increasing value—that is, uphill.

Token 2659:
It terminates when it reaches a “peak” where no neighbor has a higher value.

Token 2660:
The algorithm does not maintain a search tree, so the data structure for the current node need only recordthe state and the value of the objective function.

Token 2661:
Hill climbing does not look ahead beyondthe immediate neighbors of the current state.

Token 2662:
This resembles trying to ﬁnd the top of MountEverest in a thick fog while suffering from amnesia.

Token 2663:
To illustrate hill climbing, we will use the 8-queens problem introduced on page 71.

Token 2664:
Local search algorithms typically use a complete-state formulation , where each state has 8 queens on the board, one per column.

Token 2665:
The successors of a state are all possible statesgenerated by moving a single queen to another square in the same column (so each state has8×7=56 successors).

Token 2666:
The heuristic cost function his the number of pairs of queens that are attacking each other, either directly or indirectly.

Token 2667:
The global minimum of this function is zero, which occurs only at perfect solutions.

Token 2668:
Figure 4.3(a) shows a state with h=1 7 .T h e ﬁgure also shows the values of all its successors, with the best successors having h=1 2 .

Token 2669:
Hill-climbing algorithms typically choose randomly among the set of best successors if there is more than one.

Token 2670:
Hill climbing is sometimes called greedy local search because it grabs a good neighbor GREEDY LOCAL SEARCH state without thinking ahead about where to go next.

Token 2671:
Although greed is considered one of the seven deadly sins, it turns out that greedy algorithms often perform quite well.

Token 2672:
Hill climbingoften makes rapid progress toward a solution because it is usually quite easy to improve a badstate.

Token 2673:
For example, from the state in Figure 4.3(a), it takes just ﬁve steps to reach the statein Figure 4.3(b), which has h=1and is very nearly a solution.

Token 2674:
Unfortunately, hill climbing often gets stuck for the following reasons: •Local maxima : a local maximum is a peak that is higher than each of its neighboring LOCAL MAXIMUM states but lower than the global maximum.

Token 2675:
Hill-climbing algorithms that reach the vicinity of a local maximum will be drawn upward toward the peak but will then bestuck with nowhere else to go.

Token 2676:
Figure 4.1 illustrates the problem schematically. More

Token 2677:
Section 4.1.

Token 2678:
Local Search Algorithms and Optimization Problems 123 141817151418 14 14141414121612 13161714181314 17151815131513 12151513151213 14141416121412 12151613141214 18161616141614 (a) (b) Figure 4.3 (a) An 8-queens state with heuristic cost estimate h=17 , showing the value of hfor each possible successor obtained by movin g a queen within its column.

Token 2679:
The best moves are marked. (b) A local minimum in the 8-queens state space; the state has h=1but every successor has a higher cost.

Token 2680:
concretely, the state in Figure 4.3(b) is a local maximum (i.e., a local minimum for the costh); every move of a single queen makes the situation worse.

Token 2681:
•Ridges : a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima RIDGE that is very difﬁcult for greedy algorithms to navigate.

Token 2682:
•Plateaux : a plateau is a ﬂat area of the state-space landscape.

Token 2683:
It can be a ﬂat local PLATEAU maximum, from which no uphill exit exists, or a shoulder , from which progress is SHOULDER possible. (See Figure 4.1.)

Token 2684:
A hill-climbing search might get lost on the plateau. In each case, the algorithm reaches a point at which no progress is being made.

Token 2685:
Starting from a randomly generated 8-queens state, steepest-ascent hill climbing gets stuck 86% of the time,solving only 14% of problem instances.

Token 2686:
It works quickly, taking just 4 steps on average whenit succeeds and 3 when it gets stuck—not bad for a state space with 8 8≈17million states.

Token 2687:
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has the same value as the current state.

Token 2688:
Might it not be a good idea to keep going—to allow asideways move in the hope that the plateau is really a shoulder, as shown in Figure 4.1?

Token 2689:
The SIDEWAYS MOVE answer is usually yes, but we must take care.

Token 2690:
If we always allow sideways moves when there are no uphill moves, an inﬁnite loop will occur whenever the algorithm reaches a ﬂat local maximum that is not a shoulder.

Token 2691:
One common solution is to put a limit on the number of con- secutive sideways moves allowed.

Token 2692:
For example, we could allow up to, say, 100 consecutive sideways moves in the 8-queens problem.

Token 2693:
This raises the percentage of problem instances solved by hill climbing from 14% to 94%.

Token 2694:
Success comes at a cost: the algorithm averages roughly 21 steps for each successful instance and 64 for each failure.

Token 2695:
124 Chapter 4. Beyond Classical Search Figure 4.4 Illustration of why ridges cause difﬁculties for hill climbing.

Token 2696:
The grid of states (dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local maxima that are not directly connected to each other.

Token 2697:
From each local maximum, all the available actions point downhill. Many variants of hill climbing have been invented.

Token 2698:
Stochastic hill climbing chooses atSTOCHASTIC HILL CLIMBING random from among the uphill moves; the probability of selection can vary with the steepness of the uphill move.

Token 2699:
This usually converges more slowly than steepest ascent, but in somestate landscapes, it ﬁnds better solutions.

Token 2700:
First-choice hill climbing implements stochastic FIRST-CHOICEHILL CLIMBING hill climbing by generating successors randomly until one is generated that is better than the current state.

Token 2701:
This is a good strategy when a state has many (e.g., thousands) of successors.

Token 2702:
The hill-climbing algorithms described so far are incomplete—they often fail to ﬁnd a goal when one exists because they can get stuck on local maxima.

Token 2703:
Random-restart hill climbing adopts the well-known adage, “If at ﬁrst you don’t succeed, try, try again.” It con-RANDOM-RESTART HILL CLIMBING ducts a series of hill-climbing searches from randomly generated initial states,1until a goal is found.

Token 2704:
It is trivially complete with probability approaching 1, because it will eventuallygenerate a goal state as the initial state.

Token 2705:
If each hill-climbing search has a probability pof success, then the expected number of restarts required is 1/p.

Token 2706:
For 8-queens instances with no sideways moves allowed, p≈0.14, so we need roughly 7 iterations to ﬁnd a goal (6 fail- ures and 1 success).

Token 2707:
The expected number of steps is the cost of one successful iteration plus(1−p)/ptimes the cost of failure, or roughly 22 steps in all.

Token 2708:
When we allow sideways moves, 1/0.94≈1.06iterations are needed on average and (1×21)+(0 .06/0.94)×64≈25steps.

Token 2709:
For 8-queens, then, random-restart hill climbing is very effective indeed.

Token 2710:
Even for three mil-lion queens, the approach can ﬁnd solutions in under a minute.

Token 2711:
2 1Generating a random state from an implicitly speciﬁed state space can be a hard problem in itself. 2Luby et al.

Token 2712:
(1993) prove that it is best, in some cases, to restart a randomized search algorithm after a particular, ﬁxed amount of time and that this can be much more efﬁcient than letting each search continue indeﬁnitely.

Token 2713:
Disallowing or limiting the number of sideways moves is an example of this idea.

Token 2714:
Section 4.1.

Token 2715:
Local Search Algorithms and Optimization Problems 125 The success of hill climbing depends very much on the shape of the state-space land- scape: if there are few local maxima and plateaux, random-restart hill climbing will ﬁnd agood solution very quickly.

Token 2716:
On the other hand, many real problems have a landscape thatlooks more like a widely scattered family of balding porcupines on a ﬂat ﬂoor, with miniatureporcupines living on the tip of each porcupine needle, ad inﬁnitum .

Token 2717:
NP-hard problems typi- cally have an exponential number of local maxima to get stuck on.

Token 2718:
Despite this, a reasonably good local maximum can often be found after a small number of restarts.

Token 2719:
4.1.2 Simulated annealing A hill-climbing algorithm that never makes “downhill” moves toward states with lower value (or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-mum.

Token 2720:
In contrast, a purely random walk—that is, moving to a successor chosen uniformlyat random from the set of successors—is complete but extremely inefﬁcient.

Token 2721:
Therefore, itseems reasonable to try to combine hill climbing with a random walk in some way that yieldsboth efﬁciency and completeness.

Token 2722:
Simulated annealing is such an algorithm.

Token 2723:
In metallurgy, SIMULATED ANNEALING annealing is the process used to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them, thus allowing the material to reach a low- energy crystalline state.

Token 2724:
To explain simulated annealing, we switch our point of view fromhill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a GRADIENT DESCENT ping-pong ball into the deepest crevice in a bumpy surface.

Token 2725:
If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of thelocal minimum.

Token 2726:
The trick is to shake just hard enough to bounce the ball out of local min-ima but not hard enough to dislodge it from the global minimum.

Token 2727:
The simulated-annealingsolution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce theintensity of the shaking (i.e., lower the temperature).

Token 2728:
The innermost loop of the simulated-annealing algorithm (Figure 4.5) is quite similar to hill climbing.

Token 2729:
Instead of picking the bestmove, however, it picks a random move. If the move improves the situation, it is always accepted.

Token 2730:
Otherwise, the algorithm accepts the move with some probability less than 1.

Token 2731:
The probability decreases exponentially with the “badness” ofthe move—the amount ΔEby which the evaluation is worsened.

Token 2732:
The probability also de- creases as the “temperature” Tgoes down: “bad” moves are more likely to be allowed at the start when Tis high, and they become more unlikely as Tdecreases.

Token 2733:
If the schedule lowers Tslowly enough, the algorithm will ﬁnd a global optimum with probability approaching 1.

Token 2734:
Simulated annealing was ﬁrst used extensively to solve VLSI layout problems in the early 1980s.

Token 2735:
It has been applied widely to factory scheduling and other large-scale optimiza-tion tasks.

Token 2736:
In Exercise 4.4, you are asked to compare its performance to that of random-restarthill climbing on the 8-queens puzzle.

Token 2737:
4.1.3 Local beam search Keeping just one node in memory might seem to be an extreme reaction to the problem ofmemory limitations.

Token 2738:
The local beam search algorithm 3keeps track of kstates rather thanLOCAL BEAM SEARCH 3Local beam search is an adaptation of beam search , which is a path-based algorithm.

Token 2739:
126 Chapter 4.

Token 2740:
Beyond Classical Search function SIMULATED -ANNEALING (problem ,schedule )returns a solution state inputs :problem , a problem schedule , a mapping from time to “temperature” current←MAKE-NODE(problem .INITIAL -STATE ) fort=1to∞do T←schedule (t) ifT=0then return current next←a randomly selected successor of current ΔE←next .VALUE –current .VALUE ifΔE>0thencurrent←next elsecurrent←next only with probability eΔE/T Figure 4.5 The simulated annealing algorithm, a version of stochastic hill climbing where some downhill moves are allowed.

Token 2741:
Downhill moves are accepted readily early in the anneal-ing schedule and then less often as time goes on.

Token 2742:
The schedule input determines the value of the temperature Tas a function of time. just one. It begins with krandomly generated states.

Token 2743:
At each step, all the successors of all k states are generated. If any one is a goal, the algorithm halts.

Token 2744:
Otherwise, it selects the kbest successors from the complete list and repeats.

Token 2745:
At ﬁrst sight, a local beam search with kstates might seem to be nothing more than running krandom restarts in parallel instead of in sequence.

Token 2746:
In fact, the two algorithms are quite different. In a random-restart search, each search process runs independently ofthe others.

Token 2747:
In a local beam search, useful information is passed among the parallel search threads.

Token 2748:
In effect, the states that generate the best successors say to the others, “Come over here, the grass is greener!” The algorithm quickly abandons unfruitful searches and movesits resources to where the most progress is being made.

Token 2749:
In its simplest form, local beam search can suffer from a lack of diversity among the kstates—they can quickly become concentrated in a small region of the state space, making the search little more than an expensive version of hill climbing.

Token 2750:
A variant called stochastic beam search , analogous to stochastic hill climbing, helps alleviate this problem.

Token 2751:
Instead STOCHASTIC BEAM SEARCH of choosing the best kfrom the the pool of candidate successors, stochastic beam search chooses ksuccessors at random, with the probability of choosing a given successor being an increasing function of its value.

Token 2752:
Stochastic beam search bears some resemblance to theprocess of natural selection, whereby the “successors” (offspring) of a “state” (organism)populate the next generation according to its “value” (ﬁtness).

Token 2753:
4.1.4 Genetic algorithms Agenetic algorithm (orGA) is a variant of stochastic beam search in which successor statesGENETIC ALGORITHM are generated by combining twoparent states rather than by modifying a single state.

Token 2754:
The analogy to natural selection is the same as in stochastic beam search, except that now we aredealing with sexual rather than asexual reproduction.

Token 2755:
Section 4.1.

Token 2756:
Local Search Algorithms and Optimization Problems 127 (a) Initial Population(b) Fitness Function(c) Selection(d) Crossover(e) Mutation24 23 201129%31% 26% 14%32752411 24748552 327524112441512432748552 24752411 3275212424415411 322521242475241132748152 2441541724748552 32752411 24415124 32543213 Figure 4.6 The genetic algorithm, illustrated for digit strings representing 8-queens states.

Token 2757:
The initial population in (a) is ranked by the ﬁ tness function in (b), resulting in pairs for mating in (c).

Token 2758:
They produce offspring in (d), which are subject to mutation in (e).

Token 2759:
+= Figure 4.7 The 8-queens states corresponding to the ﬁrst two parents in Figure 4.6(c) and the ﬁrst offspring in Figure 4.6(d).

Token 2760:
The shaded columns are lost in the crossover step and theunshaded columns are retained.

Token 2761:
Like beam searches, GAs begin with a set of krandomly generated states, called the population .

Token 2762:
Each state, or individual , is represented as a string over a ﬁnite alphabet—most POPULATION INDIVIDUAL commonly, a string of 0s and 1s.

Token 2763:
For example, an 8-queens state must specify the positions of 8 queens, each in a column of 8 squares, and so requires 8×log28=24 bits.

Token 2764:
Alternatively, the state could be represented as 8 digits, each in the range from 1 to 8.

Token 2765:
(We demonstrate laterthat the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digitstrings representing 8-queens states.

Token 2766:
The production of the next generation of states is shown in Figure 4.6(b)–(e).

Token 2767:
In (b), each state is rated by the objective function, or (in GA terminology) the ﬁtness function .A FITNESS FUNCTION ﬁtness function should return higher values for better states, so, for the 8-queens problem we use the number of nonattacking pairs of queens, which has a value of 28 for a solution.

Token 2768:
The values of the four states are 24, 23, 20, and 11.

Token 2769:
In this particular variant of the genetic algorithm, the probability of being chosen for reproducing is directly proportional to theﬁtness score, and the percentages are shown next to the raw scores.

Token 2770:
In (c), two pairs are selected at random for reproduction, in accordance with the prob-

Token 2771:
128 Chapter 4. Beyond Classical Search abilities in (b).

Token 2772:
Notice that one individual is selected twice and one not at all.4For each pair to be mated, a crossover point is chosen randomly from the positions in the string.

Token 2773:
In CROSSOVER Figure 4.6, the crossover points are after the third digit in the ﬁrst pair and after the ﬁfth digit in the second pair.5 In (d), the offspring themselves are created by crossing over the parent strings at the crossover point.

Token 2774:
For example, the ﬁrst child of the ﬁrst pair gets the ﬁrst three digits from the ﬁrst parent and the remaining digits from the second parent, whereas the second child getsthe ﬁrst three digits from the second parent and the rest from the ﬁrst parent.

Token 2775:
The 8-queensstates involved in this reproduction step are shown in Figure 4.7.

Token 2776:
The example shows thatwhen two parent states are quite different, the crossover operation can produce a state that isa long way from either parent state.

Token 2777:
It is often the case that the population is quite diverseearly on in the process, so crossover (like simulated annealing) frequently takes large steps inthe state space early in the search process and smaller steps later on when most individualsare quite similar.

Token 2778:
Finally, in (e), each location is subject to random mutation with a small independent MUTATION probability.

Token 2779:
One digit was mutated in the ﬁrst, third, and fourth offspring.

Token 2780:
In the 8-queens problem, this corresponds to choosing a queen at random and moving it to a random square in its column.

Token 2781:
Figure 4.8 describes an algorithm that implements all these steps.

Token 2782:
Like stochastic beam search, genetic algorithms combine an uphill tendency with ran- dom exploration and exchange of information among parallel search threads.

Token 2783:
The primaryadvantage, if any, of genetic algorithms comes from the crossover operation.

Token 2784:
Yet it can beshown mathematically that, if the positions of the genetic code are permuted initially in arandom order, crossover conveys no advantage.

Token 2785:
Intuitively, the advantage comes from theability of crossover to combine large blocks of letters that have evolved independently to per-form useful functions, thus raising the level of granularity at which the search operates.

Token 2786:
Forexample, it could be that putting the ﬁrst three queens in positions 2, 4, and 6 (where they donot attack each other) constitutes a useful block that can be combined with other blocks toconstruct a solution.

Token 2787:
The theory of genetic algorithms explains how this works using the idea of a schema , SCHEMA which is a substring in which some of the positions can be left unspeciﬁed.

Token 2788:
For example, the schema 246***** describes all 8-queens states in which the ﬁrst three queens are inpositions 2, 4, and 6, respectively.

Token 2789:
Strings that match the schema (such as 24613578) arecalled instances of the schema.

Token 2790:
It can be shown that if the average ﬁtness of the instances of INSTANCE a schema is above the mean, then the number of instances of the schema within the population will grow over time.

Token 2791:
Clearly, this effect is unlikely to be signiﬁcant if adjacent bits are totallyunrelated to each other, because then there will be few contiguous blocks that provide aconsistent beneﬁt.

Token 2792:
Genetic algorithms work best when schemata correspond to meaningfulcomponents of a solution.

Token 2793:
For example, if the string is a representation of an antenna, then theschemata may represent components of the antenna, such as reﬂectors and deﬂectors.

Token 2794:
A good 4There are many variants of this selection rule.

Token 2795:
The method of culling , in which all individuals below a given threshold are discarded, can be shown to con verge faster than the random version (Baum et al.

Token 2796:
, 1995). 5It is here that the encoding matters.

Token 2797:
If a 24-bit encoding is used instead of 8 digits, then the crossover point has a 2/3 chance of being in the middle of a digit, which results in an essentially arbitrary mutation of that digit.

Token 2798:
Section 4.2.

Token 2799:
Local Search in Continuous Spaces 129 function GENETIC -ALGORITHM (population ,FITNESS -FN)returns an individual inputs :population , a set of individuals FITNESS -FN, a function that measures the ﬁtness of an individual repeat new population←empty set fori=1toSIZE(population )do x←RANDOM -SELECTION (population ,FITNESS -FN) y←RANDOM -SELECTION (population ,FITNESS -FN) child←REPRODUCE (x,y) if(small random probability) thenchild←MUTATE (child ) addchild tonew population population←new population until some individual is ﬁt enough, or enough time has elapsed return the best individual in population , according to F ITNESS -FN function REPRODUCE (x,y)returns an individual inputs :x,y, parent individuals n←LENGTH (x);c←random number from 1 to n return APPEND (SUBSTRING (x,1 ,c), SUBSTRING (y,c+1,n)) Figure 4.8 A genetic algorithm.

Token 2800:
The algorithm is the same as the one diagrammed in Figure 4.6, with one variation: in this more popular version, each mating of two parentsproduces only one offspring, not two.

Token 2801:
component is likely to be good in a variety of different designs.

Token 2802:
This suggests that successful use of genetic algorithms requires careful engineering of the representation.

Token 2803:
In practice, genetic algorithms have had a widespread impact on optimization problems, such as circuit layout and job-shop scheduling.

Token 2804:
At present, it is not clear whether the appealof genetic algorithms arises from their performance or from their æsthetically pleasing originsin the theory of evolution.

Token 2805:
Much work remains to be done to identify the conditions underwhich genetic algorithms perform well.

Token 2806:
4.2 L OCAL SEARCH IN CONTINUOUS SPACES In Chapter 2, we explained the distinction between discrete and continuous environments,pointing out that most real-world environments are continuous.

Token 2807:
Yet none of the algorithms we have described (except for ﬁrst-choice hill climbing and simulated annealing) can handle continuous state and action spaces, because they have inﬁnite branching factors.

Token 2808:
This sectionprovides a very brief introduction to some local search techniques for ﬁnding optimal solu- tions in continuous spaces.

Token 2809:
The literature on this topic is vast; many of the basic techniques

Token 2810:
130 Chapter 4.

Token 2811:
Beyond Classical Search EVOLUTION AND SEARCH The theory of evolution was developed in Charles Darwin’s On the Origin of Species by Means of Natural Selection (1859) and independently by Alfred Russel Wallace (1858).

Token 2812:
The central idea is simple: variations occur in reproduction andwill be preserved in successive generations approximately in proportion to theireffect on reproductive ﬁtness.

Token 2813:
Darwin’s theory was developed with no knowledge of how the traits of organ- isms can be inherited and modiﬁed.

Token 2814:
The probabilistic laws governing these pro-cesses were ﬁrst identiﬁed by Gregor Mendel (1866), a monk who experimentedwith sweet peas.

Token 2815:
Much later, Watson and Crick (1953) identiﬁed the structure of theDNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine).

Token 2816:
In the standard model, variation occurs both by point mutations in the letter sequence and by “crossover” (in which the DNA of an offspring is generated by combininglong sections of DNA from each parent).

Token 2817:
The analogy to local search algorithms has already been described; the princi- pal difference between stochastic beam search and evolution is the use of sexual re- production, wherein successors are generated from multiple organisms rather than just one.

Token 2818:
The actual mechanisms of evolution are, however, far richer than mostgenetic algorithms allow.

Token 2819:
For example, mutations can involve reversals, duplica-tions, and movement of large chunks of DNA; some viruses borrow DNA from oneorganism and insert it in another; and there are transposable genes that do nothingbut copy themselves many thousands of times within the genome.

Token 2820:
There are even genes that poison cells from potential mates that do not carry the gene, thereby in- creasing their own chances of replication.

Token 2821:
Most important is the fact that the genes themselves encode the mechanisms whereby the genome is reproduced and trans- lated into an organism.

Token 2822:
In genetic algorithms, those mechanisms are a separateprogram that is not represented within the strings being manipulated.

Token 2823:
Darwinian evolution may appear inefﬁcient, having generated blindly some 10 45or so organisms without improving its search heuristics one iota.

Token 2824:
Fifty years before Darwin, however, the otherwise great French naturalist Jean Lamarck(1809) proposed a theory of evolution whereby traits acquired by adaptation dur- ing an organism’s lifetime would be passed on to its offspring.

Token 2825:
Such a process would be effective but does not seem to occur in nature.

Token 2826:
Much later, James Bald- win (1896) proposed a superﬁcially similar theory: that behavior learned during an organism’s lifetime could accelerate the rate of evolution.

Token 2827:
Unlike Lamarck’s, Bald-win’s theory is entirely consistent with Darwinian evolution because it relies on se-lection pressures operating on individuals that have found local optima among theset of possible behaviors allowed by their genetic makeup.

Token 2828:
Computer simulationsconﬁrm that the “Baldwin effect” is real, once “ordinary” evolution has createdorganisms whose internal performance measure correlates with actual ﬁtness.

Token 2829:
Section 4.2.

Token 2830:
Local Search in Continuous Spaces 131 originated in the 17th century, after the development of calculus by Newton and Leibniz.6We ﬁnd uses for these techniques at several places in the book, including the chapters on learning,vision, and robotics.

Token 2831:
We begin with an example.

Token 2832:
Suppose we want to place three new airports anywhere in Romania, such that the sum of squared distances from each city on the map (Figure 3.2) to its nearest airport is minimized.

Token 2833:
The state space is then deﬁned by the coordinates of the airports: (x 1,y1),(x2,y2),a n d(x3,y3).T h i s i s a six-dimensional space; we also say that states are deﬁned by six variables .

Token 2834:
(In general, states are deﬁned by an n-dimensional VARIABLE vector of variables, x.)

Token 2835:
Moving around in this space corresponds to moving one or more of the airports on the map.

Token 2836:
The objective function f(x1,y1,x2,y2,x3,y3)is relatively easy to compute for any particular state once we compute the closest cities.

Token 2837:
Let Cibe the set of cities whose closest airport (in the current state) is airport i.

Token 2838:
Then, in the neighborhood of the current state ,w h e r et h e Cis remain constant, we have f(x1,y1,x2,y2,x3,y3)=3/summationdisplay i=1/summationdisplay c∈Ci(xi−xc)2+(yi−yc)2.

Token 2839:
(4.1) This expression is correct locally , but not globally because the sets Ciare (discontinuous) functions of the state.

Token 2840:
One way to avoid continuous problems is simply to discretize the neighborhood of each DISCRETIZATION state.

Token 2841:
For example, we can move only one airport at a time in either the xorydirection by a ﬁxed amount ±δ.

Token 2842:
With 6 variables, this gives 12 possible successors for each state. We can then apply any of the local search algorithms described previously.

Token 2843:
We could also ap-ply stochastic hill climbing and simulated annealing directly, without discretizing the space.These algorithms choose successors randomly, which can be done by generating random vec-tors of length δ.

Token 2844:
Many methods attempt to use the gradient of the landscape to ﬁnd a maximum.

Token 2845:
The GRADIENT gradient of the objective function is a vector ∇fthat gives the magnitude and direction of the steepest slope.

Token 2846:
For our problem, we have ∇f=/parenleftbigg∂f ∂x1,∂f ∂y1,∂f ∂x2,∂f ∂y2,∂f ∂x3,∂f ∂y3/parenrightbigg .

Token 2847:
In some cases, we can ﬁnd a maximum by solving the equation ∇f=0.

Token 2848:
(This could be done, for example, if we were placing just one airport; the solution is the arithmetic mean of all the cities’ coordinates.)

Token 2849:
In many cases, however, this equation cannot be solved in closed form.For example, with three airports, the expression for the gradient depends on what cities areclosest to each airport in the current state.

Token 2850:
This means we can compute the gradient locally (but not globally ); for example, ∂f ∂x1=2/summationdisplay c∈C1(xi−xc).

Token 2851:
(4.2) Given a locally correct expression for the gradient, we can perform steepest-ascent hill climb- 6A basic knowledge of multivariate calculus and vector arithmetic is useful for reading this section.

Token 2852:
132 Chapter 4.

Token 2853:
Beyond Classical Search ing by updating the current state according to the formula x←x+α∇f(x), where αis a small constant often called the step size .

Token 2854:
In other cases, the objective function STEP SIZE might not be available in a differentiable form at all—for example, the value of a particular set of airport locations might be determined by running some large-scale economic simulationpackage.

Token 2855:
In those cases, we can calculate a so-called empirical gradient by evaluating the EMPIRICAL GRADIENT response to small increments and decrements in each coordinate.

Token 2856:
Empirical gradient search is the same as steepest-ascent hill climbing in a discretized version of the state space.

Token 2857:
Hidden beneath the phrase “ αis a small constant” lies a huge variety of methods for adjusting α.

Token 2858:
The basic problem is that, if αis too small, too many steps are needed; if α is too large, the search could overshoot the maximum.

Token 2859:
The technique of line search tries to LINESEARCH overcome this dilemma by extending the current gradient direction—usually by repeatedly doubling α—until fstarts to decrease again.

Token 2860:
The point at which this occurs becomes the new current state.

Token 2861:
There are several schools of thought about how the new direction should bechosen at this point.

Token 2862:
For many problems, the most effective algorithm is the venerable Newton–Raphson NEWTON–RAPHSON method.

Token 2863:
This is a general technique for ﬁnding roots of functions—that is, solving equations of the form g(x)=0 .

Token 2864:
It works by computing a new estimate for the root xaccording to Newton’s formula x←x−g(x)/g/prime(x).

Token 2865:
To ﬁnd a maximum or minimum of f, we need to ﬁnd xsuch that the gradient is zero (i.e., ∇f(x)=0).

Token 2866:
Thus, g(x)in Newton’s formula becomes ∇f(x), and the update equation can be written in matrix–vector form as x←x−H−1 f(x)∇f(x), where Hf(x)is the Hessian matrix of second derivatives, whose elements Hijare given HESSIAN by∂2f/∂x i∂xj.

Token 2867:
For our airport example, we can see from Equation (4.2) that Hf(x)is particularly simple: the off-diagonal elements are zero and the diagonal elements for airportiare just twice the number of cities in C i.

Token 2868:
A moment’s calculation shows that one step of the update moves airport idirectly to the centroid of Ci, which is the minimum of the local expression for ffrom Equation (4.1).7For high-dimensional problems, however, computing then2entries of the Hessian and inverting it may be expensive, so many approximate versions of the Newton–Raphson method have been developed.

Token 2869:
Local search methods suffer from local maxima, ridges, and plateaux in continuous state spaces just as much as in discrete spaces.

Token 2870:
Random restarts and simulated annealing canbe used and are often helpful.

Token 2871:
High-dimensional continuous spaces are, however, big placesin which it is easy to get lost.

Token 2872:
A ﬁnal topic with which a passing acquaintance is useful is constrained optimization .

Token 2873:
CONSTRAINED OPTIMIZATION An optimization problem is constrained if solutions must satisfy some hard constraints on the values of the variables.

Token 2874:
For example, in our airport-siting problem, we might constrain sites 7In general, the Newton–Raphson update can be seen as ﬁtting a quadratic surface to fatxand then moving directly to the minimum of that surface—which is also the minimum of fiffis quadratic.

Token 2875:
Section 4.3. Searching with Nondeterministic Actions 133 to be inside Romania and on dry land (rather than in the middle of lakes).

Token 2876:
The difﬁculty of constrained optimization problems depends on the nature of the constraints and the objectivefunction.

Token 2877:
The best-known category is that of linear programming problems, in which con- LINEAR PROGRAMMING straints must be linear inequalities forming a convex set8and the objective function is also CONVEX SET linear.

Token 2878:
The time complexity of linear programming is polynomial in the number of variables.

Token 2879:
Linear programming is probably the most widely studied and broadly useful class of optimization problems.

Token 2880:
It is a special case of the more general problem of convex opti- mization , which allows the constraint region to be any convex region and the objective toCONVEX OPTIMIZATION be any function that is convex within the constraint region.

Token 2881:
Under certain conditions, convex optimization problems are also polynomially solvable and may be feasible in practice withthousands of variables.

Token 2882:
Several important problems in machine learning and control theorycan be formulated as convex optimization problems (see Chapter 20).

Token 2883:
4.3 S EARCHING WITH NONDETERMINISTIC ACTIONS In Chapter 3, we assumed that the environment is fully observable and deterministic and thatthe agent knows what the effects of each action are.

Token 2884:
Therefore, the agent can calculate exactlywhich state results from any sequence of actions and always knows which state it is in.

Token 2885:
Itspercepts provide no new information after each action, although of course they tell the agentthe initial state.

Token 2886:
When the environment is either partially observable or nondeterministic (or both), per- cepts become useful.

Token 2887:
In a partially observable environment, every percept helps narrow downthe set of possible states the agent might be in, thus making it easier for the agent to achieveits goals.

Token 2888:
When the environment is nondeterministic, percepts tell the agent which of the pos-sible outcomes of its actions has actually occurred.

Token 2889:
In both cases, the future percepts cannot be determined in advance and the agent’s future actions will depend on those future percepts.

Token 2890:
So the solution to a problem is not a sequence but a contingency plan (also known as a strat- CONTINGENCYPLAN egy) that speciﬁes what to do depending on what percepts are received.

Token 2891:
In this section, we STRATEGY examine the case of nondeterminism, deferring partial observability to Section 4.4.

Token 2892:
4.3.1 The erratic vacuum world As an example, we use the vacuum world, ﬁrst introduced in Chapter 2 and deﬁned as a search problem in Section 3.2.1.

Token 2893:
Recall that the state space has eight states, as shown inFigure 4.9.

Token 2894:
There are three actions— Left,Right ,a n d Suck—and the goal is to clean up all the dirt (states 7 and 8).

Token 2895:
If the environment is observable, deterministic, and completelyknown, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the solution is an action sequence.

Token 2896:
For example, if the initial state is 1, then the action sequence [Suck,Right ,Suck] will reach a goal state, 8.

Token 2897:
8A set of points Sis convex if the line joining any two points in Sis also contained in S.Aconvex function is one for which the space “above” it forms a convex set; by deﬁnition, convex functions have no local (as opposedto global) minima.

Token 2898:
134 Chapter 4. Beyond Classical Search 12 8 75634 Figure 4.9 The eight possible states of the vacuum world; states 7 and 8 are goal states.

Token 2899:
Now suppose that we introduce nondeterminism in the form of a powerful but erratic vacuum cleaner.

Token 2900:
In the erratic vacuum world ,t h e Suck action works as follows:ERRATIC VACUUM WORLD •When applied to a dirty square the action cleans the square and sometimes cleans up dirt in an adjacent square, too.

Token 2901:
•When applied to a clean square the action sometimes deposits dirt on the carpet.9 To provide a precise formulation of this problem, we need to generalize the notion of a tran- sition model from Chapter 3.

Token 2902:
Instead of deﬁning the transition model by a R ESULT function that returns a single state, we use a R ESULTS function that returns a setof possible outcome states.

Token 2903:
For example, in the erratic vacuum world, the Suck action in state 1 leads to a state in the set{5,7}—the dirt in the right-hand square may or may not be vacuumed up.

Token 2904:
We also need to generalize the notion of a solution to the problem.

Token 2905:
For example, if we start in state 1, there is no single sequence of actions that solves the problem.

Token 2906:
Instead, we need a contingency plan such as the following: [Suck,ifState=5then[Right,Suck]else[]].

Token 2907:
(4.3) Thus, solutions for nondeterministic problems can contain nested if–then –else statements; this means that they are trees rather than sequences.

Token 2908:
This allows the selection of actions based on contingencies arising during execution.

Token 2909:
Many problems in the real, physical worldare contingency problems because exact prediction is impossible.

Token 2910:
For this reason, manypeople keep their eyes open while walking around or driving.

Token 2911:
9We assume that most readers face similar problems and can sympathize with our agent.

Token 2912:
We apologize to owners of modern, efﬁcient home appliances who cannot take advantage of this pedagogical device.

Token 2913:
Section 4.3.

Token 2914:
Searching with Nondeterministic Actions 135 4.3.2 AND–ORsearch trees The next question is how to ﬁnd contingent solutions to nondeterministic problems.

Token 2915:
As in Chapter 3, we begin by constructing search trees, but here the trees have a different character.In a deterministic environment, the only branching is introduced by the agent’s own choicesin each state.

Token 2916:
We call these nodes ORnodes . In the vacuum world, for example, at an OR OR NODE node the agent chooses Left or Right or Suck .

Token 2917:
In a nondeterministic environment, branching is also introduced by the environment’s choice of outcome for each action.

Token 2918:
We call these nodes AND nodes .

Token 2919:
For example, the Suck action in state 1 leads to a state in the set {5,7}, AND NODE so the agent would need to ﬁnd a plan for state 5 andfor state 7.

Token 2920:
These two kinds of nodes alternate, leading to an AND –ORtree as illustrated in Figure 4.10.

Token 2921:
AND–OR TREE A solution for an AND –ORsearch problem is a subtree that (1) has a goal node at every leaf, (2) speciﬁes one action at each of its ORnodes, and (3) includes every outcome branch at each of its AND nodes.

Token 2922:
The solution is shown in bold lines in the ﬁgure; it corresponds to the plan given in Equation (4.3).

Token 2923:
(The plan uses if–then–else notation to handle the AND branches, but when there are more than two branches at a node, it might be better to use a case Left SuckRight Suck Right Suck 6 GOAL8 GOAL7 1 2 5 1 LOOP5 LOOP 5 LOOPLeft Suck 1 LOOP GOAL8 4 Figure 4.10 The ﬁrst two levels of the search tree for the erratic vacuum world.

Token 2924:
State nodes are ORnodes where some action must be chosen.

Token 2925:
At the AND nodes, shown as circles, every outcome must be handled, as indicated by the arc linking the outgoing branches.

Token 2926:
The solution found is shown in bold lines.

Token 2927:
136 Chapter 4.

Token 2928:
Beyond Classical Search function AND-OR-GRAPH -SEARCH (problem )returns a conditional plan ,or failure OR-SEARCH (problem .INITIAL -STATE ,problem ,[]) function OR-SEARCH (state ,problem ,path )returns a conditional plan ,or failure ifproblem .GOAL-TEST(state )then return the empty plan ifstate is onpath then return failure for each action inproblem .ACTIONS (state )do plan←AND-SEARCH (RESULTS (state ,action ),problem ,[state|path]) ifplan/negationslash=failure then return [action|plan] return failure function AND-SEARCH (states ,problem ,path )returns a conditional plan ,or failure for each siinstates do plani←OR-SEARCH (si,problem ,path ) ifplani=failure then return failure return [ifs1thenplan1else ifs2thenplan2else...ifsn−1thenplann−1elseplann] Figure 4.11 An algorithm for searching AND –ORgraphs generated by nondeterministic environments.

Token 2929:
It returns a conditional plan that reaches a goal state in all circumstances.

Token 2930:
(The notation [x|l]refers to the list formed by adding object xto the front of list l.) construct.)

Token 2931:
Modifying the basic problem-solving agent shown in Figure 3.1 to execute con- tingent solutions of this kind is straightforward.

Token 2932:
One may also consider a somewhat differentagent design, in which the agent can act before it has found a guaranteed plan and deals with some contingencies only as they arise during execution.

Token 2933:
This type of interleaving of search INTERLEAVING and execution is also useful for exploration problems (see Section 4.5) and for game playing (see Chapter 5).

Token 2934:
Figure 4.11 gives a recursive, depth-ﬁrst algorithm for AND –ORgraph search.

Token 2935:
One key aspect of the algorithm is the way in which it deals with cycles, which often arise in nondeterministic problems (e.g., if an action sometimes has no effect or if an unintendedeffect can be corrected).

Token 2936:
If the current state is identical to a state on the path from the root,then it returns with failure.

Token 2937:
This doesn’t mean that there is nosolution from the current state; it simply means that if there isa noncyclic solution, it must be reachable from the earlier incarnation of the current state, so the new incarnation can be discarded.

Token 2938:
With this check, weensure that the algorithm terminates in every ﬁnite state space, because every path must reacha goal, a dead end, or a repeated state.

Token 2939:
Notice that the algorithm does not check whether thecurrent state is a repetition of a state on some other path from the root, which is important for efﬁciency.

Token 2940:
Exercise 4.5 investigates this issue. AND –ORgraphs can also be explored by breadth-ﬁrst or best-ﬁrst methods.

Token 2941:
The concept of a heuristic function must be modiﬁed to estimate the cost of a contingent solution rather than a sequence, but the notion of admissibility carries over and there is an analog of the A∗ algorithm for ﬁnding optimal solutions.

Token 2942:
Pointers are given in the bibliographical notes at theend of the chapter.

Token 2943:
Section 4.3.

Token 2944:
Searching with Nondeterministic Actions 137 Suck Right 6 1 2 5 Right Figure 4.12 Part of the search graph for the slippery vacuum world, where we have shown (some) cycles explicitly.

Token 2945:
All solutions for this problem are cyclic plans because there is no w a yt om o v er e l i a b l y .

Token 2946:
4.3.3 Try, try again Consider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac- uum world except that movement actions sometimes fail, leaving the agent in the same loca-tion.

Token 2947:
For example, moving Right in state 1 leads to the state set {1,2}.

Token 2948:
Figure 4.12 shows part of the search graph; clearly, there are no longer any acyclic solutions from state 1, and A ND-OR-GRAPH -SEARCH would return with failure.

Token 2949:
There is, however, a cyclic solution , CYCLIC SOLUTION w h i c hi st ok e e pt r y i n g Right until it works.

Token 2950:
We can express this solution by adding a label to LABEL denote some portion of the plan and using that label later instead of repeating the plan itself.

Token 2951:
Thus, our cyclic solution is [Suck,L1:Right,ifState=5thenL1elseSuck].

Token 2952:
(A better syntax for the looping part of this plan would be “ whileState=5doRight .”) In general a cyclic plan may be considered a solution provided that every leaf is a goalstate and that a leaf is reachable from every point in the plan.

Token 2953:
The modiﬁcations neededto A ND-OR-GRAPH -SEARCH are covered in Exercise 4.6.

Token 2954:
The key realization is that a loop in the state space back to a state Ltranslates to a loop in the plan back to the point where the subplan for state Lis executed.

Token 2955:
Given the deﬁnition of a cyclic solution, an agent executing such a solution will eventu- ally reach the goal provided that each outcome of a nondeterministic action eventually occurs .

Token 2956:
Is this condition reasonable? It depends on the reason for the nondeterminism.

Token 2957:
If the action rolls a die, then it’s reasonable to suppose that eventually a six will be rolled.

Token 2958:
If the action isto insert a hotel card key into the door lock, but it doesn’t work the ﬁrst time, then perhaps itwill eventually work, or perhaps one has the wrong key (or the wrong room!).

Token 2959:
After seven or

Token 2960:
138 Chapter 4.

Token 2961:
Beyond Classical Search eight tries, most people will assume the problem is with the key and will go back to the front desk to get a new one.

Token 2962:
One way to understand this decision is to say that the initial problemformulation (observable, nondeterministic) is abandoned in favor of a different formulation(partially observable, deterministic) where the failure is attributed to an unobservable prop-erty of the key.

Token 2963:
We have more to say on this issue in Chapter 13.

Token 2964:
4.4 S EARCHING WITH PARTIAL OBSERV ATIONS We now turn to the problem of partial observability, where the agent’s percepts do not suf- ﬁce to pin down the exact state.

Token 2965:
As noted at the beginning of the previous section, if the agent is in one of several possible states, then an action may lead to one of several possibleoutcomes— even if the environment is deterministic .

Token 2966:
The key concept required for solving partially observable problems is the belief state , representing the agent’s current belief about BELIEF STATE the possible physical states it might be in, given the sequence of actions and percepts up to that point.

Token 2967:
We begin with the simplest scenario for studying belief states, which is when theagent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.

Token 2968:
4.4.1 Searching with no observation When the agent’s percepts provide no information at all , we have what is called a sensor- lessproblem or sometimes a conformant problem.

Token 2969:
At ﬁrst, one might think the sensorless SENSORLESS CONFORMANT agent has no hope of solving a problem if it has no idea what state it’s in; in fact, sensorless problems are quite often solvable.

Token 2970:
Moreover, sensorless agents can be surprisingly useful,primarily because they don’t rely on sensors working properly.

Token 2971:
In manufacturing systems, for example, many ingenious methods have been developed for orienting parts correctly froman unknown initial position by using a sequence of actions with no sensing at all.

Token 2972:
The highcost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-spectrum antibiotic rather than using the contingent plan of doing an expensive blood test, then waiting for the results to come back, and then prescribing a more speciﬁc antibiotic and perhaps hospitalization because the infection has progressed too far.

Token 2973:
We can make a sensorless version of the vacuum world.

Token 2974:
Assume that the agent knows the geography of its world, but doesn’t know its location or the distribution of dirt.

Token 2975:
In thatcase, its initial state could be any element of the set {1,2,3,4,5,6,7,8}. Now, consider what happens if it tries the action Right .

Token 2976:
This will cause it to be in one of the states {2,4,6,8}—the agent now has more information!

Token 2977:
Furthermore, the action sequence [ Right ,Suck] will always end up in one of the states {4,8}.

Token 2978:
Finally, the sequence [ Right ,Suck,Left,Suck] is guaranteed to reach the goal state 7 no matter what the start state.

Token 2979:
We say that the agent can coerce the COERCION w o r l di n t os t a t e7 .

Token 2980:
To solve sensorless problems, we search in the space of belief states rather than physical states.10Notice that in belief-state space, the problem is fully observable because the agent 10In a fully observable environment, each belief state contains one physical state.

Token 2981:
Thus, we can view the algo- rithms in Chapter 3 as searching in a belief-state space of singleton belief states.

Token 2982:
Section 4.4. Searching with Partial Observations 139 always knows its own belief state.

Token 2983:
Furthermore, the solution (if any) is always a sequence of actions.

Token 2984:
This is because, as in the ordinary problems of Chapter 3, the percepts received aftereach action are completely predictable—they’re always empty!

Token 2985:
So there are no contingenciesto plan for. This is true even if the environment is nondeterminstic .

Token 2986:
It is instructive to see how the belief-state search problem is constructed.

Token 2987:
Suppose the underlying physical problem Pis deﬁned by A CTIONS P,RESULT P,GOAL-TESTP,a n d STEP-COSTP.

Token 2988:
Then we can deﬁne the corresponding sensorless problem as follows: •Belief states : The entire belief-state space contains every possible set of physical states.

Token 2989:
IfPhasNstates, then the sensorless problem has up to 2Nstates, although many may be unreachable from the initial state.

Token 2990:
•Initial state : Typically the set of all states in P, although in some cases the agent will have more knowledge than this.

Token 2991:
•Actions : This is slightly tricky.

Token 2992:
Suppose the agent is in belief state b={s1,s2},b u t ACTIONS P(s1)/negationslash=ACTIONS P(s2); then the agent is unsure of which actions are legal.

Token 2993:
If we assume that illegal actions have no effect on the environment, then it is safe totake the union of all the actions in any of the physical states in the current belief state b: A CTIONS (b)=/uniondisplay s∈bACTIONS P(s).

Token 2994:
On the other hand, if an illegal action might be the end of the world, it is safer to allow only the intersection , that is, the set of actions legal in allthe states.

Token 2995:
For the vacuum world, every state has the same legal actions, so both methods give the same result.

Token 2996:
•Transition model : The agent doesn’t know which state in the belief state is the right one; so as far as it knows, it might get to any of the states resulting from applying theaction to one of the physical states in the belief state.

Token 2997:
For deterministic actions, the setof states that might be reached is b /prime=RESULT (b,a)={s/prime:s/prime=RESULT P(s,a)ands∈b}.

Token 2998:
(4.4) With deterministic actions, b/primeis never larger than b.

Token 2999:
With nondeterminism, we have b/prime=RESULT (b,a)={s/prime:s/prime∈RESULTS P(s,a)ands∈b} =/uniondisplay s∈bRESULTS P(s,a), which may be larger than b, as shown in Figure 4.13.

Token 3000:
The process of generating the new belief state after the action is called the prediction step; the notation b/prime= PREDICTION PREDICT P(b,a)will come in handy.

Token 3001:
•Goal test : The agent wants a plan that is sure to work, which means that a belief state satisﬁes the goal only if allthe physical states in it satisfy G OAL-TESTP.

Token 3002:
The agent may accidentally achieve the goal earlier, but it won’t know that it has done so. •Path cost : This is also tricky.

Token 3003:
If the same action can have different costs in different states, then the cost of taking an action in a given belief state could be one of several values.

Token 3004:
(This gives rise to a new class of problems, which we explore in Exercise 4.9.

Token 3005:
)For now we assume that the cost of an action is the same in all states and so can betransferred directly from the underlying physical problem.

Token 3006:
140 Chapter 4.

Token 3007:
Beyond Classical Search 2 4 1 3 2 4 1 3 1 3 (b) (a) Figure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a deterministic action, Right .

Token 3008:
(b) Prediction for the same belief state and action in the slippery version of the sensorless vacuum world.

Token 3009:
Figure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum world.

Token 3010:
There are only 12 reachable belief states out of 28= 256 possible belief states.

Token 3011:
The preceding deﬁnitions enable the automatic construction of the belief-state problem formulation from the deﬁnition of the underlying physical problem.

Token 3012:
Once this is done, wecan apply any of the search algorithms of Chapter 3. In fact, we can do a little bit morethan that.

Token 3013:
In “ordinary” graph search, newly generated states are tested to see if they areidentical to existing states.

Token 3014:
This works for belief states, too; for example, in Figure 4.14, theaction sequence [ Suck,Left,Suck] starting at the initial state reaches the same belief state as [Right ,Left,Suck], namely,{5,7}.

Token 3015:
Now, consider the belief state reached by [ Left], namely, {1,3,5,7}. Obviously, this is not identical to {5,7}, but it is a superset .

Token 3016:
It is easy to prove (Exercise 4.8) that if an action sequence is a solution for a belief state b, it is also a solution for any subset of b.

Token 3017:
Hence, we can discard a path reaching {1,3,5,7}if{5,7}has already been generated.

Token 3018:
Conversely, if {1,3,5,7}has already been generated and found to be solvable, then any subset ,s u c ha s{5,7}, is guaranteed to be solvable.

Token 3019:
This extra level of pruning may dramatically improve the efﬁciency of sensorless problem solving.

Token 3020:
Even with this improvement, however, sensorless problem-solving as we have described it is seldom feasible in practice.

Token 3021:
The difﬁculty is not so much the vastness of the belief-statespace—even though it is exponentially larger than the underlying physical state space; inmost cases the branching factor and solution length in the belief-state space and physicalstate space are not so different.

Token 3022:
The real difﬁculty lies with the size of each belief state.

Token 3023:
Forexample, the initial belief state for the 10×10vacuum world contains 100×2 100or around 1032physical states—far too many if we use the atomic representation, which is an explicit list of states.

Token 3024:
One solution is to represent the belief state by some more compact description.

Token 3025:
In English, we could say the agent knows “Nothing” in the initial state; after moving Left,w e could say, “Not in the rightmost column,” and so on.

Token 3026:
Chapter 7 explains how to do this in aformal representation scheme.

Token 3027:
Another approach is to avoid the standard search algorithms,which treat belief states as black boxes just like any other problem state.

Token 3028:
Instead, we can look

Token 3029:
Section 4.4.

Token 3030:
Searching with Partial Observations 141 L R SLR S LRS L RSL RS LR SL RS1 13 5724 6823 45 6 78 45 78 53 764 84 85 7 68873 7 Figure 4.14 The reachable portion of the belief-state space for the deterministic, sensor- less vacuum world.

Token 3031:
Each shaded box corresponds to a single belief state.

Token 3032:
At any given point, the agent is in a particular belief state but does not know which physical state it is in.

Token 3033:
The initial belief state (complete ignorance) is t he top center box. Actions are represented by labeled links. Self-loops are omitted for clarity.

Token 3034:
inside the belief states and develop incremental belief-state search algorithms that build upINCREMENTAL BELIEF-STATESEARCH the solution one physical state at a time.

Token 3035:
For example, in the sensorless vacuum world, the initial belief state is {1,2,3,4,5,6,7,8}, and we have to ﬁnd an action sequence that works in all 8 states.

Token 3036:
We can do this by ﬁrst ﬁnding a solution that works for state 1; then we checkif it works for state 2; if not, go back and ﬁnd a different solution for state 1, and so on.

Token 3037:
Justas an AND –ORsearch has to ﬁnd a solution for every branch at an AND node, this algorithm has to ﬁnd a solution for every state in the belief state; the difference is that AND –ORsearch can ﬁnd a different solution for each branch, whereas an incremental belief-state search has to ﬁnd onesolution that works for allthe states.

Token 3038:
The main advantage of the incremental approach is that it is typically able to detect failure quickly—when a belief state is unsolvable, it is usually the case that a small subset ofthe belief state, consisting of the ﬁrst few states examined, is also unsolvable.

Token 3039:
In some cases,

Token 3040:
142 Chapter 4.

Token 3041:
Beyond Classical Search this leads to a speedup proportional to the size of the belief states, which may themselves be as large as the physical state space itself.

Token 3042:
Even the most efﬁcient solution algorithm is not of much use when no solutions exist. Many things just cannot be done without sensing.

Token 3043:
For example, the sensorless 8-puzzle isimpossible. On the other hand, a little bit of sensing can go a long way.

Token 3044:
For example, every 8-puzzle instance is solvable if just one square is visible—the solution involves moving each tile in turn into the visible square and then keeping track of its location.

Token 3045:
4.4.2 Searching with observations For a general partially observable problem, we have to specify how the environment generatespercepts for the agent.

Token 3046:
For example, we might deﬁne the local-sensing vacuum world to beone in which the agent has a position sensor and a local dirt sensor but has no sensor capableof detecting dirt in other squares.

Token 3047:
The formal problem speciﬁcation includes a P ERCEPT (s) function that returns the percept received in a given state.

Token 3048:
(If sensing is nondeterministic,then we use a P ERCEPTS function that returns a set of possible percepts.)

Token 3049:
For example, in the local-sensing vacuum world, the P ERCEPT in state 1 is [A,Dirty].

Token 3050:
Fully observable problems are a special case in which P ERCEPT (s)=sfor every state s, while sensorless problems are a special case in which P ERCEPT (s)=null.

Token 3051:
When observations are partial, it will usually be the case that several states could have produced any given percept.

Token 3052:
For example, the percept [A,Dirty]is produced by state 3 as well as by state 1.

Token 3053:
Hence, given this as the initial percept, the initial belief state for thelocal-sensing vacuum world will be {1,3}.T h e A CTIONS ,STEP-COST,a n dG OAL-TEST are constructed from the underlying physical problem just as for sensorless problems, but the transition model is a bit more complicated.

Token 3054:
We can think of transitions from one belief state to the next for a particular action as occurring in three stages, as shown in Figure 4.15: •Theprediction stage is the same as for sensorless problems: given the action ain belief stateb, the predicted belief state is ˆb=PREDICT (b,a).11 •The observation prediction stage determines the set of percepts othat could be ob- served in the predicted belief state: POSSIBLE -PERCEPTS (ˆb)={o:o=PERCEPT (s)ands∈ˆb}.

Token 3055:
•The update stage determines, for each possible percept, the belief state that would result from the percept.

Token 3056:
The new belief state bois just the set of states in ˆbthat could have produced the percept: bo=UPDATE (ˆb,o)={s:o=PERCEPT (s)ands∈ˆb}.

Token 3057:
Notice that each updated belief state bocan be no larger than the predicted belief state ˆb; observations can only help reduce uncertainty compared to the sensorless case.

Token 3058:
More-over, for deterministic sensing, the belief states for the different possible percepts willbe disjoint, forming a partition of the original predicted belief state.

Token 3059:
11Here, and throughout the book, the “hat” in ˆbmeans an estimated or predicted value for b.

Token 3060:
Section 4.4.

Token 3061:
Searching with Partial Observations 143 2 4 4 1 2 4 1 3 2 1 3 3 (b)(a) 4 2 1 3 Right [A,Dirty ][B,Dirty ] [B,Clean ]Right[B,Dirty ] [B,Clean ] Figure 4.15 Two example of transitions in local-sensing vacuum worlds.

Token 3062:
(a) In the de- terministic world, Right is applied in the initial belief state, resulting in a new belief state with two possible physical states; for those states, the possible percepts are [B,Dirty]and [B,Clean ], leading to two belief states, each of which is a singleton.

Token 3063:
(b) In the slippery world, Right is applied in the initial belief state, giving a new belief state with four physi- cal states; for those states, the possible percepts are [A,Dirty],[B,Dirty],a n d[B,Clean ], leading to three belief states as shown.

Token 3064:
Putting these three stages together, we obtain the possible belief states resulting from a given action and the subsequent possible percepts: RESULTS (b,a)={bo:bo=UPDATE (PREDICT (b,a),o)and o∈POSSIBLE -PERCEPTS (PREDICT (b,a))}.

Token 3065:
(4.5) Again, the nondeterminism in the partially observable problem comes from the inability to predict exactly which percept will be received after acting; underlying nondeterminism inthe physical environment may contribute to this inability by enlarging the belief state at the prediction stage, leading to more percepts at the observation stage.

Token 3066:
4.4.3 Solving partially observable problems The preceding section showed how to derive the R ESULTS function for a nondeterministic belief-state problem from an underlying physical problem and the P ERCEPT function.

Token 3067:
Given

Token 3068:
144 Chapter 4.

Token 3069:
Beyond Classical Search 7 5 1 3 4 2 Suck [B,Dirty ] [B,Clean ]Right [A,Clean ] Figure 4.16 The ﬁrst level of the AND –ORsearch tree for a problem in the local-sensing vacuum world; Suck is the ﬁrst step of the solution.

Token 3070:
such a formulation, the AND –ORsearch algorithm of Figure 4.11 can be applied directly to derive a solution.

Token 3071:
Figure 4.16 shows part of the search tree for the local-sensing vacuumworld, assuming an initial percept [A,Dirty].

Token 3072:
The solution is the conditional plan [Suck,Right,ifBstate ={6}then Suck else[]].

Token 3073:
Notice that, because we supplied a belief-state problem to the AND –ORsearch algorithm, it returned a conditional plan that tests the belief state rather than the actual state.

Token 3074:
This is as itshould be: in a partially observable environment the agent won’t be able to execute a solutionthat requires testing the actual state.

Token 3075:
As in the case of standard search algorithms applied to sensorless problems, the AND – ORsearch algorithm treats belief states as black boxes, just like any other states.

Token 3076:
One can improve on this by checking for previously generated belief states that are subsets or supersetsof the current state, just as for sensorless problems.

Token 3077:
One can also derive incremental searchalgorithms, analogous to those described for sensorless problems, that provide substantialspeedups over the black-box approach.

Token 3078:
4.4.4 An agent for partially observable environments The design of a problem-solving agent for partially observable environments is quite similarto the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls asearch algorithm (such as A ND-OR-GRAPH -SEARCH ) to solve it, and executes the solution.

Token 3079:
There are two main differences.

Token 3080:
First, the solution to a problem will be a conditional plan rather than a sequence; if the ﬁrst step is an if–then–else expression, the agent will need to test the condition in the if-part and execute the then-part or the else-part accordingly.

Token 3081:
Second, the agent will need to maintain its belief state as it performs actions and receives percepts.

Token 3082:
This process resembles the prediction–observation–update process in Equation (4.5) but is actually simpler because the percept is given by the environment rather than calculated by the

Token 3083:
Section 4.4.

Token 3084:
Searching with Partial Observations 145 7 5 6 2 1 3 6 4 8 2 [B,Dirty ] Right [A,Clean ] 7 5 Suck Figure 4.17 Two prediction–update cycles of belief-state maintenance in the kindergarten vacuum world with local sensing.

Token 3085:
agent. Given an initial belief state b, an action a, and a percept o, the new belief state is: b/prime=UPDATE (PREDICT (b,a),o).

Token 3086:
(4.6) Figure 4.17 shows the belief state being maintained in the kindergarten vacuum world with local sensing, wherein any square may become dirty at any time unless the agent is activelycleaning it at that moment.

Token 3087:
12 In partially observable environments—which include the vast majority of real-world environments—maintaining one’s belief state is a core function of any intelligent system.This function goes under various names, including monitoring ,ﬁltering andstate estima- MONITORING FILTERING tion.

Token 3088:
Equation (4.6) is called a recursive state estimator because it computes the new belief STATE ESTIMATION RECURSIVEstate from the previous one rather than by examining the entire percept sequence.

Token 3089:
If the agent is not to “fall behind,” the computation has to happen as fast as percepts are coming in.

Token 3090:
As the environment becomes more complex, the exact update computation becomes infeasible and the agent will have to compute an approximate belief state, perhaps focusing on the im-plications of the percept for the aspects of the environment that are of current interest.

Token 3091:
Mostwork on this problem has been done for stochastic, continuous-state environments with thetools of probability theory, as explained in Chapter 15.

Token 3092:
Here we will show an example in adiscrete environment with detrministic sensors and nondeterministic actions.

Token 3093:
The example concerns a robot with the task of localization : working out where it is, LOCALIZATION given a map of the world and a sequence of percepts and actions.

Token 3094:
Our robot is placed in the maze-like environment of Figure 4.18.

Token 3095:
The robot is equipped with four sonar sensors that tell whether there is an obstacle—the outer wall or a black square in the ﬁgure—in each of the four compass directions.

Token 3096:
We assume that the sensors give perfectly correct data, and that the robot has a correct map of the enviornment.

Token 3097:
But unfortunately the robot’s navigational system is broken, so when it executes a Move action, it moves randomly to one of the adjacent squares.

Token 3098:
The robot’s task is to determine its current location. Suppose the robot has just been switched on, so it does not know where it is.

Token 3099:
Thus its initial belief state bconsists of the set of all locations.

Token 3100:
The the robot receives the percept 12The usual apologies to those who are unfamiliar with the effect of small children on the environment.

Token 3101:
146 Chapter 4.

Token 3102:
Beyond Classical Search (a) Possible locations of robot after E1=N S W (b) Possible locations of robot After E1=N S W , E 2=N S Figure 4.18 Possible positions of the robot, ⊙, (a) after one observation E1=NSW and (b) after a second observation E2=NS.

Token 3103:
When sensors are noiseless and the transition model is accurate, there are no other possible locations for the robot consistent with this sequence of two observations.

Token 3104:
NSW , meaning there are obstacles to the north, west, and south, and does an update using the equation bo=UPDATE (b), yielding the 4 locations shown in Figure 4.18(a).

Token 3105:
You can inspect the maze to see that those are the only four locations that yield the percept NWS .

Token 3106:
Next the robot executes a Move action, but the result is nondeterministic.

Token 3107:
The new be- lief state, ba=PREDICT (bo,Move), contains all the locations that are one step away from the locations in bo.

Token 3108:
When the second percept, NS, arrives, the robot does U PDATE (ba,NS)and ﬁnds that the belief state has collapsed down to the single location shown in Figure 4.18(b).That’s the only location that could be the result of U PDATE (PREDICT (UPDATE (b,NSW),Move),NS).

Token 3109:
With nondetermnistic actions the P REDICT step grows the belief state, but the U PDATE step shrinks it back down—as long as the percepts provide some useful identifying information.

Token 3110:
Sometimes the percepts don’t help much for localization: If there were one or more long east-west corridors, then a robot could receive a long sequence of NS percepts, but never know where in the corridor(s) it was.

Token 3111:
Section 4.5.

Token 3112:
Online Search Agents and Unknown Environments 147 4.5 O NLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS So far we have concentrated on agents that use ofﬂine search algorithms.

Token 3113:
They compute OFFLINE SEARCH a complete solution before setting foot in the real world and then execute the solution.

Token 3114:
In contrast, an online search13agent interleaves computation and action: ﬁrst it takes an action, ONLINESEARCH then it observes the environment and computes the next action.

Token 3115:
Online search is a good idea in dynamic or semidynamic domains—domains where there is a penalty for sitting around and computing too long.

Token 3116:
Online search is also helpful in nondeterministic domains becauseit allows the agent to focus its computational efforts on the contingencies that actually ariserather than those that might happen but probably won’t.

Token 3117:
Of course, there is a tradeoff: the more an agent plans ahead, the less often it will ﬁnd itself up the creek without a paddle.

Token 3118:
Online search is a necessary idea for unknown environments, where the agent does not know what states exist or what its actions do.

Token 3119:
In this state of ignorance, the agent faces anexploration problem and must use its actions as experiments in order to learn enough to EXPLORATION PROBLEM make deliberation worthwhile.

Token 3120:
The canonical example of online search is a robot that is placed in a new building and must explore it to build a map that it can use for getting from AtoB.

Token 3121:
Methods for escaping from labyrinths—required knowledge for aspiring heroes of antiquity—are also examples of online search algorithms.

Token 3122:
Spatial exploration is not the only form of exploration, however.Consider a newborn baby: it has many possible actions but knows the outcomes of none ofthem, and it has experienced only a few of the possible states that it can reach.

Token 3123:
The baby’sgradual discovery of how the world works is, in part, an online search process.

Token 3124:
4.5.1 Online search problems An online search problem must be solved by an agent executing actions, rather than by purecomputation.

Token 3125:
We assume a deterministic and fully observable environment (Chapter 17 re-laxes these assumptions), but we stipulate that the agent knows only the following: •A CTIONS (s), which returns a list of actions allowed in state s; •The step-cost function c(s,a,s/prime)—note that this cannot be used until the agent knows thats/primeis the outcome; and •GOAL-TEST(s).

Token 3126:
Note in particular that the agent cannot determine R ESULT (s,a)except by actually being insand doing a.

Token 3127:
For example, in the maze problem shown in Figure 4.19, the agent does not know that going Upfrom (1,1) leads to (1,2); nor, having done that, does it know that goingDown will take it back to (1,1).

Token 3128:
This degree of ignorance can be reduced in some applications—for example, a robot explorer might know how its movement actions work andbe ignorant only of the locations of obstacles.

Token 3129:
13The term “online” is commonly used in computer science to refer to algorithms that must process input data as they are received rather than waiting for the entire input data set to become available.

Token 3130:
148 Chapter 4. Beyond Classical Search G S 123 123 Figure 4.19 A simple maze problem.

Token 3131:
The agent starts at Sand must reach Gbut knows nothing of the environment.

Token 3132:
SG S GA AS G (a) (b) Figure 4.20 (a) Two state spaces that might lead an online search agent into a dead end.

Token 3133:
Any given agent will fail in at least one of these spaces.

Token 3134:
(b) A two-dimensional environment that can cause an online search agent to follow an arbitrarily inefﬁcient route to the goal.

Token 3135:
Whichever choice the agent makes, the adversary blocks that route with another long, thinwall, so that the path followed is much longer than the best possible path.

Token 3136:
Finally, the agent might have access to an admissible heuristic function h(s)that es- timates the distance from the current state to a goal state.

Token 3137:
For example, in Figure 4.19, theagent might know the location of the goal and be able to use the Manhattan-distance heuristic.

Token 3138:
Typically, the agent’s objective is to reach a goal state while minimizing cost.

Token 3139:
(Another possible objective is simply to explore the entire environment.) The cost is the total path cost of the path that the agent actually travels.

Token 3140:
It is common to compare this cost with the path cost of the path the agent would follow if it knew the search space in advance —that is, the actual shortest path (or shortest complete exploration).

Token 3141:
In the language of online algorithms,this is called the competitive ratio ; we would like it to be as small as possible. COMPETITIVE RATIO

Token 3142:
Section 4.5.

Token 3143:
Online Search Agents and Unknown Environments 149 Although this sounds like a reasonable request, it is easy to see that the best achievable competitive ratio is inﬁnite in some cases.

Token 3144:
For example, if some actions are irreversible — IRREVERSIBLE i.e., they lead to a state from which no action leads back to the previous state—the online search might accidentally reach a dead-end state from which no goal state is reachable.

Token 3145:
Per- DEAD END haps the term “accidentally” is unconvincing—after all, there might be an algorithm that happens not to take the dead-end path as it explores.

Token 3146:
Our claim, to be more precise, is that no algorithm can avoid dead ends in all state spaces. Consider the two dead-end state spaces in Figure 4.20(a).

Token 3147:
To an online search algorithm that has visited states SandA, the two state spaces look identical , so it must make the same decision in both.

Token 3148:
Therefore, it will fail in one of them.

Token 3149:
This is an example of an adversary argument —we can imagine an adversaryADVERSARY ARGUMENT constructing the state space while the agent explores it and putting the goals and dead ends wherever it chooses.

Token 3150:
Dead ends are a real difﬁculty for robot exploration—staircases, ramps, cliffs, one-way streets, and all kinds of natural terrain present opportunities for irreversible actions.

Token 3151:
To makeprogress, we simply assume that the state space is safely explorable —that is, some goal state SAFELY EXPLORABLE is reachable from every reachable state.

Token 3152:
State spaces with reversible actions, such as mazes and 8-puzzles, can be viewed as undirected graphs and are clearly safely explorable.

Token 3153:
Even in safely explorable environments, no bounded competitive ratio can be guaran- teed if there are paths of unbounded cost.

Token 3154:
This is easy to show in environments with irre-versible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)shows.

Token 3155:
For this reason, it is common to describe the performance of online search algorithmsin terms of the size of the entire state space rather than just the depth of the shallowest goal.

Token 3156:
4.5.2 Online search agents After each action, an online agent receives a percept telling it what state it has reached; fromthis information, it can augment its map of the environment.

Token 3157:
The current map is used todecide where to go next.

Token 3158:
This interleaving of planning and action means that online search algorithms are quite different from the ofﬂine search algorithms we have seen previously.

Token 3159:
For example, ofﬂine algorithms such as A ∗can expand a node in one part of the space and then immediately expand a node in another part of the space, because node expansion involvessimulated rather than real actions.

Token 3160:
An online algorithm, on the other hand, can discoversuccessors only for a node that it physically occupies.

Token 3161:
To avoid traveling all the way acrossthe tree to expand the next node, it seems better to expand nodes in a local order.

Token 3162:
Depth-ﬁrst search has exactly this property because (except when backtracking) the next node expandedis a child of the previous node expanded.

Token 3163:
An online depth-ﬁrst search agent is shown in Figure 4.21.

Token 3164:
This agent stores its map in a table, R ESULT [s,a], that records the state resulting from executing action ain state s. Whenever an action from the current state has not been explored, the agent tries that action.

Token 3165:
The difﬁculty comes when the agent has tried all the actions in a state.

Token 3166:
In ofﬂine depth-ﬁrst search, the state is simply dropped from the queue; in an online search, the agent has tobacktrack physically.

Token 3167:
In depth-ﬁrst search, this means going back to the state from which theagent most recently entered the current state.

Token 3168:
To achieve that, the algorithm keeps a table that

Token 3169:
150 Chapter 4.

Token 3170:
Beyond Classical Search function ONLINE -DFS-A GENT (s/prime)returns an action inputs :s/prime, a percept that identiﬁes the current state persistent :result , a table indexed by state and action, initially empty untried , a table that lists, for each state, the actions not yet tried unbacktracked , a table that lists, for each state, the backtracks not yet tried s,a, the previous state and action, initially null ifGOAL-TEST(s/prime)then return stop ifs/primei san e ws t a t e( n o ti n untried )thenuntried [s/prime]←ACTIONS (s/prime) ifsis not null then result [s,a]←s/prime addsto the front of unbacktracked [s/prime] ifuntried [s/prime]i se m p t y then ifunbacktracked [s/prime]i se m p t y then return stop elsea←an action bsuch that result [s/prime,b]=P OP(unbacktracked [s/prime]) elsea←POP(untried [s/prime]) s←s/prime return a Figure 4.21 An online search agent that uses depth-ﬁrst exploration.

Token 3171:
The agent is appli- cable only in state spaces in which every action can be “undone” by some other action.

Token 3172:
lists, for each state, the predecessor states to which the agent has not yet backtracked.

Token 3173:
If the agent has run out of states to which it can backtrack, then its search is complete.

Token 3174:
We recommend that the reader trace through the progress of O NLINE -DFS-A GENT when applied to the maze given in Figure 4.19.

Token 3175:
It is fairly easy to see that the agent will, in the worst case, end up traversing every link in the state space exactly twice.

Token 3176:
For exploration, this is optimal; for ﬁnding a goal, on the other hand, the agent’s competitive ratio could bearbitrarily bad if it goes off on a long excursion when there is a goal right next to the initialstate.

Token 3177:
An online variant of iterative deepening solves this problem; for an environment that isa uniform tree, the competitive ratio of such an agent is a small constant.

Token 3178:
Because of its method of backtracking, O NLINE -DFS-A GENT works only in state spaces where the actions are reversible.

Token 3179:
There are slightly more complex algorithms thatwork in general state spaces, but no such algorithm has a bounded competitive ratio.

Token 3180:
4.5.3 Online local search Like depth-ﬁrst search, hill-climbing search has the property of locality in its node expan- sions.

Token 3181:
In fact, because it keeps just one current state in memory, hill-climbing search is already an online search algorithm!

Token 3182:
Unfortunately, it is not very useful in its simplest form because it leaves the agent sitting at local maxima with nowhere to go.

Token 3183:
Moreover, random restarts cannot be used, because the agent cannot transport itself to a new state.

Token 3184:
Instead of random restarts, one might consider using a random walk to explore the RANDOM WALK environment.

Token 3185:
A random walk simply selects at random one of the available actions from the

Token 3186:
Section 4.5.

Token 3187:
Online Search Agents and Unknown Environments 151 S G Figure 4.22 An environment in which a random walk will take exponentially many steps to ﬁnd the goal.

Token 3188:
current state; preference can be given to actions that have not yet been tried.

Token 3189:
It is easy to prove that a random walk will eventually ﬁnd a goal or complete its exploration, provided that the space is ﬁnite.14On the other hand, the process can be very slow.

Token 3190:
Figure 4.22 shows an environment in which a random walk will take exponentially many steps to ﬁnd the goal because, at each step, backward progress is twice as likely as forward progress.

Token 3191:
The exampleis contrived, of course, but there are many real-world state spaces whose topology causesthese kinds of “traps” for random walks.

Token 3192:
Augmenting hill climbing with memory rather than randomness turns out to be a more effective approach.

Token 3193:
The basic idea is to store a “current best estimate” H(s)of the cost to reach the goal from each state that has been visited.

Token 3194:
H(s)starts out being just the heuristic estimate h(s)and is updated as the agent gains experience in the state space.

Token 3195:
Figure 4.23 shows a simple example in a one-dimensional state space. In (a), the agent seems to be stuck in a ﬂat local minimum at the shaded state.

Token 3196:
Rather than staying where it is, the agent should follow what seems to be the best path to the goal given the current cost estimates forits neighbors.

Token 3197:
The estimated cost to reach the goal through a neighbor s /primeis the cost to get tos/primeplus the estimated cost to get to a goal from there—that is, c(s,a,s/prime)+H(s/prime).I n t h e example, there are two actions, with estimated costs 1+9 and1+2 , so it seems best to move right.

Token 3198:
Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.Since the best move cost 1 and led to a state that is at least 2 steps from a goal, the shadedstate must be at least 3 steps from a goal, so its Hshould be updated accordingly, as shown in Figure 4.23(b).

Token 3199:
Continuing this process, the agent will move back and forth twice more,updating Heach time and “ﬂattening out” the local minimum until it escapes to the right.

Token 3200:
An agent implementing this scheme, which is called learning real-time A ∗(LRTA∗), is LRTA* shown in Figure 4.24.

Token 3201:
Like O NLINE -DFS-A GENT , it builds a map of the environment in theresult table.

Token 3202:
It updates the cost estimate for the state it has just left and then chooses the “apparently best” move according to its current cost estimates.

Token 3203:
One important detail is that actions that have not yet been tried in a state sare always assumed to lead immediately to the goal with the least possible cost, namely h(s).T h i s optimism under uncertainty encouragesOPTIMISM UNDER UNCERTAINTY the agent to explore new, possibly promising paths.

Token 3204:
An LRTA∗agent is guaranteed to ﬁnd a goal in any ﬁnite, safely explorable environment.

Token 3205:
Unlike A∗, however, it is not complete for inﬁnite state spaces—there are cases where it can be led inﬁnitely astray.

Token 3206:
It can explore an environment of nstates in O(n2)steps in the worst case, 14Random walks are complete on inﬁnite one-dimensional and two-dimensional grids.

Token 3207:
On a three-dimensional grid, the probability that the walk ever returns to the starting point is only about 0.3405 (Hughes, 1995).

Token 3208:
152 Chapter 4.

Token 3209:
Beyond Classical Search 121 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12 2 34 4 43 3 3 1 1 1 1 1 1 13 1 1 1 1 1 1 153 554(a) (b) (c) (d) (e)8 9 89 898 989 4 4 3 4 Figure 4.23 Five iterations of LRTA∗on a one-dimensional state space.

Token 3210:
Each state is labeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its step cost.

Token 3211:
The shaded state marks the location of the agent, and the updated cost estimates at each iteration are circled.

Token 3212:
function LRTA*-A GENT (s/prime)returns an action inputs :s/prime, a percept that identiﬁes the current state persistent :result , a table, indexed by state and action, initially empty H, a table of cost estimates indexed by state, initially empty s,a, the previous state and action, initially null ifGOAL-TEST(s/prime)then return stop ifs/primei san e ws t a t e( n o ti n H)thenH[s/prime]←h(s/prime) ifsis not null result [s,a]←s/prime H[s]← min b∈ACTIONS (s)LRTA*-C OST(s,b,result [s,b],H) a←an action bin A CTIONS (s/prime) that minimizes LRTA*-C OST(s/prime,b,result [s/prime,b],H) s←s/prime return a function LRTA*-C OST(s,a,s/prime,H)returns a cost estimate ifs/primeis undeﬁned then return h(s) else return c(s, a, s/prime)+H[s/prime] Figure 4.24 LRTA*-A GENT selects an action according to the values of neighboring states, which are updated as the agent moves about the state space.

Token 3213:
Section 4.6. Summary 153 but often does much better.

Token 3214:
The LRTA∗agent is just one of a large family of online agents that one can deﬁne by specifying the action selection rule and the update rule in different ways.We discuss this family, developed originally for stochastic environments, in Chapter 21.

Token 3215:
4.5.4 Learning in online search The initial ignorance of online search agents provides several opportunities for learning.

Token 3216:
First,the agents learn a “map” of the environment—more precisely, the outcome of each action ineach state—simply by recording each of their experiences.

Token 3217:
(Notice that the assumption ofdeterministic environments means that one experience is enough for each action.)

Token 3218:
Second,the local search agents acquire more accurate estimates of the cost of each state by using local updating rules, as in LRTA ∗.

Token 3219:
In Chapter 21, we show that these updates eventually converge toexact values for every state, provided that the agent explores the state space in the right way.

Token 3220:
Once exact values are known, optimal decisions can be taken simply by moving to thelowest-cost successor—that is, pure hill climbing is then an optimal strategy.

Token 3221:
If you followed our suggestion to trace the behavior of O NLINE -DFS-A GENT in the environment of Figure 4.19, you will have noticed that the agent is not very bright.

Token 3222:
Forexample, after it has seen that the Upaction goes from (1,1) to (1,2), the agent still has no idea that the Down action goes back to (1,1) or that the Upaction also goes from (2,1) to (2,2), from (2,2) to (2,3), and so on.

Token 3223:
In general, we would like the agent to learn that Up increases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.

Token 3224:
For this to happen, we need two things.

Token 3225:
First, we need a formal and explicitly manipulable representation for these kinds of general rules; so far, we have hidden the information inside the black box called the R ESULT function.

Token 3226:
Part III is devoted to this issue.

Token 3227:
Second, we need algorithms that can construct suitable general rules from the speciﬁc observations made by the agent. These are covered in Chapter 18.

Token 3228:
4.6 S UMMARY This chapter has examined search algorithms for problems beyond the “classical” case ofﬁnding the shortest path to a goal in an observable, deterministic, discrete environment.

Token 3229:
•Local search methods such as hill climbing operate on complete-state formulations, keeping only a small number of nodes in memory.

Token 3230:
Several stochastic algorithms havebeen developed, including simulated annealing , which returns optimal solutions when given an appropriate cooling schedule.

Token 3231:
•Many local search methods apply also to problems in continuous spaces.

Token 3232:
Linear pro- gramming andconvex optimization problems obey certain restrictions on the shape of the state space and the nature of the objective function, and admit polynomial-timealgorithms that are often extremely efﬁcient in practice.

Token 3233:
•Agenetic algorithm is a stochastic hill-climbing search in which a large population of states is maintained.

Token 3234:
New states are generated by mutation and by crossover ,w h i c h combines pairs of states from the population.

Token 3235:
154 Chapter 4.

Token 3236:
Beyond Classical Search •Innondeterministic environments, agents can apply AND –ORsearch to generate con- tingent plans that reach the goal regardless of which outcomes occur during execution.

Token 3237:
•When the environment is partially observable, the belief state represents the set of possible states that the agent might be in.

Token 3238:
•Standard search algorithms can be applied directly to belief-state space to solve sensor- less problems , and belief-state AND –ORsearch can solve general partially observable problems.

Token 3239:
Incremental algorithms that construct solutions state-by-state within a beliefstate are often more efﬁcient.

Token 3240:
•Exploration problems arise when the agent has no idea about the states and actions of its environment.

Token 3241:
For safely explorable environments, online search agents can build a map and ﬁnd a goal if one exists.

Token 3242:
Updating heuristic estimates from experience provides an effective method to escape from local minima.

Token 3243:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Local search techniques have a long history in mathematics and computer science.

Token 3244:
Indeed, the Newton–Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very efﬁ-cient local search method for continuous spaces in which gradient information is available.Brent (1973) is a classic reference for optimization algorithms that do not require such in-formation.

Token 3245:
Beam search, which we have presented as a local search algorithm, originatedas a bounded-width variant of dynamic programming for speech recognition in the H ARPY system (Lowerre, 1976).

Token 3246:
A related algorithm is analyzed in depth by Pearl (1984, Ch. 5).

Token 3247:
The topic of local search was reinvigorated in the early 1990s by surprisingly good re- sults for large constraint-satisfaction problems such as n-queens (Minton et al.

Token 3248:
, 1992) and logical reasoning (Selman et al. , 1992) and by the incorporation of randomness, multiple simultaneous searches, and other improvements.

Token 3249:
This renaissance of what Christos Papadim- itriou has called “New Age” algorithms also sparked increased interest among theoretical computer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994).

Token 3250:
Inthe ﬁeld of operations research, a variant of hill climbing called tabu search has gained popu- TABU SEARCH larity (Glover and Laguna, 1997).

Token 3251:
This algorithm maintains a tabu list of kpreviously visited states that cannot be revisited; as well as improving efﬁciency when searching graphs, this list can allow the algorithm to escape from some local minima.

Token 3252:
Another useful improvement on hill climbing is the S TAGE algorithm (Boyan and Moore, 1998).

Token 3253:
The idea is to use the local maxima found by random-restart hill climbing to get an idea of the overall shape of the land-scape.

Token 3254:
The algorithm ﬁts a smooth surface to the set of local maxima and then calculates theglobal maximum of that surface analytically.

Token 3255:
This becomes the new restart point. The algo-rithm has been shown to work in practice on hard problems. Gomes et al.

Token 3256:
(1998) showed that the run times of systematic backtracking algorithms often have a heavy-tailed distribution , HEAVY-TAILED DISTRIBUTION which means that the probability of a very long run time is more than would be predicted if the run times were exponentially distributed.

Token 3257:
When the run time distribution is heavy-tailed,random restarts ﬁnd a solution faster, on average, than a single run to completion.

Token 3258:
Bibliographical and Historical Notes 155 Simulated annealing was ﬁrst described by Kirkpatrick et al.

Token 3259:
(1983), who borrowed directly from the Metropolis algorithm (which is used to simulate complex systems in physics (Metropolis et al.

Token 3260:
, 1953) and was supposedly invented at a Los Alamos dinner party).

Token 3261:
Simulated annealing is now a ﬁeld in itself, with hundreds of papers published every year.

Token 3262:
Finding optimal solutions in continuous spaces is the subject matter of several ﬁelds, including optimization theory ,optimal control theory ,a n dt h e calculus of variations .T h e basic techniques are explained well by Bishop (1995); Press et al.

Token 3263:
(2007) cover a wide range of algorithms and provide working software.

Token 3264:
As Andrew Moore points out, researchers have taken inspiration for search and opti- mization algorithms from a wide variety of ﬁelds of study: metallurgy (simulated annealing),biology (genetic algorithms), economics (market-based algorithms), entomology (ant colonyoptimization), neurology (neural networks), animal behavior (reinforcement learning), moun-taineering (hill climbing), and others.

Token 3265:
Linear programming (LP) was ﬁrst studied systematically by the Russian mathemati- cian Leonid Kantorovich (1939).

Token 3266:
It was one of the ﬁrst applications of computers; the sim- plex algorithm (Dantzig, 1949) is still used despite worst-case exponential complexity.

Token 3267:
Kar- markar (1984) developed the far more efﬁcient family of interior-point methods, which was shown to have polynomial complexity for the more general class of convex optimization prob- lems by Nesterov and Nemirovski (1994).

Token 3268:
Excellent introductions to convex optimization areprovided by Ben-Tal and Nemirovski (2001) and Boyd and Vandenberghe (2004).

Token 3269:
Work by Sewall Wright (1931) on the concept of a ﬁtness landscape was an impor- tant precursor to the development of genetic algorithms.

Token 3270:
In the 1950s, several statisticians, including Box (1957) and Friedman (1959), used evolutionary techniques for optimization problems, but it wasn’t until Rechenberg (1965) introduced evolution strategies to solve op- EVOLUTION STRATEGY timization problems for airfoils that the approach gained popularity.

Token 3271:
In the 1960s and 1970s, John Holland (1975) championed genetic algorithms, both as a useful tool and as a methodto expand our understanding of adaptation, biological or otherwise (Holland, 1995).

Token 3272:
The ar- tiﬁcial life movement (Langton, 1995) takes this idea one step further, viewing the products ARTIFICIAL LIFE of genetic algorithms as organisms rather than solutions to problems.

Token 3273:
Work in this ﬁeld by Hinton and Nowlan (1987) and Ackley and Littman (1991) has done much to clarify the im-plications of the Baldwin effect.

Token 3274:
For general background on evolution, we recommend Smithand Szathm´ ary (1999), Ridley (2004), and Carroll (2007).

Token 3275:
Most comparisons of genetic algorithms to other approaches (especially stochastic hill climbing) have found that the genetic algorithms are slower to converge (O’Reilly and Op-pacher, 1994; Mitchell et al.

Token 3276:
, 1996; Juels and Wattenberg, 1996; Baluja, 1997).

Token 3277:
Such ﬁndings are not universally popular within the GA community, but recent attempts within that com-munity to understand population-based search as an approximate form of Bayesian learning(see Chapter 20) might help close the gap between the ﬁeld and its critics (Pelikan et al.

Token 3278:
, 1999). The theory of quadratic dynamical systems may also explain the performance of GAs (Rabani et al. , 1998). See Lohn et al.

Token 3279:
(2001) for an example of GAs applied to antenna design, and Renner and Ekart (2003) for an application to computer-aided design.

Token 3280:
The ﬁeld of genetic programming is closely related to genetic algorithms.

Token 3281:
The princi- GENETIC PROGRAMMING pal difference is that the representations that are mutated and combined are programs rather

Token 3282:
156 Chapter 4. Beyond Classical Search than bit strings.

Token 3283:
The programs are represented in the form of expression trees; the expressions can be in a standard language such as Lisp or can be specially designed to represent circuits,robot controllers, and so on.

Token 3284:
Crossover involves splicing together subtrees rather than sub-strings.

Token 3285:
This form of mutation guarantees that the offspring are well-formed expressions,which would not be the case if programs were manipulated as strings.

Token 3286:
Interest in genetic programming was spurred by John Koza’s work (Koza, 1992, 1994), but it goes back at least to early experiments with machine code by Friedberg (1958) andwith ﬁnite-state automata by Fogel et al.

Token 3287:
(1966). As with genetic algorithms, there is debate about the effectiveness of the technique. Koza et al.

Token 3288:
(1999) describe experiments in the use of genetic programming to design circuit devices.

Token 3289:
The journals Evolutionary Computation andIEEE Transactions on Evolutionary Com- putation cover genetic algorithms and genetic programming; articles are also found in Com- plex Systems ,Adaptive Behavior ,a n d Artiﬁcial Life .

Token 3290:
The main conference is the Genetic and Evolutionary Computation Conference (GECCO).

Token 3291:
Good overview texts on genetic algo- rithms are given by Mitchell (1996), Fogel (2000), and Langdon and Poli (2002), and by thefree online book by Poli et al.

Token 3292:
(2008).

Token 3293:
The unpredictability and partial observability of real environments were recognized early on in robotics projects that used planning techniques, including Shakey (Fikes et al.

Token 3294:
, 1972) and F REDDY (Michie, 1974).

Token 3295:
The problems received more attention after the publica- tion of McDermott’s (1978a) inﬂuential article, Planning and Acting .

Token 3296:
The ﬁrst work to make explicit use of AND –ORtrees seems to have been Slagle’s S AINT program for symbolic integration, mentioned in Chapter 1.

Token 3297:
Amarel (1967) applied the ideato propositional theorem proving, a topic discussed in Chapter 7, and introduced a search algorithm similar to A ND-OR-GRAPH -SEARCH .

Token 3298:
The algorithm was further developed and formalized by Nilsson (1971), who also described AO∗—which, as its name suggests, ﬁnds optimal solutions given an admissible heuristic.

Token 3299:
AO∗was analyzed and improved by Martelli and Montanari (1973).

Token 3300:
AO∗is a top-down algorithm; a bottom-up generalization of A∗is A∗LD, for A∗Lightest Derivation (Felzenszwalb and McAllester, 2007).

Token 3301:
Interest in AND –OR search has undergone a revival in recent years, with new algorithms for ﬁnding cyclic solu-tions (Jimenez and Torras, 2000; Hansen and Zilberstein, 2001) and new techniques inspiredby dynamic programming (Bonet and Geffner, 2005).

Token 3302:
The idea of transforming partially observable problems into belief-state problems orig- inated with Astrom (1965) for the much more complex case of probabilistic uncertainty (seeChapter 17).

Token 3303:
Erdmann and Mason (1988) studied the problem of robotic manipulation with-out sensors, using a continuous form of belief-state search.

Token 3304:
They showed that it was possibleto orient a part on a table from an arbitrary initial position by a well-designed sequence of tilt-ing actions.

Token 3305:
More practical methods, based on a series of precisely oriented diagonal barriersacross a conveyor belt, use the same algorithmic insights (Wiegley et al.

Token 3306:
, 1996).

Token 3307:
The belief-state approach was reinvented in the context of sensorless and partially ob- servable search problems by Genesereth and Nourbakhsh (1993).

Token 3308:
Additional work was done on sensorless problems in the logic-based planning community (Goldman and Boddy, 1996;Smith and Weld, 1998).

Token 3309:
This work has emphasized concise representations for belief states,as explained in Chapter 11.

Token 3310:
Bonet and Geffner (2000) introduced the ﬁrst effective heuristics

Token 3311:
Exercises 157 for belief-state search; these were reﬁned by Bryce et al. (2006).

Token 3312:
The incremental approach to belief-state search, in which solutions are constructed incrementally for subsets of stateswithin each belief state, was studied in the planning literature by Kurien et al.

Token 3313:
(2002); several new incremental algorithms were introduced for nondeterministic, partially observable prob-lems by Russell and Wolfe (2005).

Token 3314:
Additional references for planning in stochastic, partially observable environments appear in Chapter 17.

Token 3315:
Algorithms for exploring unknown state spaces have been of interest for many centuries.

Token 3316:
Depth-ﬁrst search in a maze can be implemented by keeping one’s left hand on the wall; loopscan be avoided by marking each junction.

Token 3317:
Depth-ﬁrst search fails with irreversible actions;the more general problem of exploring Eulerian graphs (i.e., graphs in which each node has EULERIAN GRAPH equal numbers of incoming and outgoing edges) was solved by an algorithm due to Hierholzer (1873).

Token 3318:
The ﬁrst thorough algorithmic study of the exploration problem for arbitrary graphswas carried out by Deng and Papadimitriou (1990), who developed a completely generalalgorithm but showed that no bounded competitive ratio is possible for exploring a generalgraph.

Token 3319:
Papadimitriou and Yannakakis (1991) examined the question of ﬁnding paths to a goalin geometric path-planning environments (where all actions are reversible).

Token 3320:
They showed thata small competitive ratio is achievable with square obstacles, but with general rectangular obstacles no bounded ratio can be achieved.

Token 3321:
(See Figure 4.20.)

Token 3322:
The LRTA ∗algorithm was developed by Korf (1990) as part of an investigation into real-time search for environments in which the agent must act after searching for only a REAL-TIME SEARCH ﬁxed amount of time (a common situation in two-player games).

Token 3323:
LRTA∗is in fact a special case of reinforcement learning algorithms for stochastic environments (Barto et al. , 1995).

Token 3324:
Its policy of optimism under uncertainty—always head for the closest unvisited state—can resultin an exploration pattern that is less efﬁcient in the uninformed case than simple depth-ﬁrstsearch (Koenig, 2000).

Token 3325:
Dasgupta et al.

Token 3326:
(1994) show that online iterative deepening search is optimally efﬁcient for ﬁnding a goal in a uniform tree with no heuristic information.

Token 3327:
Sev- eral informed variants on the LRTA ∗theme have been developed with different methods for searching and updating within the known portion of the graph (Pemberton and Korf, 1992).

Token 3328:
As yet, there is no good understanding of how to ﬁnd goals with optimal efﬁciency when using heuristic information.

Token 3329:
EXERCISES 4.1 Give the name of the algorithm that results from each of the following special cases: a. Local beam search with k=1. b.

Token 3330:
Local beam search with one initial state and no limit on the number of states retained.

Token 3331:
c. Simulated annealing with T=0at all times (and omitting the termination test). d. Simulated annealing with T=∞at all times.

Token 3332:
e. Genetic algorithm with population size N=1.

Token 3333:
158 Chapter 4.

Token 3334:
Beyond Classical Search 4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption that pieces ﬁt exactly with no slack.

Token 3335:
Now consider the real problem, in which pieces don’tﬁt exactly but allow for up to 10 degrees of rotation to either side of the “proper” alignment.Explain how to formulate the problem so it could be solved by simulated annealing.

Token 3336:
4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type deﬁned in Exercise 3.30. a.

Token 3337:
Implement and test a hill-climbing method to solve TSPs.

Token 3338:
Compare the results with op- timal solutions obtained from the A∗algorithm with the MST heuristic (Exercise 3.30).

Token 3339:
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to consult Larra˜ naga et al.

Token 3340:
(1999) for some suggestions for representations.

Token 3341:
4.4 Generate a large number of 8-puzzle and 8-queens instances and solve them (where pos- sible) by hill climbing (steepest-ascent and ﬁrst-choice variants), hill climbing with random restart, and simulated annealing.

Token 3342:
Measure the search cost and percentage of solved problemsand graph these against the optimal solution cost. Comment on your results.

Token 3343:
4.5 The A ND-OR-GRAPH -SEARCH algorithm in Figure 4.11 checks for repeated states only on the path from the root to the current state.

Token 3344:
Suppose that, in addition, the algorithmwere to store every visited state and check against that list.

Token 3345:
(See B READTH -FIRST-SEARCH in Figure 3.11 for an example.)

Token 3346:
Determine the information that should be stored and how the algorithm should use that information when a repeated state is found.

Token 3347:
( Hint: You will need to distinguish at least between states for which a successful subplan was constructed previously and states for which no subplan could be found.)

Token 3348:
Explain how to use labels, as deﬁned in Section 4.3.3, to avoid having multiple copies of subplans.

Token 3349:
4.6 Explain precisely how to modify the A ND-OR-GRAPH -SEARCH algorithm to generate a cyclic plan if no acyclic plan exists.

Token 3350:
You will need to deal with three issues: labeling the plan steps so that a cyclic plan can point back to an earlier part of the plan, modifying O R-SEARCH so that it continues to look for acyclic plans after ﬁnding a cyclic plan, and augmenting the plan representation to indicate whether a plan is cyclic.

Token 3351:
Show how your algorithm works on (a) the slippery vacuum world, and (b) the slippery, erratic vacuum world.

Token 3352:
You might wish touse a computer implementation to check your results.

Token 3353:
4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems.

Token 3354:
A sequence of actions solves a sensorless problem if it maps every physical state in the initial belief state bto a goal state.

Token 3355:
Suppose the agent knows h ∗(s), the true optimal cost of solving the physical state sin the fully observable problem, for every state sinb.

Token 3356:
Find an admissible heuristic h(b)for the sensorless problem in terms of these costs, and prove its admissibilty.

Token 3357:
Comment on the accuracy of this heuristic on the sensorless vacuum problem of Figure 4.14.How well does A ∗perform?

Token 3358:
4.8 This exercise explores subset–superset relations between belief states in sensorless or partially observable environments. a.

Token 3359:
Prove that if an action sequence is a solution for a belief state b, it is also a solution for any subset of b.

Token 3360:
Can anything be said about supersets of b?

Token 3361:
Exercises 159 b. Explain in detail how to modify graph search for sensorless problems to take advantage of your answers in (a).

Token 3362:
c. Explain in detail how to modify AND –ORsearch for partially observable problems, beyond the modiﬁcations you describe in (b).

Token 3363:
4.9 On page 139 it was assumed that a given action would have the same cost when ex- ecuted in any physical state within a given belief state.

Token 3364:
(This leads to a belief-state searchproblem with well-deﬁned step costs.) Now consider what happens when the assumptiondoes not hold.

Token 3365:
Does the notion of optimality still make sense in this context, or does it requiremodiﬁcation?

Token 3366:
Consider also various possible deﬁnitions of the “cost” of executing an action in a belief state; for example, we could use the minimum of the physical costs; or the maxi- mum ; or a cost interval with the lower bound being the minimum cost and the upper bound being the maximum; or just keep the set of all possible costs for that action.

Token 3367:
For each of these,explore whether A ∗(with modiﬁcations if necessary) can return optimal solutions.

Token 3368:
4.10 Consider the sensorless version of the erratic vacuum world.

Token 3369:
Draw the belief-state space reachable from the initial belief state {1,2,3,4,5,6,7,8}, and explain why the problem is unsolvable.

Token 3370:
4.11 We can turn the navigation problem in Exercise 3.7 into an environment as follows: •The percept will be a list of the positions, relative to the agent , of the visible vertices.

Token 3371:
The percept does notinclude the position of the robot!

Token 3372:
The robot must learn its own po- sition from the map; for now, you can assume that each location has a different “view.” •Each action will be a vector describing a straight-line path to follow.

Token 3373:
If the path is unobstructed, the action succeeds; otherwise, the robot stops at the point where itspath ﬁrst intersects an obstacle.

Token 3374:
If the agent returns a zero motion vector and is at thegoal (which is ﬁxed and known), then the environment teleports the agent to a random location (not inside an obstacle).

Token 3375:
•The performance measure charges the agent 1 point for each unit of distance traversed and awards 1000 points each time the goal is reached. a.

Token 3376:
Implement this environment and a problem-solving agent for it.

Token 3377:
After each teleporta- tion, the agent will need to formulate a new problem, which will involve discovering itscurrent location. b.

Token 3378:
Document your agent’s performance (by having the agent generate suitable commentary as it moves around) and report its performance over 100 episodes.

Token 3379:
c. Modify the environment so that 30% of the time the agent ends up at an unintended destination (chosen randomly from the other visible vertices if any; otherwise, no move at all).

Token 3380:
This is a crude model of the motion errors of a real robot.

Token 3381:
Modify the agent so that when such an error is detected, it ﬁnds out where it is and then constructs a plan to get back to where it was and resume the old plan.

Token 3382:
Remember that sometimes getting back to where it was might also fail!

Token 3383:
Show an example of the agent successfully overcoming two successive motion errors and still reaching the goal.

Token 3384:
160 Chapter 4.

Token 3385:
Beyond Classical Search d. Now try two different recovery schemes after an error: (1) head for the closest vertex on the original route; and (2) replan a route to the goal from the new location.

Token 3386:
Compare theperformance of the three recovery schemes. Would the inclusion of search costs affectthe comparison?

Token 3387:
e. Now suppose that there are locations from which the view is identical. (For example, suppose the world is a grid with square obstacles.)

Token 3388:
What kind of problem does the agentnow face? What do solutions look like?

Token 3389:
4.12 Suppose that an agent is in a 3×3maze environment like the one shown in Fig- ure 4.19.

Token 3390:
The agent knows that its initial location is (1,1), that the goal is at (3,3), and that theactions Up,Down ,Left,Right have their usual effects unless blocked by a wall.

Token 3391:
The agent does notknow where the internal walls are.

Token 3392:
In any given state, the agent perceives the set of legal actions; it can also tell whether the state is one it has visited before. a.

Token 3393:
Explain how this online search problem can be viewed as an ofﬂine search in belief-state space, where the initial belief state includes all possible environment conﬁgurations.How large is the initial belief state?

Token 3394:
How large is the space of belief states? b. How many distinct percepts are possible in the initial state?

Token 3395:
c. Describe the ﬁrst few branches of a contingency plan for this problem. How large (roughly) is the complete plan?

Token 3396:
Notice that this contingency plan is a solution for every possible environment ﬁtting the given description.

Token 3397:
Therefore, interleaving of search and execution is not strictly necessary even inunknown environments.

Token 3398:
4.13 In this exercise, we examine hill climbing in the context of robot navigation, using the environment in Figure 3.31 as an example.

Token 3399:
a. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local minimum?

Token 3400:
Is it possible for it to get stuck with convex obstacles? b. Construct a nonconvex polygonal environment in which the agent gets stuck.

Token 3401:
c. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide where to go next, it does a depth- ksearch.

Token 3402:
It should ﬁnd the best k-step path and do one step along it, and then repeat the process.

Token 3403:
d.I s t h e r e s o m e kfor which the new algorithm is guaranteed to escape from local minima?

Token 3404:
e. Explain how LRTA∗enables the agent to escape from local minima in this case.

Token 3405:
4.14 Like DFS, online DFS is incomplete for reversible state spaces with inﬁnite paths.

Token 3406:
For example, suppose that states are points on the inﬁnite two-dimensional grid and actions are unit vectors (1,0),(0,1),(−1,0),(0,−1), tried in that order.

Token 3407:
Show that online DFS starting at(0,0)will not reach (1,−1).

Token 3408:
Suppose the agent can observe, in addition to its current state, all successor states and the actions that would lead to them.

Token 3409:
Write an algorithm thatis complete even for bidirected state spaces with inﬁnite paths. What states does it visit inreaching (1,−1)?

Token 3410:
5ADVERSARIAL SEARCH In which we examine the problems that arise when we try to plan ahead in a world where other agents are planning against us.

Token 3411:
5.1 G AMES Chapter 2 introduced multiagent environments , in which each agent needs to consider the actions of other agents and how they affect its own welfare.

Token 3412:
The unpredictability of theseother agents can introduce contingencies into the agent’s problem-solving process, as dis- cussed in Chapter 4.

Token 3413:
In this chapter we cover competitive environments, in which the agents’ goals are in conﬂict, giving rise to adversarial search problems—often known as games .

Token 3414:
GAME Mathematical game theory , a branch of economics, views any multiagent environment as a game, provided that the impact of each agent on the others is “signiﬁcant,” regardlessof whether the agents are cooperative or competitive.

Token 3415:
1In AI, the most common games are of a rather specialized kind—what game theorists call deterministic, turn-taking, two-player,zero-sum games ofperfect information (such as chess).

Token 3416:
In our terminology, this means ZERO-SUM GAMES PERFECT INFORMATION deterministic, fully observable environments in which two agents act alternately and in which the utility values at the end of the game are always equal and opposite.

Token 3417:
For example, if one player wins a game of chess, the other player necessarily loses.

Token 3418:
It is this opposition between the agents’ utility functions that makes the situation adversarial.

Token 3419:
Games have engaged the intellectual faculties of humans—sometimes to an alarming degree—for as long as civilization has existed.

Token 3420:
For AI researchers, the abstract nature of games makes them an appealing subject for study.

Token 3421:
The state of a game is easy to represent, and agents are usually restricted to a small number of actions whose outcomes are deﬁned byprecise rules.

Token 3422:
Physical games, such as croquet and ice hockey, have much more complicateddescriptions, a much larger range of possible actions, and rather imprecise rules deﬁningthe legality of actions.

Token 3423:
With the exception of robot soccer, these physical games have notattracted much interest in the AI community.

Token 3424:
1Environments with very many agents are often viewed as economies rather than games. 161

Token 3425:
162 Chapter 5. Adversarial Search Games, unlike most of the toy problems studied in Chapter 3, are interesting because they are too hard to solve.

Token 3426:
For example, chess has an average branching factor of about 35,and games often go to 50 moves by each player, so the search tree has about 35 100or10154 nodes (although the search graph has “only” about 1040distinct nodes).

Token 3427:
Games, like the real world, therefore require the ability to make some decision even when calculating the optimal decision is infeasible.

Token 3428:
Games also penalize inefﬁciency severely.

Token 3429:
Whereas an implementation of A∗search that is half as efﬁcient will simply take twice as long to run to completion, a chess program that is half as efﬁcient in using its available time probably will be beaten into theground, other things being equal.

Token 3430:
Game-playing research has therefore spawned a number ofinteresting ideas on how to make the best possible use of time.

Token 3431:
We begin with a deﬁnition of the optimal move and an algorithm for ﬁnding it.

Token 3432:
We then look at techniques for choosing a good move when time is limited.

Token 3433:
Pruning allows us PRUNING to ignore portions of the search tree that make no difference to the ﬁnal choice, and heuristic evaluation functions allow us to approximate the true utility of a state without doing a com- plete search.

Token 3434:
Section 5.5 discusses games such as backgammon that include an element ofchance; we also discuss bridge, which includes elements of imperfect information because IMPERFECT INFORMATION not all cards are visible to each player.

Token 3435:
Finally, we look at how state-of-the-art game-playing programs fare against human opposition and at directions for future developments.

Token 3436:
We ﬁrst consider games with two players, whom we call MAX and MIN for reasons that will soon become obvious.

Token 3437:
MAX moves ﬁrst, and then they take turns moving until the game is over.

Token 3438:
At the end of the game, points are awarded to the winning player and penalties aregiven to the loser.

Token 3439:
A game can be formally deﬁned as a kind of search problem with thefollowing elements: •S 0:T h e initial state , which speciﬁes how the game is set up at the start.

Token 3440:
•PLAYER (s): Deﬁnes which player has the move in a state. •ACTIONS (s): Returns the set of legal moves in a state.

Token 3441:
•RESULT (s,a): The transition model , which deﬁnes the result of a move.

Token 3442:
•TERMINAL -TEST(s):Aterminal test , which is true when the game is over and false TERMINAL TEST otherwise.

Token 3443:
States where the game has ended are called terminal states .

Token 3444:
TERMINAL STATES •UTILITY (s,p):Autility function (also called an objective function or payoff function), deﬁnes the ﬁnal numeric value for a game that ends in terminal state sfor a player p.I n chess, the outcome is a win, loss, or draw, with values +1,0 ,o r1 2.

Token 3445:
Some games have a wider variety of possible outcomes; the payoffs in backgammon range from 0 to +192 .

Token 3446:
Azero-sum game is (confusingly) deﬁned as one where the total payoff to all players is the same for every instance of the game.

Token 3447:
Chess is zero-sum because every game has payoff of either 0+1 ,1+0 or1 2+1 2.

Token 3448:
“Constant-sum” would have been a better term, but zero-sum is traditional and makes sense if you imagine each player is charged anentry fee of 1 2.

Token 3449:
The initial state, A CTIONS function, and R ESULT function deﬁne the game tree for the GAME TREE game—a tree where the nodes are game states and the edges are moves.

Token 3450:
Figure 5.1 shows part of the game tree for tic-tac-toe (noughts and crosses). From the initial state, MAX has nine possible moves.

Token 3451:
Play alternates between MAX ’s placing an Xand MIN’s placing an O

Token 3452:
Section 5.2.

Token 3453:
Optimal Decisions in Games 163 until we reach leaf nodes corresponding to terminal states such that one player has three in a row or all the squares are ﬁlled.

Token 3454:
The number on each leaf node indicates the utility valueof the terminal state from the point of view of MAX ; high values are assumed to be good for MAX and bad for MIN (which is how the players get their names).

Token 3455:
For tic-tac-toe the game tree is relatively small—fewer than 9! = 362 ,880 terminal nodes.

Token 3456:
But for chess there are over 1040nodes, so the game tree is best thought of as a theoretical construct that we cannot realize in the physical world.

Token 3457:
But regardless of the sizeof the game tree, it is MAX ’s job to search for a good move.

Token 3458:
We use the term search tree for a SEARCH TREE tree that is superimposed on the full game tree, and examines enough nodes to allow a player to determine what move to make.

Token 3459:
X XX X XXX X X X X OO XO OXO XO X. . . . . . . . . . . .. . .. . . . . .

Token 3460:
X X –1 0 +1X XX XO XXO XXO O OXXXO O OOOX XMAX ( X) MIN ( O) MAX ( X) MIN ( O) TERMINAL Utility Figure 5.1 A (partial) game tree for the game of tic-tac-toe.

Token 3461:
The top node is the initial state, and MAX moves ﬁrst, placing an Xin an empty square.

Token 3462:
We show part of the tree, giving alternating moves by MIN (O)a n d MAX (X), until we eventually reach terminal states, which can be assigned utilities according to the rules of the game.

Token 3463:
5.2 O PTIMAL DECISIONS IN GAMES In a normal search problem, the optimal solution would be a sequence of actions leading to a goal state—a terminal state that is a win.

Token 3464:
In adversarial search, MIN has something to say about it.

Token 3465:
MAX therefore must ﬁnd a contingent strategy , which speciﬁes MAX ’s move in STRATEGY the initial state, then MAX ’s moves in the states resulting from every possible response by

Token 3466:
164 Chapter 5. Adversarial Search MAX A BCD 31 28 2 4 6 1 45 232 23 a1a2a3 b1 b2b3 c1 c2c3 d1 d2d3MIN Figure 5.2 A two-ply game tree.

Token 3467:
The /trianglenodes are “ MAX nodes,” in which it is MAX ’s turn to move, and the /triangleinvnodes are “ MIN nodes.” The terminal node s show the utility values forMAX ; the other nodes are labeled with their minimax values.

Token 3468:
MAX ’s best move at the root isa1, because it leads to the state with the highest minimax value, and MIN’s best reply is b1, because it leads to the state w ith the lowest minimax value.

Token 3469:
MIN,t h e n MAX ’s moves in the states resulting from every possible response by MIN tothose moves, and so on.

Token 3470:
This is exactly analogous to the AND –ORsearch algorithm (Figure 4.11) with MAX playing the role of ORand MIN equivalent to AND.

Token 3471:
Roughly speaking, an optimal strategy leads to outcomes at least as good as any other strategy when one is playing aninfallible opponent.

Token 3472:
We begin by showing how to ﬁnd this optimal strategy.

Token 3473:
Even a simple game like tic-tac-toe is too complex for us to draw the entire game tree on one page, so we will switch to the trivial game in Figure 5.2.

Token 3474:
The possible moves for MAX at the root node are labeled a1,a2,a n da3. The possible replies to a1for MIN areb1,b2, b3, and so on.

Token 3475:
This particular game ends after one move each by MAX and MIN.

Token 3476:
( I n g a m e parlance, we say that this tree is one move deep, consisting of two half-moves, each of whichis called a ply.)

Token 3477:
The utilities of the terminal states in this game range from 2 to 14.

Token 3478:
PLY Given a game tree, the optimal strategy can be determined from the minimax value MINIMAX VALUE of each node, which we write as M INIMAX (n).

Token 3479:
The minimax value of a node is the utility (for MAX ) of being in the corresponding state, assuming that both players play optimally from there to the end of the game.

Token 3480:
Obviously, the minimax value of a terminal state is justits utility.

Token 3481:
Furthermore, given a choice, M AXprefers to move to a state of maximum value, whereas M INprefers a state of minimum value.

Token 3482:
So we have the following: MINIMAX (s)=⎧ ⎨ ⎩UTILITY (s) if T ERMINAL -TEST(s) max a∈Actions (s)MINIMAX (RESULT (s,a))if PLAYER (s)= MAX mina∈Actions (s)MINIMAX (RESULT (s,a))if PLAYER (s)= MIN Let us apply these deﬁnitions to the game tree in Figure 5.2.

Token 3483:
The terminal nodes on the bottom level get their utility values from the game’s U TILITY function.

Token 3484:
The ﬁrst MIN node, labeled B, has three successor states with values 3, 12, and 8, so its minimax value is 3.

Token 3485:
Similarly, the other two MIN nodes have minimax value 2.

Token 3486:
The root node is a MAX node; its successor states have minimax values 3, 2, and 2; so it has a minimax value of 3. We can also identify

Token 3487:
Section 5.2.

Token 3488:
Optimal Decisions in Games 165 theminimax decision at the root: action a1is the optimal choice for MAX because it leads to MINIMAX DECISION the state with the highest minimax value.

Token 3489:
This deﬁnition of optimal play for MAX assumes that MIN also plays optimally—it maximizes the worst-case outcome for MAX .W h a ti f MIN does not play optimally?

Token 3490:
Then it is easy to show (Exercise 5.7) that MAX will do even better.

Token 3491:
Other strategies against suboptimal opponents may do better than the minimax strategy, but these strategies necessarily do worse against optimal opponents.

Token 3492:
5.2.1 The minimax algorithm Theminimax algorithm (Figure 5.3) computes the minimax decision from the current state.

Token 3493:
MINIMAX ALGORITHM It uses a simple recursive computation of the minimax values of each successor state, directly implementing the deﬁning equations.

Token 3494:
The recursion proceeds all the way down to the leaves of the tree, and then the minimax values are backed up through the tree as the recursion unwinds.

Token 3495:
For example, in Figure 5.2, the algorithm ﬁrst recurses down to the three bottom-left nodes and uses the U TILITY function on them to discover that their values are 3, 12, and 8, respectively.

Token 3496:
Then it takes the minimum of these values, 3, and returns it as the backed-up value of node B.

Token 3497:
A similar process gives the backed-up values of 2 for Cand 2 for D. Finally, we take the maximum of 3, 2, and 2 to get the backed-up value of 3 for the root node.

Token 3498:
The minimax algorithm performs a complete depth-ﬁrst exploration of the game tree.

Token 3499:
If the maximum depth of the tree is mand there are blegal moves at each point, then the time complexity of the minimax algorithm is O(b m).

Token 3500:
The space complexity is O(bm)for an algorithm that generates all actions at once, or O(m)for an algorithm that generates actions one at a time (see page 87).

Token 3501:
For real games, of course, the time cost is totally impractical, but this algorithm serves as the basis for the mathematical analysis of games and for morepractical algorithms.

Token 3502:
5.2.2 Optimal decisions in multiplayer games Many popular games allow more than two players.

Token 3503:
Let us examine how to extend the minimaxidea to multiplayer games.

Token 3504:
This is straightforward from the technical viewpoint, but raisessome interesting new conceptual issues.

Token 3505:
First, we need to replace the single value for each node with a vector of values.

Token 3506:
For example, in a three-player game with players A,B,a n dC, a vector/angbracketleftv A,vB,vC/angbracketrightis associated with each node.

Token 3507:
For terminal states, this vector gives the utility of the state from each player’sviewpoint.

Token 3508:
(In two-player, zero-sum games, the two-element vector can be reduced to a singlevalue because the values are always opposite.)

Token 3509:
The simplest way to implement this is to havethe U TILITY function return a vector of utilities. Now we have to consider nonterminal states.

Token 3510:
Consider the node marked Xin the game tree shown in Figure 5.4. In that state, player Cchooses what to do.

Token 3511:
The two choices lead to terminal states with utility vectors /angbracketleftvA=1,vB=2,vC=6/angbracketrightand/angbracketleftvA=4,vB=2,vC=3/angbracketright.

Token 3512:
Since 6 is bigger than 3, Cshould choose the ﬁrst move.

Token 3513:
This means that if state Xis reached, subsequent play will lead to a terminal state with utilities /angbracketleftvA=1,vB=2,vC=6/angbracketright.

Token 3514:
Hence, the backed-up value of Xis this vector. The backed-up value of a node nis always the utility

Token 3515:
166 Chapter 5.

Token 3516:
Adversarial Search function MINIMAX -DECISION (state )returns an action return argmaxa∈ACTIONS (s)MIN-VALUE (RESULT (state ,a)) function MAX-VALUE (state )returns a utility value ifTERMINAL -TEST(state )then return UTILITY (state ) v←−∞ for each ainACTIONS (state )do v←MAX(v,MIN-VALUE (RESULT (s,a))) return v function MIN-VALUE (state )returns a utility value ifTERMINAL -TEST(state )then return UTILITY (state ) v←∞ for each ainACTIONS (state )do v←MIN(v,MAX-VALUE (RESULT (s,a))) return v Figure 5.3 An algorithm for calculating minimax decisions.

Token 3517:
It returns the action corre- sponding to the best possible move, that is, the move that leads to the outcome with the best utility, under t he assumption that the opponent play s to minimize utility.

Token 3518:
The functions MAX-VALUE and M IN-VALUE go through the whole game tree, all the way to the leaves, to determine the backed-up value of a state.

Token 3519:
The notation argmaxa∈Sf(a)computes the element aof setSthat has the maximum value of f(a).

Token 3520:
to move A B C A (1, 2, 6) (4, 2, 3) (6, 1, 2) (7, 4,1) (5,1,1) (1, 5, 2) (7, 7,1) (5, 4, 5)(1, 2, 6) (6, 1, 2) (1, 5, 2) (5, 4, 5)(1, 2, 6) (1, 5, 2)(1, 2, 6) X Figure 5.4 The ﬁrst three plies of a game tree with three players ( A,B,C).

Token 3521:
Each node is labeled with values from the viewpoint of each player. The best move is marked at the root.

Token 3522:
vector of the successor state with the highest value for the player choosing at n. Anyone who plays multiplayer games, such as Diplomacy, quickly becomes aware that much more is going on than in two-player games.

Token 3523:
Multiplayer games usually involve alliances ,w h e t h e r ALLIANCE formal or informal, among the players.

Token 3524:
Alliances are made and broken as the game proceeds. How are we to understand such behavior?

Token 3525:
Are alliances a natural consequence of optimalstrategies for each player in a multiplayer game? It turns out that they can be. For example,

Token 3526:
Section 5.3. Alpha–Beta Pruning 167 suppose AandBare in weak positions and Cis in a stronger position.

Token 3527:
Then it is often optimal for both AandBto attack Crather than each other, lest Cdestroy each of them individually.

Token 3528:
In this way, collaboration emerges from purely selﬁsh behavior.

Token 3529:
Of course,as soon as Cweakens under the joint onslaught, the alliance loses its value, and either A orBcould violate the agreement.

Token 3530:
In some cases, explicit alliances merely make concrete what would have happened anyway.

Token 3531:
In other cases, a social stigma attaches to breaking an alliance, so players must balance the immediate advantage of breaking an alliance against thelong-term disadvantage of being perceived as untrustworthy.

Token 3532:
See Section 17.5 for more onthese complications. If the game is not zero-sum, then collaboration can also occur with just two players.

Token 3533:
Suppose, for example, that there is a terminal state with utilities /angbracketleftv A= 1000 ,vB= 1000/angbracketrightand that 1000 is the highest possible utility for each player.

Token 3534:
Then the optimal strategy is for bothplayers to do everything possible to reach this state—that is, the players will automaticallycooperate to achieve a mutually desirable goal.

Token 3535:
5.3 A LPHA –BETA PRUNING The problem with minimax search is that the number of game states it has to examine is exponential in the depth of the tree.

Token 3536:
Unfortunately, we can’t eliminate the exponent, but it turns out we can effectively cut it in half.

Token 3537:
The trick is that it is possible to compute the correctminimax decision without looking at every node in the game tree.

Token 3538:
That is, we can borrow theidea of pruning from Chapter 3 to eliminate large parts of the tree from consideration.

Token 3539:
The particular technique we examine is called alpha–beta pruning .

Token 3540:
When applied to a standard ALPHA–BETA PRUNING minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly inﬂuence the ﬁnal decision.

Token 3541:
Consider again the two-ply game tree from Figure 5.2.

Token 3542:
Let’s go through the calculation of the optimal decision once more, this time paying careful attention to what we know ateach point in the process.

Token 3543:
The steps are explained in Figure 5.5. The outcome is that we canidentify the minimax decision without ever evaluating two of the leaf nodes.

Token 3544:
Another way to look at this is as a simpliﬁcation of the formula for M INIMAX .L e tt h e two unevaluated successors of node Cin Figure 5.5 have values xandy.

Token 3545:
Then the value of the root node is given by MINIMAX (root) = max(min(3 ,12,8),min(2 ,x,y),min(14 ,5,2)) =m a x ( 3 ,min(2 ,x,y),2) =m a x ( 3 ,z,2) where z=m i n ( 2 ,x,y)≤2 =3.

Token 3546:
In other words, the value of the root and hence the minimax decision are independent of the values of the pruned leaves xandy.

Token 3547:
Alpha–beta pruning can be applied to trees of any depth, and it is often possible to prune entire subtrees rather than just leaves.

Token 3548:
The general principle is this: consider a node n

Token 3549:
168 Chapter 5.

Token 3550:
Adversarial Search (a) (b) (c) (d) (e) (f)3 312 31 28 31 28 2 3 12 8 2 14 3 12 8 2 14 5 2A BA B A BCDA BCDA BA BC[−∞, +∞][ − ∞, +∞] [3, +∞] [3, +∞] [3, 3] [3, 14][−∞, 2] [−∞, 2] [2, 2][3, 3] [3, 3] [3, 3][3, 3][−∞, 3][ − ∞, 3] [−∞, 2][ − ∞, 14] Figure 5.5 Stages in the calculation of the optimal decision for the game tree in Figure 5.2.

Token 3551:
At each point, we show the range of possible values for each node. (a) The ﬁrst leaf below B has the value 3.

Token 3552:
Hence, B,w h i c hi sa MIN node, has a value of at most 3.

Token 3553:
(b) The second leaf below Bhas a value of 12; MIN would avoid this move, so the value of Bis still at most 3.

Token 3554:
(c) The third leaf below Bhas a value of 8; we have seen all B’s successor states, so the value of Bis exactly 3.

Token 3555:
Now, we can infer that the value of the root is at least 3, because MAX has a choice worth 3 at the root. (d) The ﬁrst leaf below Chas the value 2.

Token 3556:
Hence, C,w h i c hi sa MIN node, has a value of at most 2.

Token 3557:
But we know that Bis worth 3, so MAX would never choose C. Therefore, there is no point in looking at the other successor states ofC.

Token 3558:
This is an example of alpha–beta pruning. (e) The ﬁrst leaf below Dhas the value 14, soDis worth at most 14.

Token 3559:
This is still higher than MAX ’s best alternative (i.e., 3), so we need to keep exploring D’s successor states.

Token 3560:
Notice also that we now have bounds on all of the successors of the root, so the root’s value is also at most 14.

Token 3561:
(f) The second successor of D is worth 5, so again we need to keep exploring. The third successor is worth 2, so now Dis worth exactly 2.

Token 3562:
MAX ’s decision at the root is to move to B, giving a value of 3. somewhere in the tree (see Figure 5.6), such that Player has a choice of moving to that node.

Token 3563:
If Player has a better choice meither at the parent node of nor at any choice point further up, thennwill never be reached in actual play.

Token 3564:
So once we have found out enough about n(by examining some of its descendants) to reach this conclusion, we can prune it.

Token 3565:
Remember that minimax search is depth-ﬁrst, so at any one time we just have to con- sider the nodes along a single path in the tree.

Token 3566:
Alpha–beta pruning gets its name from thefollowing two parameters that describe bounds on the backed-up values that appear anywherealong the path:

Token 3567:
Section 5.3. Alpha–Beta Pruning 169 Player Opponent Player Opponentm n• •• Figure 5.6 The general case for alpha–beta pruning.

Token 3568:
If mis better than nfor Player, we will never get to nin play.

Token 3569:
α=the value of the best (i.e., highest-value) choice we have found so far at any choice point along the path for MAX .

Token 3570:
β=the value of the best (i.e., lowest-value) choice we have found so far at any choice point along the path for MIN.

Token 3571:
Alpha–beta search updates the values of αandβas it goes along and prunes the remaining branches at a node (i.e., terminates the recursive call) as soon as the value of the current node is known to be worse than the current αorβvalue for MAX orMIN, respectively.

Token 3572:
The complete algorithm is given in Figure 5.7. We encourage you to trace its behavior whenapplied to the tree in Figure 5.5.

Token 3573:
5.3.1 Move ordering The effectiveness of alpha–beta pruning is highly dependent on the order in which the statesare examined.

Token 3574:
For example, in Figure 5.5(e) and (f), we could not prune any successors of D at all because the worst successors (from the point of view of MIN) were generated ﬁrst.

Token 3575:
If the third successor of Dhad been generated ﬁrst, we would have been able to prune the other two.

Token 3576:
This suggests that it might be worthwhile to try to examine ﬁrst the successors that arelikely to be best.

Token 3577:
If this can be done, 2then it turns out that alpha–beta needs to examine only O(bm/2) nodes to pick the best move, instead of O(bm)for minimax.

Token 3578:
This means that the effective branching factor becomes√ binstead of b—for chess, about 6 instead of 35.

Token 3579:
Put another way, alpha–beta can solve a tree roughly twice as deep as minimax in the same amount of time.

Token 3580:
If successors are examined in random order rather than best-ﬁrst, the total number of nodes examined will be roughly O(b3m/4)for moderate b.

Token 3581:
For chess, a fairly simple ordering function (such as trying captures ﬁrst, then threats, then forward moves, and then backwardmoves) gets you to within about a factor of 2 of the best-case O(b m/2)result.

Token 3582:
2Obviously, it cannot be done perfectly; otherwise, the or dering function could be used to play a perfect game!

Token 3583:
170 Chapter 5.

Token 3584:
Adversarial Search function ALPHA -BETA-SEARCH (state )returns an action v←MAX-VALUE (state ,−∞,+∞) return theaction in A CTIONS (state ) with value v function MAX-VALUE (state ,α,β)returns a utility value ifTERMINAL -TEST(state )then return UTILITY (state ) v←−∞ for each ainACTIONS (state )do v←MAX(v,MIN-VALUE (RESULT (s,a),α,β)) ifv≥βthen return v α←MAX(α,v) return v function MIN-VALUE (state ,α,β)returns a utility value ifTERMINAL -TEST(state )then return UTILITY (state ) v←+∞ for each ainACTIONS (state )do v←MIN(v,MAX-VALUE (RESULT (s,a),α,β)) ifv≤αthen return v β←MIN(β,v) return v Figure 5.7 The alpha–beta search algorithm.

Token 3585:
Notice that these routines are the same as the M INIMAX functions in Figure 5.3, except for the two lines in each of M IN-VALUE and MAX-VALUE that maintain αandβ(and the bookkeeping to pass these parameters along).

Token 3586:
Adding dynamic move-ordering schemes, such as trying ﬁrst the moves that were found to be best in the past, brings us quite close to the theoretical limit.

Token 3587:
The past could be theprevious move—often the same threats remain—or it could come from previous exploration of the current move.

Token 3588:
One way to gain information from the current move is with iterative deepening search. First, search 1 ply deep and record the best path of moves.

Token 3589:
Then search1 ply deeper, but use the recorded path to inform move ordering.

Token 3590:
As we saw in Chapter 3,iterative deepening on an exponential game tree adds only a constant fraction to the totalsearch time, which can be more than made up from better move ordering.

Token 3591:
The best moves areoften called killer moves and to try them ﬁrst is called the killer move heuristic.

Token 3592:
KILLER MOVES In Chapter 3, we noted that repeated states in the search tree can cause an exponential increase in search cost.

Token 3593:
In many games, repeated states occur frequently because of transpo- sitions —different permutations of the move sequence that end up in the same position.

Token 3594:
For TRANSPOSITION example, if White has one move, a1, that can be answered by Black with b1and an unre- lated move a2on the other side of the board that can be answered by b2, then the sequences [a1,b1,a2,b2]and[a2,b2,a1,b1]both end up in the same position.

Token 3595:
It is worthwhile to store the evaluation of the resulting position in a hash table the ﬁrst time it is encountered so that we don’t have to recompute it on subsequent occurrences.

Token 3596:
The hash table of previously seen positions is traditionally called a transposition table ; it is essentially identical to the exploredTRANSPOSITION TABLE

Token 3597:
Section 5.4. Imperfect Real-Time Decisions 171 list in G RAPH -SEARCH (Section 3.3).

Token 3598:
Using a transposition table can have a dramatic effect, sometimes as much as doubling the reachable search depth in chess.

Token 3599:
On the other hand, if weare evaluating a million nodes per second, at some point it is not practical to keep allof them in the transposition table.

Token 3600:
Various strategies have been used to choose which nodes to keepand which to discard.

Token 3601:
5.4 I MPERFECT REAL-TIMEDECISIONS The minimax algorithm generates the entire game search space, whereas the alpha–beta algo- rithm allows us to prune large parts of it.

Token 3602:
However, alpha–beta still has to search all the way to terminal states for at least a portion of the search space.

Token 3603:
This depth is usually not practical,because moves must be made in a reasonable amount of time—typically a few minutes atmost.

Token 3604:
Claude Shannon’s paper Programming a Computer for Playing Chess (1950) proposed instead that programs should cut off the search earlier and apply a heuristic evaluation func- tion to states in the search, effectively turning nonterminal nodes into terminal leaves.

Token 3605:
In EVALUATION FUNCTION other words, the suggestion is to alter minimax or alpha–beta in two ways: replace the utility function by a heuristic evaluation function E VA L, which estimates the position’s utility, and replace the terminal test by a cutoff test that decides when to apply E VA L. That gives us the CUTOFF TEST following for heuristic minimax for state sand maximum depth d: H-M INIMAX (s,d)=⎧ ⎨ ⎩EVA L(s) if C UTOFF -TEST(s,d) max a∈Actions (s)H-M INIMAX (RESULT (s,a),d+1 ) if PLAYER (s)= MAX mina∈Actions (s)H-M INIMAX (RESULT (s,a),d+1 ) if PLAYER (s)= MIN.

Token 3606:
5.4.1 Evaluation functions An evaluation function returns an estimate of the expected utility of the game from a given position, just as the heuristic functions of Chapter 3 return an estimate of the distance tothe goal.

Token 3607:
The idea of an estimator was not new when Shannon proposed it.

Token 3608:
For centuries,chess players (and aﬁcionados of other games) have developed ways of judging the value ofa position because humans are even more limited in the amount of search they can do thanare computer programs.

Token 3609:
It should be clear that the performance of a game-playing programdepends strongly on the quality of its evaluation function.

Token 3610:
An inaccurate evaluation functionwill guide an agent toward positions that turn out to be lost. How exactly do we design goodevaluation functions?

Token 3611:
First, the evaluation function should order the terminal states in the same way as the true utility function: states that are wins must evaluate better than draws, which in turn must be better than losses.

Token 3612:
Otherwise, an agent using the evaluation function might err even if it can see ahead all the way to the end of the game.

Token 3613:
Second, the computation must not taketoo long! (The whole point is to search faster.)

Token 3614:
Third, for nonterminal states, the evaluationfunction should be strongly correlated with the actual chances of winning.

Token 3615:
172 Chapter 5.

Token 3616:
Adversarial Search One might well wonder about the phrase “chances of winning.” After all, chess is not a game of chance: we know the current state with certainty, and no dice are involved.

Token 3617:
But if thesearch must be cut off at nonterminal states, then the algorithm will necessarily be uncertain about the ﬁnal outcomes of those states.

Token 3618:
This type of uncertainty is induced by computational,rather than informational, limitations.

Token 3619:
Given the limited amount of computation that the evaluation function is allowed to do for a given state, the best it can do is make a guess about the ﬁnal outcome.

Token 3620:
Let us make this idea more concrete.

Token 3621:
Most evaluation functions work by calculating various features of the state—for example, in chess, we would have features for the number of white pawns, black pawns, white queens, black queens, and so on.

Token 3622:
The features, takentogether, deﬁne various categories orequivalence classes of states: the states in each category have the same values for all the features.

Token 3623:
For example, one category contains all two-pawnvs. one-pawn endgames.

Token 3624:
Any given category, generally speaking, will contain some statesthat lead to wins, some that lead to draws, and some that lead to losses.

Token 3625:
The evaluationfunction cannot know which states are which, but it can return a single value that reﬂects theproportion of states with each outcome.

Token 3626:
For example, suppose our experience suggests that 72% of the states encountered in the two-pawns vs. one-pawn category lead to a win (utility +1); 20% to a loss (0), and 8% to a draw (1/2).

Token 3627:
Then a reasonable evaluation for states in the category is the expected value :(0.72×+1) + (0 .20×0) + (0 .08×1/2) = 0 .76.I n EXPECTED VALUE principle, the expected value can be determined for each category, resulting in an evaluation function that works for any state.

Token 3628:
As with terminal states, the evaluation function need notreturn actual expected values as long as the ordering of the states is the same.

Token 3629:
In practice, this kind of analysis requires too many categories and hence too much experience to estimate all the probabilities of winning.

Token 3630:
Instead, most evaluation functionscompute separate numerical contributions from each feature and then combine them to ﬁnd the total value.

Token 3631:
For example, introductory chess books give an approximate material value MATERIAL VALUE for each piece: each pawn is worth 1, a knight or bishop is worth 3, a rook 5, and the queen 9.

Token 3632:
Other features such as “good pawn structure” and “king safety” might be worth half a pawn, say.

Token 3633:
These feature values are then simply added up to obtain the evaluation of the position.

Token 3634:
A secure advantage equivalent to a pawn gives a substantial likelihood of winning, and a secure advantage equivalent to three pawns should give almost certain victory, as illustratedin Figure 5.8(a).

Token 3635:
Mathematically, this kind of evaluation function is called a weighted linear function because it can be expressed as WEIGHTED LINEAR FUNCTION EVA L(s)=w1f1(s)+w2f2(s)+···+wnfn(s)=n/summationdisplay i=1wifi(s), where each wiis a weight and each fiis a feature of the position.

Token 3636:
For chess, the ficould be the numbers of each kind of piece on the board, and the wicould be the values of the pieces (1 for pawn, 3 for bishop, etc.).

Token 3637:
Adding up the values of features seems like a reasonable thing to do, but in fact it involves a strong assumption: that the contribution of each feature is independent of the values of the other features.

Token 3638:
For example, assigning the value 3 to a bishop ignores the factthat bishops are more powerful in the endgame, when they have a lot of space to maneuver.

Token 3639:
Section 5.4.

Token 3640:
Imperfect Real-Time Decisions 173 (b)White to move (a)White to move Figure 5.8 Two chess positions that differ only in the position of the rook at lower right.

Token 3641:
In (a), Black has an advantage of a knight and two pawns, which should be enough to win the game.

Token 3642:
In (b), White will capture the queen, giving it an advantage that should be strongenough to win.

Token 3643:
For this reason, current programs for chess and other games also use nonlinear combinations of features.

Token 3644:
For example, a pair of bishops might be worth slightly more than twice the valueof a single bishop, and a bishop is worth more in the endgame (that is, when the move number feature is high or the number of remaining pieces feature is low).

Token 3645:
The astute reader will have noticed that the features and weights are notpart of the rules of chess!

Token 3646:
They come from centuries of human chess-playing experience.

Token 3647:
In games where thiskind of experience is not available, the weights of the evaluation function can be estimatedby the machine learning techniques of Chapter 18.

Token 3648:
Reassuringly, applying these techniquesto chess has conﬁrmed that a bishop is indeed worth about three pawns.

Token 3649:
5.4.2 Cutting off search The next step is to modify A LPHA -BETA-SEARCH so that it will call the heuristic E VA L function when it is appropriate to cut off the search.

Token 3650:
We replace the two lines in Figure 5.7that mention T ERMINAL -TESTwith the following line: ifCUTOFF -TEST(state ,depth )then return EVA L(state ) We also must arrange for some bookkeeping so that the current depth is incremented on each recursive call.

Token 3651:
The most straightforward approach to controlling the amount of search is to seta ﬁxed depth limit so that C UTOFF -TEST(state ,depth ) returns true for alldepth greater than some ﬁxed depth d. (It must also return true for all terminal states, just as T ERMINAL -TEST did.)

Token 3652:
The depth dis chosen so that a move is selected within the allocated time. A more robust approach is to apply iterative deepening. (See Chapter 3.)

Token 3653:
When time runs out, theprogram returns the move selected by the deepest completed search.

Token 3654:
As a bonus, iterativedeepening also helps with move ordering.

Token 3655:
174 Chapter 5. Adversarial Search These simple approaches can lead to errors due to the approximate nature of the eval- uation function.

Token 3656:
Consider again the simple evaluation function for chess based on materialadvantage.

Token 3657:
Suppose the program searches to the depth limit, reaching the position in Fig-ure 5.8(b), where Black is ahead by a knight and two pawns.

Token 3658:
It would report this as theheuristic value of the state, thereby declaring that the state is a probable win by Black.

Token 3659:
But White’s next move captures Black’s queen with no compensation.

Token 3660:
Hence, the position is really won for White, but this can be seen only by looking ahead one more ply.

Token 3661:
Obviously, a more sophisticated cutoff test is needed.

Token 3662:
The evaluation function should be applied only to positions that are quiescent —that is, unlikely to exhibit wild swings in value QUIESCENCE in the near future.

Token 3663:
In chess, for example, positions in which favorable captures can be made are not quiescent for an evaluation function that just counts material.

Token 3664:
Nonquiescent positionscan be expanded further until quiescent positions are reached.

Token 3665:
This extra search is called aquiescence search ; sometimes it is restricted to consider only certain types of moves, such QUIESCENCE SEARCH as capture moves, that will quickly resolve the uncertainties in the position.

Token 3666:
The horizon effect is more difﬁcult to eliminate.

Token 3667:
It arises when the program is facing HORIZON EFFECT an opponent’s move that causes serious damage and is ultimately unavoidable, but can be temporarily avoided by delaying tactics.

Token 3668:
Consider the chess game in Figure 5.9. It is clear that there is no way for the black bishop to escape.

Token 3669:
For example, the white rook can capture it by moving to h1, then a1, then a2; a capture at depth 6 ply.

Token 3670:
But Black does have a sequenceof moves that pushes the capture of the bishop “over the horizon.” Suppose Black searchesto depth 8 ply.

Token 3671:
Most moves by Black will lead to the eventual capture of the bishop, and thuswill be marked as “bad” moves.

Token 3672:
But Black will consider checking the white king with thepawn at e4. This will lead to the king capturing the pawn.

Token 3673:
Now Black will consider checkingagain, with the pawn at f5, leading to another pawn capture.

Token 3674:
That takes up 4 ply, and fromthere the remaining 4 ply is not enough to capture the bishop.

Token 3675:
Black thinks that the line ofplay has saved the bishop at the price of two pawns, when actually all it has done is push theinevitable capture of the bishop beyond the horizon that Black can see.

Token 3676:
One strategy to mitigate the horizon effect is the singular extension , a move that is SINGULAR EXTENSION “clearly better” than all other moves in a given position.

Token 3677:
Once discovered anywhere in the tree in the course of a search, this singular move is remembered.

Token 3678:
When the search reaches thenormal depth limit, the algorithm checks to see if the singular extension is a legal move; if itis, the algorithm allows the move to be considered.

Token 3679:
This makes the tree deeper, but becausethere will be few singular extensions, it does not add many total nodes to the tree.

Token 3680:
5.4.3 Forward pruning So far, we have talked about cutting off search at a certain level and about doing alpha–beta pruning that provably has no effect on the result (at least with respect to the heuristicevaluation values).

Token 3681:
It is also possible to do forward pruning , meaning that some moves at FORWARD PRUNING a given node are pruned immediately without further consideration.

Token 3682:
Clearly, most humans playing chess consider only a few moves from each position (at least consciously).

Token 3683:
Oneapproach to forward pruning is beam search : on each ply, consider only a “beam” of the n BEAM SEARCH best moves (according to the evaluation function) rather than considering all possible moves.

Token 3684:
Section 5.4. Imperfect Real-Time Decisions 175 a b c d e f g h1 2 3 4 5 6 7 8 Figure 5.9 The horizon effect.

Token 3685:
With Black to move, the black bishop is surely doomed.

Token 3686:
But Black can forestall that event by checking the white king with its pawns, forcing the king to capture the pawns.

Token 3687:
This pushes the inevitable loss of the bishop over the horizon, and thusthe pawn sacriﬁces are seen by the search algorithm as good moves rather than bad ones.

Token 3688:
Unfortunately, this approach is rather dangerous because there is no guarantee that the best move will not be pruned away.

Token 3689:
The P ROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver- sion of alpha–beta search that uses statistics gained from prior experience to lessen the chancethat the best move will be pruned.

Token 3690:
Alpha–beta search prunes any node that is provably out- side the current (α,β)window. P ROBCUTalso prunes nodes that are probably outside the window.

Token 3691:
It computes this probability by doing a shallow search to compute the backed-upvaluevof a node and then using past experience to estimate how likely it is that a score of v at depth din the tree would be outside (α,β).

Token 3692:
Buro applied this technique to his Othello pro- gram, L OGISTELLO , and found that a version of his program with P ROBCUTbeat the regular version 64% of the time, even when the regular version was given twice as much time.

Token 3693:
Combining all the techniques described here results in a program that can play cred- itable chess (or other games).

Token 3694:
Let us assume we have implemented an evaluation function for chess, a reasonable cutoff test with a quiescence search, and a large transposition table.

Token 3695:
Let us also assume that, after months of tedious bit-bashing, we can generate and evaluate around a million nodes per second on the latest PC, allowing us to search roughly 200 million nodes per move under standard time controls (three minutes per move).

Token 3696:
The branching factor forchess is about 35, on average, and 35 5is about 50 million, so if we used minimax search, we could look ahead only about ﬁve plies.

Token 3697:
Though not incompetent, such a program can befooled easily by an average human chess player, who can occasionally plan six or eight plies ahead.

Token 3698:
With alpha–beta search we get to about 10 plies, which results in an expert level of play.

Token 3699:
Section 5.8 describes additional pruning techniques that can extend the effective searchdepth to roughly 14 plies.

Token 3700:
To reach grandmaster status we would need an extensively tunedevaluation function and a large database of optimal opening and endgame moves.

Token 3701:
176 Chapter 5.

Token 3702:
Adversarial Search 5.4.4 Search versus lookup Somehow it seems like overkill for a chess program to start a game by considering a tree of a billion game states, only to conclude that it will move its pawn to e4.

Token 3703:
Books describing goodplay in the opening and endgame in chess have been available for about a century (Tattersall,1911).

Token 3704:
It is not surprising, therefore, that many game-playing programs use table lookup rather than search for the opening and ending of games.

Token 3705:
For the openings, the computer is mostly relying on the expertise of humans.

Token 3706:
The best advice of human experts on how to play each opening is copied from books and entered intotables for the computer’s use.

Token 3707:
However, computers can also gather statistics from a databaseof previously played games to see which opening sequences most often lead to a win.

Token 3708:
Inthe early moves there are few choices, and thus much expert commentary and past games onwhich to draw.

Token 3709:
Usually after ten moves we end up in a rarely seen position, and the programmust switch from table lookup to search.

Token 3710:
Near the end of the game there are again fewer possible positions, and thus more chance to do lookup.

Token 3711:
But here it is the computer that has the expertise: computer analysis ofendgames goes far beyond anything achieved by humans.

Token 3712:
A human can tell you the gen- eral strategy for playing a king-and-rook-versus-king (KRK) endgame: reduce the opposing king’s mobility by squeezing it toward one edge of the board, using your king to prevent theopponent from escaping the squeeze.

Token 3713:
Other endings, such as king, bishop, and knight versusking (KBNK), are difﬁcult to master and have no succinct strategy description.

Token 3714:
A computer,on the other hand, can completely solve the endgame by producing a policy , which is a map- POLICY ping from every possible state to the best move in that state.

Token 3715:
Then we can just look up the best move rather than recompute it anew. How big will the KBNK lookup table be?

Token 3716:
It turns outthere are 462 ways that two kings can be placed on the board without being adjacent.

Token 3717:
Afterthe kings are placed, there are 62 empty squares for the bishop, 61 for the knight, and twopossible players to move next, so there are just 462×62×61×2=3 ,494,568possible positions.

Token 3718:
Some of these are checkmates; mark them as such in a table.

Token 3719:
Then do a retrograde RETROGRADE minimax search: reverse the rules of chess to do unmoves rather than moves.

Token 3720:
Any move by White that, no matter what move Black responds with, ends up in a position marked as a win, must also be a win.

Token 3721:
Continue this search until all 3,494,568 positions are resolved as win,loss, or draw, and you have an infallible lookup table for all KBNK endgames.

Token 3722:
Using this technique and a tour de force of optimization tricks, Ken Thompson (1986, 1996) and Lewis Stiller (1992, 1996) solved all chess endgames with up to ﬁve pieces andsome with six pieces, making them available on the Internet.

Token 3723:
Stiller discovered one casewhere a forced mate existed but required 262 moves; this caused some consternation becausethe rules of chess require a capture or pawn move to occur within 50 moves.

Token 3724:
Later work byMarc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-pieceand some seven-piece endgames; there is a KQNKRBN endgame that with best play requires 517 moves until a capture, which then leads to a mate.

Token 3725:
If we could extend the chess endgame tables from 6 pieces to 32, then White would know on the opening move whether it would be a win, loss, or draw.

Token 3726:
This has not happenedso far for chess, but it has happened for checkers, as explained in the historical notes section.

Token 3727:
Section 5.5. Stochastic Games 177 5.5 S TOCHASTIC GAMES In real life, many unpredictable external events can put us into unforeseen situations.

Token 3728:
Many games mirror this unpredictability by including a random element, such as the throwing ofdice. We call these stochastic games .

Token 3729:
Backgammon is a typical game that combines luck STOCHASTIC GAMES and skill.

Token 3730:
Dice are rolled at the beginning of a player’s turn to determine the legal moves.

Token 3731:
In the backgammon position of Figure 5.10, for example, White has rolled a 6–5 and has fourpossible moves.

Token 3732:
1234 5 6 7 8 9 10 11 12 24 23 22 21 20 19 18 17 16 15 14 130 25 Figure 5.10 A typical backgammon position.

Token 3733:
The goal of the game is to move all one’s pieces off the board. White moves clockwise toward 25, and Black moves counterclockwisetoward 0.

Token 3734:
A piece can move to any position unless multiple opponent pieces are there; if there is one opponent, it is captured and must start ove r. In the position shown, White has rolled 6–5 and must choose among four legal moves: (5–10,5–11), (5–11,19–24), (5–10,10–16), and (5–11,11–16), where the notation (5–11,11–16) means move one piece from position 5 to 11, and then move a piece from 11 to 16.

Token 3735:
Although White knows what his or her own legal moves are, White does not know what Black is going to roll and thus does not know what Black’s legal moves will be.

Token 3736:
That meansWhite cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe.

Token 3737:
Agame tree in backgammon must include chance nodes in addition to MAX and MIN nodes. CHANCENODES Chance nodes are shown as circles in Figure 5.11.

Token 3738:
The branches leading from each chance node denote the possible dice rolls; each branch is labeled with the roll and its probability.

Token 3739:
There are 36 ways to roll two dice, each equally likely; but because a 6–5 is the same as a 5–6,there are only 21 distinct rolls.

Token 3740:
The six doubles (1–1 through 6–6) each have a probability of1/36, so we say P(1–1)=1/36. The other 15 distinct rolls each have a 1/18 probability.

Token 3741:
178 Chapter 5. Adversarial Search CHANCE MIN MAXCHANCEMAX . . .. . .B 1. .

Token 3742:
.1,11/36 1,21/18 TERMINAL1,21/18 ... ... ... ... ...... ...1,11/36 ...... ... ... ... ...C. . .

Token 3743:
1/18 6,5 6,61/36 1/18 6,5 6,61/36 2– 1 1 –1 Figure 5.11 Schematic game tree for a backgammon position.

Token 3744:
The next step is to understand how to make correct decisions. Obviously, we still want to pick the move that leads to the best position.

Token 3745:
However, positions do not have deﬁniteminimax values.

Token 3746:
Instead, we can only calculate the expected value of a position: the average EXPECTED VALUE over all possible outcomes of the chance nodes.

Token 3747:
This leads us to generalize the minimax value for deterministic games to an expecti- minimax value for games with chance nodes.

Token 3748:
Terminal nodes and MAX and MIN nodes (forEXPECTIMINIMAX VALUE which the dice roll is known) work exactly the same way as before.

Token 3749:
For chance nodes we compute the expected value, which is the sum of the value over all outcomes, weighted bythe probability of each chance action: E XPECTIMINIMAX (s)=⎧ ⎪⎪⎨ ⎪⎪⎩UTILITY (s) if T ERMINAL -TEST(s) max aEXPECTIMINIMAX (RESULT (s,a)) if PLAYER (s)= MAX minaEXPECTIMINIMAX (RESULT (s,a)) if PLAYER (s)= MIN/summationtext rP(r)EXPECTIMINIMAX (RESULT (s,r))if PLAYER (s)= CHANCE where rrepresents a possible dice roll (or other chance event) and R ESULT (s,r)is the same state as s, with the additional fact that the result of the dice roll is r. 5.5.1 Evaluation functions for games of chance As with minimax, the obvious approximation to make with expectiminimax is to cut the search off at some point and apply an evaluation function to each leaf.

Token 3750:
One might think thatevaluation functions for games such as backgammon should be just like evaluation functions

Token 3751:
Section 5.5. Stochastic Games 179 for chess—they just need to give higher scores to better positions.

Token 3752:
But in fact, the presence of chance nodes means that one has to be more careful about what the evaluation values mean.Figure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2,3, 4] to the leaves, move a 1is best; with values [1, 20, 30, 400], move a2is best.

Token 3753:
Hence, the program behaves totally differently if we make a change in the scale of some evaluation values!

Token 3754:
It turns out that to avoid this sensitivity, the evaluation function must be a positive linear transformation of the probability of winning from a position (or, more generally, of theexpected utility of the position).

Token 3755:
This is an important and general property of situations inwhich uncertainty is involved, and we discuss it further in Chapter 16.

Token 3756:
CHANCE MINMAX 22 3311 4423 1 4.9 .1 .9 .12.1 1.3 20 20 30 30 1 1 400 40020 30 1 400.9 .1 .9 .121 40.9a1a2 a1a2 Figure 5.12 An order-preserving transformation on leaf values changes the best move.

Token 3757:
If the program knew in advance all the dice rolls that would occur for the rest of the game, solving a game with dice would be just like solving a game without dice, which mini- max does in O(bm)time, where bis the branching factor and mis the maximum depth of the game tree.

Token 3758:
Because expectiminimax is also considering all the possible dice-roll sequences,it will take O(b mnm),w h e r e nis the number of distinct rolls.

Token 3759:
Even if the search depth is limited to some small depth d, the extra cost compared with that of minimax makes it unrealistic to consider looking ahead very far in most games ofchance.

Token 3760:
In backgammon nis 21 and bis usually around 20, but in some situations can be as high as 4000 for dice rolls that are doubles.

Token 3761:
Three plies is probably all we could manage.

Token 3762:
Another way to think about the problem is this: the advantage of alpha–beta is that it ignores future developments that just are not going to happen, given best play.

Token 3763:
Thus, itconcentrates on likely occurrences.

Token 3764:
In games with dice, there are nolikely sequences of moves, because for those moves to take place, the dice would ﬁrst have to come out the right way to make them legal.

Token 3765:
This is a general problem whenever uncertainty enters the picture: the possibilities are multiplied enormously, and forming detailed plans of action becomespointless because the world probably will not play along.

Token 3766:
It may have occurred to you that something like alpha–beta pruning could be applied

Token 3767:
180 Chapter 5. Adversarial Search to game trees with chance nodes. It turns out that it can.

Token 3768:
The analysis for MIN and MAX nodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity.

Token 3769:
Consider the chance node Cin Figure 5.11 and what happens to its value as we examine and evaluate its children.

Token 3770:
Is it possible to ﬁnd an upper bound on the value of Cbefore we have looked at all its children?

Token 3771:
(Recall that this is what alpha–beta needs in order to prune a node and its subtree.)

Token 3772:
At ﬁrst sight, it might seem impossible because the value of Cis the average of its children’s values, and in order to compute the average of a set of numbers, we must look atall the numbers.

Token 3773:
But if we put bounds on the possible values of the utility function, then wecan arrive at bounds for the average without looking at every number.

Token 3774:
For example, say thatall utility values are between −2and+2; then the value of leaf nodes is bounded, and in turn wecanplace an upper bound on the value of a chance node without looking at all its children.

Token 3775:
An alternative is to do Monte Carlo simulation to evaluate a position. Start with MONTE CARLO SIMULATION an alpha–beta (or other) search algorithm.

Token 3776:
From a start position, have the algorithm play thousands of games against itself, using random dice rolls.

Token 3777:
In the case of backgammon, theresulting win percentage has been shown to be a good approximation of the value of theposition, even if the algorithm has an imperfect heuristic and is searching only a few plies(Tesauro, 1995).

Token 3778:
For games with dice, this type of simulation is called a rollout .

Token 3779:
ROLLOUT 5.6 P ARTIALLY OBSERV ABLE GAMES Chess has often been described as war in miniature, but it lacks at least one major charac- teristic of real wars, namely, partial observability .

Token 3780:
In the “fog of war,” the existence and disposition of enemy units is often unknown until revealed by direct contact.

Token 3781:
As a result,warfare includes the use of scouts and spies to gather information and the use of concealmentand bluff to confuse the enemy.

Token 3782:
Partially observable games share these characteristics andare thus qualitatively different from the games described in the preceding sections.

Token 3783:
5.6.1 Kriegspiel: Partially observable chess Indeterministic partially observable games, uncertainty about the state of the board arises en- tirely from lack of access to the choices made by the opponent.

Token 3784:
This class includes children’sgames such as Battleships (where each player’s ships are placed in locations hidden from theopponent but do not move) and Stratego (where piece locations are known but piece types arehidden).

Token 3785:
We will examine the game of Kriegspiel , a partially observable variant of chess in KRIEGSPIEL which pieces can move but are completely invisible to the opponent.

Token 3786:
The rules of Kriegspiel are as follows: White and Black each see a board containing only their own pieces.

Token 3787:
A referee, who can see all the pieces, adjudicates the game and period- ically makes announcements that are heard by both players.

Token 3788:
On his turn, White proposes to the referee any move that would be legal if there were no black pieces.

Token 3789:
If the move is in fact not legal (because of the black pieces), the referee announces “illegal.” In this case, White may keep proposing moves until a legal one is found—and learns more about the location of Black’s pieces in the process.

Token 3790:
Once a legal move is proposed, the referee announces one or

Token 3791:
Section 5.6.

Token 3792:
Partially Observable Games 181 more of the following: “Capture on square X” if there is a capture, and “Check by D”i ft h e black king is in check, where Dis the direction of the check, and can be one of “Knight,” “Rank,” “File,” “Long diagonal,” or “Short diagonal.” (In case of discovered check, the ref-eree may make two “Check” announcements.)

Token 3793:
If Black is checkmated or stalemated, thereferee says so; otherwise, it is Black’s turn to move.

Token 3794:
Kriegspiel may seem terrifyingly impossible, but humans manage it quite well and com- puter programs are beginning to catch up.

Token 3795:
It helps to recall the notion of a belief state as deﬁned in Section 4.4 and illustrated in Figure 4.14—the set of all logically possible board states given the complete history of percepts to date.

Token 3796:
Initially, White’s belief state is a sin-gleton because Black’s pieces haven’t moved yet.

Token 3797:
After White makes a move and Black re-sponds, White’s belief state contains 20 positions because Black has 20 replies to any Whitemove.

Token 3798:
Keeping track of the belief state as the game progresses is exactly the problem of state estimation , for which the update step is given in Equation (4.6).

Token 3799:
We can map Kriegspiel state estimation directly onto the partially observable, nondeterministic framework of Sec-tion 4.4 if we consider the opponent as the source of nondeterminism; that is, the R ESULTS of White’s move are composed from the (predictable) outcome of White’s own move and theunpredictable outcome given by Black’s reply.

Token 3800:
3 Given a current belief state, White may ask, “Can I win the game?” For a partially observable game, the notion of a strategy is altered; instead of specifying a move to make for each possible move the opponent might make, we need a move for every possible percept sequence that might be received.

Token 3801:
For Kriegspiel, a winning strategy, or guaranteed check- mate , is one that, for each possible percept sequence, leads to an actual checkmate for everyGUARANTEED CHECKMATE possible board state in the current belief state, regardless of how the opponent moves.

Token 3802:
With this deﬁnition, the opponent’s belief state is irrelevant—the strategy has to work even if theopponent can see all the pieces.

Token 3803:
This greatly simpliﬁes the computation. Figure 5.13 showspart of a guaranteed checkmate for the KRK (king and rook against king) endgame.

Token 3804:
In thiscase, Black has just one piece (the king), so a belief state for White can be shown in a singleboard by marking each possible position of the Black king.

Token 3805:
The general AND -ORsearch algorithm can be applied to the belief-state space to ﬁnd guaranteed checkmates, just as in Section 4.4.

Token 3806:
The incremental belief-state algorithm men-tioned in that section often ﬁnds midgame checkmates up to depth 9—probably well beyondthe abilities of human players.

Token 3807:
In addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that makes no sense in fully observable games: probabilistic checkmate .

Token 3808:
Such checkmates are PROBABILISTIC CHECKMATE still required to work in every board state in the belief state; they are probabilistic with respect to randomization of the winning player’s moves.

Token 3809:
To get the basic idea, consider the problemof ﬁnding a lone black king using just the white king.

Token 3810:
Simply by moving randomly, thewhite king will eventually bump into the black king even if the latter tries to avoid this fate, since Black cannot keep guessing the right evasive moves indeﬁnitely.

Token 3811:
In the terminology of probability theory, detection occurs with probability 1.

Token 3812:
The KBNK endgame—king, bishop 3Sometimes, the belief state will become too large to represent just as a list of board states, but we will ignore this issue for now; Chapters 7 and 8 suggest methods for compactly representing very large belief states.

Token 3813:
182 Chapter 5. Adversarial Search a1234 d bcKc3 ? “Illegal” “OK” Rc3 ?

Token 3814:
“OK” “Check” Figure 5.13 Part of a guaranteed checkmate in the KRK endgame, shown on a reduced board.

Token 3815:
In the initial belief state, Black’s king is in one of three possible locations.

Token 3816:
By acombination of probing moves, the strategy narrows this down to one. Completion of the checkmate is left as an exercise.

Token 3817:
and knight against king—is won in this sense; White presents Black with an inﬁnite random sequence of choices, for one of which Black will guess incorrectly and reveal his position, leading to checkmate.

Token 3818:
The KBBK endgame, on the other hand, is won with probability 1−/epsilon1.

Token 3819:
White can force a win only by leaving one of his bishops unprotected for one move.

Token 3820:
IfBlack happens to be in the right place and captures the bishop (a move that would lose if thebishops are protected), the game is drawn.

Token 3821:
White can choose to make the risky move at somerandomly chosen point in the middle of a very long sequence, thus reducing /epsilon1to an arbitrarily small constant, but cannot reduce /epsilon1to zero.

Token 3822:
It is quite rare that a guaranteed or probabilistic checkmate can be found within any reasonable depth, except in the endgame.

Token 3823:
Sometimes a checkmate strategy works for some of the board states in the current belief state but not others.

Token 3824:
Trying such a strategy may succeed, leading to an accidental checkmate —accidental in the sense that White could not know that ACCIDENTAL CHECKMATE it would be checkmate—if Black’s pieces happen to be in the right places.

Token 3825:
(Most checkmates in games between humans are of this accidental nature.)

Token 3826:
This idea leads naturally to the question of how likely it is that a given strategy will win, which leads in turn to the question ofhow likely it is that each board state in the current belief state is the true board state.

Token 3827:
Section 5.6.

Token 3828:
Partially Observable Games 183 One’s ﬁrst inclination might be to propose that all board states in the current belief state are equally likely—but this can’t be right.

Token 3829:
Consider, for example, White’s belief state afterBlack’s ﬁrst move of the game.

Token 3830:
By deﬁnition (assuming that Black plays optimally), Blackmust have played an optimal move, so all board states resulting from suboptimal moves oughtto be assigned zero probability.

Token 3831:
This argument is not quite right either, because each player’s goal is not just to move pieces to the right squares but also to minimize the information that the opponent has about their location.

Token 3832:
Playing any predictable “optimal” strategy provides the opponent with information.

Token 3833:
Hence, optimal play in partially observable games requiresa willingness to play somewhat randomly .

Token 3834:
(This is why restaurant hygiene inspectors do random inspection visits.)

Token 3835:
This means occasionally selecting moves that may seem “intrinsi- cally” weak—but they gain strength from their very unpredictability, because the opponent isunlikely to have prepared any defense against them.

Token 3836:
From these considerations, it seems that the probabilities associated with the board states in the current belief state can only be calculated given an optimal randomized strat- egy; in turn, computing that strategy seems to require knowing the probabilities of the var- ious states the board might be in.

Token 3837:
This conundrum can be resolved by adopting the game- theoretic notion of an equilibrium solution, which we pursue further in Chapter 17.

Token 3838:
An equilibrium speciﬁes an optimal randomized strategy for each player.

Token 3839:
Computing equilib- ria is prohibitively expensive, however, even for small games, and is out of the question forKriegspiel.

Token 3840:
At present, the design of effective algorithms for general Kriegspiel play is anopen research topic.

Token 3841:
Most systems perform bounded-depth lookahead in their own belief-state space, ignoring the opponent’s belief state.

Token 3842:
Evaluation functions resemble those for theobservable game but include a component for the size of the belief state—smaller is better!

Token 3843:
5.6.2 Card games Card games provide many examples of stochastic partial observability, where the missing information is generated randomly.

Token 3844:
For example, in many games, cards are dealt randomly atthe beginning of the game, with each player receiving a hand that is not visible to the otherplayers.

Token 3845:
Such games include bridge, whist, hearts, and some forms of poker.

Token 3846:
At ﬁrst sight, it might seem that these card games are just like dice games: the cards are dealt randomly and determine the moves available to each player, but all the “dice” are rolledat the beginning!

Token 3847:
Even though this analogy turns out to be incorrect, it suggests an effectivealgorithm: consider all possible deals of the invisible cards; solve each one as if it were afully observable game; and then choose the move that has the best outcome averaged over allthe deals.

Token 3848:
Suppose that each deal soccurs with probability P(s); then the move we want is argmax a/summationdisplay sP(s)MINIMAX (RESULT (s,a)).

Token 3849:
(5.1) Here, we run exact M INIMAX if computationally feasible; otherwise, we run H-M INIMAX .

Token 3850:
Now, in most card games, the number of possible deals is rather large.

Token 3851:
For example, in bridge play, each player sees just two of the four hands; there are two unseen hands of 13cards each, so the number of deals is/parenleftbig 26 13/parenrightbig =1 0,400,600.

Token 3852:
Solving even one deal is quite difﬁcult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo

Token 3853:
184 Chapter 5.

Token 3854:
Adversarial Search approximation: instead of adding up allthe deals, we take a random sample ofNdeals, where the probability of deal sappearing in the sample is proportional to P(s): argmax a1 NN/summationdisplay i=1MINIMAX (RESULT (si,a)).

Token 3855:
(5.2) (Notice that P(s)does not appear explicitly in the summation, because the samples are al- ready drawn according to P(s).)

Token 3856:
AsNgrows large, the sum over the random sample tends to the exact value, but even for fairly small N—say, 100 to 1,000—the method gives a good approximation.

Token 3857:
It can also be applied to deterministic games such as Kriegspiel, given somereasonable estimate of P(s).

Token 3858:
For games like whist and hearts, where there is no bidding or betting phase before play commences, each deal will be equally likely and so the values of P(s)are all equal.

Token 3859:
For bridge, play is preceded by a bidding phase in which each team indicates how many tricks it expects to win.

Token 3860:
Since players bid based on the cards they hold, the other players learn moreabout the probability of each deal.

Token 3861:
Taking this into account in deciding how to play the handis tricky, for the reasons mentioned in our description of Kriegspiel: players may bid in sucha way as to minimize the information conveyed to their opponents.

Token 3862:
Even so, the approach isquite effective for bridge, as we show in Section 5.7.

Token 3863:
The strategy described in Equations 5.1 and 5.2 is sometimes called averaging over clairvoyance because it assumes that the game will become observable to both players im- mediately after the ﬁrst move.

Token 3864:
Despite its intuitive appeal, the strategy can lead one astray.Consider the following story: Day 1: Road Aleads to a heap of gold; Road Bleads to a fork.

Token 3865:
Take the left fork and you’ll ﬁnd a bigger heap of gold, but take the right fork and you’ll be run over by a bus.

Token 3866:
Day 2: Road Aleads to a heap of gold; Road Bleads to a fork.

Token 3867:
Take the right fork and you’ll ﬁnd a bigger heap of gold, but take the left fork and you’ll be run over by a bus.Day 3: Road Aleads to a heap of gold; Road Bleads to a fork.

Token 3868:
One branch of the fork leads to a bigger heap of gold, but take the wrong fork and you’ll be hit by a bus.

Token 3869:
Unfortunately you don’t know which fork is which.

Token 3870:
Averaging over clairvoyance leads to the following reasoning: on Day 1, Bis the right choice; on Day 2, Bis the right choice; on Day 3, the situation is the same as either Day 1 or Day 2, soBmust still be the right choice.

Token 3871:
Now we can see how averaging over clairvoyance fails: it does not consider the belief state that the agent will be in after acting.

Token 3872:
A belief state of total ignorance is not desirable, es- pecially when one possibility is certain death.

Token 3873:
Because it assumes that every future state willautomatically be one of perfect knowledge, the approach never selects actions that gather in- formation (like the ﬁrst move in Figure 5.13); nor will it choose actions that hide information from the opponent or provide information to a partner because it assumes that they alreadyknow the information; and it will never bluff in poker, 4because it assumes the opponent can BLUFF see its cards.

Token 3874:
In Chapter 17, we show how to construct algorithms that do all these things by virtue of solving the true partially observable decision problem.

Token 3875:
4Blufﬁng—betting as if one’s hand is good, even when it’s not—is a core part of poker strategy.

Token 3876:
Section 5.7.

Token 3877:
State-of-the-Art Game Programs 185 5.7 S TATE -OF-THE-ARTGAME PROGRAMS In 1965, the Russian mathematician Alexander Kronrod called chess “the Drosophila of ar- tiﬁcial intelligence.” John McCarthy disagrees: whereas geneticists use fruit ﬂies to makediscoveries that apply to biology more broadly, AI has used chess to do the equivalent ofbreeding very fast fruit ﬂies.

Token 3878:
Perhaps a better analogy is that chess is to AI as Grand Prixmotor racing is to the car industry: state-of-the-art game programs are blindingly fast, highlyoptimized machines that incorporate the latest engineering advances, but they aren’t muchuse for doing the shopping or driving off-road.

Token 3879:
Nonetheless, racing and game-playing gen- erate excitement and a steady stream of innovations that have been adopted by the wider community.

Token 3880:
In this section we look at what it takes to come out on top in various games.

Token 3881:
Chess :I B M ’ sD EEPBLUE chess program, now retired, is well known for defeating world CHESS champion Garry Kasparov in a widely publicized exhibition match.

Token 3882:
Deep Blue ran on a par- allel computer with 30 IBM RS/6000 processors doing alpha–beta search.

Token 3883:
The unique partwas a conﬁguration of 480 custom VLSI chess processors that performed move generation and move ordering for the last few levels of the tree, and evaluated the leaf nodes.

Token 3884:
Deep Blue searched up to 30 billion positions per move, reaching depth 14 routinely.

Token 3885:
The key to itssuccess seems to have been its ability to generate singular extensions beyond the depth limitfor sufﬁciently interesting lines of forcing/forced moves.

Token 3886:
In some cases the search reached adepth of 40 plies.

Token 3887:
The evaluation function had over 8000 features, many of them describinghighly speciﬁc patterns of pieces.

Token 3888:
An “opening book” of about 4000 positions was used, aswell as a database of 700,000 grandmaster games from which consensus recommendationscould be extracted.

Token 3889:
The system also used a large endgame database of solved positions con-taining all positions with ﬁve pieces and many with six pieces.

Token 3890:
This database had the effectof substantially extending the effective search depth, allowing Deep Blue to play perfectly insome cases even when it was many moves away from checkmate.

Token 3891:
The success of D EEPBLUE reinforced the widely held belief that progress in computer game-playing has come primarily from ever-more-powerful hardware—a view encouraged by IBM.

Token 3892:
But algorithmic improvements have allowed programs running on standard PCsto win World Computer Chess Championships.

Token 3893:
A variety of pruning heuristics are used toreduce the effective branching factor to less than 3 (compared with the actual branching factorof about 35).

Token 3894:
The most important of these is the null move heuristic, which generates a good NULL MOVE lower bound on the value of a position, using a shallow search in which the opponent gets to move twice at the beginning.

Token 3895:
This lower bound often allows alpha–beta pruning withoutthe expense of a full-depth search.

Token 3896:
Also important is futility pruning , which helps decide in FUTILITYPRUNING advance which moves will cause a beta cutoff in the successor nodes.

Token 3897:
HYDRA can be seen as the successor to D EEPBLUE.HYDRA runs on a 64-processor cluster with 1 gigabyte per processor and with custom hardware in the form of FPGA (Field Programmable Gate Array) chips.

Token 3898:
H YDRA reaches 200 million evaluations per second, about the same as Deep Blue, but H YDRA reaches 18 plies deep rather than just 14 because of aggressive use of the null move heuristic and forward pruning.

Token 3899:
186 Chapter 5.

Token 3900:
Adversarial Search RYBKA , winner of the 2008 and 2009 World Computer Chess Championships, is con- sidered the strongest current computer player.

Token 3901:
It uses an off-the-shelf 8-core 3.2 GHz IntelXeon processor, but little is known about the design of the program.

Token 3902:
R YBKA ’s main ad- vantage appears to be its evaluation function, which has been tuned by its main developer,International Master Vasik Rajlich, and at least three other grandmasters.

Token 3903:
The most recent matches suggest that the top computer chess programs have pulled ahead of all human contenders.

Token 3904:
(See the historical notes for details.)

Token 3905:
Checkers : Jonathan Schaeffer and colleagues developed C HINOOK , which runs on regular CHECKERS PCs and uses alpha–beta search.

Token 3906:
Chinook defeated the long-running human champion in an abbreviated match in 1990, and since 2007 C HINOOK has been able to play perfectly by using alpha–beta search combined with a database of 39 trillion endgame positions.

Token 3907:
Othello , also called Reversi, is probably more popular as a computer game than as a board OTHELLO game.

Token 3908:
It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation expertise had to be developed from scratch.

Token 3909:
In 1997, the L OGISTELLO program (Buro, 2002) defeated the human world champion, Takeshi Murakami, by six games to none.

Token 3910:
It is generallyacknowledged that humans are no match for computers at Othello.

Token 3911:
Backgammon : Section 5.5 explained why the inclusion of uncertainty from dice rolls makes BACKGAMMON deep search an expensive luxury.

Token 3912:
Most work on backgammon has gone into improving the evaluation function.

Token 3913:
Gerry Tesauro (1992) combined reinforcement learning with neuralnetworks to develop a remarkably accurate evaluator that is used with a search to depth 2or 3.

Token 3914:
After playing more than a million training games against itself, Tesauro’s program, TD-G AMMON , is competitive with top human players.

Token 3915:
The program’s opinions on the open- ing moves of the game have in some cases radically altered the received wisdom.

Token 3916:
Gois the most popular board game in Asia.

Token 3917:
Because the board is 19×19and moves are GO allowed into (almost) every empty square, the branching factor starts at 361, which is too daunting for regular alpha–beta search methods.

Token 3918:
In addition, it is difﬁcult to write an eval- uation function because control of territory is often very unpredictable until the endgame.

Token 3919:
Therefore the top programs, such as M OGO, avoid alpha–beta search and instead use Monte Carlo rollouts.

Token 3920:
The trick is to decide what moves to make in the course of the rollout. There isno aggressive pruning; all moves are possible.

Token 3921:
The UCT (upper conﬁdence bounds on trees)method works by making random moves in the ﬁrst few iterations, and over time guiding the sampling process to prefer moves that have led to wins in previous samples.

Token 3922:
Some tricks are added, including knowledge-based rules that suggest particular moves whenever a given pattern is detected and limited local search to decide tactical questions.

Token 3923:
Some programs also include special techniques from combinatorial game theory to analyze endgames.

Token 3924:
These COMBINATORIAL GAME THEORY techniques decompose a position into sub-positions that can be analyzed separately and then combined (Berlekamp and Wolfe, 1994; M¨ uller, 2003).

Token 3925:
The optimal solutions obtained in this way have surprised many professional Go players, who thought they had been playingoptimally all along.

Token 3926:
Current Go programs play at the master level on a reduced 9×9board, but are still at advanced amateur level on a full board.

Token 3927:
Bridge is a card game of imperfect information: a player’s cards are hidden from the other BRIDGE players.

Token 3928:
Bridge is also a multiplayer game with four players instead of two, although the

Token 3929:
Section 5.8. Alternative Approaches 187 players are paired into two teams.

Token 3930:
As in Section 5.6, optimal play in partially observable games like bridge can include elements of information gathering, communication, and carefulweighing of probabilities.

Token 3931:
Many of these techniques are used in the Bridge Baron program(Smith et al. , 1998), which won the 1997 computer bridge championship.

Token 3932:
While it does not play optimally, Bridge Baron is one of the few successful game-playing systems to use complex, hierarchical plans (see Chapter 11) involving high-level ideas, such as ﬁnessing and squeezing , that are familiar to bridge players.

Token 3933:
The GIB program (Ginsberg, 1999) won the 2000 computer bridge championship quite decisively using the Monte Carlo method.

Token 3934:
Since then, other winning programs have followed GIB’s lead.

Token 3935:
GIB’s major innovation is using explanation-based generalization to compute EXPLANATION- BASED GENERALIZATIONand cache general rules for optimal play in various standard classes of situations rather than evaluating each situation individually.

Token 3936:
For example, in a situation where one player has thecards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7×6=4 2 ways that the ﬁrst player can lead from that suit and the second player can follow.

Token 3937:
But GIBtreats these situations as just two: the ﬁrst player can lead either a high card or a low card;the exact cards played don’t matter.

Token 3938:
With this optimization (and a few others), GIB can solvea 52-card, fully observable deal exactly in about a second.

Token 3939:
GIB’s tactical accuracy makes up for its inability to reason about information.

Token 3940:
It ﬁnished 12th in a ﬁeld of 35 in the par contest (involving just play of the hand, not bidding) at the 1998 human world championship, farexceeding the expectations of many human experts.

Token 3941:
There are several reasons why GIB plays at expert level with Monte Carlo simulation, whereas Kriegspiel programs do not.

Token 3942:
First, GIB’s evaluation of the fully observable version of the game is exact, searching the full game tree, while Kriegspiel programs rely on inexact heuristics.

Token 3943:
But far more important is the fact that in bridge, most of the uncertainty in thepartially observable information comes from the randomness of the deal, not from the adver-sarial play of the opponent.

Token 3944:
Monte Carlo simulation handles randomness well, but does notalways handle strategy well, especially when the strategy involves the value of information.

Token 3945:
Scrabble : Most people think the hard part about Scrabble is coming up with good words, but SCRABBLE given the ofﬁcial dictionary, it turns out to be rather easy to program a move generator to ﬁnd the highest-scoring move (Gordon, 1994).

Token 3946:
That doesn’t mean the game is solved, however:merely taking the top-scoring move each turn results in a good but not expert player.

Token 3947:
Theproblem is that Scrabble is both partially observable and stochastic: you don’t know whatletters the other player has or what letters you will draw next.

Token 3948:
So playing Scrabble wellcombines the difﬁculties of backgammon and bridge.

Token 3949:
Nevertheless, in 2006, the Q UACKLE program defeated the former world champion, David Boys, 3–2.

Token 3950:
5.8 A LTERNATIVE APPROACHES Because calculating optimal decisions in games is intractable in most cases, all algorithmsmust make some assumptions and approximations.

Token 3951:
The standard approach, based on mini-max, evaluation functions, and alpha–beta, is just one way to do this. Probably because it has

Token 3952:
188 Chapter 5.

Token 3953:
Adversarial Search MAX 99 1000 1000 1000 100 101 102 100100 99 MIN Figure 5.14 A two-ply game tree for which heuristic minimax may make an error.

Token 3954:
been worked on for so long, the standard approach dominates other methods in tournament play.

Token 3955:
Some believe that this has caused game playing to become divorced from the main-stream of AI research: the standard approach no longer provides much room for new insightinto general questions of decision making.

Token 3956:
In this section, we look at the alternatives. First, let us consider heuristic minimax.

Token 3957:
It selects an optimal move in a given search tree provided that the leaf node evaluations are exactly correct .

Token 3958:
In reality, evaluations are usually crude estimates of the value of a position and can be considered to have large errorsassociated with them.

Token 3959:
Figure 5.14 shows a two-ply game tree for which minimax suggeststaking the right-hand branch because 100>99.

Token 3960:
That is the correct move if the evaluations are all correct. But of course the evaluation function is only approximate.

Token 3961:
Suppose thatthe evaluation of each node has an error that is independent of other nodes and is randomlydistributed with mean zero and standard deviation of σ.

Token 3962:
Then when σ=5, the left-hand branch is actually better 71% of the time, and 58% of the time when σ=2.

Token 3963:
The intuition behind this is that the right-hand branch has four nodes that are close to 99; if an error in the evaluation of any one of the four makes the right-hand branch slip below 99, then the left-hand branch is better.

Token 3964:
In reality, circumstances are actually worse than this because the error in the evaluation function is notindependent.

Token 3965:
If we get one node wrong, the chances are high that nearby nodes in the tree will also be wrong.

Token 3966:
The fact that the node labeled 99 has siblings labeled 1000suggests that in fact it might have a higher true value.

Token 3967:
We can use an evaluation functionthat returns a probability distribution over possible values, but it is difﬁcult to combine thesedistributions properly, because we won’t have a good model of the very strong dependenciesthat exist between the values of sibling nodes Next, we consider the search algorithm that generates the tree.

Token 3968:
The aim of an algorithm designer is to specify a computation that runs quickly and yields a good move.

Token 3969:
The alpha–betaalgorithm is designed not just to select a good move but also to calculate bounds on the valuesof all the legal moves.

Token 3970:
To see why this extra information is unnecessary, consider a position in which there is only one legal move.

Token 3971:
Alpha–beta search still will generate and evaluate a large search tree, telling us that the only move is the best move and assigning it a value.

Token 3972:
Butsince we have to make the move anyway, knowing the move’s value is useless.

Token 3973:
Similarly, ifthere is one obviously good move and several moves that are legal but lead to a quick loss, we

Token 3974:
Section 5.9. Summary 189 would not want alpha–beta to waste time determining a precise value for the lone good move.

Token 3975:
Better to just make the move quickly and save the time for later. This leads to the idea of theutility of a node expansion .

Token 3976:
A good search algorithm should select node expansions of high utility—that is, ones that are likely to lead to the discovery of a signiﬁcantly better move.

Token 3977:
Ifthere are no node expansions whose utility is higher than their cost (in terms of time), then the algorithm should stop searching and make a move.

Token 3978:
Notice that this works not only for clear-favorite situations but also for the case of symmetrical moves, for which no amount of search will show that one move is better than another.

Token 3979:
This kind of reasoning about what computations to do is called metareasoning (rea- METAREASONING soning about reasoning).

Token 3980:
It applies not just to game playing but to any kind of reasoning at all.

Token 3981:
All computations are done in the service of trying to reach better decisions, all havecosts, and all have some likelihood of resulting in a certain improvement in decision quality.Alpha–beta incorporates the simplest kind of metareasoning, namely, a theorem to the effectthat certain branches of the tree can be ignored without loss.

Token 3982:
It is possible to do much better.In Chapter 16, we see how these ideas can be made precise and implementable.

Token 3983:
Finally, let us reexamine the nature of search itself.

Token 3984:
Algorithms for heuristic search and for game playing generate sequences of concrete states, starting from the initial state and then applying an evaluation function.

Token 3985:
Clearly, this is not how humans play games.

Token 3986:
In chess, one often has a particular goal in mind—for example, trapping the opponent’s queen—and can use this goal to selectively generate plausible plans for achieving it.

Token 3987:
This kind of goal-directed reasoning or planning sometimes eliminates combinatorial search altogether.

Token 3988:
David Wilkins’ (1980) P ARADISE is the only program to have used goal-directed reasoning successfully in chess: it was capable of solving some chess problems requiring an 18-move combination.

Token 3989:
As yet there is no good understanding of how to combine the two kinds of algorithms into a robust and efﬁcient system, although Bridge Baron might be a step in theright direction.

Token 3990:
A fully integrated system would be a signiﬁcant achievement not just forgame-playing research but also for AI research in general, because it would be a good basisfor a general intelligent agent.

Token 3991:
5.9 S UMMARY We have looked at a variety of games to understand what optimal play means and to under-stand how to play well in practice.

Token 3992:
The most important ideas are as follows: •A game can be deﬁned by the initial state (how the board is set up), the legal actions in each state, the result of each action, a terminal test (which says when the game is over), and a utility function that applies to terminal states.

Token 3993:
•In two-player zero-sum games with perfect information ,t h e minimax algorithm can select optimal moves by a depth-ﬁrst enumeration of the game tree.

Token 3994:
•The alpha–beta search algorithm computes the same optimal move as minimax, but achieves much greater efﬁciency by eliminating subtrees that are provably irrelevant.

Token 3995:
•Usually, it is not feasible to consider the whole game tree (even with alpha–beta), so we

Token 3996:
190 Chapter 5.

Token 3997:
Adversarial Search need to cut the search off at some point and apply a heuristic evaluation function that estimates the utility of a state.

Token 3998:
•Many game programs precompute tables of best moves in the opening and endgame so that they can look up a move rather than search.

Token 3999:
•Games of chance can be handled by an extension to the minimax algorithm that eval- uates a chance node by taking the average utility of all its children, weighted by the probability of each child.

Token 4000:
•Optimal play in games of imperfect information , such as Kriegspiel and bridge, re- quires reasoning about the current and future belief states of each player.

Token 4001:
A simple approximation can be obtained by averaging the value of an action over each possibleconﬁguration of missing information.

Token 4002:
•Programs have bested even champion human players at games such as chess, checkers, and Othello.

Token 4003:
Humans retain the edge in several games of imperfect information, suchas poker, bridge, and Kriegspiel, and in games with very large branching factors andlittle good heuristic knowledge, such as Go.

Token 4004:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The early history of mechanical game playing was marred by numerous frauds.

Token 4005:
The mostnotorious of these was Baron Wolfgang von Kempelen’s (1734–1804) “The Turk,” a supposedchess-playing automaton that defeated Napoleon before being exposed as a magician’s trickcabinet housing a human chess expert (see Levitt, 2000).

Token 4006:
It played from 1769 to 1854.

Token 4007:
In1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributedthe ﬁrst serious discussion of the feasibility of computer chess and checkers (Morrison andMorrison, 1961).

Token 4008:
He did not understand the exponential complexity of search trees, claiming “the combinations involved in the Analytical Engine enormously surpassed any required, even by the game of chess.” Babbage also designed, but did not build, a special-purposemachine for playing tic-tac-toe.

Token 4009:
The ﬁrst true game-playing machine was built around 1890by the Spanish engineer Leonardo Torres y Quevedo.

Token 4010:
It specialized in the “KRK” (king androok vs. king) chess endgame, guaranteeing a win with king and rook from any position.

Token 4011:
The minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of modern set theory.

Token 4012:
The paper unfortunately contained several errors and did not describe min-imax correctly.

Token 4013:
On the other hand, it did lay out the ideas of retrograde analysis and proposed(but did not prove) what became known as Zermelo’s theorem: that chess is determined—White can force a win or Black can or it is a draw; we just don’t know which.

Token 4014:
Zermelo saysthat should we eventually know, “Chess would of course lose the character of a game at all.” A solid foundation for game theory was developed in the seminal work Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944), which included an analysis showing that some games require strategies that are randomized (or otherwise unpredictable).

Token 4015:
See Chapter 17 for more information.

Token 4016:
Bibliographical and Historical Notes 191 John McCarthy conceived the idea of alpha–beta search in 1956, although he did not publish it.

Token 4017:
The NSS chess program (Newell et al. , 1958) used a simpliﬁed version of alpha– beta; it was the ﬁrst chess program to do so.

Token 4018:
Alpha–beta pruning was described by Hart andEdwards (1961) and Hart et al. (1972).

Token 4019:
Alpha–beta was used by the “Kotok–McCarthy” chess program written by a student of John McCarthy (Kotok, 1962).

Token 4020:
Knuth and Moore (1975) proved the correctness of alpha–beta and analysed its time complexity.

Token 4021:
Pearl (1982b) shows alpha–beta to be asymptotically optimal among all ﬁxed-depth game-tree search algorithms.

Token 4022:
Several attempts have been made to overcome the problems with the “standard ap- proach” that were outlined in Section 5.8.

Token 4023:
The ﬁrst nonexhaustive heuristic search algorithmwith some theoretical grounding was probably B ∗(Berliner, 1979), which attempts to main- tain interval bounds on the possible value of a node in the game tree rather than giving ita single point-valued estimate.

Token 4024:
Leaf nodes are selected for expansion in an attempt to re-ﬁne the top-level bounds until one move is “clearly best.” Palay (1985) extends the B ∗idea using probability distributions on values in place of intervals.

Token 4025:
David McAllester’s (1988)conspiracy number search expands leaf nodes that, by changing their values, could causethe program to prefer a new move at the root.

Token 4026:
MGSS ∗(Russell and Wefald, 1989) uses the decision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in terms of the expected improvement in decision quality at the root.

Token 4027:
It outplayed an alpha– beta algorithm at Othello despite searching an order of magnitude fewer nodes.

Token 4028:
The MGSS∗ approach is, in principle, applicable to the control of any form of deliberation.

Token 4029:
Alpha–beta search is in many ways the two-player analog of depth-ﬁrst branch-and- bound, which is dominated by A∗in the single-agent case.

Token 4030:
The SSS∗algorithm (Stockman, 1979) can be viewed as a two-player A∗and never expands more nodes than alpha–beta to reach the same decision.

Token 4031:
The memory requirements and computational overhead of the queuemake SSS ∗in its original form impractical, but a linear-space version has been developed from the RBFS algorithm (Korf and Chickering, 1996).

Token 4032:
Plaat et al.

Token 4033:
(1996) developed a new view of SSS∗as a combination of alpha–beta and transposition tables, showing how to over- come the drawbacks of the original algorithm and developing a new variant called MTD( f) that has been adopted by a number of top programs.

Token 4034:
D. F. Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax ap- plied to approximate evaluations.

Token 4035:
They showed that under certain assumptions about the dis-tribution of leaf values in the tree, minimaxing can yield values at the root that are actually less reliable than the direct use of the evaluation function itself.

Token 4036:
Pearl’s book Heuristics (1984) partially explains this apparent paradox and analyzes many game-playing algorithms.

Token 4037:
Baumand Smith (1997) propose a probability-based replacement for minimax, showing that it re-sults in better choices in certain games.

Token 4038:
The expectiminimax algorithm was proposed byDonald Michie (1966).

Token 4039:
Bruce Ballard (1983) extended alpha–beta pruning to cover treeswith chance nodes and Hauk (2004) reexamines this work and provides empirical results.

Token 4040:
Koller and Pfeffer (1997) describe a system for completely solving partially observ- able games.

Token 4041:
The system is quite general, handling games whose optimal strategy requires randomized moves and games that are more complex than those handled by any previoussystem.

Token 4042:
Still, it can’t handle games as complex as poker, bridge, and Kriegspiel. Franket al.

Token 4043:
(1998) describe several variants of Monte Carlo search, including one where MIN has

Token 4044:
192 Chapter 5. Adversarial Search complete information but MAX does not.

Token 4045:
Among deterministic, partially observable games, Kriegspiel has received the most attention.

Token 4046:
Ferguson demonstrated hand-derived random-ized strategies for winning Kriegspiel with a bishop and knight (1992) or two bishops (1995)against a king.

Token 4047:
The ﬁrst Kriegspiel programs concentrated on ﬁnding endgame checkmatesand performed AND –ORsearch in belief-state space (Sakuta and Iida, 2002; Bolognesi and Ciancarini, 2003).

Token 4048:
Incremental belief-state algorithms enabled much more complex midgame checkmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efﬁcientstate estimation remains the primary obstacle to effective general play (Parker et al.

Token 4049:
, 2005).

Token 4050:
Chess was one of the ﬁrst tasks undertaken in AI, with early efforts by many of the pio- neers of computing, including Konrad Zuse in 1945, Norbert Wiener in his book Cybernetics (1948), and Alan Turing in 1950 (see Turing et al.

Token 4051:
, 1953).

Token 4052:
But it was Claude Shannon’s article Programming a Computer for Playing Chess (1950) that had the most complete set of ideas, describing a representation for board positions, an evaluation function, quiescencesearch, and some ideas for selective (nonexhaustive) game-tree search.

Token 4053:
Slater (1950) and thecommentators on his article also explored the possibilities for computer chess play.

Token 4054:
D. G. Prinz (1952) completed a program that solved chess endgame problems but did not play a full game.

Token 4055:
Stan Ulam and a group at the Los Alamos National Lab produced a program that played chess on a 6×6board with no bishops (Kister et al. , 1957).

Token 4056:
It could search 4 plies deep in about 12 minutes.

Token 4057:
Alex Bernstein wrote the ﬁrst documented programto play a full game of standard chess (Bernstein and Roberts, 1958).

Token 4058:
5 The ﬁrst computer chess match featured the Kotok–McCarthy program from MIT (Ko- tok, 1962) and the ITEP program written in the mid-1960s at Moscow’s Institute of Theo- retical and Experimental Physics (Adelson-Velsky et al.

Token 4059:
, 1970). This intercontinental match was played by telegraph. It ended with a 3–1 victory for the ITEP program in 1967.

Token 4060:
The ﬁrstchess program to compete successfully with humans was MIT’s M ACHACK-6 (Greenblatt et al. , 1967).

Token 4061:
Its Elo rating of approximately 1400 was well above the novice level of 1000.

Token 4062:
The Fredkin Prize, established in 1980, offered awards for progressive milestones in chess play.

Token 4063:
The $5,000 prize for the ﬁrst program to achieve a master rating went to B ELLE (Condon and Thompson, 1982), which achieved a rating of 2250.

Token 4064:
The $10,000 prize for theﬁrst program to achieve a USCF (United States Chess Federation) rating of 2500 (near thegrandmaster level) was awarded to D EEPTHOUGHT (Hsu et al.

Token 4065:
, 1990) in 1989. The grand prize, $100,000, went to D EEPBLUE (Campbell et al.

Token 4066:
, 2002; Hsu, 2004) for its landmark victory over world champion Garry Kasparov in a 1997 exhibition match.

Token 4067:
Kasparov wrote: The decisive game of the match was Game 2, which left a scar in my memory ...we saw something that went well beyond our wildest expectations of how well a computer would be able to foresee the long-term positional c onsequences of its decisions.

Token 4068:
The machine refused to move to a position that had a decisive short-term advantage—showing a veryhuman sense of danger.

Token 4069:
(Kasparov, 1997) Probably the most complete description of a modern chess program is provided by Ernst Heinz (2000), whose D ARKTHOUGHT program was the highest-ranked noncommercial PC program at the 1999 world championships.

Token 4070:
5A Russian program, BESM may have predated Bernstein’s program.

Token 4071:


Token 4072:
Bibliographical and Historical Notes 193 (a) (b) Figure 5.15 Pioneers in computer chess: (a) Herbert Simon and Allen Newell, developers of the NSS program (1958); (b) John McCarthy and the Kotok–McCarthy program on an IBM 7090 (1967).

Token 4073:
In recent years, chess programs are pulling ahead of even the world’s best humans.

Token 4074:
In 2004–2005 H YDRA defeated grand master Evgeny Vladimirov 3.5–0.5, world champion Ruslan Ponomariov 2–0, and seventh-ranked Michael Adams 5.5–0.5.

Token 4075:
In 2006, D EEPFRITZ beat world champion Vladimir Kramnik 4–2, and in 2007 R YBKA defeated several grand masters in games in which it gave odds (such as a pawn) to the human players.

Token 4076:
As of 2009,the highest Elo rating ever recorded was Kasparov’s 2851.

Token 4077:
H YDRA (Donninger and Lorenz, 2004) is rated somewhere between 2850 and 3000, based mostly on its trouncing of MichaelAdams.

Token 4078:
The R YBKA program is rated between 2900 and 3100, but this is based on a small number of games and is not considered reliable.

Token 4079:
Ross (2004) shows how human players havelearned to exploit some of the weaknesses of the computer programs.

Token 4080:
Checkers was the ﬁrst of the classic games fully played by a computer. Christopher Strachey (1952) wrote the ﬁrst working program for checkers.

Token 4081:
Beginning in 1952, ArthurSamuel of IBM, working in his spare time, developed a checkers program that learned itsown evaluation function by playing itself thousands of times (Samuel, 1959, 1967).

Token 4082:
Wedescribe this idea in more detail in Chapter 21.

Token 4083:
Samuel’s program began as a novice butafter only a few days’ self-play had improved itself beyond Samuel’s own level.

Token 4084:
In 1962 itdefeated Robert Nealy, a champion at “blind checkers,” through an error on his part.

Token 4085:
Whenone considers that Samuel’s computing equipment (an IBM 704) had 10,000 words of mainmemory, magnetic tape for long-term storage, and a .000001 GHz processor, the win remainsa great accomplishment.

Token 4086:
The challenge started by Samuel was taken up by Jonathan Schaeffer of the University of Alberta.

Token 4087:
His C HINOOK program came in second in the 1990 U.S. Open and earned the right to challenge for the world championship.

Token 4088:
It then ran up against a problem, in the formof Marion Tinsley.

Token 4089:
Dr. Tinsley had been world champion for over 40 years, losing onlythree games in all that time.

Token 4090:
In the ﬁrst match against C HINOOK , Tinsley suffered his fourth

Token 4091:
194 Chapter 5. Adversarial Search and ﬁfth losses, but won the match 20.5–18.5.

Token 4092:
A rematch at the 1994 world championship ended prematurely when Tinsley had to withdraw for health reasons.

Token 4093:
C HINOOK became the ofﬁcial world champion. Schaeffer kept on building on his database of endgames, and in2007 “solved” checkers (Schaeffer et al.

Token 4094:
, 2007; Schaeffer, 2008). This had been predicted by Richard Bellman (1965).

Token 4095:
In the paper that introduced the dynamic programming approach to retrograde analysis, he wrote, “In checkers, the number of possible moves in any given situation is so small that we can conﬁdently expect a complete digital computer solution tothe problem of optimal play in this game.” Bellman did not, however, fully appreciate thesize of the checkers game tree.

Token 4096:
There are about 500 quadrillion positions.

Token 4097:
After 18 yearsof computation on a cluster of 50 or more machines, Jonathan Schaeffer’s team completedan endgame table for all checkers positions with 10 or fewer pieces: over 39 trillion entries.From there, they were able to do forward alpha–beta search to derive a policy that provesthat checkers is in fact a draw with best play by both sides.

Token 4098:
Note that this is an applicationof bidirectional search (Section 3.4.6).

Token 4099:
Building an endgame table for all of checkers wouldbe impractical: it would require a billion gigabytes of storage.

Token 4100:
Searching without any tablewould also be impractical: the search tree has about 8 47positions, and would take thousands of years to search with today’s technology.

Token 4101:
Only a combination of clever search, endgame data, and a drop in the price of processors and memory could solve checkers.

Token 4102:
Thus, checkers joins Qubic (Patashnik, 1980), Connect Four (Allis, 1988), and Nine-Men’s Morris (Gasser,1998) as games that have been solved by computer analysis.

Token 4103:
Backgammon , a game of chance, was analyzed mathematically by Gerolamo Cardano (1663), but only taken up for computer play in the late 1970s, ﬁrst with the BKG pro- gram (Berliner, 1980b); it used a complex, manually constructed evaluation function and searched only to depth 1.

Token 4104:
It was the ﬁrst program to defeat a human world champion at a ma-jor classic game (Berliner, 1980a).

Token 4105:
Berliner readily acknowledged that BKG was very luckywith the dice. Gerry Tesauro’s (1995) TD-G AMMON played consistently at world champion level.

Token 4106:
The BGB LITZ program was the winner of the 2008 Computer Olympiad. Gois a deterministic game, but the large branching factor makes it challeging.

Token 4107:
The key issues and early literature in computer Go are summarized by Bouzy and Cazenave (2001) and M¨uller (2002).

Token 4108:
Up to 1997 there were no competent Go programs.

Token 4109:
Now the best programs play most of their moves at the master level; the only problem is that over the course of a game they usually make at least one serious blunder that allows a strong opponent to win.Whereas alpha–beta search reigns in most games, many recent Go programs have adoptedMonte Carlo methods based on the UCT (upper conﬁdence bounds on trees) scheme (Kocsisand Szepesvari, 2006).

Token 4110:
The strongest Go program as of 2009 is Gelly and Silver’s M OGO (Wang and Gelly, 2007; Gelly and Silver, 2008).

Token 4111:
In August 2008, M OGOscored a surprising win against top professional Myungwan Kim, albeit with M OGOreceiving a handicap of nine stones (about the equivalent of a queen handicap in chess).

Token 4112:
Kim estimated M OGO’s strength at 2–3 dan, the low end of advanced amateur.

Token 4113:
For this match, M OGOwas run on an 800-processor 15 teraﬂop supercomputer (1000 times Deep Blue).

Token 4114:
A few weeks later, MOGO, with only a ﬁve-stone handicap, won against a 6-dan professional.

Token 4115:
In the 9×9form of Go, M OGOis at approximately the 1-dan professional level.

Token 4116:
Rapid advances are likely as experimentation continues with new forms of Monte Carlo search. The Computer Go

Token 4117:
Exercises 195 Newsletter , published by the Computer Go Association, describes current developments. Bridge : Smith et al.

Token 4118:
(1998) report on how their planning-based program won the 1998 computer bridge championship, and (Ginsberg, 2001) describes how his GIB program, basedon Monte Carlo simulation, won the following computer championship and did surprisinglywell against human players and standard book problem sets.

Token 4119:
From 2001–2007, the computer bridge championship was won ﬁve times by J ACK and twice by W BRIDGE 5.

Token 4120:
Neither has had academic articles explaining their structure, but both are rumored to use the Monte Carlotechnique, which was ﬁrst proposed for bridge by Levy (1989).

Token 4121:
Scrabble : A good description of a top program, M AV EN , is given by its creator, Brian Sheppard (2002).

Token 4122:
Generating the highest-scoring move is described by Gordon (1994), andmodeling opponents is covered by Richards and Amir (2007). Soccer (Kitano et al.

Token 4123:
, 1997b; Visser et al. , 2008) and billiards (Lam and Greenspan, 2008; Archibald et al.

Token 4124:
, 2009) and other stochastic games with a continuous space of actions are beginning to attract attention in AI, both in simulation and with physical robot players.

Token 4125:
Computer game competitions occur annually, and papers appear in a variety of venues.

Token 4126:
The rather misleadingly named conference proceedings Heuristic Programming in Artiﬁcial Intelligence report on the Computer Olympiads, which include a wide variety of games.

Token 4127:
The General Game Competition (Love et al.

Token 4128:
, 2006) tests programs that must learn to play an un- known game given only a logical description of the rules of the game.

Token 4129:
There are also severaledited collections of important papers on game-playing research (Levy, 1988a, 1988b; Mars-land and Schaeffer, 1990).

Token 4130:
The Internationa l Computer Chess Associ ation (I CCA), f ounded in 1977, publishes the ICGA Journal (formerly the ICCA Journal ).

Token 4131:
Important papers have been published in the serial anthology Advances in Computer Chess , starting with Clarke (1977).

Token 4132:
Volume 134 of the journal Artiﬁcial Intelligence (2002) contains descriptions of state-of-the-art programs for chess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble,and other games.

Token 4133:
Since 1998, a biennial Computers and Games conference has been held.

Token 4134:
EXERCISES 5.1 Suppose you have an oracle, OM(s), that correctly predicts the opponent’s move in any state.

Token 4135:
Using this, formulate the deﬁnition of a game as a (single-agent) search problem.Describe an algorithm for ﬁnding the optimal move.

Token 4136:
5.2 Consider the problem of solving two 8-puzzles. a. Give a complete problem formulation in the style of Chapter 3. b.

Token 4137:
How large is the reachable state space? Give an exact numerical expression.

Token 4138:
c. Suppose we make the problem adversarial as follows: the two players take turns mov- ing; a coin is ﬂipped to determine the puzzle on which to make a move in that turn; andthe winner is the ﬁrst to solve one puzzle.

Token 4139:
Which algorithm can be used to choose amove in this setting? d. Give an informal proof that someone will eventually win if both play perfectly.

Token 4140:
196 Chapter 5. Adversarial Search (b)(a) a fe d c b bd cd ad ce cf cc ae af ac de df dd dd?? ? ?

Token 4141:
?PE Figure 5.16 (a) A map where the cost of every edge is 1.

Token 4142:
Initially the pursuer Pis at node band the evader Eis at node d. (b) A partial game tree for this map. Each node is labeled with the P,E positions.

Token 4143:
Pmoves ﬁrst. Branches marked “?” have yet to be explored. 5.3 Imagine that, in Exercise 3.3, one of the friends wants to avoid the other.

Token 4144:
The problem then becomes a two-player pursuit–evasion game. We assume now that the players take PURSUIT–EVASION turns moving.

Token 4145:
The game ends only when the players are on the same node; the terminal payoff to the pursuer is minus the total time taken.

Token 4146:
(The evader “wins” by never losing.) Anexample is shown in Figure 5.16. a. Copy the game tree and mark the values of the terminal nodes. b.

Token 4147:
Next to each internal node, write the strongest fact you can infer about its value (a number, one or more inequalities such as “ ≥14”, or a “?”).

Token 4148:
c. Beneath each question mark, write the name of the node reached by that branch.

Token 4149:
d. Explain how a bound on the value of the nodes in (c) can be derived from consideration of shortest-path lengths on the map, and derive such bounds for these nodes.

Token 4150:
Rememberthe cost to get to each leaf as well as the cost to solve it.

Token 4151:
e. Now suppose that the tree as given, with the leaf bounds from (d), is evaluated from left to right.

Token 4152:
Circle those “?” nodes that would notneed to be expanded further, given the bounds from part (d), and cross out those that need not be considered at all.

Token 4153:
f. Can you prove anything in general about who wins the game on a map that is a tree?

Token 4154:


Token 4155:
Exercises 197 5.4 Describe and implement state descriptions, move generators, terminal tests, utility func- tions, and evaluation functions for one or more of the following stochastic games: Monopoly,Scrabble, bridge play with a given contract, or Texas hold’em poker.

Token 4156:
5.5 Describe and implement a real-time ,multiplayer game-playing environment, where time is part of the environment state and players are given ﬁxed time allocations.

Token 4157:
5.6 Discuss how well the standard approach to game playing would apply to games such as tennis, pool, and croquet, which take place in a continuous physical state space.

Token 4158:
5.7 Prove the following assertion: For every game tree, the utility obtained by MAX using minimax decisions against a suboptimal MIN will be never be lower than the utility obtained playing against an optimal MIN.

Token 4159:
Can you come up with a game tree in which MAX can do still better using a suboptimal strategy against a suboptimal MIN?

Token 4160:
A B 14 3 2 Figure 5.17 The starting position of a simple game. Player Amoves ﬁrst.

Token 4161:
The two players take turns moving, and each player must move his token to an open adjacent space in either direction.

Token 4162:
If the opponent occupies an adj acent space, then a player may jump over the opponent to the next open space if any.

Token 4163:
(For example, if Ais on 3 and Bis on 2, then Amay move back to 1.) The game ends when one player reaches the opposite end of the board.

Token 4164:
If player Areaches space 4 ﬁrst, then the value of the game to Ais+1;i fp l a y e r Breaches space 1 ﬁrst, then the value of the game to Ais−1.

Token 4165:
5.8 Consider the two-player game described in Figure 5.17. a.

Token 4166:
Draw the complete game tree, using the following conventions: •Write each state as (sA,sB),w h e r e sAandsBdenote the token locations.

Token 4167:
•Put each terminal state in a square box and write its game value in a circle.

Token 4168:
•Putloop states (states that already appear on the path to the root) in double square boxes.

Token 4169:
Since their value is unclear, annotate each with a “?” in a circle. b. Now mark each node with its backed-up minimax value (also in a circle).

Token 4170:
Explain how you handled the “?” values and why.

Token 4171:
c. Explain why the standard minimax algorithm would fail on this game tree and brieﬂy sketch how you might ﬁx it, drawing on your answer to (b).

Token 4172:
Does your modiﬁed algo-rithm give optimal decisions for all games with loops? d. This 4-square game can be generalized to nsquares for any n>2.

Token 4173:
Prove that Awins ifnis even and loses if nis odd.

Token 4174:
5.9 This problem exercises the basic concepts of game playing, using tic-tac-toe (noughts and crosses) as an example.

Token 4175:
We deﬁne X nas the number of rows, columns, or diagonals

Token 4176:
198 Chapter 5. Adversarial Search with exactly nX’s and no O’s. Similarly, Onis the number of rows, columns, or diagonals with just nO’s.

Token 4177:
The utility function assigns +1to any position with X3=1and−1to any position with O3=1. All other terminal positions have utility 0.

Token 4178:
For nonterminal positions, we use a linear evaluation function deﬁned as Eval(s)=3X2(s)+X1(s)−(3O2(s)+O1(s)). a.

Token 4179:
Approximately how many possible games of tic-tac-toe are there? b.

Token 4180:
Show the whole game tree starting from an empty board down to depth 2 (i.e., one X and one Oon the board), taking symmetry into account.

Token 4181:
c. Mark on your tree the evaluations of all the positions at depth 2. d. Using the minimax algorithm, mark on your tree the backed-up values for the positions at depths 1 and 0, and use those values to choose the best starting move.

Token 4182:
e. Circle the nodes at depth 2 that would notbe evaluated if alpha–beta pruning were applied, assuming the nodes are generated in the optimal order for alpha–beta pruning.

Token 4183:
5.10 Consider the family of generalized tic-tac-toe games, deﬁned as follows.

Token 4184:
Each partic- ular game is speciﬁed by a set Sofsquares and a collection Wofwinning positions.

Token 4185:
Each winning position is a subset of S. For example, in standard tic-tac-toe, Sis a set of 9 squares andWis a collection of 8 subsets of W: the three rows, the three columns, and the two diag- onals.

Token 4186:
In other respects, the game is identical to standard tic-tac-toe.

Token 4187:
Starting from an empty board, players alternate placing their marks on an empty square.

Token 4188:
A player who marks every square in a winning position wins the game. It is a tie if all squares are marked and neither player has won.

Token 4189:
a.L e tN=|S|, the number of squares.

Token 4190:
Give an upper bound on the number of nodes in the complete game tree for generalized tic-tac-toe as a function of N. b.

Token 4191:
Give a lower bound on the size of the game tree for the worst case, where W={}.

Token 4192:
c. Propose a plausible evaluation function that can be used for any instance of generalized tic-tac-toe. The function may depend on SandW.

Token 4193:
d. Assume that it is possible to generate a new board and check whether it is a winning position in 100 Nmachine instructions and assume a 2 gigahertz processor.

Token 4194:
Ignore memory limitations. Using your estimate in (a), roughly how large a game tree can becompletely solved by alpha–beta in a second of CPU time?

Token 4195:
a minute? an hour? 5.11 Develop a general game-playing program, capable of playing a variety of games. a.

Token 4196:
Implement move generators and evaluation functions for one or more of the following games: Kalah, Othello, checkers, and chess.

Token 4197:
b. Construct a general alpha–beta game-playing agent.

Token 4198:
c. Compare the effect of increasing search depth, improving move ordering, and improv- ing the evaluation function.

Token 4199:
How close does your effective branching factor come to theideal case of perfect move ordering?

Token 4200:
d. Implement a selective search algorithm, such as B* (Berliner, 1979), conspiracy number search (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare itsperformance to A*.

Token 4201:
Exercises 199 n1 n2 nj Figure 5.18 Situation when considering whether to prune node nj.

Token 4202:
5.12 Describe how the minimax and alpha–beta algorithms change for two-player, non- zero-sum games in which each player has a distinct utility function and both utility functions are known to both players.

Token 4203:
If there are no constraints on the two terminal utilities, is it possiblefor any node to be pruned by alpha–beta?

Token 4204:
What if the player’s utility functions on any statediffer by at most a constant k, making the game almost cooperative?

Token 4205:
5.13 Develop a formal proof of correctness for alpha–beta pruning. To do this, consider the situation shown in Figure 5.18.

Token 4206:
The question is whether to prune node n j, which is a max- node and a descendant of node n1.

Token 4207:
The basic idea is to prune it if and only if the minimax value of n1can be shown to be independent of the value of nj.

Token 4208:
a. Mode n1takes on the minimum value among its children: n1=m i n ( n2,n21,...,n 2b2).

Token 4209:
Find a similar expression for n2and hence an expression for n1in terms of nj.

Token 4210:
b.L e tlibe the minimum (or maximum) value of the nodes to the leftof node niat depth i, whose minimax value is already known.

Token 4211:
Similarly, let ribe the minimum (or maximum) value of the unexplored nodes to the right of niat depth i. Rewrite your expression for n1in terms of the liandrivalues.

Token 4212:
c. Now reformulate the expression to show that in order to affect n1,njmust not exceed a certain bound derived from the livalues.

Token 4213:
d. Repeat the process for the case where njis a min-node.

Token 4214:
5.14 Prove that alpha–beta pruning takes time O(2m/2)with optimal move ordering, where mis the maximum depth of the game tree.

Token 4215:
5.15 Suppose you have a chess program that can evaluate 10 million nodes per second.

Token 4216:
Decide on a compact representation of a game state for storage in a transposition table.

Token 4217:
About how many entries can you ﬁt in a 2-gigabyte in-memory table? Will that be enough for the

Token 4218:
200 Chapter 5. Adversarial Search 0.5 0.5 0.5 0.5 2 2 12 0 2 -1 0 Figure 5.19 The complete game tree for a trivial game with chance nodes.

Token 4219:
three minutes of search allocated for one move? How many table lookups can you do in the time it would take to do one evaluation?

Token 4220:
Now suppose the transposition table is stored ondisk.

Token 4221:
About how many evaluations could you do in the time it takes to do one disk seek withstandard disk hardware?

Token 4222:
5.16 This question considers pruning in games with chance nodes. Figure 5.19 shows the complete game tree for a trivial game.

Token 4223:
Assume that the leaf nodes are to be evaluated in left-to-right order, and that before a leaf node is evaluated, we know nothing about its value—therange of possible values is −∞ to∞.

Token 4224:
a. Copy the ﬁgure, mark the value of all the internal nodes, and indicate the best move at the root with an arrow. b.

Token 4225:
Given the values of the ﬁrst six leaves, do we need to evaluate the seventh and eighth leaves?

Token 4226:
Given the values of the ﬁrst seven leaves, do we need to evaluate the eighthleaf? Explain your answers.

Token 4227:
c. Suppose the leaf node values are known to lie between –2 and 2 inclusive.

Token 4228:
After the ﬁrst two leaves are evaluated, what is the value range for the left-hand chance node?

Token 4229:
d. Circle all the leaves that need not be evaluated under the assumption in (c).

Token 4230:
5.17 Implement the expectiminimax algorithm and the *-alpha–beta algorithm, which is described by Ballard (1983), for pruning game trees with chance nodes.

Token 4231:
Try them on a game such as backgammon and measure the pruning effectiveness of *-alpha–beta.

Token 4232:
5.18 Prove that with a positive linear transformation of leaf values (i.e., transforming a valuextoax+bwhere a>0), the choice of move remains unchanged in a game tree, even when there are chance nodes.

Token 4233:
5.19 Consider the following procedure for choosing moves in games with chance nodes: •Generate some dice-roll sequences (say, 50) down to a suitable depth (say, 8).

Token 4234:
•With known dice rolls, the game tree becomes deterministic.

Token 4235:
For each dice-roll se- quence, solve the resulting deterministic game tree using alpha–beta.

Token 4236:
Exercises 201 •Use the results to estimate the value of each move and to choose the best. Will this procedure work well? Why (or why not)?

Token 4237:
5.20 In the following, a “max” tree consists only of max nodes, whereas an “expectimax” tree consists of a max node at the root with alternating layers of chance and max nodes.

Token 4238:
Atchance nodes, all outcome probabilities are nonzero. The goal is to ﬁnd the value of the root with a bounded-depth search.

Token 4239:
For each of (a)–(f), either give an example or explain why thisis impossible. a.

Token 4240:
Assuming that leaf values are ﬁnite but unbounded, is pruning (as in alpha–beta) ever possible in a max tree? b.

Token 4241:
Is pruning ever possible in an expectimax tree under the same conditions?

Token 4242:
c. If leaf values are all nonnegative, is pruning ever possible in a max tree? Give an example, or explain why not.

Token 4243:
d. If leaf values are all nonnegative, is pruning ever possible in an expectimax tree? Give an example, or explain why not.

Token 4244:
e. If leaf values are all in the range [0,1], is pruning ever possible in a max tree? Give an example, or explain why not.

Token 4245:
f. If leaf values are all in the range [0,1], is pruning ever possible in an expectimax tree?

Token 4246:
g. Consider the outcomes of a chance node in an expectimax tree.

Token 4247:
Which of the following evaluation orders is most likely to yield pruning opportunities?

Token 4248:
(i) Lowest probability ﬁrst (ii) Highest probability ﬁrst (iii) Doesn’t make any difference 5.21 Which of the following are true and which are false?

Token 4249:
Give brief explanations. a.

Token 4250:
In a fully observable, turn-taking, zero-sum game between two perfectly rational play- ers, it does not help the ﬁrst player to know what strategy the second player is using— that is, what move the second player will make, given the ﬁrst player’s move.

Token 4251:
b.

Token 4252:
In a partially observable, turn-taking, zero-sum game between two perfectly rational players, it does not help the ﬁrst player to know what move the second player willmake, given the ﬁrst player’s move.

Token 4253:
c. A perfectly rational backgammon agent never loses.

Token 4254:
5.22 Consider carefully the interplay of chance events and partial information in each of the games in Exercise 5.4. a.

Token 4255:
For which is the standard expectiminimax model appropriate?

Token 4256:
Implement the algorithm and run it in your game-playing agent, with appropriate modiﬁcations to the game-playing environment. b.

Token 4257:
For which would the scheme described in Exercise 5.19 be appropriate?

Token 4258:
c. Discuss how you might deal with the fact that in some of the games, the players do not have the same knowledge of the current state.

Token 4259:


Token 4260:
6CONSTRAINT SATISFACTION PROBLEMS In which we see how treating states as more than just little black boxes leads to the invention of a range of powerful new search methods and a deeper understandingof problem structure and complexity.

Token 4261:
Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of states .

Token 4262:
These states can be evaluated by domain-speciﬁc heuristics and tested to see whether they are goal states.

Token 4263:
From the point of view of the search algorithm, however, each state is atomic, or indivisible—a black box with no internal structure.

Token 4264:
This chapter describes a way to solve a wide variety of problems more efﬁciently.

Token 4265:
We use a factored representation for each state: a set of variables, each of which has a value.

Token 4266:
A problem is solved when each variable has a value that satisﬁes all the constraints on thevariable.

Token 4267:
A problem described this way is called a constraint satisfaction problem ,o rC S P .

Token 4268:
CONSTRAINT SATISFACTION PROBLEMCSP search algorithms take advantage of the structure of states and use general-purpose rather than problem-speciﬁc heuristics to enable the solution of complex problems.

Token 4269:
The main idea is to eliminate large portions of the search space all at once by identifying variable/value combinations that violate the constraints.

Token 4270:
6.1 D EFINING CONSTRAINT SATISFACTION PROBLEMS A constraint satisfaction problem consists of three components, X,D, andC: Xis a set of variables, {X1,...,X n}.

Token 4271:
Dis a set of domains, {D1,...,D n}, one for each variable. Cis a set of constraints that specify allowable combinations of values.

Token 4272:
Each domain Diconsists of a set of allowable values, {v1,...,v k}for variable Xi.

Token 4273:
Each constraint Ciconsists of a pair /angbracketleftscope,rel/angbracketright,w h e r e scope is a tuple of variables that participate in the constraint and relis a relation that deﬁnes the values that those variables can take on.

Token 4274:
A relation can be represented as an explicit list of all tuples of values that satisfy the constraint,or as an abstract relation that supports two operations: testing if a tuple is a member of therelation and enumerating the members of the relation.

Token 4275:
For example, if X 1andX2both have 202

Token 4276:
Section 6.1.

Token 4277:
Deﬁning Constraint Satisfaction Problems 203 the domain{A,B}, then the constraint saying the two variables must have different values can be written as /angbracketleft(X1,X2),[(A,B),(B,A)]/angbracketrightor as/angbracketleft(X1,X2),X1/negationslash=X2/angbracketright.

Token 4278:
To solve a CSP, we need to deﬁne a state space and the notion of a solution.

Token 4279:
Each state in a CSP is deﬁned by an assignment of values to some or all of the variables, {Xi= ASSIGNMENT vi,Xj=vj,...}.

Token 4280:
An assignment that does not violate any constraints is called a consistent CONSISTENT or legal assignment.

Token 4281:
A complete assignment is one in which every variable is assigned, andCOMPLETE ASSIGNMENT asolution to a CSP is a consistent, complete assignment.

Token 4282:
A partial assignment is one that SOLUTION PARTIAL ASSIGNMENT assigns values to only some of the variables.

Token 4283:
6.1.1 Example problem: Map coloring Suppose that, having tired of Romania, we are looking at a map of Australia showing each of its states and territories (Figure 6.1(a)).

Token 4284:
We are given the task of coloring each regioneither red, green, or blue in such a way that no neighboring regions have the same color.

Token 4285:
Toformulate this as a CSP, we deﬁne the variables to be the regions X={WA,NT,Q,NSW,V,SA,T}.

Token 4286:
The domain of each variable is the set D i={red,green,blue}. The constraints require neighboring regions to have distinct colors.

Token 4287:
Since there are nine places where regions border,there are nine constraints: C={SA/negationslash=WA,SA/negationslash=NT,SA/negationslash=Q,SA/negationslash=NSW,SA/negationslash=V, WA/negationslash=NT,NT/negationslash=Q,Q/negationslash=NSW,NSW/negationslash=V}.

Token 4288:
Here we are using abbreviations; SA/negationslash=WA is a shortcut for /angbracketleft(SA,WA),SA/negationslash=WA/angbracketright,w h e r e SA/negationslash=WA can be fully enumerated in turn as {(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue, green)}.

Token 4289:
There are many possible solutions to this problem, such as {WA=red,NT=green,Q=red,NSW =green,V=red,SA=blue,T=red}.

Token 4290:
It can be helpful to visualize a CSP as a constraint graph , as shown in Figure 6.1(b).

Token 4291:
The CONSTRAINT GRAPH nodes of the graph correspond to variables of the problem, and a link connects any two vari- ables that participate in a constraint.

Token 4292:
Why formulate a problem as a CSP?

Token 4293:
One reason is that the CSPs yield a natural rep- resentation for a wide variety of problems; if you already have a CSP-solving system, it isoften easier to solve a problem using it than to design a custom solution using another searchtechnique.

Token 4294:
In addition, CSP solvers can be faster than state-space searchers because the CSPsolver can quickly eliminate large swatches of the search space.

Token 4295:
For example, once we havechosen{SA=blue}in the Australia problem, we can conclude that none of the ﬁve neighbor- ing variables can take on the value blue.

Token 4296:
Without taking advantage of constraint propagation, a search procedure would have to consider 3 5= 243 assignments for the ﬁve neighboring variables; with constraint propagation we never have to consider blue as a value, so we have only25=3 2 assignments to look at, a reduction of 87%.

Token 4297:
In regular state-space search we can only ask: is this speciﬁc state a goal? No? What about this one?

Token 4298:
With CSPs, once we ﬁnd out that a partial assignment is not a solution, we can

Token 4299:
204 Chapter 6.

Token 4300:
Constraint Satisfaction Problems Western AustraliaNorthern Territory South AustraliaQueensland New South Wales Victoria TasmaniaWANT SAQ NSW V T (a) (b) Figure 6.1 (a) The principal states and territories of Australia.

Token 4301:
Coloring this map can be viewed as a constraint satisfaction problem (CSP).

Token 4302:
The goal is to assign colors to each region so that no neighboring regions have the same color.

Token 4303:
(b) The map-coloring problemrepresented as a constraint graph. immediately discard further reﬁnements of the partial assignment.

Token 4304:
Furthermore, we can see whythe assignment is not a solution—we see which variables violate a constraint—so we can focus attention on the variables that matter.

Token 4305:
As a result, many problems that are intractablefor regular state-space search can be solved quickly when formulated as a CSP.

Token 4306:
6.1.2 Example problem: Job-shop scheduling Factories have the problem of scheduling a day’s worth of jobs, subject to various constraints.In practice, many of these problems are solved with CSP techniques.

Token 4307:
Consider the problem ofscheduling the assembly of a car.

Token 4308:
The whole job is composed of tasks, and we can model eachtask as a variable, where the value of each variable is the time that the task starts, expressedas an integer number of minutes.

Token 4309:
Constraints can assert that one task must occur beforeanother—for example, a wheel must be installed before the hubcap is put on—and that only so many tasks can go on at once.

Token 4310:
Constraints can also specify that a task takes a certain amount of time to complete.

Token 4311:
We consider a small part of the car assembly, consisting of 15 tasks: install axles (front and back), afﬁx all four wheels (right and left, front and back), tighten nuts for each wheel, afﬁx hubcaps, and inspect the ﬁnal assembly.

Token 4312:
We can represent the tasks with 15 variables: X={Axle F,Axle B,Wheel RF,Wheel LF,Wheel RB,Wheel LB,Nuts RF, Nuts LF,Nuts RB,Nuts LB,CapRF,CapLF,CapRB,CapLB,Inspect}.

Token 4313:
The value of each variable is the time that the task starts. Next we represent precedence constraints between individual tasks.

Token 4314:
Whenever a task T1must occur before task T2,a n dPRECEDENCE CONSTRAINTS taskT1takes duration d1to complete, we add an arithmetic constraint of the form T1+d1≤T2.

Token 4315:
Section 6.1.

Token 4316:
Deﬁning Constraint Satisfaction Problems 205 In our example, the axles have to be in place before the wheels are put on, and it takes 10 minutes to install an axle, so we write Axle F+1 0≤Wheel RF;Axle F+1 0≤Wheel LF; Axle B+1 0≤Wheel RB;Axle B+1 0≤Wheel LB.

Token 4317:
Next we say that, for each wheel, we must afﬁx the wheel (which takes 1 minute), then tighten the nuts (2 minutes), and ﬁnally attach the hubcap (1 minute, but not represented yet): Wheel RF+1≤Nuts RF;Nuts RF+2≤CapRF; Wheel LF+1≤Nuts LF;Nuts LF+2≤CapLF; Wheel RB+1≤Nuts RB;Nuts RB+2≤CapRB; Wheel LB+1≤Nuts LB;Nuts LB+2≤CapLB.

Token 4318:
Suppose we have four workers to install wheels, but they have to share one tool that helps put the axle in place.

Token 4319:
We need a disjunctive constraint to say that Axle FandAxle Bmust notDISJUNCTIVE CONSTRAINT overlap in time; either one comes ﬁrst or the other does: (Axle F+1 0≤Axle B)or(Axle B+1 0≤Axle F).

Token 4320:
This looks like a more complicated constraint, combining arithmetic and logic.

Token 4321:
But it still reduces to a set of pairs of values that Axle FandAxle Fcan take on.

Token 4322:
We also need to assert that the inspection comes last and takes 3 minutes.

Token 4323:
For every variable except Inspect we add a constraint of the form X+dX≤Inspect .

Token 4324:
Finally, suppose there is a requirement to get the whole assembly done in 30 minutes.

Token 4325:
We can achieve that by limiting the domain of all variables: Di={1,2,3,...,27}.

Token 4326:
This particular problem is trivial to solve, but CSPs have been applied to job-shop schedul- ing problems like this with thousands of variables.

Token 4327:
In some cases, there are complicatedconstraints that are difﬁcult to specify in the CSP formalism, and more advanced planningtechniques are used, as discussed in Chapter 11.

Token 4328:
6.1.3 Variations on the CSP formalism The simplest kind of CSP involves variables that have discrete ,ﬁnite domains .M a p - DISCRETE DOMAIN FINITE DOMAIN coloring problems and scheduling with time limits are both of this kind.

Token 4329:
The 8-queens prob- lem described in Chapter 3 can also be viewed as a ﬁnite-domain CSP, where the variablesQ 1,...,Q 8are the positions of each queen in columns 1,...,8and each variable has the domain Di={1,2,3,4,5,6,7,8}.

Token 4330:
A discrete domain can be inﬁnite , such as the set of integers or strings.

Token 4331:
(If we didn’t put INFINITE a deadline on the job-scheduling problem, there would be an inﬁnite number of start times for each variable.)

Token 4332:
With inﬁnite domains, it is no longer possible to describe constraints byenumerating all allowed combinations of values.

Token 4333:
Instead, a constraint language must be CONSTRAINT LANGUAGE used that understands constraints such as T1+d1≤T2directly, without enumerating the set of pairs of allowable values for (T1,T2).

Token 4334:
Special solution algorithms (which we do not discuss here) exist for linear constraints on integer variables—that is, constraints, such asLINEAR CONSTRAINTS the one just given, in which each variable appears only in linear form.

Token 4335:
It can be shown that no algorithm exists for solving general nonlinear constraints on integer variables.NONLINEAR CONSTRAINTS

Token 4336:
206 Chapter 6.

Token 4337:
Constraint Satisfaction Problems Constraint satisfaction problems with continuous domains are common in the realCONTINUOUS DOMAINS world and are widely studied in the ﬁeld of operations research.

Token 4338:
For example, the scheduling of experiments on the Hubble Space Telescope requires very precise timing of observations;the start and ﬁnish of each observation and maneuver are continuous-valued variables thatmust obey a variety of astronomical, precedence, and power constraints.

Token 4339:
The best-known category of continuous-domain CSPs is that of linear programming problems, where con- straints must be linear equalities or inequalities.

Token 4340:
Linear programming problems can be solvedin time polynomial in the number of variables.

Token 4341:
Problems with different types of constraintsand objective functions have also been studied—quadratic programming, second-order conicprogramming, and so on.

Token 4342:
In addition to examining the types of variables that can appear in CSPs, it is useful to look at the types of constraints.

Token 4343:
The simplest type is the unary constraint , which restricts UNARY CONSTRAINT the value of a single variable.

Token 4344:
For example, in the map-coloring problem it could be the case that South Australians won’t tolerate the color green; we can express that with the unaryconstraint/angbracketleft(SA),SA/negationslash=green/angbracketright Abinary constraint relates two variables.

Token 4345:
For example, SA/negationslash=NSW is a binary BINARY CONSTRAINT constraint.

Token 4346:
A binary CSP is one with only binary constraints; it can be represented as a constraint graph, as in Figure 6.1(b).

Token 4347:
We can also describe higher-order constraints, such as asserting that the value of Yis between XandZ, with the ternary constraint Between (X,Y,Z ).

Token 4348:
A constraint involving an arbitrary number of variables is called a global constraint .GLOBAL CONSTRAINT (The name is traditional but confusing because it need not involve allthe variables in a prob- lem).

Token 4349:
One of the most common global constraints is Alldiﬀ , which says that all of the variables involved in the constraint must have different values.

Token 4350:
In Sudoku problems (seeSection 6.2.6), all variables in a row or column must satisfy an Alldiﬀ constraint.

Token 4351:
An- other example is provided by cryptarithmetic puzzles. (See Figure 6.2(a).)

Token 4352:
Each letter in a CRYPTARITHMETIC cryptarithmetic puzzle represents a different digit.

Token 4353:
For the case in Figure 6.2(a), this would be represented as the global constraint Alldiﬀ (F,T,U,W,R,O ).

Token 4354:
The addition constraints on the four columns of the puzzle can be written as the following n-ary constraints: O+O=R+1 0·C10 C10+W+W=U+1 0·C100 C100+T+T=O+1 0·C1000 C1000=F, where C10,C100,a n dC1000are auxiliary variables representing the digit carried over into the tens, hundreds, or thousands column.

Token 4355:
These constraints can be represented in a constraint hypergraph , such as the one shown in Figure 6.2(b).

Token 4356:
A hypergraph consists of ordinary nodesCONSTRAINT HYPERGRAPH (the circles in the ﬁgure) and hypernodes (the squares), which represent n-ary constraints.

Token 4357:
Alternatively, as Exercise 6.6 asks you to prove, every ﬁnite-domain constraint can be reduced to a set of binary constraints if enough auxiliary variables are introduced, so we could transform any CSP into one with only binary constraints; this makes the algorithms simpler.Another way to convert an n-ary CSP to a binary one is the dual graph transformation: create DUAL GRAPH a new graph in which there will be one variable for each constraint in the original graph, and

Token 4358:
Section 6.1. Deﬁning Constraint Satisfaction Problems 207 (a)O W T F U R (b)+ FT T OW W UO O R C3 C1 C2 Figure 6.2 (a) A cryptarithmetic problem.

Token 4359:
Each letter stands for a distinct digit; the aim is to ﬁnd a substitution of digits for letters such that the resulting sum is arithmetically correct, with the added restriction that no leading zeroes are allowed.

Token 4360:
(b) The constraint hypergraphfor the cryptarithmetic problem, showing the Alldiﬀ constraint (square box at the top) as well as the column addition constraints (four square boxes in the middle).

Token 4361:
The variables C 1, C2,a n dC3represent the carry digits for the three columns.

Token 4362:
one binary constraint for each pair of constraints in the original graph that share variables.

Token 4363:
For example, if the original graph has variables {X,Y,Z}and constraints /angbracketleft(X,Y,Z ),C1/angbracketrightand /angbracketleft(X,Y),C2/angbracketrightthen the dual graph would have variables {C1,C2}with the binary constraint /angbracketleft(X,Y),R1/angbracketright,w h e r e (X,Y)are the shared variables and R1is a new relation that deﬁnes the constraint between the shared variables, as speciﬁed by the original C1andC2.

Token 4364:
There are however two reasons why we might prefer a global constraint such as Alldiﬀ rather than a set of binary constraints.

Token 4365:
First, it is easier and less error-prone to write theproblem description using Alldiﬀ .

Token 4366:
Second, it is possible to design special-purpose inference algorithms for global constraints that are not available for a set of more primitive constraints.

Token 4367:
We describe these inference algorithms in Section 6.2.5.

Token 4368:
The constraints we have described so far have all been absolute constraints, violation of which rules out a potential solution.

Token 4369:
Many real-world CSPs include preference constraints PREFERENCE CONSTRAINTS indicating which solutions are preferred.

Token 4370:
For example, in a university class-scheduling prob- lem there are absolute constraints that no professor can teach two classes at the same time.

Token 4371:
But we also may allow preference constraints: Prof. R might prefer teaching in the morning, whereas Prof. N prefers teaching in the afternoon.

Token 4372:
A schedule that has Prof. R teaching at2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair)but would not be an optimal one.

Token 4373:
Preference constraints can often be encoded as costs on in-dividual variable assignments—for example, assigning an afternoon slot for Prof. R costs 2 points against the overall objective function, whereas a morning slot costs 1.

Token 4374:
With this formulation, CSPs with preferences can be solved with optimization search methods, eitherpath-based or local.

Token 4375:
We call such a problem a constraint optimization problem , or COP.

Token 4376:
CONSTRAINT OPTIMIZATIONPROBLEM Linear programming problems do this kind of optimization.

Token 4377:
208 Chapter 6.

Token 4378:
Constraint Satisfaction Problems 6.2 C ONSTRAINT PROPAGATION :INFERENCE IN CSP S In regular state-space search, an algorithm can do only one thing: search.

Token 4379:
In CSPs there is a choice: an algorithm can search (choose a new variable assignment from several possibilities) or do a speciﬁc type of inference called constraint propagation : using the constraints to INFERENCE CONSTRAINT PROPAGATION reduce the number of legal values for a variable, which in turn can reduce the legal values for another variable, and so on.

Token 4380:
Constraint propagation may be intertwined with search, or itmay be done as a preprocessing step, before search starts.

Token 4381:
Sometimes this preprocessing cansolve the whole problem, so no search is required at all. The key idea is local consistency .

Token 4382:
If we treat each variable as a node in a graph (see LOCAL CONSISTENCY Figure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local con- sistency in each part of the graph causes inconsistent values to be eliminated throughout thegraph.

Token 4383:
There are different types of local consistency, which we now cover in turn.

Token 4384:
6.2.1 Node consistency A single variable (corresponding to a node in the CSP network) is node-consistent if all NODE CONSISTENCY the values in the variable’s domain satisfy the variable’s unary constraints.

Token 4385:
For example, in the variant of the Australia map-coloring problem (Figure 6.1) where South Australiansdislike green, the variable SAstarts with domain {red,green,blue}, and we can make it node consistent by eliminating green , leaving SAwith the reduced domain {red,blue}.W e say that a network is node-consistent if every variable in the network is node-consistent.

Token 4386:
It is always possible to eliminate all the unary constraints in a CSP by running node consistency.

Token 4387:
It is also possible to transform all n-ary constraints into binary ones (see Ex- ercise 6.6).

Token 4388:
Because of this, it is common to deﬁne CSP solvers that work with only binary constraints; we make that assumption for the rest of this chapter, except where noted.

Token 4389:
6.2.2 Arc consistency Av a r i a b l ei naC S Pi s arc-consistent if every value in its domain satisﬁes the variable’s ARC CONSISTENCY binary constraints.

Token 4390:
More formally, Xiis arc-consistent with respect to another variable Xjif for every value in the current domain Dithere is some value in the domain Djthat satisﬁes the binary constraint on the arc (Xi,Xj).

Token 4391:
A network is arc-consistent if every variable is arc consistent with every other variable.

Token 4392:
For example, consider the constraint Y=X2where the domain of both XandYis the set of digits.

Token 4393:
We can write this constraint explicitly as /angbracketleft(X,Y),{(0,0),(1,1),(2,4),(3,9))}/angbracketright.

Token 4394:
To make Xarc-consistent with respect to Y, we reduce X’s domain to{0,1,2,3}.I f w e also make Yarc-consistent with respect to X,t h e nY’s domain becomes {0,1,4,9}and the whole CSP is arc-consistent.

Token 4395:
On the other hand, arc consistency can do nothing for the Australia map-coloring prob- lem.

Token 4396:
Consider the following inequality constraint on (SA,WA): {(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}.

Token 4397:
Section 6.2.

Token 4398:
Constraint Propagation: Inference in CSPs 209 function AC-3( csp)returns false if an inconsistency is found and true otherwise inputs :csp, a binary CSP with components (X, D, C ) local variables :queue , a queue of arcs, initially all the arcs in csp whilequeue is not empty do (Xi,Xj)←REMOVE -FIRST (queue ) ifREVISE (csp,Xi,Xj)then ifsize of Di=0 then return false for each XkinXi.NEIGHBORS -{Xj}do add (Xk,Xi)t oqueue return true function REVISE (csp,Xi,Xj)returns true iff we revise the domain of Xi revised←false for each xinDido ifno value yinDjallows ( x,y) to satisfy the constraint between XiandXjthen deletexfromDi revised←true return revised Figure 6.3 The arc-consistency algorithm AC-3.

Token 4399:
After applying AC-3, either every arc is arc-consistent, or some variable has an empty domain, indicating that the CSP cannot be solved.

Token 4400:
The name “AC-3” was used by the algorithm’s inventor (Mackworth, 1977) becauseit’s the third version developed in the paper.

Token 4401:
No matter what value you choose for SA(or for WA), there is a valid value for the other variable.

Token 4402:
So applying arc consistency has no effect on the domains of either variable.

Token 4403:
The most popular algorithm for arc consistency is called AC-3 (see Figure 6.3).

Token 4404:
To make every variable arc-consistent, the AC-3 algorithm maintains a queue of arcs to consider.

Token 4405:
(Actually, the order of consideration is not important, so the data structure is really a set, but tradition calls it a queue.)

Token 4406:
Initially, the queue contains all the arcs in the CSP.

Token 4407:
AC-3 then popsoff an arbitrary arc (X i,Xj)from the queue and makes Xiarc-consistent with respect to Xj.

Token 4408:
If this leaves Diunchanged, the algorithm just moves on to the next arc.

Token 4409:
But if this revises Di(makes the domain smaller), then we add to the queue all arcs (Xk,Xi)where Xkis a neighbor of Xi.

Token 4410:
We need to do that because the change in Dimight enable further reductions in the domains of Dk, even if we have previously considered Xk.I fDiis revised down to nothing, then we know the whole CSP has no consistent solution, and AC-3 can immediatelyreturn failure.

Token 4411:
Otherwise, we keep checking, trying to remove values from the domains ofvariables until no more arcs are in the queue.

Token 4412:
At that point, we are left with a CSP that isequivalent to the original CSP—they both have the same solutions—but the arc-consistent CSP will in most cases be faster to search because its variables have smaller domains.

Token 4413:
The complexity of AC-3 can be analyzed as follows. Assume a CSP with nvariables, each with domain size at most d, and with cbinary constraints (arcs).

Token 4414:
Each arc (X k,Xi)can be inserted in the queue only dtimes because Xihas at most dvalues to delete. Checking

Token 4415:
210 Chapter 6.

Token 4416:
Constraint Satisfaction Problems consistency of an arc can be done in O(d2)time, so we get O(cd3)total worst-case time.1 It is possible to extend the notion of arc consistency to handle n-ary rather than just binary constraints; this is called generalized arc consistency or sometimes hyperarc consis-tency, depending on the author.

Token 4417:
A variable X iisgeneralized arc consistent with respect toGENERALIZED ARC CONSISTENT ann-ary constraint if for every value vin the domain of Xithere exists a tuple of values that is a member of the constraint, has all its values taken from the domains of the corresponding variables, and has its Xicomponent equal to v. For example, if all variables have the do- main{0,1,2,3},t h e nt om a k et h ev a r i a b l e Xconsistent with the constraint X<Y<Z , we would have to eliminate 2 and 3 from the domain of Xbecause the constraint cannot be satisﬁed when Xis 2 or 3.

Token 4418:
6.2.3 Path consistency Arc consistency can go a long way toward reducing the domains of variables, sometimes ﬁnding a solution (by reducing every domain to size 1) and sometimes ﬁnding that the CSPcannot be solved (by reducing some domain to size 0).

Token 4419:
But for other networks, arc consistencyfails to make enough inferences.

Token 4420:
Consider the map-coloring problem on Australia, but withonly two colors allowed, red and blue.

Token 4421:
Arc consistency can do nothing because every variableis already arc consistent: each can be red with blue at the other end of the arc (or vice versa).But clearly there is no solution to the problem: because Western Australia, Northern Territoryand South Australia all touch each other, we need at least three colors for them alone.

Token 4422:
Arc consistency tightens down the domains (unary constraints) using the arcs (binary constraints).

Token 4423:
To make progress on problems like map coloring, we need a stronger notion of consistency.

Token 4424:
Path consistency tightens the binary constraints by using implicit constraints PATH CONSISTENCY that are inferred by looking at triples of variables.

Token 4425:
A two-variable set {Xi,Xj}is path-consistent with respect to a third variable Xmif, for every assignment {Xi=a,Xj=b}consistent with the constraints on {Xi,Xj},t h e r ei s an assignment to Xmthat satisﬁes the constraints on {Xi,Xm}and{Xm,Xj}.

Token 4426:
This is called path consistency because one can think of it as looking at a path from XitoXjwithXmin the middle.

Token 4427:
Let’s see how path consistency fares in coloring the Australia map with two colors. We will make the set {WA,SA}path consistent with respect to NT.

Token 4428:
We start by enumerating the consistent assignments to the set. In this case, there are only two: {WA=red,SA=blue} and{WA=blue,SA=red}.

Token 4429:
We can see that with both of these assignments NT can be neither rednorblue (because it would conﬂict with either WA orSA).

Token 4430:
Because there is no valid choice for NT, we eliminate both assignments, and we end up with no valid assignments for{WA,SA}.

Token 4431:
Therefore, we know that there can be no solution to this problem.

Token 4432:
The PC-2 algorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3achieves arc consistency.

Token 4433:
Because it is so similar, we do not show it here.

Token 4434:
1The AC-4 algorithm (Mohr and Henderson, 1986) runs in O(cd2)worst-case time but can be slower than AC-3 on average cases. See Exercise 6.13.

Token 4435:
Section 6.2.

Token 4436:
Constraint Propagation: Inference in CSPs 211 6.2.4 K-consistency Stronger forms of propagation can be deﬁned with the notion of k-consistency .A C S P i s K-CONSISTENCY k-consistent if, for any set of k−1variables and for any consistent assignment to those variables, a consistent value can always be assigned to any kth variable.

Token 4437:
1-consistency says that, given the empty set, we can make any set of one variable consistent: this is what wecalled node consistency.

Token 4438:
2-consistency is the same as arc consistency. For binary constraint networks, 3-consistency is the same as path consistency.

Token 4439:
AC S Pi s strongly k-consistent if it is k-consistent and is also (k−1)-consistent, STRONGLY K-CONSISTENT (k−2)-consistent, ...all the way down to 1-consistent.

Token 4440:
Now suppose we have a CSP with nnodes and make it strongly n-consistent (i.e., strongly k-consistent for k=n).

Token 4441:
We can then solve the problem as follows: First, we choose a consistent value for X1.W e a r e t h e n guaranteed to be able to choose a value for X2because the graph is 2-consistent, for X3 because it is 3-consistent, and so on.

Token 4442:
For each variable Xi, we need only search through the d values in the domain to ﬁnd a value consistent with X1,...,X i−1.

Token 4443:
We are guaranteed to ﬁnd a solution in time O(n2d).

Token 4444:
Of course, there is no free lunch: any algorithm for establishing n-consistency must take time exponential in nin the worst case.

Token 4445:
Worse, n-consistency also requires space that is exponential in n. The memory issue is even more severe than the time.

Token 4446:
In practice, determining the appropriate level of consistency checking is mostly an empirical science.

Token 4447:
It can be said practitioners commonly compute 2-consistency and less commonly 3-consistency.

Token 4448:
6.2.5 Global constraints Remember that a global constraint is one involving an arbitrary number of variables (but not necessarily all variables).

Token 4449:
Global constraints occur frequently in real problems and can behandled by special-purpose algorithms that are more efﬁcient than the general-purpose meth-ods described so far.

Token 4450:
For example, the Alldiﬀ constraint says that all the variables involved must have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be- low).

Token 4451:
One simple form of inconsistency detection for Alldiﬀ constraints works as follows: ifmvariables are involved in the constraint, and if they have npossible distinct values alto- gether, and m>n , then the constraint cannot be satisﬁed.

Token 4452:
This leads to the following simple algorithm: First, remove any variable in the con- straint that has a singleton domain, and delete that variable’s value from the domains of theremaining variables.

Token 4453:
Repeat as long as there are singleton variables.

Token 4454:
If at any point an emptydomain is produced or there are more variables than domain values left, then an inconsistencyhas been detected.

Token 4455:
This method can detect the inconsistency in the assignment {WA=red,NSW =red} for Figure 6.1.

Token 4456:
Notice that the variables SA,NT,a n dQare effectively connected by an Alldiﬀ constraint because each pair must have two different colors.

Token 4457:
After applying AC-3 with the partial assignment, the domain of each variable is reduced to {green,blue}.T h a t is, we have three variables and only two colors, so the Alldiﬀ constraint is violated.

Token 4458:
Thus, a simple consistency procedure for a higher-order constraint is sometimes more effectivethan applying arc consistency to an equivalent set of binary constraints.

Token 4459:
There are more

Token 4460:
212 Chapter 6.

Token 4461:
Constraint Satisfaction Problems complex inference algorithms for Alldiﬀ (see van Hoeve and Katriel, 2006) that propagate more constraints but are more computationally expensive to run.

Token 4462:
Another important higher-order constraint is the resource constraint , sometimes calledRESOURCE CONSTRAINT theatmost constraint.

Token 4463:
For example, in a scheduling problem, let P1,...,P 4denote the numbers of personnel assigned to each of four tasks.

Token 4464:
The constraint that no more than 10 personnel are assigned in total is written as Atmost (10,P1,P2,P3,P4).

Token 4465:
We can detect an inconsistency simply by checking the sum of the minimum values of the current domains;for example, if each variable has the domain {3,4,5,6},t h eAtmost constraint cannot be satisﬁed.

Token 4466:
We can also enforce consistency by deleting the maximum value of any domain if itis not consistent with the minimum values of the other domains.

Token 4467:
Thus, if each variable in ourexample has the domain {2,3,4,5,6}, the values 5 and 6 can be deleted from each domain.

Token 4468:
For large resource-limited problems with integer values—such as logistical problems involving moving thousands of people in hundreds of vehicles—it is usually not possible torepresent the domain of each variable as a large set of integers and gradually reduce that set byconsistency-checking methods.

Token 4469:
Instead, domains are represented by upper and lower boundsand are managed by bounds propagation .

Token 4470:
For example, in an airline-scheduling problem, BOUNDS PROPAGATION let’s suppose there are two ﬂights, F1andF2, for which the planes have capacities 165 and 385, respectively.

Token 4471:
The initial domains for the numbers of passengers on each ﬂight are then D1=[ 0,165] andD2=[ 0,385].

Token 4472:
Now suppose we have the additional constraint that the two ﬂights together must carry 420 people: F1+F2= 420 .

Token 4473:
Propagating bounds constraints, we reduce the domains to D1=[ 3 5,165] andD2= [255 ,385].

Token 4474:
We say that a CSP is bounds consistent if for every variable X, and for both the lower-BOUNDS CONSISTENT bound and upper-bound values of X, there exists some value of Ythat satisﬁes the constraint between XandYfor every variable Y.

Token 4475:
This kind of bounds propagation is widely used in practical constraint problems.

Token 4476:
6.2.6 Sudoku example The popular Sudoku puzzle has introduced millions of people to constraint satisfaction prob- SUDOKU lems, although they may not recognize it.

Token 4477:
A Sudoku board consists of 81 squares, some of which are initially ﬁlled with digits from 1 to 9.

Token 4478:
The puzzle is to ﬁll in all the remainingsquares such that no digit appears twice in any row, column, or 3×3box (see Figure 6.4).

Token 4479:
A row, column, or box is called a unit.

Token 4480:
The Sudoku puzzles that are printed in newspapers and puzzle books have the property that there is exactly one solution.

Token 4481:
Although some can be tricky to solve by hand, taking tensof minutes, even the hardest Sudoku problems yield to a CSP solver in less than 0.1 second.

Token 4482:
A Sudoku puzzle can be considered a CSP with 81 variables, one for each square.

Token 4483:
We use the variable names A1through A9for the top row (left to right), down to I1through I9 for the bottom row.

Token 4484:
The empty squares have the domain {1,2,3,4,5,6,7,8,9}and the pre- ﬁlled squares have a domain consisting of a single value.

Token 4485:
In addition, there are 27 different

Token 4486:
Section 6.2.

Token 4487:
Constraint Propagation: Inference in CSPs 213 326 93 51 18 6481 29 78 67 8226 95 82 39 513326 93 51 18 6481 29 78 67 8226 95 82 39 5134 8915 7 6 748 2 2 579 35 437 6 2956413 1 394 53 781 4 1 457 6 6 9478 2123456789 A BCDEFGH IA BCDEFGH I123456789 (a) (b) Figure 6.4 (a) A Sudoku puzzle and (b) its solution.

Token 4488:
Alldiﬀ constraints: one for each row, column, and box of 9 squares.

Token 4489:
Alldiﬀ (A1,A2,A3,A4,A5,A6,A7,A8,A9) Alldiﬀ (B1,B2,B3,B4,B5,B6,B7,B8,B9) ···Alldiﬀ (A1,B1,C1,D1,E1,F1,G1,H1,I1) Alldiﬀ (A2,B2,C2,D2,E2,F2,G2,H2,I2) ···Alldiﬀ (A1,A2,A3,B1,B2,B3,C1,C2,C 3) Alldiﬀ (A4,A5,A6,B4,B5,B6,C4,C5,C6) ··· Let us see how far arc consistency can take us.

Token 4490:
Assume that the Alldiﬀ constraints have been expanded into binary constraints (such as A1/negationslash=A2) so that we can apply the AC-3 algorithm directly.

Token 4491:
Consider variable E6from Figure 6.4(a)—the empty square between the 2 and the 8 in the middle box.

Token 4492:
From the constraints in the box, we can remove not only 2 and 8 but also1a n d7f r o m E6’s domain.

Token 4493:
From the constraints in its column, we can eliminate 5, 6, 2, 8, 9, and 3.

Token 4494:
That leaves E6with a domain of {4}; in other words, we know the answer for E6.

Token 4495:
Now consider variable I6—the square in the bottom middle box surrounded by 1, 3, and 3.

Token 4496:
Applying arc consistency in its column, we eliminate 5, 6, 2, 4 (since we now know E6must be 4), 8, 9, and 3.

Token 4497:
We eliminate 1 by arc consistency with I5, and we are left with only the value 7 in the domain of I6.

Token 4498:
Now there are 8 known values in column 6, so arc consistency can infer that A6must be 1.

Token 4499:
Inference continues along these lines, and eventually, AC-3 can solve the entire puzzle—all the variables have their domains reduced to a single value, asshown in Figure 6.4(b).

Token 4500:
Of course, Sudoku would soon lose its appeal if every puzzle could be solved by a

Token 4501:
214 Chapter 6. Constraint Satisfaction Problems mechanical application of AC-3, and indeed AC-3 works only for the easiest Sudoku puzzles.

Token 4502:
Slightly harder ones can be solved by PC-2, but at a greater computational cost: there are255,960 different path constraints to consider in a Sudoku puzzle.

Token 4503:
To solve the hardest puzzlesand to make efﬁcient progress, we will have to be more clever.

Token 4504:
Indeed, the appeal of Sudoku puzzles for the human solver is the need to be resourceful in applying more complex inference strategies.

Token 4505:
Aﬁcionados give them colorful names, such as “naked triples.” That strategy works as follows: in any unit (row, column or box), ﬁndthree squares that each have a domain that contains the same three numbers or a subset ofthose numbers.

Token 4506:
For example, the three domains might be {1,8},{3,8},a n d{1,3,8}.F r o m that we don’t know which square contains 1, 3, or 8, but we do know that the three numbersmust be distributed among the three squares.

Token 4507:
Therefore we can remove 1, 3, and 8 from thedomains of every other square in the unit.

Token 4508:
It is interesting to note how far we can go without saying much that is speciﬁc to Su- doku.

Token 4509:
We do of course have to say that there are 81 variables, that their domains are the digits1t o9 ,a n dt h a tt h e r ea r e2 7 Alldiﬀ constraints.

Token 4510:
But beyond that, all the strategies—arc con- sistency, path consistency, etc.—apply generally to all CSPs, not just to Sudoku problems.Even naked triples is really a strategy for enforcing consistency of Alldiﬀ constraints and has nothing to do with Sudoku per se .

Token 4511:
This is the power of the CSP formalism: for each new problem area, we only need to deﬁne the problem in terms of constraints; then the generalconstraint-solving mechanisms can take over.

Token 4512:
6.3 B ACKTRACKING SEARCH FOR CSP S Sudoku problems are designed to be solved by inference over constraints.

Token 4513:
But many otherCSPs cannot be solved by inference alone; there comes a time when we must search for asolution.

Token 4514:
In this section we look at backtracking search algorithms that work on partial as- signments; in the next section we look at local search algorithms over complete assignments.

Token 4515:
We could apply a standard depth-limited search (from Chapter 3).

Token 4516:
A state would be a partial assignment, and an action would be adding var=value to the assignment.

Token 4517:
But for a CSP with nvariables of domain size d, we quickly notice something terrible: the branching factor at the top level is ndbecause any of dvalues can be assigned to any of nvariables.

Token 4518:
At the next level, the branching factor is (n−1)d, and so on for nlevels.

Token 4519:
We generate a tree withn!·d nleaves, even though there are only dnpossible complete assignments!

Token 4520:
Our seemingly reasonable but naive formulation ignores crucial property common to all CSPs: commutativity .

Token 4521:
A problem is commutative if the order of application of any given COMMUTATIVITY set of actions has no effect on the outcome.

Token 4522:
CSPs are commutative because when assigning values to variables, we reach the same partial assignment regardless of order.

Token 4523:
Therefore, we need only consider a single variable at each node in the search tree.

Token 4524:
For example, at the root node of a search tree for coloring the map of Australia, we might make a choice between SA=red,SA=green ,a n dSA=blue, but we would never choose between SA=redand WA=blue.

Token 4525:
With this restriction, the number of leaves is dn, as we would hope.

Token 4526:
Section 6.3.

Token 4527:
Backtracking Search for CSPs 215 function BACKTRACKING -SEARCH (csp)returns a solution, or failure return BACKTRACK ({},csp) function BACKTRACK (assignment ,csp)returns a solution, or failure ifassignment is complete then return assignment var←SELECT -UNASSIGNED -VARIABLE (csp) for each value inORDER -DOMAIN -VALUES (var,assignment ,csp)do ifvalue is consistent with assignment then add{var=value}toassignment inferences←INFERENCE (csp,var,value ) ifinferences/negationslash=failure then addinferences toassignment result←BACKTRACK (assignment ,csp) ifresult/negationslash=failure then return result remove{var=value}andinferences fromassignment return failure Figure 6.5 A simple backtracking algorithm for constraint satisfaction problems.

Token 4528:
The al- gorithm is modeled on the recursive depth-ﬁrst search of Chapter 3.

Token 4529:
By varying the functions SELECT -UNASSIGNED -VARIABLE and O RDER -DOMAIN -VALUES , we can implement the general-purpose heuristics discussed in the text.

Token 4530:
The function I NFERENCE can optionally be used to impose arc-, path-, or k-consistency, as desired.

Token 4531:
If a value choice leads to failure (noticed either by I NFERENCE or by B ACKTRACK ), then value assignments (including those made by I NFERENCE ) are removed from the current assignment and a new value is tried.

Token 4532:
The term backtracking search is used for a depth-ﬁrst search that chooses values forBACKTRACKING SEARCH one variable at a time and backtracks when a variable has no legal values left to assign.

Token 4533:
The algorithm is shown in Figure 6.5.

Token 4534:
It repeatedly chooses an unassigned variable, and then triesall values in the domain of that variable in turn, trying to ﬁnd a solution.

Token 4535:
If an inconsistency isdetected, then B ACKTRACK returns failure, causing the previous call to try another value.

Token 4536:
Part of the search tree for the Australia problem is shown in Figure 6.6, where we have assigned variables in the order WA,NT,Q,... .

Token 4537:
Because the representation of CSPs is standardized, there is no need to supply B ACKTRACKING -SEARCH with a domain-speciﬁc initial state, action function, transition model, or goal test.

Token 4538:
Notice that B ACKTRACKING -SEARCH keeps only a single representation of a state and alters that representation rather than creating new ones, as described on page 87.

Token 4539:
In Chapter 3 we improved the poor performance of uninformed search algorithms by supplying them with domain-speciﬁc heuristic functions derived from our knowledge of the problem.

Token 4540:
It turns out that we can solve CSPs efﬁciently without such domain-speciﬁc knowl- edge.

Token 4541:
Instead, we can add some sophistication to the unspeciﬁed functions in Figure 6.5,using them to address the following questions: 1.

Token 4542:
Which variable should be assigned next (S ELECT -UNASSIGNED -VARIABLE ), and in what order should its values be tried (O RDER -DOMAIN -VALUES )?

Token 4543:
216 Chapter 6.

Token 4544:
Constraint Satisfaction Problems WA=red WA=blue WA=green WA=red NT=blueWA=redNT=green WA=redNT=green Q=redWA=red NT=green Q=blue Figure 6.6 Part of the search tree for the map-coloring problem in Figure 6.1.

Token 4545:
2. What inferences should be performed at each step in the search (I NFERENCE )? 3.

Token 4546:
When the search arrives at an assignment that violates a constraint, can the search avoid repeating this failure?

Token 4547:
The subsections that follow answer each of these questions in turn.

Token 4548:
6.3.1 Variable and value ordering The backtracking algorithm contains the line var←SELECT -UNASSIGNED -VARIABLE (csp).

Token 4549:
The simplest strategy for S ELECT -UNASSIGNED -VARIABLE is to choose the next unassigned variable in order, {X1,X2,...}.

Token 4550:
This static variable ordering seldom results in the most efﬁ- cient search.

Token 4551:
For example, after the assignments for WA=redandNT=green in Figure 6.6, there is only one possible value for SA, so it makes sense to assign SA=blue next rather than assigning Q.

Token 4552:
In fact, after SAis assigned, the choices for Q,NSW ,a n dVare all forced.

Token 4553:
This intuitive idea—choosing the variable with the fewest “legal” values—is called the minimum- remaining-values (MRV) heuristic.

Token 4554:
It also has been called the “most constrained variable” orMINIMUM- REMAINING-VALUES “fail-ﬁrst” heuristic, the latter because it picks a variable that is most likely to cause a failure soon, thereby pruning the search tree.

Token 4555:
If some variable Xhas no legal values left, the MRV heuristic will select Xand failure will be detected immediately—avoiding pointless searches through other variables.

Token 4556:
The MRV heuristic usually performs better than a random or staticordering, sometimes by a factor of 1,000 or more, although the results vary widely dependingon the problem.

Token 4557:
The MRV heuristic doesn’t help at all in choosing the ﬁrst region to color in Australia, because initially every region has three legal colors.

Token 4558:
In this case, the degree heuristic comes DEGREE HEURISTIC in handy.

Token 4559:
It attempts to reduce the branching factor on future choices by selecting the vari- able that is involved in the largest number of constraints on other unassigned variables.

Token 4560:
In Figure 6.1, SAis the variable with highest degree, 5; the other variables have degree 2 or 3, except for T, which has degree 0.

Token 4561:
In fact, once SAis chosen, applying the degree heuris- tic solves the problem without any false steps—you can choose anyconsistent color at each choice point and still arrive at a solution with no backtracking.

Token 4562:
The minimum-remaining-

Token 4563:
Section 6.3.

Token 4564:
Backtracking Search for CSPs 217 values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a tie-breaker.

Token 4565:
Once a variable has been selected, the algorithm must decide on the order in which to examine its values.

Token 4566:
For this, the least-constraining-value heuristic can be effective in someLEAST- CONSTRAINING- VALUEcases.

Token 4567:
It prefers the value that rules out the fewest choices for the neighboring variables in the constraint graph.

Token 4568:
For example, suppose that in Figure 6.1 we have generated the partial assignment with WA=redandNT=green and that our next choice is for Q.

Token 4569:
Blue would be a bad choice because it eliminates the last legal value left for Q’s neighbor, SA.T h e least-constraining-value heuristic therefore prefers red to blue.

Token 4570:
In general, the heuristic is trying to leave the maximum ﬂexibility for subsequent variable assignments.

Token 4571:
Of course, if we are trying to ﬁnd all the solutions to a problem, not just the ﬁrst one, then the ordering does not matter because we have to consider every value anyway.

Token 4572:
The same holds if there are nosolutions to the problem. Why should variable selection be fail-ﬁrst, but value selection be fail-last?

Token 4573:
It turns out that, for a wide variety of problems, a variable ordering that chooses a variable with theminimum number of remaining values helps minimize the number of nodes in the search treeby pruning larger parts of the tree earlier.

Token 4574:
For value ordering, the trick is that we only need one solution; therefore it makes sense to look for the most likely values ﬁrst.

Token 4575:
If we wanted to enumerate all solutions rather than just ﬁnd one, then value ordering would be irrelevant.

Token 4576:
6.3.2 Interleaving search and inference So far we have seen how AC-3 and other algorithms can infer reductions in the domain ofvariables before we begin the search.

Token 4577:
But inference can be even more powerful in the course of a search: every time we make a choice of a value for a variable, we have a brand-newopportunity to infer new domain reductions on the neighboring variables.

Token 4578:
One of the simplest forms of inference is called forward checking .

Token 4579:
Whenever a vari- FORWARD CHECKING ableXis assigned, the forward-checking process establishes arc consistency for it: for each unassigned variable Ythat is connected to Xby a constraint, delete from Y’s domain any value that is inconsistent with the value chosen for X.

Token 4580:
Because forward checking only does arc consistency inferences, there is no reason to do forward checking if we have already donearc consistency as a preprocessing step.

Token 4581:
Figure 6.7 shows the progress of backtracking search on the Australia CSP with for- ward checking.

Token 4582:
There are two important points to notice about this example.

Token 4583:
First, noticethat after WA=redandQ=green are assigned, the domains of NT andSAare reduced to a single value; we have eliminated branching on these variables altogether by propagat-ing information from WA andQ.

Token 4584:
A second point to notice is that after V=blue, the do- main of SAis empty.

Token 4585:
Hence, forward checking has detected that the partial assignment {WA=red,Q=green,V=blue}is inconsistent with the constraints of the problem, and the algorithm will therefore backtrack immediately.

Token 4586:
For many problems the search will be more effective if we combine the MRV heuris- tic with forward checking.

Token 4587:
Consider Figure 6.7 after assigning {WA=red}. Intuitively, it seems that that assignment constrains its neighbors, NTandSA, so we should handle those

Token 4588:
218 Chapter 6.

Token 4589:
Constraint Satisfaction Problems Initial domains AfterWA=red AfterQ=green AfterV=blueRGB RRBRGBRGB BRGBRGBRGB RRRRGB BBGBRGB GGRGBRGB BGBRGB RGBRGBRGBWA T SAV NSWQ NT Figure 6.7 The progress of a map-coloring search with forward checking.

Token 4590:
WA=red is assigned ﬁrst; then forward checking deletes redfrom the domains of the neighboring variables NT andSA.A f t e r Q=green is assigned, green is deleted from the domains of NT,SA,a n dNSW .A f t e r V=blue is assigned, blue is deleted from the domains of NSW andSA, leaving SAwith no legal values.

Token 4591:
variables next, and then all the other variables will fall into place.

Token 4592:
That’s exactly what hap- pens with MRV: NTandSAhave two values, so one of them is chosen ﬁrst, then the other, thenQ,NSW ,a n dVin order.

Token 4593:
Finally Tstill has three values, and any one of them works.

Token 4594:
We can view forward checking as an efﬁcient way to incrementally compute the informationthat the MRV heuristic needs to do its job.

Token 4595:
Although forward checking detects many inconsistencies, it does not detect all of them.

Token 4596:
The problem is that it makes the current variable arc-consistent, but doesn’t look ahead andmake all the other variables arc-consistent.

Token 4597:
For example, consider the third row of Figure 6.7.It shows that when WAisredandQisgreen , bothNTandSAare forced to be blue.

Token 4598:
Forward checking does not look far enough ahead to notice that this is an inconsistency: NTandSA are adjacent and so cannot have the same value.

Token 4599:
The algorithm called MAC (for Maintaining Arc Consistency (MAC) ) detects this MAINTAINING ARC CONSISTENCY (MAC) inconsistency.

Token 4600:
After a variable Xiis assigned a value, the I NFERENCE procedure calls AC-3, but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj,Xi)for all Xjthat are unassigned variables that are neighbors of Xi.

Token 4601:
From there, AC-3 does constraint propagation in the usual way, and if any variable has its domain reduced to the empty set, the call to AC-3 fails and we know to backtrack immediately.

Token 4602:
We can see that MAC is strictly more powerful than forward checking because forward checking does the same thing as MACon the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursivelypropagate constraints when changes are made to the domains of variables.

Token 4603:
6.3.3 Intelligent backtracking: Looking backward The B ACKTRACKING -SEARCH algorithm in Figure 6.5 has a very simple policy for what to do when a branch of the search fails: back up to the preceding variable and try a differentvalue for it.

Token 4604:
This is called chronological backtracking because the most recent decision CHRONOLOGICAL BACKTRACKING point is revisited.

Token 4605:
In this subsection, we consider better possibilities.

Token 4606:
Consider what happens when we apply simple backtracking in Figure 6.1 with a ﬁxed variable ordering Q,NSW ,V,T,SA,WA,NT.

Token 4607:
Suppose we have generated the partial assignment{Q=red,NSW =green,V=blue,T=red}.

Token 4608:
When we try the next variable, SA, we see that every value violates a constraint. We back up to Tand try a new color for

Token 4609:
Section 6.3. Backtracking Search for CSPs 219 Tasmania!

Token 4610:
Obviously this is silly—recoloring Tasmania cannot possibly resolve the problem with South Australia.

Token 4611:
A more intelligent approach to backtracking is to backtrack to a variable that might ﬁx the problem—a variable that was responsible for making one of the possible values of SA impossible.

Token 4612:
To do this, we will keep track of a set of assignments that are in conﬂict with some value for SA.

Token 4613:
The set (in this case {Q=red,NSW =green,V=blue,}), is called the conﬂict set forSA.T h e backjumping method backtracks to the most recent assignment in CONFLICT SET BACKJUMPING the conﬂict set; in this case, backjumping would jump over Tasmania and try a new value forV.

Token 4614:
This method is easily implemented by a modiﬁcation to B ACKTRACK such that it accumulates the conﬂict set while checking for a legal value to assign.

Token 4615:
If no legal value isfound, the algorithm should return the most recent element of the conﬂict set along with thefailure indicator.

Token 4616:
The sharp-eyed reader will have noticed that forward checking can supply the conﬂict set with no extra work: whenever forward checking based on an assignment X=xdeletes a value from Y’s domain, it should add X=xtoY’s conﬂict set.

Token 4617:
If the last value is deleted fromY’s domain, then the assignments in the conﬂict set of Yare added to the conﬂict set ofX.

Token 4618:
Then, when we get to Y, we know immediately where to backtrack if needed.

Token 4619:
The eagle-eyed reader will have noticed something odd: backjumping occurs when every value in a domain is in conﬂict with the current assignment; but forward checkingdetects this event and prevents the search from ever reaching such a node!

Token 4620:
In fact, it can beshown that every branch pruned by backjumping is also pruned by forward checking.

Token 4621:
Hence, simple backjumping is redundant in a forward-checking search or, indeed, in a search that uses stronger consistency checking, such as MAC.

Token 4622:
Despite the observations of the preceding paragraph, the idea behind backjumping re- mains a good one: to backtrack based on the reasons for failure.

Token 4623:
Backjumping notices failurewhen a variable’s domain becomes empty, but in many cases a branch is doomed long beforethis occurs.

Token 4624:
Consider again the partial assignment {WA=red,NSW =red}(which, from our earlier discussion, is inconsistent).

Token 4625:
Suppose we try T=rednext and then assign NT,Q, V,SA.

Token 4626:
We know that no assignment can work for these last four variables, so eventually we run out of values to try at NT.

Token 4627:
Now, the question is, where to backtrack?

Token 4628:
Backjumping cannot work, because NT does have values consistent with the preceding assigned variables— NT doesn’t have a complete conﬂict set of preceding variables that caused it to fail.

Token 4629:
We know,however, that the four variables NT,Q,V,a n dSA,taken together , failed because of a set of preceding variables, which must be those variables that directly conﬂict with the four.

Token 4630:
Thisleads to a deeper notion of the conﬂict set for a variable such as NT: it is that set of preced- ing variables that caused NT,together with any subsequent variables , to have no consistent solution.

Token 4631:
In this case, the set is WA andNSW , so the algorithm should backtrack to NSW and skip over Tasmania.

Token 4632:
A backjumping algorithm that uses conﬂict sets deﬁned in this wayis called conﬂict-directed backjumping .

Token 4633:
CONFLICT-DIRECTED BACKJUMPING We must now explain how these new conﬂict sets are computed. The method is in fact quite simple.

Token 4634:
The “terminal” failure of a branch of the search always occurs because avariable’s domain becomes empty; that variable has a standard conﬂict set.

Token 4635:
In our example,SAfails, and its conﬂict set is (say) {WA,NT,Q}. We backjump to Q,a n dQabsorbs

Token 4636:
220 Chapter 6.

Token 4637:
Constraint Satisfaction Problems the conﬂict set from SA(minus Qitself, of course) into its own direct conﬂict set, which is {NT,NSW}; the new conﬂict set is {WA,NT,NSW}.

Token 4638:
That is, there is no solution from Qonward, given the preceding assignment to {WA,NT,NSW}. Therefore, we backtrack toNT, the most recent of these.

Token 4639:
NT absorbs{WA,NT,NSW}−{NT}into its own direct conﬂict set {WA},g i v i n g{WA,NSW}(as stated in the previous paragraph).

Token 4640:
Now the algorithm backjumps to NSW , as we would hope. To summarize: let Xjbe the current variable, and let conf(Xj)be its conﬂict set.

Token 4641:
If every possible value for Xjfails, backjump to the most recent variable Xiinconf(Xj), and set conf(Xi)←conf(Xi)∪conf(Xj)−{Xi}.

Token 4642:
When we reach a contradiction, backjumping can tell us how far to back up, so we don’t waste time changing variables that won’t ﬁx the problem.

Token 4643:
But we would also like to avoid running into the same problem again.

Token 4644:
When the search arrives at a contradiction, we know that some subset of the conﬂict set is responsible for the problem.

Token 4645:
Constraint learning is theCONSTRAINT LEARNING idea of ﬁnding a minimum set of variables from the conﬂict set that causes the problem.

Token 4646:
This set of variables, along with their corresponding values, is called a no-good .

Token 4647:
We then record NO-GOOD the no-good, either by adding a new constraint to the CSP or by keeping a separate cache of no-goods.

Token 4648:
For example, consider the state {WA=red,NT=green,Q=blue}in the bottom row of Figure 6.6.

Token 4649:
Forward checking can tell us this state is a no-good because there is novalid assignment to SA.

Token 4650:
In this particular case, recording the no-good would not help, because once we prune this branch from the search tree, we will never encounter this combinationagain.

Token 4651:
But suppose that the search tree in Figure 6.6 were actually part of a larger search tree that started by ﬁrst assigning values for VandT.

Token 4652:
Then it would be worthwhile to record {WA=red,NT=green,Q=blue}as a no-good because we are going to run into the same problem again for each possible set of assignments to VandT.

Token 4653:
No-goods can be effectively used by forward checking or by backjumping.

Token 4654:
Constraint learning is one of the most important techniques used by modern CSP solvers to achieveefﬁciency on complex problems.

Token 4655:
6.4 L OCAL SEARCH FOR CSP S Local search algorithms (see Section 4.1) turn out to be effective in solving many CSPs.

Token 4656:
Theyuse a complete-state formulation: the initial state assigns a value to every variable, and the search changes the value of one variable at a time.

Token 4657:
For example, in the 8-queens problem (see Figure 4.3), the initial state might be a random conﬁguration of 8 queens in 8 columns, andeach step moves a single queen to a new position in its column.

Token 4658:
Typically, the initial guessviolates several constraints. The point of local search is to eliminate the violated constraints.

Token 4659:
2 In choosing a new value for a variable, the most obvious heuristic is to select the value that results in the minimum number of conﬂicts with other variables—the min-conﬂicts MIN-CONFLICTS 2Local search can easily be extended to constraint optimization problems (COPs).

Token 4660:
In that case, all the techniques for hill climbing and simulated annealing can be applied to optimize the objective function.

Token 4661:
Section 6.4.

Token 4662:
Local Search for CSPs 221 function MIN-CONFLICTS (csp,max steps )returns a solution or failure inputs :csp, a constraint satisfaction problem max steps , the number of steps allowed before giving up current←an initial complete assignment for csp fori=1t omax steps do ifcurrent is a solution for cspthen return current var←a randomly chosen conﬂicted variable from csp.VARIABLES value←the value vforvarthat minimizes C ONFLICTS (var,v,current ,csp) setvar=value incurrent return failure Figure 6.8 The M IN-CONFLICTS algorithm for solving CSPs by local search.

Token 4663:
The initial state may be chosen randomly or by a greedy assignment process that chooses a minimal- conﬂict value for each variable in turn.

Token 4664:
The C ONFLICTS function counts the number of constraints violated by a particular value, given the rest of the current assignment.

Token 4665:
2 2 1 23 1 23 3 2 32 30 Figure 6.9 A two-step solution using min-conﬂicts for an 8-queens problem.

Token 4666:
At each stage, a queen is chosen for reassignment in its column.

Token 4667:
The number of conﬂicts (in thiscase, the number of attacking queens) is shown in each square.

Token 4668:
The algorithm moves the queen to the min-conﬂicts square, breaking ties randomly. heuristic.

Token 4669:
The algorithm is shown in Figure 6.8 and its application to an 8-queens problem is diagrammed in Figure 6.9.

Token 4670:
Min-conﬂicts is surprisingly effective for many CSPs.

Token 4671:
Amazingly, on the n-queens problem, if you don’t count the initial placement of queens, the run time of min-conﬂicts isroughly independent of problem size .

Token 4672:
It solves even the million -queens problem in an aver- age of 50 steps (after the initial assignment).

Token 4673:
This remarkable observation was the stimulusleading to a great deal of research in the 1990s on local search and the distinction betweeneasy and hard problems, which we take up in Chapter 7.

Token 4674:
Roughly speaking, n-queens is easy for local search because solutions are densely distributed throughout the state space.

Token 4675:
Min-conﬂicts also works well for hard problems.

Token 4676:
For example, it has been used to scheduleobservations for the Hubble Space Telescope, reducing the time taken to schedule a week ofobservations from three weeks (!)

Token 4677:
to around 10 minutes.

Token 4678:
222 Chapter 6.

Token 4679:
Constraint Satisfaction Problems All the local search techniques from Section 4.1 are candidates for application to CSPs, and some of those have proved especially effective.

Token 4680:
The landscape of a CSP under the min-conﬂicts heuristic usually has a series of plateaux.

Token 4681:
There may be millions of variable as-signments that are only one conﬂict away from a solution.

Token 4682:
Plateau search—allowing side-ways moves to another state with the same score—can help local search ﬁnd its way off this plateau.

Token 4683:
This wandering on the plateau can be directed with tabu search : keeping a small list of recently visited states and forbidding the algorithm to return to those states.

Token 4684:
Simulatedannealing can also be used to escape from plateaux.

Token 4685:
Another technique, called constraint weighting , can help concentrate the search on the CONSTRAINT WEIGHTING important constraints.

Token 4686:
Each constraint is given a numeric weight, Wi, initially all 1.

Token 4687:
At each step of the search, the algorithm chooses a variable/value pair to change that will result in thelowest total weight of all violated constraints.

Token 4688:
The weights are then adjusted by incrementingthe weight of each constraint that is violated by the current assignment.

Token 4689:
This has two beneﬁts:it adds topography to plateaux, making sure that it is possible to improve from the currentstate, and it also, over time, adds weight to the constraints that are proving difﬁcult to solve.

Token 4690:
Another advantage of local search is that it can be used in an online setting when the problem changes.

Token 4691:
This is particularly important in scheduling problems.

Token 4692:
A week’s airline schedule may involve thousands of ﬂights and tens of thousands of personnel assignments, but bad weather at one airport can render the schedule infeasible.

Token 4693:
We would like to repair theschedule with a minimum number of changes.

Token 4694:
This can be easily done with a local searchalgorithm starting from the current schedule.

Token 4695:
A backtracking search with the new set ofconstraints usually requires much more time and might ﬁnd a solution with many changesfrom the current schedule.

Token 4696:
6.5 T HESTRUCTURE OF PROBLEMS In this section, we examine ways in which the structure of the problem, as represented by the constraint graph, can be used to ﬁnd solutions quickly.

Token 4697:
Most of the approaches here alsoapply to other problems besides CSPs, such as probabilistic reasoning.

Token 4698:
After all, the only way we can possibly hope to deal with the real world is to decompose it into many subproblems.

Token 4699:
Looking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)),one fact stands out: Tasmania is not connected to the mainland.

Token 4700:
3Intuitively, it is obvious that coloring Tasmania and coloring the mainland are independent subproblems —any solutionINDEPENDENT SUBPROBLEMS for the mainland combined with any solution for Tasmania yields a solution for the whole map.

Token 4701:
Independence can be ascertained simply by ﬁnding connected components of theCONNECTED COMPONENT constraint graph.

Token 4702:
Each component corresponds to a subproblem CSP i. If assignment Siis a solution of CSP i,t h e n/uniontext iSiis a solution of/uniontext iCSP i.

Token 4703:
Why is this important? Consider the following: suppose each CSP ihascvariables from the total of nvariables, where cis a constant.

Token 4704:
Then there are n/csubproblems, each of which takes at most dcwork to solve, 3A careful cartographer or patriotic Tasmanian might object that Tasmania should not be colored the same as its nearest mainland neighbor, to avoid the impression that it might be part of that state.

Token 4705:
Section 6.5. The Structure of Problems 223 where dis the size of the domain.

Token 4706:
Hence, the total work is O(dcn/c),w h i c hi s linear inn; without the decomposition, the total work is O(dn), which is exponential in n.L e t ’ s m a k e this more concrete: dividing a Boolean CSP with 80 variables into four subproblems reducesthe worst-case solution time from the lifetime of the universe down to less than a second.

Token 4707:
Completely independent subproblems are delicious, then, but rare. Fortunately, some other graph structures are also easy to solve.

Token 4708:
For example, a constraint graph is a tree when any two variables are connected by only one path.

Token 4709:
We show that any tree-structured CSP can be solved in time linear in the number of variables.4The key is a new notion of consistency, called directed arc consistency or DAC.

Token 4710:
A CSP is deﬁned to be directed arc-consistent underDIRECTED ARC CONSISTENCY an ordering of variables X1,X2,...,X nif and only if every Xiis arc-consistent with each Xjforj>i .

Token 4711:
To solve a tree-structured CSP, ﬁrst pick any variable to be the root of the tree, and choose an ordering of the variables such that each variable appears after its parent in the tree.Such an ordering is called a topological sort .

Token 4712:
Figure 6.10(a) shows a sample tree and (b) TOPOLOGICAL SORT shows one possible ordering.

Token 4713:
Any tree with nnodes has n−1arcs, so we can make this graph directed arc-consistent in O(n)steps, each of which must compare up to dpossible domain values for two variables, for a total time of O(nd2).

Token 4714:
Once we have a directed arc-consistent graph, we can just march down the list of variables and choose any remaining value.

Token 4715:
Since each link from a parent to its child is arc consistent, we know that for any value we choose forthe parent, there will be a valid value left to choose for the child.

Token 4716:
That means we won’t haveto backtrack; we can move linearly through the variables. The complete algorithm is shownin Figure 6.11.

Token 4717:
A CBDE F (a)ACBD E F (b) Figure 6.10 (a) The constraint graph of a tree-structured CSP.

Token 4718:
(b) A linear ordering of the variables consistent with the tree with Aas the root. This is known as a topological sort of the variables.

Token 4719:
Now that we have an efﬁcient algorithm for trees, we can consider whether more general constraint graphs can be reduced to trees somehow.

Token 4720:
There are two primary ways to do this, one based on removing nodes and one based on collapsing nodes together.

Token 4721:
The ﬁrst approach involves assigning values to some variables so that the remaining variables form a tree.

Token 4722:
Consider the constraint graph for Australia, shown again in Fig-ure 6.12(a).

Token 4723:
If we could delete South Australia, the graph would become a tree, as in (b).Fortunately, we can do this (in the graph, not the continent) by ﬁxing a value for SAand 4Sadly, very few regions of the world have tree-structured maps, although Sulawesi comes close.

Token 4724:
224 Chapter 6.

Token 4725:
Constraint Satisfaction Problems function TREE-CSP-S OLVER (csp)returns a solution, or failure inputs :csp, a CSP with components X, D, C n←number of variables in X assignment←an empty assignment root←any variable in X X←TOPOLOGICAL SORT(X,root) forj=ndown to 2do MAKE-ARC-CONSISTENT (PARENT (Xj),Xj) ifit cannot be made consistent then return failure fori=1tondo assignment [Xi]←any consistent value from Di ifthere is no consistent value then return failure return assignment Figure 6.11 The T REE-CSP-S OLVER algorithm for solving tree-structured CSPs.

Token 4726:
If the CSP has a solution, we will ﬁnd it in linear time; if not, we will detect a contradiction.

Token 4727:
WANT SAQ NSW V TWANT Q NSW V T (a) (b) Figure 6.12 (a) The original constraint graph from Figure 6.1.

Token 4728:
(b) The constraint graph after the removal of SA.

Token 4729:
deleting from the domains of the other variables any values that are inconsistent with the value chosen for SA.

Token 4730:
Now, any solution for the CSP after SAand its constraints are removed will be con- sistent with the value chosen for SA.

Token 4731:
(This works for binary CSPs; the situation is more complicated with higher-order constraints.)

Token 4732:
Therefore, we can solve the remaining tree with the algorithm given above and thus solve the whole problem.

Token 4733:
Of course, in the general case(as opposed to map coloring), the value chosen for SAcould be the wrong one, so we would need to try each possible value.

Token 4734:
The general algorithm is as follows:

Token 4735:
Section 6.5. The Structure of Problems 225 1.

Token 4736:
Choose a subset Sof the CSP’s variables such that the constraint graph becomes a tree after removal of S.Sis called a cycle cutset . CYCLE CUTSET 2.

Token 4737:
For each possible assignment to the variables in Sthat satisﬁes all constraints on S, (a) remove from the domains of the remaining variables any values that are inconsis- tent with the assignment for S,a n d (b) If the remaining CSP has a solution, return it together with the assignment for S. If the cycle cutset has size c, then the total run time is O(dc·(n−c)d2): we have to try each of the dccombinations of values for the variables in S, and for each combination we must solve a tree problem of size n−c.

Token 4738:
If the graph is “nearly a tree,” then cwill be small and the savings over straight backtracking will be huge.

Token 4739:
In the worst case, however, ccan be as large as(n−2).

Token 4740:
Finding the smallest cycle cutset is NP-hard, but several efﬁcient approximation algorithms are known.

Token 4741:
The overall algorithmic approach is called cutset conditioning ;i tCUTSET CONDITIONING comes up again in Chapter 14, where it is used for reasoning about probabilities.

Token 4742:
The second approach is based on constructing a tree decomposition of the constraintTREE DECOMPOSITION graph into a set of connected subproblems.

Token 4743:
Each subproblem is solved independently, and the resulting solutions are then combined.

Token 4744:
Like most divide-and-conquer algorithms, this workswell if no subproblem is too large.

Token 4745:
Figure 6.13 shows a tree decomposition of the map-coloring problem into ﬁve subproblems.

Token 4746:
A tree decomposition must satisfy the following three requirements: •Every variable in the original problem appears in at least one of the subproblems.

Token 4747:
•If two variables are connected by a constraint in the original problem, they must appear together (along with the constraint) in at least one of the subproblems.

Token 4748:
•If a variable appears in two subproblems in the tree, it must appear in every subproblem along the path connecting those subproblems.

Token 4749:
The ﬁrst two conditions ensure that all the variables and constraints are represented in the decomposition.

Token 4750:
The third condition seems rather technical, but simply reﬂects the constraintthat any given variable must have the same value in every subproblem in which it appears; the links joining subproblems in the tree enforce this constraint.

Token 4751:
For example, SAappears in all four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 thatthis decomposition makes sense.

Token 4752:
We solve each subproblem independently; if any one has no solution, we know the en- tire problem has no solution.

Token 4753:
If we can solve all the subproblems, then we attempt to construct a global solution as follows.

Token 4754:
First, we view each subproblem as a “mega-variable” whose do- main is the set of all solutions for the subproblem.

Token 4755:
For example, the leftmost subproblem in Figure 6.13 is a map-coloring problem with three variables and hence has six solutions—oneis{WA=red,SA=blue,NT=green}.

Token 4756:
Then, we solve the constraints connecting the subproblems, using the efﬁcient algorithm for trees given earlier.

Token 4757:
The constraints betweensubproblems simply insist that the subproblem solutions agree on their shared variables.

Token 4758:
For example, given the solution {WA=red,SA=blue,NT=green}for the ﬁrst subproblem, the only consistent solution for the next subproblem is {SA=blue,NT=green,Q=red}.

Token 4759:
A given constraint graph admits many tree decompositions; in choosing a decompo- sition, the aim is to make the subproblems as small as possible.

Token 4760:
The tree width of a tree TREE WIDTH

Token 4761:
226 Chapter 6.

Token 4762:
Constraint Satisfaction Problems TWANT SANT SAQ SAQ NSW SA NSW V Figure 6.13 A tree decomposition of the constraint graph in Figure 6.12(a).

Token 4763:
decomposition of a graph is one less than the size of the largest subproblem; the tree width of the graph itself is deﬁned to be the minimum tree width among all its tree decompositions.If a graph has tree width wand we are given the corresponding tree decomposition, then the problem can be solved in O(nd w+1)time.

Token 4764:
Hence, CSPs with constraint graphs of bounded tree width are solvable in polynomial time.

Token 4765:
Unfortunately, ﬁnding the decomposition with minimal tree width is NP-hard, but there are heuristic methods that work well in practice.

Token 4766:
So far, we have looked at the structure of the constraint graph. There can be important structure in the values of variables as well.

Token 4767:
Consider the map-coloring problem with ncolors.

Token 4768:
For every consistent solution, there is actually a set of n!solutions formed by permuting the color names.

Token 4769:
For example, on the Australia map we know that WA,NT,a n dSAmust all have different colors, but there are 3!

Token 4770:
= 6 ways to assign the three colors to these three regions. This is called value symmetry .

Token 4771:
We would like to reduce the search space by a factor of VALUE SYMMETRY n!by breaking the symmetry.

Token 4772:
We do this by introducing a symmetry-breaking constraint .SYMMETRY- BREAKING CONSTRAINTFor our example, we might impose an arbitrary ordering constraint, NT<SA<WA,t h a t requires the three values to be in alphabetical order.

Token 4773:
This constraint ensures that only one ofthen!solutions is possible: {NT=blue,SA=green,WA=red}.

Token 4774:
For map coloring, it was easy to ﬁnd a constraint that eliminates the symmetry, and in general it is possible to ﬁnd constraints that eliminate all but one symmetric solution in polynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of values during search.

Token 4775:
In practice, breaking value symmetry has proved to be important and effective on a wide range of problems.

Token 4776:
Section 6.6.

Token 4777:
Summary 227 6.6 S UMMARY •Constraint satisfaction problems (CSPs) represent a state with a set of variable/value pairs and represent the conditions for a solution by a set of constraints on the variables.Many important real-world problems can be described as CSPs.

Token 4778:
•A number of inference techniques use the constraints to infer which variable/value pairs are consistent and which are not.

Token 4779:
These include node, arc, path, and k-consistency. •Backtracking search , a form of depth-ﬁrst search, is commonly used for solving CSPs.

Token 4780:
Inference can be interwoven with search.

Token 4781:
•Theminimum-remaining-values anddegree heuristics are domain-independent meth- ods for deciding which variable to choose next in a backtracking search.

Token 4782:
The least- constraining-value heuristic helps in deciding which value to try ﬁrst for a given variable.

Token 4783:
Backtracking occurs when no legal assignment can be found for a variable.

Token 4784:
Conﬂict-directed backjumping backtracks directly to the source of the problem.

Token 4785:
•Local search using the min-conﬂicts heuristic has also been applied to constraint satis- faction problems with great success.

Token 4786:
•The complexity of solving a CSP is strongly related to the structure of its constraint graph. Tree-structured problems can be solved in linear time.

Token 4787:
Cutset conditioning can reduce a general CSP to a tree-structured one and is quite efﬁcient if a small cutset canbe found.

Token 4788:
Tree decomposition techniques transform the CSP into a tree of subproblems and are efﬁcient if the tree width of the constraint graph is small.

Token 4789:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The earliest work related to constraint satisfaction dealt largely with numerical constraints.

Token 4790:
Equational constraints with integer domains were studied by the Indian mathematician Brah- magupta in the seventh century; they are often called Diophantine equations , after the GreekDIOPHANTINE EQUATIONS mathematician Diophantus (c. 200–284), who actually considered the domain of positive ra- tionals.

Token 4791:
Systematic methods for solving linear equations by variable elimination were studiedby Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).

Token 4792:
Finite-domain constraint satisfaction problems also have a long history.

Token 4793:
For example, graph coloring (of which map coloring is a special case) is an old problem in mathematics.

Token 4794:
GRAPH COLORING The four-color conjecture (that every planar graph can be colored with four or fewer colors) was ﬁrst made by Francis Guthrie, a student of De Morgan, in 1852.

Token 4795:
It resisted solution—despite several published claims to the contrary—until a proof was devised by Appel andHaken (1977) (see the book Four Colors Sufﬁce (Wilson, 2004)).

Token 4796:
Purists were disappointed that part of the proof relied on a computer, so Georges Gonthier (2008), using the C OQ theorem prover, derived a formal proof that Appel and Haken’s proof was correct.

Token 4797:
Speciﬁc classes of constraint satisfaction problems occur throughout the history of computer science.

Token 4798:
One of the most inﬂuential early examples was the S KETCHPAD sys-

Token 4799:
228 Chapter 6.

Token 4800:
Constraint Satisfaction Problems tem (Sutherland, 1963), which solved geometric constraints in diagrams and was the fore- runner of modern drawing programs and CAD tools.

Token 4801:
The identiﬁcation of CSPs as a general class is due to Ugo Montanari (1974).

Token 4802:
The reduction of higher-order CSPs to purely binaryCSPs with auxiliary variables (see Exercise 6.6) is due originally to the 19th-century logicianCharles Sanders Peirce.

Token 4803:
It was introduced into the CSP literature by Dechter (1990b) and was elaborated by Bacchus and van Beek (1998).

Token 4804:
CSPs with preferences among solutions are studied widely in the optimization literature; see Bistarelli et al.

Token 4805:
(1997) for a generalization of the CSP framework to allow for preferences.

Token 4806:
The bucket-elimination algorithm (Dechter,1999) can also be applied to optimization problems.

Token 4807:
Constraint propagation methods were popularized by Waltz’s (1975) success on poly- hedral line-labeling problems for computer vision.

Token 4808:
Waltz showed that, in many problems,propagation completely eliminates the need for backtracking.

Token 4809:
Montanari (1974) introducedthe notion of constraint networks and propagation by path consistency.

Token 4810:
Alan Mackworth(1977) proposed the AC-3 algorithm for enforcing arc consistency as well as the general ideaof combining backtracking with some degree of consistency enforcement.

Token 4811:
AC-4, a moreefﬁcient arc-consistency algorithm, was developed by Mohr and Henderson (1986).

Token 4812:
Soon af-ter Mackworth’s paper appeared, researchers began experimenting with the tradeoff between the cost of consistency enforcement and the beneﬁts in terms of search reduction.

Token 4813:
Haralick and Elliot (1980) favored the minimal forward-checking algorithm described by McGregor(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each vari-able assignment—an algorithm later called MAC by Sabin and Freuder (1994).

Token 4814:
The latterpaper provides somewhat convincing evidence that, on harder CSPs, full arc-consistencychecking pays off.

Token 4815:
Freuder (1978, 1982) investigated the notion of k-consistency and its relationship to the complexity of solving CSPs.

Token 4816:
Apt (1999) describes a generic algorithmicframework within which consistency propagation algorithms can be analyzed, and Bessi` ere (2006) presents a current survey.

Token 4817:
Special methods for handling higher-order or global constraints were developed ﬁrst within the context of constraint logic programming .

Token 4818:
Marriott and Stuckey (1998) provide excellent coverage of research in this area.

Token 4819:
The Alldiﬀ constraint was studied by Regin (1994), Stergiou and Walsh (1999), and van Hoeve (2001).

Token 4820:
Bounds constraints were incorpo-rated into constraint logic programming by Van Hentenryck et al. (1998).

Token 4821:
A survey of global constraints is provided by van Hoeve and Katriel (2006).

Token 4822:
Sudoku has become the most widely known CSP and was described as such by Simonis (2005).

Token 4823:
Agerbeck and Hansen (2008) describe some of the strategies and show that Sudokuon an n 2×n2board is in the class of NP-hard problems. Reeson et al.

Token 4824:
(2007) show an interactive solver based on CSP techniques.

Token 4825:
The idea of backtracking search goes back to Golomb and Baumert (1965), and its application to constraint satisfaction is due to Bitner and Reingold (1975), although they tracethe basic algorithm back to the 19th century.

Token 4826:
Bitner and Reingold also introduced the MRV heuristic, which they called the most-constrained-variable heuristic.

Token 4827:
Brelaz (1979) used the degree heuristic as a tiebreaker after applying the MRV heuristic.

Token 4828:
The resulting algorithm,despite its simplicity, is still the best method for k-coloring arbitrary graphs.

Token 4829:
Haralick and Elliot (1980) proposed the least-constraining-value heuristic.

Token 4830:
Bibliographical and Historical Notes 229 The basic backjumping method is due to John Gaschnig (1977, 1979).

Token 4831:
Kondrak and van Beek (1997) showed that this algorithm is essentially subsumed by forward checking.Conﬂict-directed backjumping was devised by Prosser (1993).

Token 4832:
The most general and pow-erful form of intelligent backtracking was actually developed very early on by Stallman andSussman (1977).

Token 4833:
Their technique of dependency-directed backtracking led to the develop- DEPENDENCY- DIRECTEDBACKTRACKING ment of truth maintenance systems (Doyle, 1979), which we discuss in Section 12.6.2.

Token 4834:
The connection between the two areas is analyzed by de Kleer (1989).

Token 4835:
The work of Stallman and Sussman also introduced the idea of constraint learning , in which partial results obtained by search can be saved and reused later in the search.

Token 4836:
Theidea was formalized Dechter (1990a).

Token 4837:
Backmarking (Gaschnig, 1979) is a particularly sim- BACKMARKING ple method in which consistent and inconsistent pairwise assignments are saved and used to avoid rechecking constraints.

Token 4838:
Backmarking can be combined with conﬂict-directed back-jumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumeseither method taken separately.

Token 4839:
The method of dynamic backtracking (Ginsberg, 1993) re- DYNAMIC BACKTRACKING tains successful partial assignments from later subsets of variables when backtracking over an earlier choice that does not invalidate the later success.

Token 4840:
Empirical studies of several randomized backtracking methods were done by Gomes et al. (2000) and Gomes and Selman (2001).

Token 4841:
Van Beek (2006) surveys backtracking. Local search in constraint satisfaction problems was popularized by the work of Kirk- patrick et al.

Token 4842:
(1983) on simulated annealing (see Chapter 4), which is widely used for schedul- ing problems.

Token 4843:
The min-conﬂicts heuristic was ﬁrst proposed by Gu (1989) and was developedindependently by Minton et al. (1992).

Token 4844:
Sosic and Gu (1994) showed how it could be applied to solve the 3,000,000 queens problem in less than a minute.

Token 4845:
The astounding success of local search using min-conﬂicts on the n-queens problem led to a reappraisal of the nature and prevalence of “easy” and “hard” problems.

Token 4846:
Peter Cheeseman et al.

Token 4847:
(1991) explored the difﬁculty of randomly generated CSPs and discovered that almost all such problems eitherare trivially easy or have no solutions.

Token 4848:
Only if the parameters of the problem generator areset in a certain narrow range, within which roughly half of the problems are solvable, do we ﬁnd “hard” problem instances.

Token 4849:
We discuss this phenomenon further in Chapter 7.

Token 4850:
Konolige (1994) showed that local search is inferior to backtracking search on problems with a certaindegree of local structure; this led to work that combined local search and inference, such asthat by Pinkas and Dechter (1995).

Token 4851:
Hoos and Tsang (2006) survey local search techniques.

Token 4852:
Work relating the structure and complexity of CSPs originates with Freuder (1985), who showed that search on arc consistent trees works without any backtracking.

Token 4853:
A similar result,with extensions to acyclic hypergraphs, was developed in the database community (Beeriet al. , 1983).

Token 4854:
Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs that runs in linear time without any preprocessing.

Token 4855:
Since those papers were published, there has been a great deal of progress in developing more general results relating the complexity of solving a CSP to the structure of its constraint graph.

Token 4856:
The notion of tree width was introduced by the graph theorists Robertson and Seymour (1986).

Token 4857:
Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a relatednotion (which they called induced width ) to constraint satisfaction problems and developed the tree decomposition approach sketched in Section 6.5.

Token 4858:
Drawing on this work and on results

Token 4859:
230 Chapter 6. Constraint Satisfaction Problems from database theory, Gottlob et al.

Token 4860:
(1999a, 1999b) developed a notion, hypertree width ,t h a t is based on the characterization of the CSP as a hypergraph.

Token 4861:
In addition to showing that anyCSP with hypertree width wcan be solved in time O(n w+1logn), they also showed that hypertree width subsumes all previously deﬁned measures of “width” in the sense that thereare cases where the hypertree width is bounded and the other measures are unbounded.

Token 4862:
Interest in look-back approaches to backtracking was rekindled by the work of Bayardo and Schrag (1997), whose R ELSAT algorithm combined constraint learning and backjumping and was shown to outperform many other algorithms of the time.

Token 4863:
This led to AND/ORsearch algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Ma-teescu, 2007). Brown et al.

Token 4864:
(1988) introduce the idea of symmetry breaking in CSPs, and Gent et al. (2006) give a recent survey.

Token 4865:
The ﬁeld of distributed constraint satisfaction looks at solving CSPs when there is a DISTRIBUTED CONSTRAINTSATISFACTION collection of agents, each of which controls a subset of the constraint variables.

Token 4866:
There have been annual workshops on this problem since 2000, and good coverage elsewhere (Collinet al. , 1999; Pearce et al.

Token 4867:
, 2008; Shoham and Leyton-Brown, 2009).

Token 4868:
Comparing CSP algorithms is mostly an empirical science: few theoretical results show that one algorithm dominates another on all problems; instead, we need to run experiments to see which algorithms perform better on typical instances of problems.

Token 4869:
As Hooker (1995) points out, we need to be careful to distinguish between competitive testing—as occurs incompetitions among algorithms based on run time—and scientiﬁc testing, whose goal is toidentify the properties of an algorithm that determine its efﬁcacy on a class of problems.

Token 4870:
The recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi et al. (2006) are excellent resources on constraint processing.

Token 4871:
There are several good earlier surveys, including those by Kumar (1992), Dechter and Frost (2002), and Bartak (2001); andthe encyclopedia articles by Dechter (1992) and Mackworth (1992).

Token 4872:
Pearson and Jeavons(1997) survey tractable classes of CSPs, covering both structural decomposition methodsand methods that rely on properties of the domains or constraints themselves.

Token 4873:
Kondrak andvan Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus and van Run (1995) give a more empirical survey.

Token 4874:
Constraint programming is covered in the books by Apt (2003) and Fruhwirth and Abdennadher (2003).

Token 4875:
Several interesting applicationsare described in the collection edited by Freuder and Mackworth (1994).

Token 4876:
Papers on constraintsatisfaction appear regularly in Artiﬁcial Intelligence and in the specialist journal Constraints .

Token 4877:
The primary conference venue is the International Conference on Principles and Practice ofConstraint Programming, often called CP.

Token 4878:
EXERCISES 6.1 How many solutions are there for the map-coloring problem in Figure 6.1? How many solutions if four colors are allowed? Two colors?

Token 4879:
6.2 Consider the problem of placing kknights on an n×nchessboard such that no two knights are attacking each other, where kis given and k≤n2.

Token 4880:
Exercises 231 a. Choose a CSP formulation. In your formulation, what are the variables? b. What are the possible values of each variable?

Token 4881:
c. What sets of variables are constrained, and how?

Token 4882:
d. Now consider the problem of putting as many knights as possible on the board with- out any attacks.

Token 4883:
Explain how to solve this with local search by deﬁning appropriate ACTIONS and R ESULT functions and a sensible objective function.

Token 4884:
6.3 Consider the problem of constructing (not solving) crossword puzzles:5ﬁtting words into a rectangular grid.

Token 4885:
The grid, which is given as part of the problem, speciﬁes whichsquares are blank and which are shaded.

Token 4886:
Assume that a list of words (i.e., a dictionary)is provided and that the task is to ﬁll in the blank squares by using any subset of the list.

Token 4887:
Formulate this problem precisely in two ways: a. As a general search problem. Choose an appropriate search algorithm and specify a heuristic function.

Token 4888:
Is it better to ﬁll in blanks one letter at a time or one word at a time? b. As a constraint satisfaction problem.

Token 4889:
Should the variables be words or letters? Which formulation do you think will be better? Why?

Token 4890:
6.4 Give precise formulations for each of the following as constraint satisfaction problems: a. Rectilinear ﬂoor-planning: ﬁnd non-overlapping places in a large rectangle for a number of smaller rectangles.

Token 4891:
b.

Token 4892:
Class scheduling: There is a ﬁxed number of professors and classrooms, a list of classes to be offered, and a list of possible time slots for classes.

Token 4893:
Each professor has a set ofclasses that he or she can teach.

Token 4894:
c. Hamiltonian tour: given a network of cities connected by roads, choose an order to visit all cities in a country without repeating any.

Token 4895:
6.5 Solve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back- tracking with forward checking and the MRV and least-constraining-value heuristics.

Token 4896:
6.6 Show how a single ternary constraint such as “ A+B=C” can be turned into three binary constraints by using an auxiliary variable.

Token 4897:
You may assume ﬁnite domains.

Token 4898:
( Hint: Consider a new variable that takes on values that are pairs of other values, and considerconstraints such as “ Xis the ﬁrst element of the pair Y.”) Next, show how constraints with more than three variables can be treated similarly.

Token 4899:
Finally, show how unary constraints can be eliminated by altering the domains of variables.

Token 4900:
This completes the demonstration that any CSP can be transformed into a CSP with only binary constraints.

Token 4901:
6.7 Consider the following logic puzzle: In ﬁve houses, each with a different color, live ﬁve persons of different nationalities, each of whom prefers a different brand of candy, a differentdrink, and a different pet.

Token 4902:
Given the following facts, the questions to answer are “Where doesthe zebra live, and in which house do they drink water?” 5Ginsberg et al.

Token 4903:
(1990) discuss several methods for cons tructing crossword puzzles. Littman et al. (1999) tackle the harder problem of solving them.

Token 4904:
232 Chapter 6. Constraint Satisfaction Problems The Englishman lives in the red house.

Token 4905:
The Spaniard owns the dog.The Norwegian lives in the ﬁrst house on the left. The green house is immediately to the right of the ivory house.

Token 4906:
The man who eats Hershey bars lives in the house next to the man with the fox. Kit Kats are eaten in the yellow house.

Token 4907:
The Norwegian lives next to the blue house.The Smarties eater owns snails.The Snickers eater drinks orange juice.The Ukrainian drinks tea.The Japanese eats Milky Ways.Kit Kats are eaten in a house next to the house where the horse is kept.Coffee is drunk in the green house.Milk is drunk in the middle house.

Token 4908:
Discuss different representations of this problem as a CSP. Why would one prefer one repre- sentation over another?

Token 4909:
6.8 Consider the graph with 8 nodes A 1,A2,A3,A4,H,T,F1,F2.Aiis connected to Ai+1for all i, each Aiis connected to H,His connected to T,a n dTis connected to each Fi.

Token 4910:
Find a 3-coloring of this graph by hand using the following strategy: backtracking with conﬂict-directed backjumping, the variable order A1,H,A4,F1,A2,F2,A3,T,a n dt h e value order R,G,B.

Token 4911:
6.9 Explain why it is a good heuristic to choose the variable that is most constrained but the value that is least constraining in a CSP search.

Token 4912:
6.10 Generate random instances of map-coloring problems as follows: scatter npoints on the unit square; select a point Xat random, connect Xby a straight line to the nearest point Ysuch that Xis not already connected to Yand the line crosses no other line; repeat the previous step until no more connections are possible.

Token 4913:
The points represent regions on the map and the lines connect neighbors.

Token 4914:
Now try to ﬁnd k-colorings of each map, for both k=3andk=4, using min-conﬂicts, backtracking, backtracking with forward checking, and backtracking with MAC.

Token 4915:
Construct a table of average run times for each algorithm for valuesofnup to the largest you can manage. Comment on your results.

Token 4916:
6.11 Use the AC-3 algorithm to show that arc consistency can detect the inconsistency of the partial assignment {WA=green,V=red}for the problem shown in Figure 6.1.

Token 4917:
6.12 What is the worst-case complexity of running AC-3 on a tree-structured CSP?

Token 4918:
6.13 AC-3 puts back on the queue every arc (X k,Xi) whenever anyvalue is deleted from the domain of Xi, even if each value of Xkis consistent with several remaining values of Xi.

Token 4919:
Suppose that, for every arc ( Xk,Xi), we keep track of the number of remaining values of Xi that are consistent with each value of Xk.

Token 4920:
Explain how to update these numbers efﬁciently and hence show that arc consistency can be enforced in total time O(n2d2).

Token 4921:
Exercises 233 6.14 The T REE-CSP-S OLVER (Figure 6.10) makes arcs consistent starting at the leaves and working backwards towards the root.

Token 4922:
Why does it do that? What would happen if it went inthe opposite direction?

Token 4923:
6.15 We introduced Sudoku as a CSP to be solved by search over partial assignments be- cause that is the way people generally undertake solving Sudoku problems.

Token 4924:
It is also possible,of course, to attack these problems with local search over complete assignments.

Token 4925:
How wellwould a local solver using the min-conﬂicts heuristic do on Sudoku problems?

Token 4926:
6.16 Deﬁne in your own words the terms constraint, backtracking search, arc consistency, backjumping, min-conﬂicts, and cycle cutset.

Token 4927:
6.17 Suppose that a graph is known to have a cycle cutset of no more than knodes.

Token 4928:
Describe a simple algorithm for ﬁnding a minimal cycle cutset whose run time is not much more than O(nk)for a CSP with nvariables.

Token 4929:
Search the literature for methods for ﬁnding approximately minimal cycle cutsets in time that is polynomial in the size of the cutset.

Token 4930:
Does the existenceof such algorithms make the cycle cutset method practical?

Token 4931:


Token 4932:
7LOGICAL AGENTS In which we design agents that can form representations of a complex world, use a process of inference to derive new representations about the world, and use thesenew representations to deduce what to do.

Token 4933:
Humans, it seems, know things; and what they know helps them do things. These are not empty statements.

Token 4934:
They make strong claims about how the intelligence of humans is achieved—not by purely reﬂex mechanisms but by processes of reasoning that operate on REASONING internal representations of knowledge.

Token 4935:
In AI, this approach to intelligence is embodied in REPRESENTATION knowledge-based agents .KNOWLEDGE-BASED AGENTS The problem-solving agents of Chapters 3 and 4 know things, but only in a very limited, inﬂexible sense.

Token 4936:
For example, the transition model for the 8-puzzle—knowledge of what theactions do—is hidden inside the domain-speciﬁc code of the R ESULT function.

Token 4937:
It can be used to predict the outcome of actions but not to deduce that two tiles cannot occupy thesame space or that states with odd parity cannot be reached from states with even parity.

Token 4938:
Theatomic representations used by problem-solving agents are also very limiting.

Token 4939:
In a partiallyobservable environment, an agent’s only choice for representing what it knows about the current state is to list all possible concrete states—a hopeless prospect in large environments.

Token 4940:
Chapter 6 introduced the idea of representing states as assignments of values to vari- ables; this is a step in the right direction, enabling some parts of the agent to work in adomain-independent way and allowing for more efﬁcient algorithms.

Token 4941:
In this chapter andthose that follow, we take this step to its logical conclusion, so to speak—we develop logic LOGIC as a general class of representations to support knowledge-based agents.

Token 4942:
Such agents can combine and recombine information to suit myriad purposes.

Token 4943:
Often, this process can be quite far removed from the needs of the moment—as when a mathematician proves a theorem oran astronomer calculates the earth’s life expectancy.

Token 4944:
Knowledge-based agents can accept newtasks in the form of explicitly described goals; they can achieve competence quickly by beingtold or learning new knowledge about the environment; and they can adapt to changes in the environment by updating the relevant knowledge.

Token 4945:
We begin in Section 7.1 with the overall agent design.

Token 4946:
Section 7.2 introduces a sim- ple new environment, the wumpus world, and illustrates the operation of a knowledge-basedagent without going into any technical detail.

Token 4947:
Then we explain the general principles of logic 234

Token 4948:
Section 7.1. Knowledge-Based Agents 235 in Section 7.3 and the speciﬁcs of propositional logic in Section 7.4.

Token 4949:
While less expressive than ﬁrst-order logic (Chapter 8), propositional logic illustrates all the basic concepts of logic; it also comes with well-developed inference technologies, which we describe in sec-tions 7.5 and 7.6.

Token 4950:
Finally, Section 7.7 combines the concept of knowledge-based agents withthe technology of propositional logic to build some simple agents for the wumpus world.

Token 4951:
7.1 K NOWLEDGE -BASED AGENTS The central component of a knowledge-based agent is its knowledge base , or KB.

Token 4952:
A knowl- KNOWLEDGE BASE edge base is a set of sentences . (Here “sentence” is used as a technical term.

Token 4953:
It is related SENTENCE but not identical to the sentences of English and other natural languages.)

Token 4954:
Each sentence is expressed in a language called a knowledge representation language and represents someKNOWLEDGE REPRESENTATIONLANGUAGE assertion about the world.

Token 4955:
Sometimes we dignify a sentence with the name axiom , when the AXIOM sentence is taken as given without being derived from other sentences.

Token 4956:
There must be a way to add new sentences to the knowledge base and a way to query what is known.

Token 4957:
The standard names for these operations are T ELL and A SK, respectively.

Token 4958:
Both operations may involve inference —that is, deriving new sentences from old.

Token 4959:
Inference INFERENCE must obey the requirement that when one A SKs a question of the knowledge base, the answer should follow from what has been told (or T ELLed) to the knowledge base previously.

Token 4960:
Later in this chapter, we will be more precise about the crucial word “follow.” For now, take it tomean that the inference process should not make things up as it goes along.

Token 4961:
Figure 7.1 shows the outline of a knowledge-based agent program. Like all our agents, it takes a percept as input and returns an action.

Token 4962:
The agent maintains a knowledge base, KB, which may initially contain some background knowledge .

Token 4963:
BACKGROUND KNOWLEDGE Each time the agent program is called, it does three things. First, it T ELLs the knowl- edge base what it perceives.

Token 4964:
Second, it A SKs the knowledge base what action it should perform.

Token 4965:
In the process of answering this query, extensive reasoning may be done about the current state of the world, about the outcomes of possible action sequences, and so on.Third, the agent program T ELLs the knowledge base which action was chosen, and the agent executes the action.

Token 4966:
The details of the representation language are hidden inside three functions that imple- ment the interface between the sensors and actuators on one side and the core representationand reasoning system on the other.

Token 4967:
M AKE-PERCEPT -SENTENCE constructs a sentence as- serting that the agent perceived the given percept at the given time.

Token 4968:
M AKE-ACTION -QUERY constructs a sentence that asks what action should be done at the current time.

Token 4969:
Finally, MAKE-ACTION -SENTENCE constructs a sentence asserting that the chosen action was ex- ecuted.

Token 4970:
The details of the inference mechanisms are hidden inside T ELL and A SK. Later sections will reveal these details.

Token 4971:
The agent in Figure 7.1 appears quite similar to the agents with internal state described in Chapter 2.

Token 4972:
Because of the deﬁnitions of T ELL and A SK, however, the knowledge-based agent is not an arbitrary program for calculating actions.

Token 4973:
It is amenable to a description at

Token 4974:
236 Chapter 7.

Token 4975:
Logical Agents function KB-A GENT (percept )returns anaction persistent :KB, a knowledge base t, a counter, initially 0, indicating time TELL(KB,MAKE-PERCEPT -SENTENCE (percept ,t)) action←ASK(KB,MAKE-ACTION -QUERY (t)) TELL(KB,MAKE-ACTION -SENTENCE (action ,t)) t←t+1 return action Figure 7.1 A generic knowledge-based agent.

Token 4976:
Given a percept, the agent adds the percept to its knowledge base, asks the knowledge base for the best action, and tells the knowledge base that it has in fact taken that action.

Token 4977:
theknowledge level , where we need specify only what the agent knows and what its goals KNOWLEDGE LEVEL are, in order to ﬁx its behavior.

Token 4978:
For example, an automated taxi might have the goal of taking a passenger from San Francisco to Marin County and might know that the GoldenGate Bridge is the only link between the two locations.

Token 4979:
Then we can expect it to cross the Golden Gate Bridge because it knows that that will achieve its goal .

Token 4980:
Notice that this analysis is independent of how the taxi works at the implementation level .

Token 4981:
It doesn’t matter whether IMPLEMENTATION LEVEL its geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons by manipulating strings of symbols stored in registers or by propagating noisy signals in anetwork of neurons.

Token 4982:
A knowledge-based agent can be built simply by T ELLing it what it needs to know.

Token 4983:
Starting with an empty knowledge base, the agent designer can T ELL sentences one by one until the agent knows how to operate in its environment.

Token 4984:
This is called the declarative ap- DECLARATIVE proach to system building.

Token 4985:
In contrast, the procedural approach encodes desired behaviors directly as program code.

Token 4986:
In the 1970s and 1980s, advocates of the two approaches engagedin heated debates.

Token 4987:
We now understand that a successful agent often combines both declarative and procedural elements in its design, and that declarative knowledge can often be compiled into more efﬁcient procedural code.

Token 4988:
We can also provide a knowledge-based agent with mechanisms that allow it to learn for itself.

Token 4989:
These mechanisms, which are discussed in Chapter 18, create general knowledgeabout the environment from a series of percepts.

Token 4990:
A learning agent can be fully autonomous.

Token 4991:
7.2 T HEWUMPUS WORLD In this section we describe an environment in which knowledge-based agents can show theirworth.

Token 4992:
The wumpus world is a cave consisting of rooms connected by passageways.

Token 4993:
Lurking WUMPUS WORLD somewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.

Token 4994:
The wumpus can be shot by an agent, but the agent has only one arrow. Some rooms contain

Token 4995:
Section 7.2.

Token 4996:
The Wumpus World 237 bottomless pits that will trap anyone who wanders into these rooms (except for the wumpus, which is too big to fall in).

Token 4997:
The only mitigating feature of this bleak environment is thepossibility of ﬁnding a heap of gold.

Token 4998:
Although the wumpus world is rather tame by moderncomputer game standards, it illustrates some important points about intelligence.

Token 4999:
A sample wumpus world is shown in Figure 7.2.

Token 5000:
The precise deﬁnition of the task environment is given, as suggested in Section 2.3, by the PEAS description: •Performance measure : +1000 for climbing out of the cave with the gold, –1000 for falling into a pit or being eaten by the wumpus, –1 for each action taken and –10 forusing up the arrow.

Token 5001:
The game ends either when the agent dies or when the agent climbsout of the cave. •Environment :A4×4grid of rooms.

Token 5002:
The agent always starts in the square labeled [1,1], facing to the right.

Token 5003:
The locations of the gold and the wumpus are chosen ran-domly, with a uniform distribution, from the squares other than the start square.

Token 5004:
Inaddition, each square other than the start can be a pit, with probability 0.2.

Token 5005:
•Actuators : The agent can move Forward ,TurnLeft by90 ◦,o rTurnRight by90◦.T h e agent dies a miserable death if it enters a square containing a pit or a live wumpus.

Token 5006:
(Itis safe, albeit smelly, to enter a square with a dead wumpus.)

Token 5007:
If an agent tries to moveforward and bumps into a wall, then the agent does not move.

Token 5008:
The action Grab can be used to pick up the gold if it is in the same square as the agent.

Token 5009:
The action Shoot can be used to ﬁre an arrow in a straight line in the direction the agent is facing.

Token 5010:
The arrowcontinues until it either hits (and hence kills) the wumpus or hits a wall.

Token 5011:
The agent has only one arrow, so only the ﬁrst Shoot action has any effect.

Token 5012:
Finally, the action Climb can be used to climb out of the cave, but only from square [1,1].

Token 5013:
•Sensors : The agent has ﬁve sensors, each of which gives a single bit of information: –In the square containing the wumpus and in the directly (not diagonally) adjacent squares, the agent will perceive a Stench .

Token 5014:
–In the squares directly adjacent to a pit, the agent will perceive a Breeze . –In the square where the gold is, the agent will perceive a Glitter .

Token 5015:
–When an agent walks into a wall, it will perceive a Bump .

Token 5016:
–When the wumpus is killed, it emits a woeful Scream that can be perceived any- where in the cave.

Token 5017:
The percepts will be given to the agent program in the form of a list of ﬁve symbols; for example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent program will get [Stench ,Breeze ,None,None,None].

Token 5018:
We can characterize the wumpus environment along the various dimensions given in Chap- ter 2. Clearly, it is discrete, static, and single-agent.

Token 5019:
(The wumpus doesn’t move, fortunately.) It is sequential, because rewards may come only after many actions are taken.

Token 5020:
It is partially observable, because some aspects of the state are not directly perceivable: the agent’s lo- cation, the wumpus’s state of health, and the availability of an arrow.

Token 5021:
As for the locations of the pits and the wumpus: we could treat them as unobserved parts of the state that hap- pen to be immutable—in which case, the transition model for the environment is completely

Token 5022:
238 Chapter 7.

Token 5023:
Logical Agents PIT 12341234 STARTStenchStench BreezeGoldPIT PITBreezeBreezeBreeze BreezeBreeze Stench Figure 7.2 A typical wumpus world.

Token 5024:
The agent is in the bottom left corner, facing right.

Token 5025:
known; or we could say that the transition model itself is unknown because the agent doesn’t know which Forward actions are fatal—in which case, discovering the locations of pits and wumpus completes the agent’s knowledge of the transition model.

Token 5026:
For an agent in the environment, the main challenge is its initial ignorance of the con- ﬁguration of the environment; overcoming this ignorance seems to require logical reasoning.In most instances of the wumpus world, it is possible for the agent to retrieve the gold safely.Occasionally, the agent must choose between going home empty-handed and risking death toﬁnd the gold.

Token 5027:
About 21% of the environments are utterly unfair, because the gold is in a pitor surrounded by pits.

Token 5028:
Let us watch a knowledge-based wumpus agent exploring the environment shown in Figure 7.2.

Token 5029:
We use an informal knowledge representation language consisting of writingdown symbols in a grid (as in Figures 7.3 and 7.4).

Token 5030:
The agent’s initial knowledge base contains the rules of the environment, as described previously; in particular, it knows that it is in [1,1] and that [1,1] is a safe square; we denotethat with an “A” and “OK,” respectively, in square [1,1].

Token 5031:
The ﬁrst percept is [None,None,None,None,None], from which the agent can con- clude that its neighboring squares, [1,2] and [2,1], are free of dangers—they are OK. Fig-ure 7.3(a) shows the agent’s state of knowledge at this point.

Token 5032:
A cautious agent will move only into a square that it knows to be OK. Let us suppose the agent decides to move forward to [2,1].

Token 5033:
The agent perceives a breeze (denoted by “B”) in[2,1], so there must be a pit in a neighboring square.

Token 5034:
The pit cannot be in [1,1], by the rules ofthe game, so there must be a pit in [2,2] or [3,1] or both.

Token 5035:
The notation “P?” in Figure 7.3(b)indicates a possible pit in those squares.

Token 5036:
At this point, there is only one known square that is OK and that has not yet been visited.

Token 5037:
So the prudent agent will turn around, go back to [1,1], and then proceed to [1,2].

Token 5038:
The agent perceives a stench in [1,2], resulting in the state of knowledge shown in Figure 7.4(a).

Token 5039:
The stench in [1,2] means that there must be a wumpus nearby. But the

Token 5040:
Section 7.2.

Token 5041:
The Wumpus World 239 A B G P S W = Agent = Breeze = Glitter, Gold = Pit = Stench = WumpusOK = Safe square V = Visited AOK 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 3,4 4,4 OK OKBP?

Token 5042:
P?

Token 5043:
A OK OKOK 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 3,4 4,4 V (a) (b) Figure 7.3 The ﬁrst step taken by the agent in the wumpus world.

Token 5044:
(a) The initial sit- uation, after percept [None,None,None,None,None]. (b) After one move, with percept [None,Breeze ,None,None,None].

Token 5045:
B B P!A OK OKOK 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 3,4 4,4 VOKW!

Token 5046:
VP!A OK OKOK 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 3,4 4,4 VS OKW! VVVBSGP?P?

Token 5047:
(b) (a)SA B G P S W = Agent = Breeze = Glitter, Gold = Pit = Stench = WumpusOK = Safe square V = Visited Figure 7.4 Two later stages in the progress of the agent.

Token 5048:
(a) After the third move, with percept [Stench ,None,None,None,None]. (b) After the ﬁfth move, with percept [Stench ,Breeze ,Glitter ,None,None].

Token 5049:
wumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent would have detected a stench when it was in [2,1]).

Token 5050:
Therefore, the agent can infer that thewumpus is in [1,3]. The notation W! indicates this inference.

Token 5051:
Moreover, the lack of a breeze in [1,2] implies that there is no pit in [2,2].

Token 5052:
Yet the agent has already inferred that there must be a pit in either [2,2] or [3,1], so this means it must be in [3,1].

Token 5053:
This is a fairly difﬁcultinference, because it combines knowledge gained at different times in different places andrelies on the lack of a percept to make one crucial step.

Token 5054:
240 Chapter 7. Logical Agents The agent has now proved to itself that there is neither a pit nor a wumpus in [2,2], so it is OK to move there.

Token 5055:
We do not show the agent’s state of knowledge at [2,2]; we just assumethat the agent turns and moves to [2,3], giving us Figure 7.4(b).

Token 5056:
In [2,3], the agent detects aglitter, so it should grab the gold and then return home.

Token 5057:
Note that in each case for which the agent draws a conclusion from the available in- formation, that conclusion is guaranteed to be correct if the available information is correct.

Token 5058:
This is a fundamental property of logical reasoning.

Token 5059:
In the rest of this chapter, we describehow to build logical agents that can represent information and draw conclusions such as thosedescribed in the preceding paragraphs.

Token 5060:
7.3 L OGIC This section summarizes the fundamental concepts of logical representation and reasoning.These beautiful ideas are independent of any of logic’s particular forms.

Token 5061:
We therefore post- pone the technical details of those forms until the next section, using instead the familiar example of ordinary arithmetic.

Token 5062:
In Section 7.1, we said that knowledge bases consist of sentences.

Token 5063:
These sentences are expressed according to the syntax of the representation language, which speciﬁes all the SYNTAX sentences that are well formed.

Token 5064:
The notion of syntax is clear enough in ordinary arithmetic: “x+y=4” is a well-formed sentence, whereas “ x4y+=” is not.

Token 5065:
A logic must also deﬁne the semantics or meaning of sentences.

Token 5066:
The semantics deﬁnes SEMANTICS thetruth of each sentence with respect to each possible world .

Token 5067:
For example, the semantics TRUTH POSSIBLE WORLD for arithmetic speciﬁes that the sentence “ x+y=4” is true in a world where xis 2 and y is 2, but false in a world where xis 1 and yis 1.

Token 5068:
In standard logics, every sentence must be either true or false in each possible world—there is no “in between.”1 When we need to be precise, we use the term model in place of “possible world.” MODEL Whereas possible worlds might be thought of as (potentially) real environments that the agent might or might not be in, models are mathematical abstractions, each of which simply ﬁxesthe truth or falsehood of every relevant sentence.

Token 5069:
Informally, we may think of a possible worldas, for example, having xmen and ywomen sitting at a table playing bridge, and the sentence x+y=4is true when there are four people in total.

Token 5070:
Formally, the possible models are just all possible assignments of real numbers to the variables xandy.

Token 5071:
Each such assignment ﬁxes the truth of any sentence of arithmetic whose variables are xandy.

Token 5072:
If a sentence αis true in model m, we say that msatisﬁes αor sometimes mis a model of α.

Token 5073:
We use the notation SATISFACTION M(α)to mean the set of all models of α.

Token 5074:
Now that we have a notion of truth, we are ready to talk about logical reasoning.

Token 5075:
This involves the relation of logical entailment between sentences—the idea that a sentence fol- ENTAILMENT lows logically from another sentence.

Token 5076:
In mathematical notation, we write α|=β 1Fuzzy logic , discussed in Chapter 14, allows for degrees of truth.

Token 5077:
Section 7.3.

Token 5078:
Logic 241 12312 PIT12312 PIT 12312 PIT PIT PIT12312 PIT PIT12312 PIT 12312 PIT PIT 12312 PIT PIT12312KB α1 BreezeBreeze Breeze Breeze BreezeBreezeBreezeBreeze 12312 PIT12312 PIT 12312 PIT PIT PIT12312 PIT PIT12312 PIT 12312 PIT PIT 12312 PIT PIT12312KB Breezeα2 BreezeBreeze BreezeBreeze Breeze Breeze Breeze (a) (b) Figure 7.5 Possible models for the presence of pits in squares [1,2], [2,2], and [3,1].

Token 5079:
The KB corresponding to the observations of nothing in [1,1] and a breeze in [2,1] is shown by the solid line.

Token 5080:
(a) Dotted line shows models of α1(no pit in [1,2]). (b) Dotted line shows models of α2(no pit in [2,2]).

Token 5081:
to mean that the sentence αentails the sentence β.

Token 5082:
The formal deﬁnition of entailment is this: α|=βif and only if, in every model in which αis true, βis also true.

Token 5083:
Using the notation just introduced, we can write α|=βif and only if M(α)⊆M(β).

Token 5084:
(Note the direction of the ⊆here: if α|=β,t h e nαis astronger assertion than β: it rules out more possible worlds.)

Token 5085:
The relation of entailment is familiar from arithmetic; we are happy with the idea that the sentence x=0 entails the sentence xy=0.

Token 5086:
Obviously, in any model where xis zero, it is the case that xyis zero (regardless of the value of y).

Token 5087:
We can apply the same kind of analysis to the wumpus-world reasoning example given in the preceding section.

Token 5088:
Consider the situation in Figure 7.3(b): the agent has detected nothing in [1,1] and a breeze in [2,1].

Token 5089:
These percepts, combined with the agent’s knowledge of the rules of the wumpus world, constitute the KB.

Token 5090:
The agent is interested (among other things) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits.

Token 5091:
Each of the three squares might or might not contain a pit, so (for the purposes of this example) there are 23=8 possible models.

Token 5092:
These eight models are shown in Figure 7.5.2 The KB can be thought of as a set of sentences or as a single sentence that asserts all the individual sentences.

Token 5093:
The KB is false in models that contradict what the agent knows—for example, the KB is false in any model in which [1,2] contains a pit, because there isno breeze in [1,1].

Token 5094:
There are in fact just three models in which the KB is true, and these are 2Although the ﬁgure shows the models as partial wumpus worlds, they are really nothing more than assignments oftrue andfalse to the sentences “there is a pit in [1,2]” etc.

Token 5095:
Models, in the mathematical sense, do not need to have ’orrible ’airy wumpuses in them.

Token 5096:
242 Chapter 7. Logical Agents shown surrounded by a solid line in Figure 7.5.

Token 5097:
Now let us consider two possible conclusions: α1=“There is no pit in [1,2].” α2=“There is no pit in [2,2].” We have surrounded the models of α1andα2with dotted lines in Figures 7.5(a) and 7.5(b), respectively.

Token 5098:
By inspection, we see the following: in every model in which KBis true, α1is also true. Hence, KB|=α1: there is no pit in [1,2].

Token 5099:
We can also see that in some models in which KBis true, α2is false.

Token 5100:
Hence, KB/negationslash|=α2: the agent cannot conclude that there is no pit in [2,2]. (Nor can it conclude that there isa pit in [2,2].

Token 5101:
)3 The preceding example not only illustrates entailment but also shows how the deﬁnition of entailment can be applied to derive conclusions—that is, to carry out logical inference .

Token 5102:
LOGICAL INFERENCE The inference algorithm illustrated in Figure 7.5 is called model checking , because it enu- MODEL CHECKING merates all possible models to check that αis true in all models in which KBis true, that is, thatM(KB)⊆M(α).

Token 5103:
In understanding entailment and inference, it might help to think of the set of all conse- quences of KBas a haystack and of αas a needle.

Token 5104:
Entailment is like the needle being in the haystack; inference is like ﬁnding it.

Token 5105:
This distinction is embodied in some formal notation: ifan inference algorithm ican derive αfromKB, we write KB/turnstileleft iα, which is pronounced “ αis derived from KBbyi”o r“iderives αfromKB.” An inference algorithm that derives only entailed sentences is called sound ortruth- SOUND preserving .

Token 5106:
Soundness is a highly desirable property.

Token 5107:
An unsound inference procedure es- TRUTH-PRESERVING sentially makes things up as it goes along—it announces the discovery of nonexistent needles.

Token 5108:
It is easy to see that model checking, when it is applicable,4is a sound procedure.

Token 5109:
The property of completeness is also desirable: an inference algorithm is complete if COMPLETENESS it can derive any sentence that is entailed.

Token 5110:
For real haystacks, which are ﬁnite in extent, it seems obvious that a systematic examination can always decide whether the needle is inthe haystack.

Token 5111:
For many knowledge bases, however, the haystack of consequences is inﬁnite,and completeness becomes an important issue.

Token 5112:
5Fortunately, there are complete inference procedures for logics that are sufﬁciently expressive to handle many knowledge bases.

Token 5113:
We have described a reasoning process whose conclusions are guaranteed to be true in any world in which the premises are true; in particular, ifKB is true in the real world, then any sentence αderived from KB by a sound inference procedure is also true in the real world.

Token 5114:
So, while an inference process operates on “syntax”—internal physical conﬁgurations such as bits in registers or patterns of electrical blips in brains—the process corresponds 3The agent can calculate the probability that there is a pit in [2,2]; Chapter 13 shows how.

Token 5115:
4Model checking works if the space of models is ﬁnite—for example, in wumpus worlds of ﬁxed size.

Token 5116:
For arithmetic, on the other hand, the space of models is inﬁnite : even if we restrict ourselves to the integers, there are inﬁnitely many pairs of values for xandyin the sentence x+y=4.

Token 5117:
5Compare with the case of inﬁnite search spaces in Chapter 3, where depth-ﬁrst search is not complete.

Token 5118:
Section 7.4.

Token 5119:
Propositional Logic: A Very Simple Logic 243 FollowsSentences Sentence EntailsSemantics Semantics Representation World Aspects of the real worldAspect of the real world Figure 7.6 Sentences are physical conﬁgurations of the agent, and reasoning is a process of constructing new physical conﬁgurations from old ones.

Token 5120:
Logical reasoning should en- sure that the new conﬁgurations represent aspects of the world that actually follow from the aspects that the old conﬁgurations represent.

Token 5121:
to the real-world relationship whereby some aspect of the real world is the case6by virtue of other aspects of the real world being the case.

Token 5122:
This correspondence between world andrepresentation is illustrated in Figure 7.6.

Token 5123:
The ﬁnal issue to consider is grounding —the connection between logical reasoning GROUNDING processes and the real environment in which the agent exists.

Token 5124:
In particular, how do we know thatKB is true in the real world? (After all, KB is just “syntax” inside the agent’s head.)

Token 5125:
This is a philosophical question about which many, many books have been written. (SeeChapter 26.)

Token 5126:
A simple answer is that the agent’s sensors create the connection. For example,our wumpus-world agent has a smell sensor.

Token 5127:
The agent program creates a suitable sentence whenever there is a smell.

Token 5128:
Then, whenever that sentence is in the knowledge base, it is true in the real world.

Token 5129:
Thus, the meaning and truth of percept sentences are deﬁned by theprocesses of sensing and sentence construction that produce them.

Token 5130:
What about the rest of theagent’s knowledge, such as its belief that wumpuses cause smells in adjacent squares?

Token 5131:
Thisis not a direct representation of a single percept, but a general rule—derived, perhaps, fromperceptual experience but not identical to a statement of that experience.

Token 5132:
General rules likethis are produced by a sentence construction process called learning , which is the subject of Part V. Learning is fallible.

Token 5133:
It could be the case that wumpuses cause smells except on February 29 in leap years , which is when they take their baths.

Token 5134:
Thus, KBmay not be true in the real world, but with good learning procedures, there is reason for optimism.

Token 5135:
7.4 P ROPOSITIONAL LOGIC :AV ERY SIMPLE LOGIC We now present a simple but powerful logic called propositional logic .

Token 5136:
We cover the syntaxPROPOSITIONAL LOGIC of propositional logic and its semantics—the way in which the truth of sentences is deter- mined.

Token 5137:
Then we look at entailment —the relation between a sentence and another sentence that follows from it—and see how this leads to a simple algorithm for logical inference.

Token 5138:
Ev-erything takes place, of course, in the wumpus world.

Token 5139:
6As Wittgenstein (1922) put it in his famous Tractatus : “The world is everything that is the case.”

Token 5140:
244 Chapter 7. Logical Agents 7.4.1 Syntax The syntax of propositional logic deﬁnes the allowable sentences.

Token 5141:
The atomic sentences ATOMIC SENTENCES consist of a single proposition symbol .

Token 5142:
Each such symbol stands for a proposition that canPROPOSITION SYMBOL be true or false.

Token 5143:
We use symbols that start with an uppercase letter and may contain other letters or subscripts, for example: P,Q,R,W1,3andNorth .

Token 5144:
The names are arbitrary but are often chosen to have some mnemonic value—we use W1,3to stand for the proposition that the wumpus is in [1,3].

Token 5145:
(Remember that symbols such as W1,3areatomic , i.e.,W,1 , and 3 are not meaningful parts of the symbol.)

Token 5146:
There are two proposition symbols with ﬁxedmeanings: True is the always-true proposition and False is the always-false proposition.

Token 5147:
Complex sentences are constructed from simpler sentences, using parentheses and logical COMPLEX SENTENCES connectives .

Token 5148:
There are ﬁve connectives in common use:LOGICAL CONNECTIVES ¬(not).

Token 5149:
A sentence such as ¬W1,3is called the negation ofW1,3.Aliteral is either an NEGATION LITERAL atomic sentence (a positive literal ) or a negated atomic sentence (a negative literal ).

Token 5150:
∧(and). A sentence whose main connective is ∧,s u c ha s W1,3∧P3,1, is called a con- junction ; its parts are the conjuncts .

Token 5151:
( T h e∧looks like an “A” for “And.”) CONJUNCTION ∨(or).

Token 5152:
A sentence using ∨,s u c ha s (W1,3∧P3,1)∨W2,2,i sa disjunction of the disjuncts DISJUNCTION (W1,3∧P3,1)andW2,2.

Token 5153:
(Historically, the ∨comes from the Latin “vel,” which means “or.” For most people, it is easier to remember ∨as an upside-down ∧.) ⇒(implies).

Token 5154:
A sentence such as (W1,3∧P3,1)⇒¬W2,2is called an implication (or con- IMPLICATION ditional).

Token 5155:
Its premise orantecedent is(W1,3∧P3,1), and its conclusion orconsequent PREMISE CONCLUSION is¬W2,2.

Token 5156:
Implications are also known as rules orif–then statements. The implication RULES symbol is sometimes written in other books as ⊃or→.

Token 5157:
⇔(if and only if). The sentence W1,3⇔¬W2,2is abiconditional . Some other books BICONDITIONAL write this as≡.

Token 5158:
Sentence→AtomicSentence |ComplexSentence AtomicSentence →True|False|P|Q|R|... ComplexSentence → (Sentence )|[Sentence ] |¬Sentence |Sentence∧Sentence |Sentence∨Sentence |Sentence⇒Sentence |Sentence⇔Sentence OPERATOR PRECEDENCE :¬,∧,∨,⇒,⇔ Figure 7.7 A BNF (Backus–Naur Form) grammar of sentences in propositional logic, along with operator precedences, from highest to lowest.

Token 5159:
Section 7.4.

Token 5160:
Propositional Logic: A Very Simple Logic 245 Figure 7.7 gives a formal grammar of propositional logic; see page 1060 if you are not familiar with the BNF notation.

Token 5161:
The BNF grammar by itself is ambiguous; a sentence withseveral operators can be parsed by the grammar in multiple ways.

Token 5162:
To eliminate the ambiguitywe deﬁne a precedence for each operator.

Token 5163:
The “not” operator ( ¬) has the highest precedence, which means that in the sentence ¬A∧Bthe¬binds most tightly, giving us the equivalent of(¬A)∧Brather than¬(A∧B).

Token 5164:
(The notation for ordinary arithmetic is the same: −2+4 is 2, not –6.) When in doubt, use parentheses to make sure of the right interpretation.

Token 5165:
Squarebrackets mean the same thing as parentheses; the choice of square brackets or parentheses issolely to make it easier for a human to read a sentence.

Token 5166:
7.4.2 Semantics Having speciﬁed the syntax of propositional logic, we now specify its semantics.

Token 5167:
The se-mantics deﬁnes the rules for determining the truth of a sentence with respect to a particularmodel.

Token 5168:
In propositional logic, a model simply ﬁxes the truth value —true orfalse —for ev- TRUTH VALUE ery proposition symbol.

Token 5169:
For example, if the sentences in the knowledge base make use of the proposition symbols P1,2,P2,2,a n dP3,1, then one possible model is m1={P1,2=false,P2,2=false,P3,1=true}.

Token 5170:
With three proposition symbols, there are 23=8possible models—exactly those depicted in Figure 7.5.

Token 5171:
Notice, however, that the models are purely mathematical objects with nonecessary connection to wumpus worlds.

Token 5172:
P 1,2is just a symbol; it might mean “there is a pit in [1,2]” or “I’m in Paris today and tomorrow.” The semantics for propositional logic must specify how to compute the truth value of anysentence, given a model.

Token 5173:
This is done recursively.

Token 5174:
All sentences are constructed from atomic sentences and the ﬁve connectives; therefore, we need to specify how to compute thetruth of atomic sentences and how to compute the truth of sentences formed with each of theﬁve connectives.

Token 5175:
Atomic sentences are easy: •True is true in every model and False is false in every model.

Token 5176:
•The truth value of every other proposition symbol must be speciﬁed directly in the model. For example, in the model m 1given earlier, P1,2is false.

Token 5177:
For complex sentences, we have ﬁve rules, which hold for any subsentences PandQin any model m(here “iff” means “if and only if”): •¬Pis true iff Pis false in m. •P∧Qis true iff both PandQare true in m. •P∨Qis true iff either PorQis true in m. •P⇒Qis true unless Pis true and Qis false in m. •P⇔Qis true iff PandQare both true or both false in m. The rules can also be expressed with truth tables that specify the truth value of a complex TRUTH TABLE sentence for each possible assignment of truth values to its components.

Token 5178:
Truth tables for the ﬁve connectives are given in Figure 7.8.

Token 5179:
From these tables, the truth value of any sentence s can be computed with respect to any model mby a simple recursive evaluation. For example,

Token 5180:
246 Chapter 7.

Token 5181:
Logical Agents P Q ¬P P∧Q P∨Q P⇒Q P⇔Q false false true false false true true false true true false true true false true false false false true false false true true false true true true true Figure 7.8 Truth tables for the ﬁve logical connectives.

Token 5182:
To use the table to compute, for example, the value of P∨QwhenPis true and Qis false, ﬁrst look on the left for the row where Pistrue andQisfalse (the third row).

Token 5183:
Then look in that row under the P∨Qcolumn to see the result: true.

Token 5184:
the sentence¬P1,2∧(P2,2∨P3,1), evaluated in m1,g i v e s true∧(false∨true)=true∧ true=true. Exercise 7.3 asks you to write the algorithm PL-T RUE?

Token 5185:
(s,m), which computes the truth value of a propositional logic sentence sin a model m. The truth tables for “and,” “or,” and “not” are in close accord with our intuitions about the English words.

Token 5186:
The main point of possible confusion is that P∨Qis true when Pis true orQis true or both .

Token 5187:
A different connective, called “exclusive or” (“xor” for short), yields false when both disjuncts are true.7There is no consensus on the symbol for exclusive or; some choices are ˙∨or/negationslash=or⊕.

Token 5188:
The truth table for ⇒may not quite ﬁt one’s intuitive understanding of “ Pimplies Q” or “ifPthenQ.” For one thing, propositional logic does not require any relation of causation orrelevance between PandQ.

Token 5189:
The sentence “5 is odd implies Tokyo is the capital of Japan” is a true sentence of propositional logic (under the normal interpretation), even though it isa decidedly odd sentence of English.

Token 5190:
Another point of confusion is that any implication istrue whenever its antecedent is false.

Token 5191:
For example, “5 is even implies Sam is smart” is true,regardless of whether Sam is smart.

Token 5192:
This seems bizarre, but it makes sense if you think of“P⇒Q” as saying, “If Pis true, then I am claiming that Qis true.

Token 5193:
Otherwise I am making no claim.” The only way for this sentence to be false is ifPis true but Qis false.

Token 5194:
The biconditional, P⇔Q, is true whenever both P⇒QandQ⇒Pare true.

Token 5195:
In English, this is often written as “ Pif and only if Q.” Many of the rules of the wumpus world are best written using ⇔.

Token 5196:
For example, a square is breezy ifa neighboring square has a pit, and a square is breezy only if a neighboring square has a pit.

Token 5197:
So we need a biconditional, B 1,1⇔(P1,2∨P2,1), where B1,1means that there is a breeze in [1,1].

Token 5198:
7.4.3 A simple knowledge base Now that we have deﬁned the semantics for propositional logic, we can construct a knowledge base for the wumpus world.

Token 5199:
We focus ﬁrst on the immutable aspects of the wumpus world, leaving the mutable aspects for a later section.

Token 5200:
For now, we need the following symbols for each[x,y]location: 7Latin has a separate word, aut, for exclusive or.

Token 5201:
Section 7.4. Propositional Logic: A Very Simple Logic 247 Px,yis true if there is a pit in [x,y].

Token 5202:
Wx,yis true if there is a wumpus in [x,y], dead or alive. Bx,yis true if the agent perceives a breeze in [x,y].

Token 5203:
Sx,yis true if the agent perceives a stench in [x,y].

Token 5204:
The sentences we write will sufﬁce to derive ¬P1,2(there is no pit in [1,2]), as was done informally in Section 7.3.

Token 5205:
We label each sentence Riso that we can refer to them: •There is no pit in [1,1]: R1:¬P1,1.

Token 5206:
•A square is breezy if and only if there is a pit in a neighboring square.

Token 5207:
This has to be stated for each square; for now, we include just the relevant squares: R2:B1,1⇔(P1,2∨P2,1). R3:B2,1⇔(P1,1∨P2,2∨P3,1).

Token 5208:
•The preceding sentences are true in all wumpus worlds.

Token 5209:
Now we include the breeze percepts for the ﬁrst two squares visited in the speciﬁc world the agent is in, leading upto the situation in Figure 7.3(b).

Token 5210:
R 4:¬B1,1. R5:B2,1. 7.4.4 A simple inference procedure Our goal now is to decide whether KB|=αfor some sentence α.

Token 5211:
For example, is ¬P1,2 entailed by our KB?

Token 5212:
Our ﬁrst algorithm for inference is a model-checking approach that is a direct implementation of the deﬁnition of entailment: enumerate the models, and check thatαis true in every model in which KB is true.

Token 5213:
Models are assignments of true orfalse to every proposition symbol.

Token 5214:
Returning to our wumpus-world example, the relevant proposi-tion symbols are B 1,1,B2,1,P1,1,P1,2,P2,1,P2,2,a n dP3,1.

Token 5215:
With seven symbols, there are 27= 128 possible models; in three of these, KBis true (Figure 7.9).

Token 5216:
In those three models, ¬P1,2is true, hence there is no pit in [1,2].

Token 5217:
On the other hand, P2,2is true in two of the three models and false in one, so we cannot yet tell whether there is a pit in [2,2].

Token 5218:
Figure 7.9 reproduces in a more precise form the reasoning illustrated in Figure 7.5.

Token 5219:
A general algorithm for deciding entailment in propositional logic is shown in Figure 7.10.

Token 5220:
Likethe B ACKTRACKING -SEARCH algorithm on page 215, TT-E NTAILS ? performs a recursive enumeration of a ﬁnite space of assignments to symbols.

Token 5221:
The algorithm is sound because it implements directly the deﬁnition of entailment, and complete because it works for any KB andαand always terminates—there are only ﬁnitely many models to examine.

Token 5222:
Of course, “ﬁnitely many” is not always the same as “few.” If KB andαcontain n symbols in all, then there are 2nmodels.

Token 5223:
Thus, the time complexity of the algorithm is O(2n). (The space complexity is only O(n)because the enumeration is depth-ﬁrst.)

Token 5224:
Later in this chapter we show algorithms that are much more efﬁcient in many cases.

Token 5225:
Unfortunately, propositional entailment is co-NP-complete (i.e., probably no easier than NP-complete—seeAppendix A), so every known inference algorithm for propositional logic has a worst-case complexity that is exponential in the size of the input.

Token 5226:
248 Chapter 7.

Token 5227:
Logical Agents B1,1 B2,1 P1,1 P1,2 P2,1 P2,2 P3,1 R1 R2 R3 R4 R5 KB false false false false false false false true true true true false false false false false false false false true true true false true false false ... ... ... ... ... ... ... ... ... ... ... ... ... false true false false false false false true true false true true false false true false false false false true true true true true true true false true false false false true false true true true true true true false true false false false true true true true true true true true false true false false true false false true false false true true false ... ... ... ... ... ... ... ... ... ... ... ... ... true true true true true true true false true true false true false Figure 7.9 A truth table constructed for the knowledge base given in the text.

Token 5228:
KBis true ifR1through R5are true, which occurs in just 3 of the 128 rows (the ones underlined in the right-hand column).

Token 5229:
In all 3 rows, P1,2is false, so there is no pit in [1,2]. On the other hand, there might (or might not) be a pit in [2,2]. function TT-E NTAILS ?

Token 5230:
(KB,α)returns true orfalse inputs :KB, the knowledge base, a sent ence in propositional logic α, the query, a sentence in propositional logic symbols←a list of the proposition symbols in KBandα return TT-C HECK -ALL(KB,α,symbols ,{}) function TT-C HECK -ALL(KB,α,symbols ,model )returns true orfalse ifEMPTY ?

Token 5231:
(symbols )then ifPL-T RUE? (KB,model )then return PL-T RUE?

Token 5232:
(α,model ) else return true //when KB is false, always return true else do P←FIRST (symbols ) rest←REST(symbols ) return (TT-C HECK -ALL(KB,α,rest,model∪{P=true}) and TT-C HECK -ALL(KB,α,rest,model∪{P=false})) Figure 7.10 A truth-table enumeration algorithm for deciding propositional entailment.

Token 5233:
(TT stands for truth table.) PL-T RUE? returns true if a sentence holds within a model.

Token 5234:
The variable model represents a partial model—an assignment to some of the symbols.

Token 5235:
The key- word “ and” is used here as a logical operation on its two arguments, returning true orfalse .

Token 5236:
Section 7.5.

Token 5237:
Propositional Theorem Proving 249 (α∧β)≡(β∧α)commutativity of ∧ (α∨β)≡(β∨α)commutativity of ∨ ((α∧β)∧γ)≡(α∧(β∧γ))associativity of ∧ ((α∨β)∨γ)≡(α∨(β∨γ))associativity of ∨ ¬(¬α)≡αdouble-negation elimination (α⇒β)≡(¬β⇒¬α)contraposition (α⇒β)≡(¬α∨β)implication elimination (α⇔β)≡((α⇒β)∧(β⇒α))biconditional elimination ¬(α∧β)≡(¬α∨¬β)De Morgan ¬(α∨β)≡(¬α∧¬β)De Morgan (α∧(β∨γ))≡((α∧β)∨(α∧γ))distributivity of ∧over∨ (α∨(β∧γ))≡((α∨β)∧(α∨γ))distributivity of ∨over∧ Figure 7.11 Standard logical equivalences.

Token 5238:
The symbols α,β,a n dγstand for arbitrary sentences of propositional logic.

Token 5239:
7.5 P ROPOSITIONAL THEOREM PROVING So far, we have shown how to determine entailment by model checking : enumerating models and showing that the sentence must hold in all models.

Token 5240:
In this section, we show how entail-ment can be done by theorem proving —applying rules of inference directly to the sentences THEOREM PROVING in our knowledge base to construct a proof of the desired sentence without consulting models.

Token 5241:
If the number of models is large but the length of the proof is short, then theorem proving can be more efﬁcient than model checking.

Token 5242:
Before we plunge into the details of theorem-proving algorithms, we will need some additional concepts related to entailment.

Token 5243:
The ﬁrst concept is logical equivalence : two sen-LOGICAL EQUIVALENCE tences αandβare logically equivalent if they are true in the same set of models.

Token 5244:
We write this as α≡β.

Token 5245:
For example, we can easily show (using truth tables) that P∧QandQ∧P are logically equivalent; other equivalences are shown in Figure 7.11.

Token 5246:
These equivalencesplay much the same role in logic as arithmetic identities do in ordinary mathematics.

Token 5247:
Analternative deﬁnition of equivalence is as follows: any two sentences αandβare equivalent only if each of them entails the other: α≡βif and only if α|=βandβ|=α.

Token 5248:
The second concept we will need is validity . A sentence is valid if it is true in allmodels. For VALIDITY example, the sentence P∨¬Pis valid.

Token 5249:
Valid sentences are also known as tautologies —they TAUTOLOGY arenecessarily true.

Token 5250:
Because the sentence True is true in all models, every valid sentence is logically equivalent to True . What good are valid sentences?

Token 5251:
From our deﬁnition of entailment, we can derive the deduction theorem , which was known to the ancient Greeks:DEDUCTION THEOREM For any sentences αandβ,α|=βif and only if the sentence (α⇒β)is valid.

Token 5252:
(Exercise 7.5 asks for a proof.)

Token 5253:
Hence, we can decide if α|=βby checking that (α⇒β)is true in every model—which is essentially what the inference algorithm in Figure 7.10 does—

Token 5254:
250 Chapter 7. Logical Agents or by proving that (α⇒β)is equivalent to True .

Token 5255:
Conversely, the deduction theorem states that every valid implication sentence describes a legitimate inference.

Token 5256:
The ﬁnal concept we will need is satisﬁability . A sentence is satisﬁable if it is true SATISFIABILITY in, or satisﬁed by, some model.

Token 5257:
For example, the knowledge base given earlier, ( R1∧R2∧ R3∧R4∧R5), is satisﬁable because there are three models in which it is true, as shown in Figure 7.9.

Token 5258:
Satisﬁability can be checked by enumerating the possible models until one is found that satisﬁes the sentence.

Token 5259:
The problem of determining the satisﬁability of sentencesin propositional logic—the SAT problem—was the ﬁrst problem proved to be NP-complete.

Token 5260:
SAT Many problems in computer science are really satisﬁability problems.

Token 5261:
For example, all the constraint satisfaction problems in Chapter 6 ask whether the constraints are satisﬁable bysome assignment.

Token 5262:
Validity and satisﬁability are of course connected: αis valid iff¬αis unsatisﬁable; contrapositively, αis satisﬁable iff ¬αis not valid.

Token 5263:
We also have the following useful result: α|=βif and only if the sentence (α∧¬β)is unsatisﬁable.

Token 5264:
Proving βfromαby checking the unsatisﬁability of (α∧¬β)corresponds exactly to the standard mathematical proof technique of reductio ad absurdum (literally, “reduction to anREDUCTIOAD ABSURDUM absurd thing”).

Token 5265:
It is also called proof by refutation or proof by contradiction .

Token 5266:
One assumes a REFUTATION CONTRADICTION sentence βto be false and shows that this leads to a contradiction with known axioms α.T h i s contradiction is exactly what is meant by saying that the sentence (α∧¬β)is unsatisﬁable.

Token 5267:
7.5.1 Inference and proofs This section covers inference rules that can be applied to derive a proof —a chain of conclu- INFERENCE RULES PROOF sions that leads to the desired goal.

Token 5268:
The best-known rule is called Modus Ponens (Latin for MODUS PONENS mode that afﬁrms ) and is written α⇒β, α β.

Token 5269:
The notation means that, whenever any sentences of the form α⇒βandαare given, then the sentence βcan be inferred.

Token 5270:
For example, if (WumpusAhead ∧WumpusAlive )⇒Shoot and(WumpusAhead ∧WumpusAlive )are given, then Shoot can be inferred.

Token 5271:
Another useful inference rule is And-Elimination , which says that, from a conjunction, AND-ELIMINATION any of the conjuncts can be inferred: α∧β α.

Token 5272:
For example, from (WumpusAhead ∧WumpusAlive ),WumpusAlive can be inferred.

Token 5273:
By considering the possible truth values of αandβ, one can show easily that Modus Ponens and And-Elimination are sound once and for all.

Token 5274:
These rules can then be used inany particular instances where they apply, generating sound inferences without the need forenumerating models.

Token 5275:
All of the logical equivalences in Figure 7.11 can be used as inference rules.

Token 5276:
For exam- ple, the equivalence for biconditional elimination yields the two inference rules α⇔β (α⇒β)∧(β⇒α)and(α⇒β)∧(β⇒α) α⇔β.

Token 5277:
Section 7.5. Propositional Theorem Proving 251 Not all inference rules work in both directions like this.

Token 5278:
For example, we cannot run Modus Ponens in the opposite direction to obtain α⇒βandαfromβ.

Token 5279:
Let us see how these inference rules and equivalences can be used in the wumpus world.

Token 5280:
We start with the knowledge base containing R1through R5and show how to prove ¬P1,2, that is, there is no pit in [1,2].

Token 5281:
First, we apply biconditional elimination to R2to obtain R6:(B1,1⇒(P1,2∨P2,1))∧((P1,2∨P2,1)⇒B1,1).

Token 5282:
Then we apply And-Elimination to R6to obtain R7:( (P1,2∨P2,1)⇒B1,1). Logical equivalence for contrapositives gives R8:(¬B1,1⇒¬(P1,2∨P2,1)).

Token 5283:
Now we can apply Modus Ponens with R8and the percept R4(i.e.,¬B1,1), to obtain R9:¬(P1,2∨P2,1).

Token 5284:
Finally, we apply De Morgan’s rule, giving the conclusion R10:¬P1,2∧¬P2,1. That is, neither [1,2] nor [2,1] contains a pit.

Token 5285:
We found this proof by hand, but we can apply any of the search algorithms in Chapter 3 to ﬁnd a sequence of steps that constitutes a proof.

Token 5286:
We just need to deﬁne a proof problem asfollows: •I NITIAL STATE : the initial knowledge base.

Token 5287:
•ACTIONS : the set of actions consists of all the inference rules applied to all the sen- tences that match the top half of the inference rule.

Token 5288:
•RESULT : the result of an action is to add the sentence in the bottom half of the inference rule.

Token 5289:
•GOAL: the goal is a state that contains the sentence we are trying to prove. Thus, searching for proofs is an alternative to enumerating models.

Token 5290:
In many practical cases ﬁnding a proof can be more efﬁcient because the proof can ignore irrelevant propositions, no matter how many of them there are.

Token 5291:
For example, the proof given earlier leading to ¬P1,2∧ ¬P2,1does not mention the propositions B2,1,P1,1,P2,2,o rP3,1.

Token 5292:
They can be ignored because the goal proposition, P1,2, appears only in sentence R2; the other propositions in R2 appear only in R4andR2;s oR1,R3,a n dR5have no bearing on the proof.

Token 5293:
The same would hold even if we added a million more sentences to the knowledge base; the simple truth-tablealgorithm, on the other hand, would be overwhelmed by the exponential explosion of models.

Token 5294:
One ﬁnal property of logical systems is monotonicity , which says that the set of en- MONOTONICITY tailed sentences can only increase as information is added to the knowledge base.8For any sentences αandβ, ifKB|=αthen KB∧β|=α.

Token 5295:
8Nonmonotonic logics, which violate the monotonicity property, capture a common property of human rea- soning: changing one’s mind.

Token 5296:
They are discussed in Section 12.6.

Token 5297:
252 Chapter 7.

Token 5298:
Logical Agents For example, suppose the knowledge base contains the additional assertion βstating that there are exactly eight pits in the world.

Token 5299:
This knowledge might help the agent draw additional con- clusions, but it cannot invalidate any conclusion αalready inferred—such as the conclusion that there is no pit in [1,2].

Token 5300:
Monotonicity means that inference rules can be applied wheneversuitable premises are found in the knowledge base—the conclusion of the rule must follow regardless of what else is in the knowledge base .

Token 5301:
7.5.2 Proof by resolution We have argued that the inference rules covered so far are sound , but we have not discussed the question of completeness for the inference algorithms that use them.

Token 5302:
Search algorithms such as iterative deepening search (page 89) are complete in the sense that they will ﬁnd any reachable goal, but if the available inference rules are inadequate, then the goal is notreachable—no proof exists that uses only those inference rules.

Token 5303:
For example, if we removedthe biconditional elimination rule, the proof in the preceding section would not go through.The current section introduces a single inference rule, resolution , that yields a complete inference algorithm when coupled with any complete search algorithm.

Token 5304:
We begin by using a simple version of the resolution rule in the wumpus world.

Token 5305:
Let us consider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and thengoes to [1,2], where it perceives a stench, but no breeze.

Token 5306:
We add the following facts to theknowledge base: R 11:¬B1,2. R12:B1,2⇔(P1,1∨P2,2∨P1,3).

Token 5307:
By the same process that led to R10earlier, we can now derive the absence of pits in [2,2] and [1,3] (remember that [1,1] is already known to be pitless): R13:¬P2,2.

Token 5308:
R14:¬P1,3.

Token 5309:
We can also apply biconditional elimination to R3, followed by Modus Ponens with R5,t o obtain the fact that there is a pit in [1,1], [2,2], or [3,1]: R15:P1,1∨P2,2∨P3,1.

Token 5310:
Now comes the ﬁrst application of the resolution rule: the literal ¬P2,2inR13resolves with the literal P2,2inR15to give the resolvent RESOLVENT R16:P1,1∨P3,1.

Token 5311:
In English; if there’s a pit in one of [1,1], [2,2], and [3,1] and it’s not in [2,2], then it’s in [1,1] or [3,1].

Token 5312:
Similarly, the literal ¬P1,1inR1resolves with the literal P1,1inR16to give R17:P3,1.

Token 5313:
In English: if there’s a pit in [1,1] or [3,1] and it’s not in [1,1], then it’s in [3,1].

Token 5314:
These last two inference steps are examples of the unit resolution inference rule, UNIT RESOLUTION /lscript1∨···∨ /lscriptk,m /lscript1∨···∨ /lscripti−1∨/lscripti+1∨···∨ /lscriptk, where each /lscriptis a literal and /lscriptiandmarecomplementary literals (i.e., one is the negationCOMPLEMENTARY LITERALS

Token 5315:
Section 7.5. Propositional Theorem Proving 253 of the other).

Token 5316:
Thus, the unit resolution rule takes a clause —a disjunction of literals—and a CLAUSE literal and produces a new clause.

Token 5317:
Note that a single literal can be viewed as a disjunction of one literal, also known as a unit clause .

Token 5318:
UNIT CLAUSE The unit resolution rule can be generalized to the full resolution rule, RESOLUTION /lscript1∨···∨ /lscriptk,m 1∨···∨ mn /lscript1∨···∨ /lscripti−1∨/lscripti+1∨···∨ /lscriptk∨m1∨···∨ mj−1∨mj+1∨···∨ mn, where /lscriptiandmjare complementary literals.

Token 5319:
This says that resolution takes two clauses and produces a new clause containing all the literals of the two original clauses except the two complementary literals.

Token 5320:
For example, we have P1,1∨P3,1,¬P1,1∨¬P2,2 P3,1∨¬P2,2.

Token 5321:
There is one more technical aspect of the resolution rule: the resulting clause should contain only one copy of each literal.9The removal of multiple copies of literals is called factoring .

Token 5322:
FACTORING For example, if we resolve (A∨B)with(A∨¬B), we obtain (A∨A), which is reduced to justA.

Token 5323:
The soundness of the resolution rule can be seen easily by considering the literal /lscriptithat is complementary to literal mjin the other clause.

Token 5324:
If /lscriptiis true, then mjis false, and hence m1∨···∨ mj−1∨mj+1∨···∨ mnmust be true, because m1∨···∨ mnis given.

Token 5325:
If /lscriptiis false, then /lscript1∨···∨ /lscripti−1∨/lscripti+1∨···∨ /lscriptkmust be true because /lscript1∨···∨ /lscriptkis given.

Token 5326:
Now /lscriptiis either true or false, so one or other of these conclusions holds—exactly as the resolution rule states.

Token 5327:
What is more surprising about the resolution rule is that it forms the basis for a family ofcomplete inference procedures.

Token 5328:
A resolution-based theorem prover can, for any sentences αandβin propositional logic, decide whether α|=β.The next two subsections explain how resolution accomplishes this.

Token 5329:
Conjunctive normal form The resolution rule applies only to clauses (that is, disjunctions of literals), so it would seem to be relevant only to knowledge bases and queries consisting of clauses.

Token 5330:
How, then, can it lead to a complete inference procedure for all of propositional logic?

Token 5331:
The answer is that every sentence of propositional logic is logically equivalent to a conjunction of clauses.

Token 5332:
A sentence expressed as a conjunction of clauses is said to be in conjunctive normal form orCONJUNCTIVE NORMAL FORM CNF (see Figure 7.14).

Token 5333:
We now describe a procedure for converting to CNF. We illustrate the procedure by converting the sentence B1,1⇔(P1,2∨P2,1)into CNF.

Token 5334:
The steps are as follows: 1. Eliminate⇔, replacing α⇔βwith(α⇒β)∧(β⇒α). (B1,1⇒(P1,2∨P2,1))∧((P1,2∨P2,1)⇒B1,1). 2.

Token 5335:
Eliminate⇒, replacing α⇒βwith¬α∨β: (¬B1,1∨P1,2∨P2,1)∧(¬(P1,2∨P2,1)∨B1,1).

Token 5336:
9If a clause is viewed as a setof literals, then this restriction is automatically respected.

Token 5337:
Using set notation for clauses makes the resolution rule much cleaner, at the cost of introducing additional notation.

Token 5338:
254 Chapter 7. Logical Agents 3.

Token 5339:
CNF requires ¬to appear only in literals, so we “move ¬inwards” by repeated appli- cation of the following equivalences from Figure 7.11: ¬(¬α)≡α(double-negation elimination) ¬(α∧β)≡(¬α∨¬β)(De Morgan) ¬(α∨β)≡(¬α∧¬β)(De Morgan) In the example, we require just one application of the last rule: (¬B1,1∨P1,2∨P2,1)∧((¬P1,2∧¬P2,1)∨B1,1).

Token 5340:
4. Now we have a sentence containing nested ∧and∨operators applied to literals.

Token 5341:
We apply the distributivity law from Figure 7.11, distributing ∨over∧wherever possible. (¬B1,1∨P1,2∨P2,1)∧(¬P1,2∨B1,1)∧(¬P2,1∨B1,1).

Token 5342:
The original sentence is now in CNF, as a conjunction of three clauses.

Token 5343:
It is much harder to read, but it can be used as input to a resolution procedure.

Token 5344:
A resolution algorithm Inference procedures based on resolution work by using the principle of proof by contradic- tion introduced on page 250.

Token 5345:
That is, to show that KB|=α, we show that (KB∧¬α)is unsatisﬁable. We do this by proving a contradiction.

Token 5346:
A resolution algorithm is shown in Figure 7.12. First, (KB∧¬α)is converted into CNF. Then, the resolution rule is applied to the resulting clauses.

Token 5347:
Each pair that containscomplementary literals is resolved to produce a new clause, which is added to the set if it isnot already present.

Token 5348:
The process continues until one of two things happens: •there are no new clauses that can be added, in which case KBdoes not entail α;o r , •two clauses resolve to yield the empty clause, in which case KBentails α.

Token 5349:
The empty clause—a disjunction of no disjuncts—is equivalent to False because a disjunction is true only if at least one of its disjuncts is true.

Token 5350:
Another way to see that an empty clauserepresents a contradiction is to observe that it arises only from resolving two complementaryunit clauses such as Pand¬P.

Token 5351:
We can apply the resolution procedure to a very simple inference in the wumpus world.

Token 5352:
When the agent is in [1,1], there is no breeze, so there can be no pits in neighboring squares.

Token 5353:
The relevant knowledge base is KB=R 2∧R4=(B1,1⇔(P1,2∨P2,1))∧¬B1,1 and we wish to prove αwhich is, say, ¬P1,2.

Token 5354:
When we convert (KB∧¬α)into CNF, we obtain the clauses shown at the top of Figure 7.13.

Token 5355:
The second row of the ﬁgure shows clauses obtained by resolving pairs in the ﬁrst row.

Token 5356:
Then, when P1,2is resolved with ¬P1,2, we obtain the empty clause, shown as a small square.

Token 5357:
Inspection of Figure 7.13 reveals that many resolution steps are pointless.

Token 5358:
For example, the clause B1,1∨¬B1,1∨P1,2is equivalent toTrue∨P1,2which is equivalent to True . Deducing that True is true is not very helpful.

Token 5359:
Therefore, any clause in which two complementary literals appear can be discarded.

Token 5360:
Section 7.5.

Token 5361:
Propositional Theorem Proving 255 function PL-R ESOLUTION (KB,α)returns true orfalse inputs :KB, the knowledge base, a sent ence in propositional logic α, the query, a sentence in propositional logic clauses←the set of clauses in the CNF representation of KB∧¬α new←{} loop do for each pair of clauses Ci,Cjinclauses do resolvents←PL-R ESOLVE (Ci,Cj) ifresolvents contains the empty clause then return true new←new∪resolvents ifnew⊆clauses then return false clauses←clauses∪new Figure 7.12 A simple resolution algorithm f or propositional logic.

Token 5362:
The function PL-R ESOLVE returns the set of all possible clauses obtained by resolving its two inputs.

Token 5363:
¬P2,1 B1,1 ¬B1,1 P1,2 P2,1 ¬P1,2 B1,1 ¬B1,1 P1,2 ¬P2,1 ¬P1,2 P1,2 P2,1 ¬P2,1 ¬B1,1 P2,1 B1,1P1,2 P2,1 ¬P1,2 ¬B1,1 P1,2 B1,1 ^ ^ ^^^ ^ ^ ^ ^ ^ ^ ^ Figure 7.13 Partial application of PL-R ESOLUTION to a simple inference in the wumpus world.¬P1,2is shown to follow from the ﬁrst four clauses in the top row.

Token 5364:
Completeness of resolution To conclude our discussion of resolution, we now show why PL-R ESOLUTION is complete.

Token 5365:
To do this, we introduce the resolution closure RC(S)of a set of clauses S, which is the setRESOLUTION CLOSURE of all clauses derivable by repeated application of the resolution rule to clauses in Sor their derivatives.

Token 5366:
The resolution closure is what PL-R ESOLUTION computes as the ﬁnal value of the variable clauses .

Token 5367:
It is easy to see that RC(S)must be ﬁnite, because there are only ﬁnitely many distinct clauses that can be constructed out of the symbols P1,...,P kthat appear in S. (Notice that this would not be true without the factoring step that removes multiple copies of literals.)

Token 5368:
Hence, PL-R ESOLUTION always terminates.

Token 5369:
The completeness theorem for resolution in propositional logic is called the ground resolution theorem :GROUND RESOLUTIONTHEOREM If a set of clauses is unsatisﬁable, then the resolution closure of those clauses contains the empty clause.

Token 5370:
This theorem is proved by demonstrating its contrapositive: if the closure RC(S)does not

Token 5371:
256 Chapter 7. Logical Agents contain the empty clause, then Sis satisﬁable.

Token 5372:
In fact, we can construct a model for Swith suitable truth values for P1,...,P k. The construction procedure is as follows: Forifrom 1 to k, –If a clause in RC(S)contains the literal ¬Piand all its other literals are false under the assignment chosen for P1,...,P i−1, then assign false toPi.

Token 5373:
–Otherwise, assign true toPi.

Token 5374:
This assignment to P1,...,P kis a model of S. To see this, assume the opposite—that, at some stage iin the sequence, assigning symbol Picauses some clause Cto become false.

Token 5375:
For this to happen, it must be the case that all the other literals in Cmust already have been falsiﬁed by assignments to P1,...,P i−1.

Token 5376:
Thus, Cmust now look like either (false∨false∨ ···false∨Pi)or like (false∨false∨···false∨¬Pi).

Token 5377:
If just one of these two is in RC(S),t h e n the algorithm will assign the appropriate truth value to Pito make Ctrue, so Ccan only be falsiﬁed if both of these clauses are in RC(S).N o w ,s i n c e RC(S)is closed under resolution, it will contain the resolvent of these two clauses, and that resolvent will have all of its literalsalready falsiﬁed by the assignments to P 1,...,P i−1.

Token 5378:
This contradicts our assumption that the ﬁrst falsiﬁed clause appears at stage i.

Token 5379:
Hence, we have proved that the construction never falsiﬁes a clause in RC(S); that is, it produces a model of RC(S)and thus a model of S itself (since Sis contained in RC(S)).

Token 5380:
7.5.3 Horn clauses and deﬁnite clauses The completeness of resolution makes it a very important inference method.

Token 5381:
In many practical situations, however, the full power of resolution is not needed.

Token 5382:
Some real-world knowledgebases satisfy certain restrictions on the form of sentences they contain, which enables themto use a more restricted and efﬁcient inference algorithm.

Token 5383:
One such restricted form is the deﬁnite clause , which is a disjunction of literals of DEFINITE CLAUSE which exactly one is positive .

Token 5384:
For example, the clause (¬L1,1∨¬Breeze∨B1,1)is a deﬁnite clause, whereas (¬B1,1∨P1,2∨P2,1)is not.

Token 5385:
Slightly more general is the Horn clause , which is a disjunction of literals of which at HORN CLAUSE most one is positive .

Token 5386:
So all deﬁnite clauses are Horn clauses, as are clauses with no positive literals; these are called goal clauses .

Token 5387:
Horn clauses are closed under resolution: if you resolve GOAL CLAUSES two Horn clauses, you get back a Horn clause.

Token 5388:
Knowledge bases containing only deﬁnite clauses are interesting for three reasons: 1.

Token 5389:
Every deﬁnite clause can be written as an implication whose premise is a conjunction of positive literals and whose conclusion is a single positive literal.

Token 5390:
(See Exercise 7.13.) For example, the deﬁnite clause (¬L1,1∨¬Breeze∨B1,1)can be written as the im- plication (L1,1∧Breeze )⇒B1,1.

Token 5391:
In the implication form, the sentence is easier to understand: it says that if the agent is in [1,1] and there is a breeze, then [1,1] is breezy.

Token 5392:
In Horn form, the premise is called the body and the conclusion is called the head .A BODY HEAD sentence consisting of a single positive literal, such as L1,1, is called a fact.

Token 5393:
It too can FACT be written in implication form as True⇒L1,1, but it is simpler to write just L1,1.

Token 5394:
Section 7.5.

Token 5395:
Propositional Theorem Proving 257 CNFSentence →Clause 1∧···∧ Clause n Clause→Literal 1∨···∨ Literal m Literal→Symbol|¬Symbol Symbol→P|Q|R|... HornClauseForm →DeﬁniteClauseForm |GoalClauseForm DeﬁniteClauseForm → (Symbol1∧···∧ Symboll)⇒Symbol GoalClauseForm → (Symbol1∧···∧ Symboll)⇒False Figure 7.14 A grammar for conjunctive normal form, Horn clauses, and deﬁnite clauses.

Token 5396:
A clause such as A∧B⇒Cis still a deﬁnite clause when it is written as ¬A∨¬B∨C, but only the former is considered the canonical form for deﬁnite clauses.

Token 5397:
One more class is thek-CNF sentence, which is a CNF sentence where each clause has at most kliterals. 2.

Token 5398:
Inference with Horn clauses can be done through the forward-chaining andbackward- FORWARD-CHAINING chaining algorithms, which we explain next.

Token 5399:
Both of these algorithms are natural,BACKWARD- CHAINING in that the inference steps are obvious and easy for humans to follow.

Token 5400:
This type of inference is the basis for logic programming , which is discussed in Chapter 9. 3.

Token 5401:
Deciding entailment with Horn clauses can be done in time that is linear in the size of the knowledge base—a pleasant surprise.

Token 5402:
7.5.4 Forward and backward chaining The forward-chaining algorithm PL-FC-E NTAILS ?

Token 5403:
(KB,q) determines if a single proposi- tion symbol q—the query—is entailed by a knowledge base of deﬁnite clauses.

Token 5404:
It begins from known facts (positive literals) in the knowledge base.

Token 5405:
If all the premises of an implica- tion are known, then its conclusion is added to the set of known facts.

Token 5406:
For example, if L1,1 andBreeze are known and (L1,1∧Breeze )⇒B1,1is in the knowledge base, then B1,1can be added.

Token 5407:
This process continues until the query qis added or until no further inferences can be made.

Token 5408:
The detailed algorithm is shown in Figure 7.15; the main point to remember is thatit runs in linear time.

Token 5409:
The best way to understand the algorithm is through an example and a picture.

Token 5410:
Fig- ure 7.16(a) shows a simple knowledge base of Horn clauses with AandBas known facts.

Token 5411:
Figure 7.16(b) shows the same knowledge base drawn as an AND–OR graph (see Chap- ter 4).

Token 5412:
In AND–OR graphs, multiple links joined by an arc indicate a conjunction—every link must be proved—while multiple links without an arc indicate a disjunction—any link can be proved.

Token 5413:
It is easy to see how forward chaining works in the graph.

Token 5414:
The known leaves (here, AandB) are set, and inference propagates up the graph as far as possible.

Token 5415:
Wher- ever a conjunction appears, the propagation waits until all the conjuncts are known before proceeding.

Token 5416:
The reader is encouraged to work through the example in detail.

Token 5417:
258 Chapter 7. Logical Agents function PL-FC-E NTAILS ?

Token 5418:
(KB,q)returns true orfalse inputs :KB, the knowledge base, a set of p ropositional deﬁnite clauses q, the query, a pr oposition s ymbol count←at a b l e ,w h e r e count [c] is the number of symbols in c’s premise inferred←at a b l e ,w h e r e inferred [s] is initially false for all symbols agenda←a queue of symbols, initially symbols known to be true in KB whileagenda is not empty do p←POP(agenda ) ifp=qthen return true ifinferred [p]=false then inferred [p]←true for each clause cinKBwhere pis inc.PREMISE do decrement count [c] ifcount [c]=0 then addc.CONCLUSION toagenda return false Figure 7.15 The forward-chaining algorithm for propositional logic.

Token 5419:
The agenda keeps track of symbols known to be true but not yet “processed.” The count table keeps track of how many premises of each implication are as yet unknown.

Token 5420:
Whenever a new symbol pfrom the agenda is processed, the count is reduced by one for each implication in whose premise pappears (easily identiﬁed in constant time with appropriate indexing.)

Token 5421:
If a count reaches zero, all the premises of the implication are known, so its conclusion can be added to theagenda.

Token 5422:
Finally, we need to keep track of which symbols have been processed; a symbol that is already in the set of inferred symbols need not be added to the agenda again.

Token 5423:
This avoids redundant work and prevents loops caused by implications such as P⇒QandQ⇒P.

Token 5424:
It is easy to see that forward chaining is sound : every inference is essentially an appli- cation of Modus Ponens.

Token 5425:
Forward chaining is also complete : every entailed atomic sentence will be derived.

Token 5426:
The easiest way to see this is to consider the ﬁnal state of the inferred table (after the algorithm reaches a ﬁxed point where no new inferences are possible).

Token 5427:
The table FIXED POINT contains true for each symbol inferred during the process, and false for all other symbols.

Token 5428:
We can view the table as a logical model; moreover, every deﬁnite clause in the original KB is true in this model.

Token 5429:
To see this, assume the opposite, namely that some clause a1∧...∧ak⇒b is false in the model.

Token 5430:
Then a1∧...∧akmust be true in the model and bmust be false in the model.

Token 5431:
But this contradicts our assumption that the algorithm has reached a ﬁxed point!We can conclude, therefore, that the set of atomic sentences inferred at the ﬁxed point deﬁnesa model of the original KB.

Token 5432:
Furthermore, any atomic sentence qthat is entailed by the KB must be true in all its models and in this model in particular.

Token 5433:
Hence, every entailed atomicsentence qmust be inferred by the algorithm.

Token 5434:
Forward chaining is an example of the general concept of data-driven reasoning—that DATA-DRIVEN is, reasoning in which the focus of attention starts with the known data.

Token 5435:
It can be used within an agent to derive conclusions from incoming percepts, often without a speciﬁc query inmind.

Token 5436:
For example, the wumpus agent might T ELLits percepts to the knowledge base using

Token 5437:
Section 7.6. Effective Propositional Model Checking 259 P⇒Q L∧M⇒P B∧L⇒M A∧P⇒L A∧B⇒L A BQ P M L B A (a) (b) Figure 7.16 (a) A set of Horn clauses.

Token 5438:
(b) The corresponding AND –ORgraph.

Token 5439:
an incremental forward-chaining algorithm in which new facts can be added to the agenda to initiate new inferences.

Token 5440:
In humans, a certain amount of data-driven reasoning occurs as newinformation arrives.

Token 5441:
For example, if I am indoors and hear rain starting to fall, it might occurto me that the picnic will be canceled.

Token 5442:
Yet it will probably not occur to me that the seventeenthpetal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining under careful control, lest they be swamped with irrelevant consequences.

Token 5443:
The backward-chaining algorithm, as its name suggests, works backward from the query. If the query qis known to be true, then no work is needed.

Token 5444:
Otherwise, the algorithm ﬁnds those implications in the knowledge base whose conclusion is q.

Token 5445:
If all the premises of one of those implications can be proved true (by backward chaining), then qis true.

Token 5446:
When applied to the query Qin Figure 7.16, it works back down the graph until it reaches a set of known facts, AandB, that forms the basis for a proof.

Token 5447:
The algorithm is essentially identical to the A ND-OR-GRAPH -SEARCH algorithm in Figure 4.11.

Token 5448:
As with forward chaining, an efﬁcient implementation runs in linear time. Backward chaining is a form of goal-directed reasoning .

Token 5449:
It is useful for answeringGOAL-DIRECTED REASONING speciﬁc questions such as “What shall I do now?” and “Where are my keys?” Often, the cost of backward chaining is much less than linear in the size of the knowledge base, because the process touches only relevant facts.

Token 5450:
7.6 E FFECTIVE PROPOSITIONAL MODEL CHECKING In this section, we describe two families of efﬁcient algorithms for general propositional inference based on model checking: One approach based on backtracking search, and oneon local hill-climbing search.

Token 5451:
These algorithms are part of the “technology” of propositionallogic. This section can be skimmed on a ﬁrst reading of the chapter.

Token 5452:
260 Chapter 7. Logical Agents The algorithms we describe are for checking satisﬁability: the SAT problem.

Token 5453:
(As noted earlier, testing entailment, α|=β, can be done by testing unsatisﬁability of α∧¬β.)

Token 5454:
We have already noted the connection between ﬁnding a satisfying model for a logical sentenceand ﬁnding a solution for a constraint satisfaction problem, so it is perhaps not surprising thatthe two families of algorithms closely resemble the backtracking algorithms of Section 6.3 and the local search algorithms of Section 6.4.

Token 5455:
They are, however, extremely important in their own right because so many combinatorial problems in computer science can be reducedto checking the satisﬁability of a propositional sentence.

Token 5456:
Any improvement in satisﬁabilityalgorithms has huge consequences for our ability to handle complexity in general.

Token 5457:
7.6.1 A complete backtracking algorithm The ﬁrst algorithm we consider is often called the Davis–Putnam algorithm , after the sem-DAVIS–PUTNAM ALGORITHM inal paper by Martin Davis and Hilary Putnam (1960).

Token 5458:
The algorithm is in fact the version described by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini-tials of all four authors.

Token 5459:
DPLL takes as input a sentence in conjunctive normal form—a setof clauses.

Token 5460:
Like B ACKTRACKING -SEARCH and TT-E NTAILS ?, it is essentially a recursive, depth-ﬁrst enumeration of possible models.

Token 5461:
It embodies three improvements over the simplescheme of TT-E NTAILS ?

Token 5462:
: •Early termination : The algorithm detects whether the sentence must be true or false, even with a partially completed model.

Token 5463:
A clause is true if anyliteral is true, even if the other literals do not yet have truth values; hence, the sentence as a whole could be judged true even before the model is complete.

Token 5464:
For example, the sentence (A∨B)∧ (A∨C)is true if Ais true, regardless of the values of BandC.

Token 5465:
Similarly, a sentence is false if anyclause is false, which occurs when each of its literals is false.

Token 5466:
Again, this can occur long before the model is complete. Early termination avoids examination of entire subtrees in the search space.

Token 5467:
•Pure symbol heuristic :Apure symbol is a symbol that always appears with the same PURE SYMBOL “sign” in all clauses.

Token 5468:
For example, in the three clauses (A∨¬B),(¬B∨¬C),a n d (C∨A), the symbol Ais pure because only the positive literal appears, Bis pure because only the negative literal appears, and Cis impure.

Token 5469:
It is easy to see that if a sentence has a model, then it has a model with the pure symbols assigned so as tomake their literals true, because doing so can never make a clause false.

Token 5470:
Note that, in determining the purity of a symbol, the algorithm can ignore clauses that are alreadyknown to be true in the model constructed so far.

Token 5471:
For example, if the model contains B=false , then the clause (¬B∨¬C)is already true, and in the remaining clauses C appears only as a positive literal; therefore Cbecomes pure.

Token 5472:
•Unit clause heuristic :Aunit clause was deﬁned earlier as a clause with just one lit- eral.

Token 5473:
In the context of DPLL, it also means clauses in which all literals but one are already assigned false by the model.

Token 5474:
For example, if the model contains B=true, then(¬B∨¬C)simpliﬁes to¬C, which is a unit clause.

Token 5475:
Obviously, for this clause to be true, Cmust be set to false . The unit clause heuristic assigns all such symbols before branching on the remainder.

Token 5476:
One important consequence of the heuristic is that

Token 5477:
Section 7.6. Effective Propositional Model Checking 261 function DPLL-S ATISFIABLE ?

Token 5478:
(s)returns true orfalse inputs :s, a sentence in propositional logic clauses←the set of clauses in the CNF representation of s symbols←a list of the proposition symbols in s return DPLL( clauses ,symbols ,{}) function DPLL( clauses ,symbols ,model )returns true orfalse ifevery clause in clauses is true in model then return true ifsome clause in clauses is false in model then return false P,value←FIND-PURE-SYMBOL (symbols ,clauses ,model ) ifPis non-null then return DPLL( clauses ,symbols –P,model∪{P=value}) P,value←FIND-UNIT-CLAUSE (clauses ,model ) ifPis non-null then return DPLL( clauses ,symbols –P,model∪{P=value}) P←FIRST (symbols );rest←REST(symbols ) return DPLL( clauses ,rest,model∪{P=true})or DPLL( clauses ,rest,model∪{P=false})) Figure 7.17 The DPLL algorithm for checking satisﬁability of a sentence in propositional logic.

Token 5479:
The ideas behind F IND-PURE-SYMBOL and F IND-UNIT-CLAUSE are described in the text; each returns a symbol (or null) and the truth value to assign to that symbol.

Token 5480:
Like TT-E NTAILS ?, DPLL operates over partial models.

Token 5481:
any attempt to prove (by refutation) a literal that is already in the knowledge base will succeed immediately (Exercise 7.23).

Token 5482:
Notice also that assigning one unit clause can create another unit clause—for example, when Cis set to false ,(C∨A)becomes a unit clause, causing true to be assigned to A.

Token 5483:
This “cascade” of forced assignments is called unit propagation .

Token 5484:
It resembles the process of forward chaining with deﬁnite UNIT PROPAGATION clauses, and indeed, if the CNF expression contains only deﬁnite clauses then DPLL essentially replicates forward chaining.

Token 5485:
(See Exercise 7.24.) The DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the search process.

Token 5486:
What Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to large problems.

Token 5487:
It is interesting that most of these tricks are in fact rather general, and wehave seen them before in other guises: 1.Component analysis (as seen with Tasmania in CSPs): As DPLL assigns truth values to variables, the set of clauses may become separated into disjoint subsets, called com- ponents , that share no unassigned variables.

Token 5488:
Given an efﬁcient way to detect when this occurs, a solver can gain considerable speed by working on each component separately.

Token 5489:
2.Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple imple- mentation of DPLL uses an arbitrary variable ordering and always tries the value true before false.T h e degree heuristic (see page 216) suggests choosing the variable that appears most frequently over all remaining clauses.

Token 5490:
262 Chapter 7.

Token 5491:
Logical Agents 3.Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that can- not be solved in hours of run time with chronological backtracking can be solved inseconds with intelligent backtracking that backs up all the way to the relevant point ofconﬂict.

Token 5492:
All SAT solvers that do intelligent backtracking use some form of conﬂict clause learning to record conﬂicts so that they won’t be repeated later in the search.

Token 5493:
Usually a limited-size set of conﬂicts is kept, and rarely used ones are dropped.

Token 5494:
4.Random restarts (as seen on page 124 for hill-climbing): Sometimes a run appears not to be making progress.

Token 5495:
In this case, we can start over from the top of the search tree,rather than trying to continue.

Token 5496:
After restarting, different random choices (in variable and value selection) are made.

Token 5497:
Clauses that are learned in the ﬁrst run are retained after the restart and can help prune the search space.

Token 5498:
Restarting does not guarantee that asolution will be found faster, but it does reduce the variance on the time to solution.

Token 5499:
5.Clever indexing (as seen in many algorithms): The speedup methods used in DPLL itself, as well as the tricks used in modern solvers, require fast indexing of such things as “the set of clauses in which variable X iappears as a positive literal.” This task is complicated by the fact that the algorithms are interested only in the clauses that havenot yet been satisﬁed by previous assignments to variables, so the indexing structuresmust be updated dynamically as the computation proceeds.

Token 5500:
With these enhancements, modern solvers can handle problems with tens of millions of vari- ables.

Token 5501:
They have revolutionized areas such as hardware veriﬁcation and security protocolveriﬁcation, which previously required laborious, hand-guided proofs.

Token 5502:
7.6.2 Local search algorithms We have seen several local search algorithms so far in this book, including H ILL-CLIMBING (page 122) and S IMULATED -ANNEALING (page 126).

Token 5503:
These algorithms can be applied di- rectly to satisﬁability problems, provided that we choose the right evaluation function.

Token 5504:
Be-cause the goal is to ﬁnd an assignment that satisﬁes every clause, an evaluation function that counts the number of unsatisﬁed clauses will do the job.

Token 5505:
In fact, this is exactly the measure used by the M IN-CONFLICTS algorithm for CSPs (page 221).

Token 5506:
All these algorithms take steps in the space of complete assignments, ﬂipping the truth value of one symbol at a time.

Token 5507:
Thespace usually contains many local minima, to escape from which various forms of random-ness are required.

Token 5508:
In recent years, there has been a great deal of experimentation to ﬁnd agood balance between greediness and randomness.

Token 5509:
One of the simplest and most effective algorithms to emerge from all this work is called W ALKSAT (Figure 7.18).

Token 5510:
On every iteration, the algorithm picks an unsatisﬁed clause and picks a symbol in the clause to ﬂip.

Token 5511:
It chooses randomly between two ways to pick whichsymbol to ﬂip: (1) a “min-conﬂicts” step that minimizes the number of unsatisﬁed clauses inthe new state and (2) a “random walk” step that picks the symbol randomly.

Token 5512:
When W ALKSAT returns a model, the input sentence is indeed satisﬁable, but when it returns failure , there are two possible causes: either the sentence is unsatisﬁable or we need to give the algorithm more time.

Token 5513:
If we set max ﬂips=∞andp>0,WALKSAT will eventually return a model (if one exists), because the random-walk steps will eventually hit

Token 5514:
Section 7.6.

Token 5515:
Effective Propositional Model Checking 263 function WALKSAT(clauses ,p,max ﬂips )returns a satisfying model or failure inputs :clauses , a set of clauses in propositional logic p, the probability of choosing to do a “random walk” move, typically around 0.5 max ﬂips , number of ﬂips allowed before giving up model←a random assignment of true/false to the symbols in clauses fori=1 tomax ﬂips do ifmodel satisﬁes clauses then return model clause←a randomly selected clause from clauses that is false in model with probability pﬂip the value in model of a randomly selected symbol from clause elseﬂip whichever symbol in clause maximizes the number of satisﬁed clauses return failure Figure 7.18 The W ALKSAT algorithm for checking satisﬁability by randomly ﬂipping the values of variables.

Token 5516:
Many versions of the algorithm exist. upon the solution.

Token 5517:
Alas, if max ﬂips is inﬁnity and the sentence is unsatisﬁable, then the algorithm never terminates!

Token 5518:
For this reason, W ALKSAT is most useful when we expect a solution to exist—for ex- ample, the problems discussed in Chapters 3 and 6 usually have solutions.

Token 5519:
On the other hand, WALKSAT cannot always detect unsatisﬁability , which is required for deciding entailment.

Token 5520:
For example, an agent cannot reliably use W ALKSAT to prove that a square is safe in the wumpus world.

Token 5521:
Instead, it can say, “I thought about it for an hour and couldn’t come up witha possible world in which the square isn’t safe.” This may be a good empirical indicator that the square is safe, but it’s certainly not a proof.

Token 5522:
7.6.3 The landscape of random SAT problems Some SAT problems are harder than others.

Token 5523:
Easy problems can be solved by any old algo- rithm, but because we know that SAT is NP-complete, at least some problem instances mustrequire exponential run time.

Token 5524:
In Chapter 6, we saw some surprising discoveries about certainkinds of problems.

Token 5525:
For example, the n-queens problem—thought to be quite tricky for back- tracking search algorithms—turned out to be trivially easy for local search methods, such as min-conﬂicts.

Token 5526:
This is because solutions are very densely distributed in the space of assign-ments, and any initial assignment is guaranteed to have a solution nearby.

Token 5527:
Thus, n-queens is easy because it is underconstrained .

Token 5528:
UNDERCONSTRAINED When we look at satisﬁability problems in conjunctive normal form, an undercon- strained problem is one with relatively fewclauses constraining the variables.

Token 5529:
For example, here is a randomly generated 3-CNF sentence with ﬁve symbols and ﬁve clauses: (¬D∨¬B∨C)∧(B∨¬A∨¬C)∧(¬C∨¬B∨E) ∧(E∨¬D∨B)∧(B∨E∨¬C).

Token 5530:
Sixteen of the 32 possible assignments are models of this sentence, so, on average, it would take just two random guesses to ﬁnd a model.

Token 5531:
This is an easy satisﬁability problem, as are

Token 5532:
264 Chapter 7. Logical Agents most such underconstrained problems.

Token 5533:
On the other hand, an overconstrained problem has many clauses relative to the number of variables and is likely to have no solutions.

Token 5534:
To go beyond these basic intuitions, we must deﬁne exactly how random sentences are generated.

Token 5535:
The notation CNF k(m,n)denotes a k-CNF sentence with mclauses and n symbols, where the clauses are chosen uniformly, independently, and without replacement from among all clauses with kdifferent literals, which are positive or negative at random.

Token 5536:
(A symbol may not appear twice in a clause, nor may a clause appear twice in a sentence.)

Token 5537:
Given a source of random sentences, we can measure the probability of satisﬁability.

Token 5538:
Figure 7.19(a) plots the probability for CNF 3(m,50), that is, sentences with 50 variables and 3 literals per clause, as a function of the clause/symbol ratio, m/n .

Token 5539:
As we expect, for small m/n the probability of satisﬁability is close to 1, and at large m/n the probability is close to 0.

Token 5540:
The probability drops fairly sharply around m/n=4.3.

Token 5541:
Empirically, we ﬁnd that the “cliff” stays in roughly the same place (for k=3) and gets sharper and sharper as n increases.

Token 5542:
Theoretically, the satisﬁability threshold conjecture says that for every k≥3,SATISFIABILITY THRESHOLDCONJECTURE there is a threshold ratio rksuch that, as ngoes to inﬁnity, the probability that CNF k(n,rn) is satisﬁable becomes 1 for all values of rbelow the threshold, and 0 for all values above.

Token 5543:
The conjecture remains unproven.

Token 5544:
00.20.40.60.81 0 1 2 3 4 5 6 7 8P(satisfiable) Clause/symbol ratio m/n0200400600800100012001400160018002000 0 1 2 3 4 5 6 7 8Runtime Clause/symbol ratio m/nDPLL WalkSAT (a) (b) Figure 7.19 (a) Graph showing the probability that a random 3-CNF sentence with n=50 symbols is satisﬁable, as a function of the clause/symbol ratio m/n .

Token 5545:
(b) Graph of the median run time (measured in number of recursive calls to DPLL, a good proxy) on random 3-CNF sentences.

Token 5546:
The most difﬁcult problems have a clause/symbol ratio of about 4.3.

Token 5547:
Now that we have a good idea where the satisﬁable and unsatisﬁable problems are, the next question is, where are the hard problems?

Token 5548:
It turns out that they are also often at thethreshold value.

Token 5549:
Figure 7.19(b) shows that 50-symbol problems at the threshold value of 4.3 are about 20 times more difﬁcult to solve than those at a ratio of 3.3.

Token 5550:
The underconstrained problems are easiest to solve (because it is so easy to guess a solution); the overconstrainedproblems are not as easy as the underconstrained, but still are much easier than the ones rightat the threshold.

Token 5551:
Section 7.7.

Token 5552:
Agents Based on Propositional Logic 265 7.7 A GENTS BASED ON PROPOSITIONAL LOGIC In this section, we bring together what we have learned so far in order to construct wumpus world agents that use propositional logic.

Token 5553:
The ﬁrst step is to enable the agent to deduce, to the extent possible, the state of the world given its percept history.

Token 5554:
This requires writing down a complete logical model of the effects of actions.

Token 5555:
We also show how the agent can keep track ofthe world efﬁciently without going back into the percept history for each inference.

Token 5556:
Finally,we show how the agent can use logical inference to construct plans that are guaranteed toachieve its goals.

Token 5557:
7.7.1 The current state of the world As stated at the beginning of the chapter, a logical agent operates by deducing what to dofrom a knowledge base of sentences about the world.

Token 5558:
The knowledge base is composed ofaxioms—general knowledge about how the world works—and percept sentences obtainedfrom the agent’s experience in a particular world.

Token 5559:
In this section, we focus on the problem ofdeducing the current state of the wumpus world—where am I, is that square safe, and so on.

Token 5560:
We began collecting axioms in Section 7.4.3. The agent knows that the starting square contains no pit ( ¬P 1,1) and no wumpus ( ¬W1,1).

Token 5561:
Furthermore, for each square, it knows that the square is breezy if and only if a neighboring square has a pit; and a square is smelly if and only if a neighboring square has a wumpus.

Token 5562:
Thus, we include a large collection of sentences of the following form: B1,1⇔(P1,2∨P2,1) S1,1⇔(W1,2∨W2,1) ··· The agent also knows that there is exactly one wumpus.

Token 5563:
This is expressed in two parts. First, we have to say that there is at least one wumpus: W1,1∨W1,2∨···∨ W4,3∨W4,4.

Token 5564:
Then, we have to say that there is at most one wumpus.

Token 5565:
For each pair of locations, we add a sentence saying that at least one of them must be wumpus-free: ¬W1,1∨¬W1,2 ¬W1,1∨¬W1,3 ··· ¬W4,3∨¬W4,4.

Token 5566:
So far, so good. Now let’s consider the agent’s percepts.

Token 5567:
If there is currently a stench, one might suppose that a proposition Stench should be added to the knowledge base.

Token 5568:
This is not quite right, however: if there was no stench at the previous time step, then ¬Stench would al- ready be asserted, and the new assertion would simply result in a contradiction.

Token 5569:
The problemis solved when we realize that a percept asserts something only about the current time .

Token 5570:
Thus, if the time step (as supplied to M AKE-PERCEPT -SENTENCE in Figure 7.1) is 4, then we add

Token 5571:
266 Chapter 7. Logical Agents Stench4to the knowledge base, rather than Stench —neatly avoiding any contradiction with ¬Stench3.

Token 5572:
The same goes for the breeze, bump, glitter, and scream percepts.

Token 5573:
The idea of associating propositions with time steps extends to any aspect of the world that changes over time.

Token 5574:
For example, the initial knowledge base includes L0 1,1—the agent is in square [1,1]at time 0—as well as FacingEast0,HaveArrow0,a n dWumpusAlive0.W eu s e the word ﬂuent (from the Latin ﬂuens , ﬂowing) to refer an aspect of the world that changes.

Token 5575:
FLUENT “Fluent” is a synonym for “state variable,” in the sense described in the discussion of factored representations in Section 2.4.7 on page 57.

Token 5576:
Symbols associated with permanent aspects ofthe world do not need a time superscript and are sometimes called atemporal variables .

Token 5577:
ATEMPORAL VARIABLE We can connect stench and breeze percepts directly to the properties of the squares where they are experienced through the location ﬂuent as follows.10For any time step tand any square [x,y], we assert Lt x,y⇒(Breezet⇔Bx,y) Lt x,y⇒(Stencht⇔Sx,y).

Token 5578:
Now, of course, we need axioms that allow the agent to keep track of ﬂuents such as Lt x,y.

Token 5579:
These ﬂuents change as the result of actions taken by the agent, so, in the terminology ofChapter 3, we need to write down the transition model of the wumpus world as a set of logical sentences.

Token 5580:
First, we need proposition symbols for the occurrences of actions.

Token 5581:
As with percepts, these symbols are indexed by time; thus, Forward 0means that the agent executes the Forward action at time 0.

Token 5582:
By convention, the percept for a given time step happens ﬁrst, followed by the action for that time step, followed by a transition to the next time step.

Token 5583:
To describe how the world changes, we can try writing effect axioms that specify the EFFECT AXIOM outcome of an action at the next time step.

Token 5584:
For example, if the agent is at location [1,1]facing east at time 0 and goes Forward , the result is that the agent is in square [2,1]and no longer is in[1,1]: L0 1,1∧FacingEast0∧Forward0⇒(L1 2,1∧¬L1 1,1).

Token 5585:
(7.1) We would need one such sentence for each possible time step, for each of the 16 squares, and each of the four orientations.

Token 5586:
We would also need similar sentences for the other actions:Grab ,Shoot ,Climb ,TurnLeft ,a n d TurnRight .

Token 5587:
Let us suppose that the agent does decide to move Forward at time 0 and asserts this fact into its knowledge base.

Token 5588:
Given the effect axiom in Equation (7.1), combined with theinitial assertions about the state at time 0, the agent can now deduce that it is in [2,1].T h a t is, A SK(KB,L1 2,1)=true.

Token 5589:
So far, so good.

Token 5590:
Unfortunately, the news elsewhere is less good: if we A SK(KB,HaveArrow1), the answer is false , that is, the agent cannot prove it still has the arrow; nor can it prove it doesn’t have it!

Token 5591:
The information has been lost because the effect axiom fails to state what remains unchanged as the result of an action.

Token 5592:
The need to do this gives rise to the frame problem .11One possible solution to the frame problem would FRAME PROBLEM 10Section 7.4.3 conveniently glossed over this requirement.

Token 5593:
11The name “frame problem” comes from “frame of refe rence” in physics—the assumed stationary background with respect to which motion is measured.

Token 5594:
It also has an analogy to the frames of a movie, in which normallymost of the background stays constant while changes occur in the foreground.

Token 5595:
Section 7.7. Agents Based on Propositional Logic 267 be to add frame axioms explicitly asserting all the propositions that remain the same.

Token 5596:
For FRAME AXIOM example, for each time twe would have Forwardt⇒(HaveArrowt⇔HaveArrowt+1) Forwardt⇒(WumpusAlivet⇔WumpusAlivet+1) ··· where we explicitly mention every proposition that stays unchanged from time tto time t+1 under the action Forward .

Token 5597:
Although the agent now knows that it still has the arrow after moving forward and that the wumpus hasn’t died or come back to life, the proliferationof frame axioms seems remarkably inefﬁcient.

Token 5598:
In a world with mdifferent actions and n ﬂuents, the set of frame axioms will be of size O(mn).

Token 5599:
This speciﬁc manifestation of the frame problem is sometimes called the representational frame problem .

Token 5600:
Historically, the REPRESENTATIONAL FRAME PROBLEM problem was a signiﬁcant one for AI researchers; we explore it further in the notes at the end of the chapter.

Token 5601:
The representational frame problem is signiﬁcant because the real world has very many ﬂuents, to put it mildly.

Token 5602:
Fortunately for us humans, each action typically changes no morethan some small number kof those ﬂuents—the world exhibits locality .

Token 5603:
Solving the repre- LOCALITY sentational frame problem requires deﬁning the transition model with a set of axioms of size O(mk)rather than size O(mn).

Token 5604:
There is also an inferential frame problem : the problemINFERENTIAL FRAME PROBLEM of projecting forward the results of a tstep plan of action in time O(kt)rather than O(nt).

Token 5605:
The solution to the problem involves changing one’s focus from writing axioms about actions to writing axioms about ﬂuents .

Token 5606:
Thus, for each ﬂuent F, we will have an axiom that deﬁnes the truth value of Ft+1in terms of ﬂuents (including Fitself) at time tand the actions that may have occurred at time t. Now, the truth value of Ft+1can be set in one of two ways: either the action at time tcauses Fto be true at t+1,o rFwas already true at time tand the action at time tdoes not cause it to be false.

Token 5607:
An axiom of this form is called a successor-state axiom and has this schema:SUCCESSOR-STATE AXIOM Ft+1⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt).

Token 5608:
One of the simplest successor-state axioms is the one for HaveArrow .

Token 5609:
Because there is no action for reloading, the ActionCausesFtpart goes away and we are left with HaveArrowt+1⇔(HaveArrowt∧¬Shoott).

Token 5610:
(7.2) For the agent’s location, the successor-state axioms are more elaborate.

Token 5611:
For example, Lt+1 1,1 is true if either (a) the agent moved Forward from[1,2]when facing south, or from [2,1] when facing west; or (b) Lt 1,1was already true and the action did not cause movement (either because the action was not Forward or because the action bumped into a wall).

Token 5612:
Written out in propositional logic, this becomes Lt+1 1,1⇔(Lt 1,1∧(¬Forwardt∨Bumpt+1)) ∨(Lt 1,2∧(Southt∧Forwardt)) (7.3) ∨(Lt 2,1∧(Westt∧Forwardt)).

Token 5613:
Exercise 7.26 asks you to write out axioms for the remaining wumpus world ﬂuents.

Token 5614:
268 Chapter 7.

Token 5615:
Logical Agents Given a complete set of successor-state axioms and the other axioms listed at the begin- ning of this section, the agent will be able to A SKand answer any answerable question about the current state of the world.

Token 5616:
For example, in Section 7.2 the initial sequence of percepts andactions is ¬Stench 0∧¬Breeze0∧¬Glitter0∧¬Bump0∧¬Scream0;Forward0 ¬Stench1∧Breeze1∧¬Glitter1∧¬Bump1∧¬Scream1;TurnRight1 ¬Stench2∧Breeze2∧¬Glitter2∧¬Bump2∧¬Scream2;TurnRight2 ¬Stench3∧Breeze3∧¬Glitter3∧¬Bump3∧¬Scream3;Forward3 ¬Stench4∧¬Breeze4∧¬Glitter4∧¬Bump4∧¬Scream4;TurnRight4 ¬Stench5∧¬Breeze5∧¬Glitter5∧¬Bump5∧¬Scream5;Forward5 Stench6∧¬Breeze6∧¬Glitter6∧¬Bump6∧¬Scream6 At this point, we have A SK(KB,L6 1,2)=true, so the agent knows where it is.

Token 5617:
Moreover, ASK(KB,W1,3)=true and A SK(KB,P3,1)=true, so the agent has found the wumpus and one of the pits.

Token 5618:
The most important question for the agent is whether a square is OK to moveinto, that is, the square contains no pit nor live wumpus.

Token 5619:
It’s convenient to add axioms forthis, having the form OK t x,y⇔¬Px,y∧¬(Wx,y∧WumpusAlivet).

Token 5620:
Finally, A SK(KB,OK6 2,2)=true , so the square [2,2]is OK to move into.

Token 5621:
In fact, given a sound and complete inference algorithm such as DPLL, the agent can answer any answerablequestion about which squares are OK—and can do so in just a few milliseconds for small-to-medium wumpus worlds.

Token 5622:
Solving the representational and inferential frame problems is a big step forward, but a pernicious problem remains: we need to conﬁrm that allthe necessary preconditions of an action hold for it to have its intended effect.

Token 5623:
We said that the Forward action moves the agent ahead unless there is a wall in the way, but there are many other unusual exceptions that couldcause the action to fail: the agent might trip and fall, be stricken with a heart attack, be carriedaway by giant bats, etc.

Token 5624:
Specifying all these exceptions is called the qualiﬁcation problem .

Token 5625:
QUALIFICATION PROBLEM There is no complete solution within logic; system designers have to use good judgment in deciding how detailed they want to be in specifying their model, and what details they wantto leave out.

Token 5626:
We will see in Chapter 13 that probability theory allows us to summarize all theexceptions without explicitly naming them.

Token 5627:
7.7.2 A hybrid agent The ability to deduce various aspects of the state of the world can be combined fairly straight-forwardly with condition–action rules and with problem-solving algorithms from Chapters 3and 4 to produce a hybrid agent for the wumpus world.

Token 5628:
Figure 7.20 shows one possible way HYBRID AGENT to do this. The agent program maintains and updates a knowledge base as well as a current plan.

Token 5629:
The initial knowledge base contains the atemporal axioms—those that don’t depend ont, such as the axiom relating the breeziness of squares to the presence of pits.

Token 5630:
At each time step, the new percept sentence is added along with all the axioms that depend on t,s u c h

Token 5631:
Section 7.7. Agents Based on Propositional Logic 269 as the successor-state axioms.

Token 5632:
(The next section explains why the agent doesn’t need axioms forfuture time steps.)

Token 5633:
Then, the agent uses logical inference, by A SKing questions of the knowledge base, to work out which squares are safe and which have yet to be visited.

Token 5634:
The main body of the agent program constructs a plan based on a decreasing priority of goals.

Token 5635:
First, if there is a glitter, the program constructs a plan to grab the gold, follow a route back to the initial location, and climb out of the cave.

Token 5636:
Otherwise, if there is no current plan, the program plans a route to the closest safe square that it has not visited yet, making surethe route goes through only safe squares.

Token 5637:
Route planning is done with A ∗search, not with ASK.

Token 5638:
If there are no safe squares to explore, the next step—if the agent still has an arrow—is to try to make a safe square by shooting at one of the possible wumpus locations.

Token 5639:
These aredetermined by asking where A SK(KB,¬Wx,y)is false—that is, where it is notknown that there is nota wumpus.

Token 5640:
The function P LAN-SHOT (not shown) uses P LAN-ROUTE to plan a sequence of actions that will line up this shot.

Token 5641:
If this fails, the program looks for a square toexplore that is not provably unsafe—that is, a square for which A SK(KB,¬OKt x,y)returns false.

Token 5642:
If there is no such square, then the mission is impossible and the agent retreats to [1,1] and climbs out of the cave.

Token 5643:
7.7.3 Logical state estimation The agent program in Figure 7.20 works quite well, but it has one major weakness: as timegoes by, the computational expense involved in the calls to A SKgoes up and up.

Token 5644:
This happens mainly because the required inferences have to go back further and further in time and involve more and more proposition symbols.

Token 5645:
Obviously, this is unsustainable—we cannot have an agent whose time to process each percept grows in proportion to the length of its life!

Token 5646:
What we really need is a constant update time—that is, independent of t. The obvious answer is to save, or cache , the results of inference, so that the inference process at the next time step can CACHING build on the results of earlier steps instead of having to start again from scratch.

Token 5647:
As we saw in Section 4.4, the past history of percepts and all their ramiﬁcations can be replaced by the belief state —that is, some representation of the set of all possible current states of the world.12The process of updating the belief state as new percepts arrive is called state estimation .

Token 5648:
Whereas in Section 4.4 the belief state was an explicit list of states, here we can use a logical sentence involving the proposition symbols associated with the currenttime step, as well as the atemporal symbols.

Token 5649:
For example, the logical sentence WumpusAlive 1∧L1 2,1∧B2,1∧(P3,1∨P2,2) (7.4) represents the set of all states at time 1 in which the wumpus is alive, the agent is at [2,1], that square is breezy, and there is a pit in [3,1]or[2,2]or both.

Token 5650:
Maintaining an exact belief state as a logical formula turns out not to be easy.

Token 5651:
If there arenﬂuent symbols for time t, then there are 2npossible states—that is, assignments of truth values to those symbols.

Token 5652:
Now, the set of belief states is the powerset (set of all subsets) of theset of physical states. There are 2 nphysical states, hence 22nbelief states.

Token 5653:
Even if we used the most compact possible encoding of logical formulas, with each belief state represented 12We can think of the percept history itself as a representation of the belief state, but one that makes inference increasingly expensive as the history gets longer.

Token 5654:
270 Chapter 7.

Token 5655:
Logical Agents function HYBRID -WUMPUS -AGENT (percept )returns anaction inputs :percept , a list, [ stench ,breeze ,glitter ,bump ,scream ] persistent :KB, a knowledge base, initially the atemporal “wumpus physics” t, a counter, initially 0, indicating time plan , an action sequence, initially empty TELL(KB,MAKE-PERCEPT -SENTENCE (percept ,t)) TELLtheKBthe temporal “physics” sentences for time t safe←{[x,y]:ASK(KB,OKt x,y)=true} ifASK(KB,Glittert)=true then plan←[Grab ]+P LAN-ROUTE (current ,{[1,1]},safe)+[Climb ] ifplan is empty then unvisited←{[x,y]:ASK(KB,Lt/prime x,y)=false for all t/prime≤t} plan←PLAN-ROUTE (current ,unvisited∩safe,safe) ifplan is empty and A SK(KB,HaveArrowt)=true then possible wumpus←{[x,y]:ASK(KB,¬Wx,y)=false} plan←PLAN-SHOT(current ,possible wumpus ,safe) ifplan is empty then // no choice but to take a risk not unsafe←{[x,y]:ASK(KB,¬OKt x,y)=false} plan←PLAN-ROUTE (current ,unvisited∩not unsafe ,safe) ifplan is empty then plan←PLAN-ROUTE (current ,{[1,1]},safe)+[Climb ] action←POP(plan ) TELL(KB,MAKE-ACTION -SENTENCE (action ,t)) t←t+1 return action function PLAN-ROUTE (current ,goals ,allowed )returns an action sequence inputs :current , the agent’s current position goals , a set of squares; try to plan a route to one of them allowed , a set of squares that can form part of the route problem←ROUTE -PROBLEM (current ,goals ,allowed ) return A*-G RAPH -SEARCH (problem ) Figure 7.20 A hybrid agent program for the wumpus world.

Token 5656:
It uses a propositional knowl- edge base to infer the state of the world, and a combination of problem-solving search and domain-speciﬁc code to decide what actions to take.

Token 5657:
by a unique binary number, we would need numbers with log2(22n)=2nbits to label the current belief state.

Token 5658:
That is, exact state estimation may require logical formulas whose size isexponential in the number of symbols.

Token 5659:
One very common and natural scheme for approximate state estimation is to represent belief states as conjunctions of literals, that is, 1-CNF formulas.

Token 5660:
To do this, the agent programsimply tries to prove X tand¬Xtfor each symbol Xt(as well as each atemporal symbol whose truth value is not yet known), given the belief state at t−1.

Token 5661:
The conjunction of

Token 5662:
Section 7.7.

Token 5663:
Agents Based on Propositional Logic 271 Figure 7.21 Depiction of a 1-CNF belief state (bold outline) as a simply representable, conservative approximation to the exact (wi ggly) belief state (shaded region with dashed outline).

Token 5664:
Each possible world is shown as a circle; the shaded ones are consistent with all the percepts.

Token 5665:
provable literals becomes the new belief state, and the previous belief state is discarded.

Token 5666:
It is important to understand that this scheme may lose some information as time goes along.

Token 5667:
For example, if the sentence in Equation (7.4) were the true belief state, then neitherP 3,1norP2,2would be provable individually and neither would appear in the 1-CNF belief state.

Token 5668:
(Exercise 7.27 explores one possible solution to this problem.)

Token 5669:
On the other hand,because every literal in the 1-CNF belief state is proved from the previous belief state, andthe initial belief state is a true assertion, we know that entire 1-CNF belief state must betrue.

Token 5670:
Thus, the set of possible states represented by the 1-CNF belief state includes all states that are in fact possible given the full percept history.

Token 5671:
As illustrated in Figure 7.21, the 1- CNF belief state acts as a simple outer envelope, or conservative approximation , around theCONSERVATIVE APPROXIMATION exact belief state.

Token 5672:
We see this idea of conservative approximations to complicated sets as a recurring theme in many areas of AI.

Token 5673:
7.7.4 Making plans by propositional inference The agent in Figure 7.20 uses logical inference to determine which squares are safe, but usesA ∗search to make plans.

Token 5674:
In this section, we show how to make plans by logical inference. The basic idea is very simple: 1.

Token 5675:
Construct a sentence that includes (a)Init0, a collection of assertions about the initial state; (b)Transition1,...,Transitiont, the successor-state axioms for all possible actions at each time up to some maximum time t; (c) the assertion that the goal is achieved at time t:HaveGoldt∧ClimbedOutt.

Token 5676:
272 Chapter 7. Logical Agents 2. Present the whole sentence to a SAT solver.

Token 5677:
If the solver ﬁnds a satisfying model, then the goal is achievable; if the sentence is unsatisﬁable, then the planning problem isimpossible. 3.

Token 5678:
Assuming a model is found, extract from the model those variables that represent ac- tions and are assigned true.

Token 5679:
Together they represent a plan to achieve the goals. A propositional planning procedure, SATP LAN, is shown in Figure 7.22.

Token 5680:
It implements the basic idea just given, with one twist.

Token 5681:
Because the agent does not know how many steps itwill take to reach the goal, the algorithm tries each possible number of steps t,u pt os o m e maximum conceivable plan length T max.

Token 5682:
In this way, it is guaranteed to ﬁnd the shortest plan if one exists.

Token 5683:
Because of the way SATP LAN searches for a solution, this approach cannot be used in a partially observable environment; SATP LAN would just set the unobservable variables to the values it needs to create a solution.

Token 5684:
function SAT PLAN (init,transition ,goal,Tmax)returns solution or failure inputs :init,transition ,goal, constitute a description of the problem Tmax, an upper limit for plan length fort=0toTmaxdo cnf←TRANSLATE -TO-SAT( init,transition ,goal,t) model←SAT-S OLVER (cnf) ifmodel is not null then return EXTRACT -SOLUTION (model ) return failure Figure 7.22 The SATP LAN algorithm.

Token 5685:
The planning problem is translated into a CNF sentence in which the goal is asserted to hold at a ﬁxed time step tand axioms are included for each time step up to t. If the satisﬁability algorithm ﬁnds a model, then a plan is extracted by looking at those proposition symbols t hat refer to actions and are assigned true in the model.

Token 5686:
If no model exists, then the process is repeated with the goal moved one step later.

Token 5687:
The key step in using SATP LAN is the construction of the knowledge base.

Token 5688:
It might seem, on casual inspection, that the wumpus world axioms in Section 7.7.1 sufﬁce for steps1(a) and 1(b) above.

Token 5689:
There is, however, a signiﬁcant difference between the requirements forentailment (as tested by A SK) and those for satisﬁability.

Token 5690:
Consider, for example, the agent’s location, initially [1,1], and suppose the agent’s unambitious goal is to be in [2,1]at time 1.

Token 5691:
The initial knowledge base contains L0 1,1and the goal is L1 2,1.U s i n gA SK, we can prove L1 2,1 ifForward0is asserted, and, reassuringly, we cannot prove L1 2,1if, say, Shoot0is asserted instead.

Token 5692:
Now, SATP LAN will ﬁnd the plan [Forward0]; so far, so good. Unfortunately, SATP LAN also ﬁnds the plan [Shoot0]. How could this be?

Token 5693:
To ﬁnd out, we inspect the model that SATP LAN constructs: it includes the assignment L0 2,1, that is, the agent can be in [2,1] at time 1 by being there at time 0 and shooting.

Token 5694:
One might ask, “Didn’t we say the agent is in[1,1]at time 0?” Yes, we did, but we didn’t tell the agent that it can’t be in two places at once!

Token 5695:
For entailment, L 0 2,1is unknown and cannot, therefore, be used in a proof; for satisﬁability,

Token 5696:
Section 7.7.

Token 5697:
Agents Based on Propositional Logic 273 on the other hand, L0 2,1is unknown and can, therefore, be set to whatever value helps to make the goal true.

Token 5698:
For this reason, SATP LAN is a good debugging tool for knowledge bases because it reveals places where knowledge is missing.

Token 5699:
In this particular case, we can ﬁx theknowledge base by asserting that, at each time step, the agent is in exactly one location, usinga collection of sentences similar to those used to assert the existence of exactly one wumpus.

Token 5700:
Alternatively, we can assert ¬L 0 x,yfor all locations other than [1,1]; the successor-state axiom for location takes care of subsequent time steps.

Token 5701:
The same ﬁxes also work to make sure theagent has only one orientation. SATP LAN has more surprises in store, however.

Token 5702:
The ﬁrst is that it ﬁnds models with impossible actions, such as shooting with no arrow.

Token 5703:
To understand why, we need to look morecarefully at what the successor-state axioms (such as Equation (7.3)) say about actions whosepreconditions are not satisﬁed.

Token 5704:
The axioms dopredict correctly that nothing will happen when such an action is executed (see Exercise 10.14), but they do notsay that the action cannot be executed!

Token 5705:
To avoid generating plans with illegal actions, we must add precondition axioms PRECONDITION AXIOMS stating that an action occurrence requires the preconditions to be satisﬁed.13For example, we need to say, for each time t,t h a t Shoott⇒HaveArrowt.

Token 5706:
This ensures that if a plan selects the Shoot action at any time, it must be the case that the agent has an arrow at that time.

Token 5707:
SATP LAN’s second surprise is the creation of plans with multiple simultaneous actions.

Token 5708:
For example, it may come up with a model in which both Forward0andShoot0are true, which is not allowed.

Token 5709:
To eliminate this problem, we introduce action exclusion axioms :f o rACTION EXCLUSION AXIOM every pair of actions At iandAt jwe add the axiom ¬At i∨¬At j.

Token 5710:
It might be pointed out that walking forward and shooting at the same time is not so hard to do, whereas, say, shooting and grabbing at the same time is rather impractical.

Token 5711:
By imposingaction exclusion axioms only on pairs of actions that really do interfere with each other, wecan allow for plans that include multiple simultaneous actions—and because SATP LAN ﬁnds the shortest legal plan, we can be sure that it will take advantage of this capability.

Token 5712:
To summarize, SATP LAN ﬁnds models for a sentence containing the initial state, the goal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.

Token 5713:
It can be shown that this collection of axioms is sufﬁcient, in the sense that there are no longer any spurious “solutions.” Any model satisfying the propositional sentence will be avalid plan for the original problem.

Token 5714:
Modern SAT-solving technology makes the approachquite practical.

Token 5715:
For example, a DPLL-style solver has no difﬁculty in generating the 11-stepsolution for the wumpus world instance shown in Figure 7.2.

Token 5716:
This section has described a declarative approach to agent construction: the agent works by a combination of asserting sentences in the knowledge base and performing logical infer-ence.

Token 5717:
This approach has some weaknesses hidden in phrases such as “for each time t”a n d 13Notice that the addition of precondition axioms means that we need not include preconditions for actions in the successor-state axioms.

Token 5718:
274 Chapter 7.

Token 5719:
Logical Agents “for each square [x,y].” For any practical agent, these phrases have to be implemented by code that generates instances of the general sentence schema automatically for insertion intothe knowledge base.

Token 5720:
For a wumpus world of reasonable size—one comparable to a smallishcomputer game—we might need a 100×100board and 1000 time steps, leading to knowl- edge bases with tens or hundreds of millions of sentences.

Token 5721:
Not only does this become rather impractical, but it also illustrates a deeper problem: we know something about the wum- pus world—namely, that the “physics” works the same way across all squares and all timesteps—that we cannot express directly in the language of propositional logic.

Token 5722:
To solve thisproblem, we need a more expressive language, one in which phrases like “for each time t” and “for each square [x,y]” can be written in a natural way.

Token 5723:
First-order logic, described in Chapter 8, is such a language; in ﬁrst-order logic a wumpus world of any size and durationcan be described in about ten sentences rather than ten million or ten trillion.

Token 5724:
7.8 S UMMARY We have introduced knowledge-based agents and have shown how to deﬁne a logic withwhich such agents can reason about the world.

Token 5725:
The main points are as follows: •Intelligent agents need knowledge about the world in order to reach good decisions.

Token 5726:
•Knowledge is contained in agents in the form of sentences in aknowledge represen- tation language that are stored in a knowledge base .

Token 5727:
•A knowledge-based agent is composed of a knowledge base and an inference mecha- nism.

Token 5728:
It operates by storing sentences about the world in its knowledge base, using theinference mechanism to infer new sentences, and using these sentences to decide whataction to take.

Token 5729:
•A representation language is deﬁned by its syntax , which speciﬁes the structure of sentences, and its semantics , which deﬁnes the truth of each sentence in each possible world ormodel .

Token 5730:
•The relationship of entailment between sentences is crucial to our understanding of reasoning.

Token 5731:
A sentence αentails another sentence βifβis true in all worlds where αis true.

Token 5732:
Equivalent deﬁnitions include the validity of the sentence α⇒βand the unsatisﬁability of the sentence α∧¬β.

Token 5733:
•Inference is the process of deriving new sentences from old ones.

Token 5734:
Sound inference algo- rithms derive only sentences that are entailed; complete algorithms derive allsentences that are entailed.

Token 5735:
•Propositional logic is a simple language consisting of proposition symbols andlogical connectives .

Token 5736:
It can handle propositions that are known true, known false, or completely unknown.

Token 5737:
•The set of possible models, given a ﬁxed propositional vocabulary, is ﬁnite, so en- tailment can be checked by enumerating models.

Token 5738:
Efﬁcient model-checking inference algorithms for propositional logic include backtracking and local search methods and can often solve large problems quickly.

Token 5739:
Bibliographical and Historical Notes 275 •Inference rules are patterns of sound inference that can be used to ﬁnd proofs.

Token 5740:
The resolution rule yields a complete inference algorithm for knowledge bases that are expressed in conjunctive normal form .Forward chaining andbackward chaining are very natural reasoning algorithms for knowledge bases in Horn form .

Token 5741:
•Local search methods such as W ALKSAT can be used to ﬁnd solutions. Such algo- rithms are sound but not complete.

Token 5742:
•Logical state estimation involves maintaining a logical sentence that describes the set of possible states consistent with the observation history.

Token 5743:
Each update step requires inference using the transition model of the environment, which is built from successor- state axioms that specify how each ﬂuent changes.

Token 5744:
•Decisions within a logical agent can be made by SAT solving: ﬁnding possible models specifying future action sequences that reach the goal.

Token 5745:
This approach works only forfully observable or sensorless environments.

Token 5746:
•Propositional logic does not scale to environments of unbounded size because it lacks the expressive power to deal concisely with time, space, and universal patterns of rela-tionships among objects.

Token 5747:
BIBLIOGRAPHICAL AND HISTORICAL NOTES John McCarthy’s paper “Programs with Common Sense” (McCarthy, 1958, 1968) promul-gated the notion of agents that use logical reasoning to mediate between percepts and actions.It also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to knowis an elegant way to build software.

Token 5748:
Allen Newell’s (1982) article “The Knowledge Level”makes the case that rational agents can be described and analyzed at an abstract level deﬁnedby the knowledge they possess rather than the programs they run.

Token 5749:
The declarative and proce-dural approaches to AI are analyzed in depth by Boden (1977).

Token 5750:
The debate was revived by, among others, Brooks (1991) and Nilsson (1991), and continues to this day (Shaparau et al. , 2008).

Token 5751:
Meanwhile, the declarative approach has spread into other areas of computer sciencesuch as networking (Loo et al. , 2006).

Token 5752:
Logic itself had its origins in ancient Greek philosophy and mathematics.

Token 5753:
Various log- ical principles—principles connecting the syntactic structure of sentences with their truthand falsity, with their meaning, or with the validity of arguments in which they ﬁgure—arescattered in the works of Plato.

Token 5754:
The ﬁrst known systematic study of logic was carried outby Aristotle, whose work was assembled by his students after his death in 322 B.C.a sa treatise called the Organon .

Token 5755:
Aristotle’s syllogisms were what we would now call inference SYLLOGISM rules.

Token 5756:
Although the syllogisms included elements of both propositional and ﬁrst-order logic, the system as a whole lacked the compositional properties required to handle sentences of arbitrary complexity.

Token 5757:
The closely related Megarian and Stoic schools (originating in the ﬁfth century B.C.

Token 5758:
and continuing for several centuries thereafter) began the systematic study of the basic logicalconnectives.

Token 5759:
The use of truth tables for deﬁning connectives is due to Philo of Megara. The

Token 5760:
276 Chapter 7. Logical Agents Stoics took ﬁve basic inference rules as valid without proof, including the rule we now call Modus Ponens.

Token 5761:
They derived a number of other rules from these ﬁve, using, among otherprinciples, the deduction theorem (page 249) and were much clearer about the notion ofproof than was Aristotle.

Token 5762:
A good account of the history of Megarian and Stoic logic is givenby Benson Mates (1953).

Token 5763:
The idea of reducing logical inference to a purely mechanical process applied to a for- mal language is due to Wilhelm Leibniz (1646–1716), although he had limited success in im-plementing the ideas.

Token 5764:
George Boole (1847) introduced the ﬁrst comprehensive and workablesystem of formal logic in his book The Mathematical Analysis of Logic .

Token 5765:
Boole’s logic was closely modeled on the ordinary algebra of real numbers and used substitution of logicallyequivalent expressions as its primary inference method.

Token 5766:
Although Boole’s system still fellshort of full propositional logic, it was close enough that other mathematicians could quicklyﬁll in the gaps.

Token 5767:
Schr¨ oder (1877) described conjunctive normal form, while Horn form was introduced much later by Alfred Horn (1951).

Token 5768:
The ﬁrst comprehensive exposition of modernpropositional logic (and ﬁrst-order logic) is found in Gottlob Frege’s (1879) Begriffschrift (“Concept Writing” or “Conceptual Notation”).

Token 5769:
The ﬁrst mechanical device to carry out logical inferences was constructed by the third Earl of Stanhope (1753–1816).

Token 5770:
The Stanhope Demonstrator could handle syllogisms and certain inferences in the theory of probability.

Token 5771:
William Stanley Jevons, one of those whoimproved upon and extended Boole’s work, constructed his “logical piano” in 1869 to per-form inferences in Boolean logic.

Token 5772:
An entertaining and instructive history of these and otherearly mechanical devices for reasoning is given by Martin Gardner (1968).

Token 5773:
The ﬁrst pub-lished computer program for logical inference was the Logic Theorist of Newell, Shaw,and Simon (1957).

Token 5774:
This program was intended to model human thought processes.

Token 5775:
Mar-tin Davis (1957) had actually designed a program that came up with a proof in 1954, but theLogic Theorist’s results were published slightly earlier.

Token 5776:
Truth tables as a method of testing validity or unsatisﬁability in propositional logic were introduced independently by Emil Post (1921) and Ludwig Wittgenstein (1922).

Token 5777:
In the 1930s, a great deal of progress was made on inference methods for ﬁrst-order logic.

Token 5778:
In particular, G¨odel (1930) showed that a complete procedure for inference in ﬁrst-order logic could be obtained via a reduction to propositional logic, using Herbrand’s theorem (Herbrand, 1930).We take up this history again in Chapter 9; the important point here is that the developmentof efﬁcient propositional algorithms in the 1960s was motivated largely by the interest ofmathematicians in an effective theorem prover for ﬁrst-order logic.

Token 5779:
The Davis–Putnam algo-rithm (Davis and Putnam, 1960) was the ﬁrst effective algorithm for propositional resolutionbut was in most cases much less efﬁcient than the DPLL backtracking algorithm introducedtwo years later (1962).

Token 5780:
The full resolution rule and a proof of its completeness appeared in aseminal paper by J.

Token 5781:
A. Robinson (1965), which also showed how to do ﬁrst-order reasoningwithout resort to propositional techniques.

Token 5782:
Stephen Cook (1971) showed that deciding satisﬁability of a sentence in propositional logic (the SAT problem) is NP-complete.

Token 5783:
Since deciding entailment is equivalent to decid-ing unsatisﬁability, it is co-NP-complete.

Token 5784:
Many subsets of propositional logic are known forwhich the satisﬁability problem is polynomially solvable; Horn clauses are one such subset.

Token 5785:


Token 5786:
Bibliographical and Historical Notes 277 The linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier (1984), who describe their algorithm as a dataﬂow process similar to the propagation of sig-nals in a circuit.

Token 5787:
Early theoretical investigations showed that DPLL has polynomial average-case com- plexity for certain natural distributions of problems.

Token 5788:
This potentially exciting fact became less exciting when Franco and Paull (1983) showed that the same problems could be solved in constant time simply by guessing random assignments.

Token 5789:
The random-generation methoddescribed in the chapter produces much harder problems.

Token 5790:
Motivated by the empirical successof local search on these problems, Koutsoupias and Papadimitriou (1992) showed that a sim-ple hill-climbing algorithm can solve almost all satisﬁability problem instances very quickly, suggesting that hard problems are rare.

Token 5791:
Moreover, Sch¨ oning (1999) exhibited a randomized hill-climbing algorithm whose worst-case expected run time on 3-SAT problems (that is, sat- isﬁability of 3-CNF sentences) is O(1.333 n)—still exponential, but substantially faster than previous worst-case bounds.

Token 5792:
The current record is O(1.324n)(Iwama and Tamaki, 2004). Achlioptas et al. (2004) and Alekhnovich et al.

Token 5793:
(2005) exhibit families of 3-SAT instances for which all known DPLL-like algorithms require exponential running time.

Token 5794:
On the practical side, efﬁciency gains in propositional solvers have been marked.

Token 5795:
Given ten minutes of computing time, the original DPLL algorithm in 1962 could only solve prob- lems with no more than 10 or 15 variables.

Token 5796:
By 1995 the S ATZ solver (Li and Anbulagan, 1997) could handle 1,000 variables, thanks to optimized data structures for indexing vari- ables.

Token 5797:
Two crucial contributions were the watched literal indexing technique of Zhang and Stickel (1996), which makes unit propagation very efﬁcient, and the introduction of clause (i.e., constraint) learning techniques from the CSP community by Bayardo and Schrag (1997).

Token 5798:
Using these ideas, and spurred by the prospect of solving industrial-scale circuit veriﬁcationproblems, Moskewicz et al.

Token 5799:
(2001) developed the C HAFF solver, which could handle prob- lems with millions of variables.

Token 5800:
Beginning in 2002, SAT competitions have been held reg-ularly; most of the winning entries have either been descendants of C HAFF or have used the same general approach.

Token 5801:
RS AT(Pipatsrisawat and Darwiche, 2007), the 2007 winner, falls in the latter category.

Token 5802:
Also noteworthy is M INISAT (Een and S¨ orensson, 2003), an open-source implementation available at http://minisat.se that is designed to be easily modiﬁed and improved.

Token 5803:
The current landscape of solvers is surveyed by Gomes et al. (2008).

Token 5804:
Local search algorithms for satisﬁability were tried by various authors throughout the 1980s; all of the algorithms were based on the idea of minimizing the number of unsatisﬁedclauses (Hansen and Jaumard, 1990).

Token 5805:
A particularly effective algorithm was developed byGu (1989) and independently by Selman et al.

Token 5806:
(1992), who called it GSAT and showed that it was capable of solving a wide range of very hard problems very quickly.

Token 5807:
The W ALKSAT algorithm described in the chapter is due to Selman et al. (1996).

Token 5808:
The “phase transition” in satisﬁability of random k-SAT problems was ﬁrst observed by Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical research—due, in part, to the obvious connection to phase transition phenomena in statistical physics.

Token 5809:
Cheeseman et al. (1991) observed phase transitions in several CSPs and conjecture that all NP-hard problems have a phase transition.

Token 5810:
Crawford and Auton (1993) located the3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a

Token 5811:
278 Chapter 7. Logical Agents sharp peak in the run time of their SAT solver.

Token 5812:
Cook and Mitchell (1997) provide an excellent summary of the early literature on the problem.

Token 5813:
The current state of theoretical understanding is summarized by Achlioptas (2009).

Token 5814:
The satisﬁability threshold conjecture states that, for each k, there is a sharp satisﬁabilitySATISFIABILITY THRESHOLD CONJECTUREthreshold rk, such that as the number of variables n→∞ , instances below the threshold are satisﬁable with probability 1, while those above the threshold are unsatisﬁable with proba- bility 1.

Token 5815:
The conjecture was not quite proved by Friedgut (1999): a sharp threshold exists butits location might depend on neven as n→∞ .

Token 5816:
Despite signiﬁcant progress in asymptotic analysis of the threshold location for large k(Achlioptas and Peres, 2004; Achlioptas et al.

Token 5817:
, 2007), all that can be proved for k=3is that it lies in the range [3.52,4.51].

Token 5818:
Current theory suggests that a peak in the run time of a SAT solver is not necessarily related to the satisﬁa- bility threshold, but instead to a phase transition in the solution distribution and structure ofSAT instances.

Token 5819:
Empirical results due to Coarfa et al. (2003) support this view.

Token 5820:
In fact, al- gorithms such as survey propagation (Parisi and Zecchina, 2002; Maneva et al.

Token 5821:
, 2007) take SURVEY PROPAGATION advantage of special properties of random SAT instances near the satisﬁability threshold and greatly outperform general SAT solvers on such instances.

Token 5822:
The best sources for information on satisﬁability, both theoretical and practical, are the Handbook of Satisﬁability (Biere et al.

Token 5823:
, 2009) and the regular International Conferences on Theory and Applications of Satisﬁability Testing , known as SAT.

Token 5824:
The idea of building agents with propositional logic can be traced back to the seminal paper of McCulloch and Pitts (1943), which initiated the ﬁeld of neural networks.

Token 5825:
Con- trary to popular supposition, the paper was concerned with the implementation of a Boolean circuit-based agent design in the brain.

Token 5826:
Circuit-based agents, which perform computation by propagating signals in hardware circuits rather than running algorithms in general-purposecomputers, have received little attention in AI, however.

Token 5827:
The most notable exception is thework of Stan Rosenschein (Rosenschein, 1985; Kaelbling and Rosenschein, 1990), who de-veloped ways to compile circuit-based agents from declarative descriptions of the task envi-ronment.

Token 5828:
(Rosenschein’s approach is described at some length in the second edition of this book.)

Token 5829:
The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based designs for controlling robots—a topic we take up in Chapter 25.

Token 5830:
Brooks (1991) arguesthat circuit-based designs are allthat is needed for AI—that representation and reasoning are cumbersome, expensive, and unnecessary.

Token 5831:
In our view, neither approach is sufﬁcient byitself. Williams et al.

Token 5832:
(2003) show how a hybrid agent design not too different from our wumpus agent has been used to control NASA spacecraft, planning sequences of actions anddiagnosing and recovering from faults.

Token 5833:
The general problem of keeping track of a partially observable environment was intro- duced for state-based representations in Chapter 4.

Token 5834:
Its instantiation for propositional repre-sentations was studied by Amir and Russell (2003), who identiﬁed several classes of envi-ronments that admit efﬁcient state-estimation algorithms and showed that for several other classes the problem is intractable.

Token 5835:
The temporal-projection problem, which involves deter- TEMPORAL- PROJECTION mining what propositions hold true after an action sequence is executed, can be seen as a special case of state estimation with empty percepts.

Token 5836:
Many authors have studied this problembecause of its importance in planning; some important hardness results were established by

Token 5837:
Exercises 279 Liberatore (1997). The idea of representing a belief state with propositions can be traced to Wittgenstein (1922).

Token 5838:
Logical state estimation, of course, requires a logical representation of the effects of actions—a key problem in AI since the late 1950s.

Token 5839:
The dominant proposal has been the sit- uation calculus formalism (McCarthy, 1963), which is couched within ﬁrst-order logic.

Token 5840:
We discuss situation calculus, and various extensions and alternatives, in Chapters 10 and 12.

Token 5841:
The approach taken in this chapter—using temporal indices on propositional variables—is morerestrictive but has the beneﬁt of simplicity.

Token 5842:
The general approach embodied in the SATP LAN algorithm was proposed by Kautz and Selman (1992).

Token 5843:
Later generations of SATP LAN were able to take advantage of the advances in SAT solvers, described earlier, and remain amongthe most effective ways of solving difﬁcult problems (Kautz, 2006).

Token 5844:
The frame problem was ﬁrst recognized by McCarthy and Hayes (1969).

Token 5845:
Many re- searchers considered the problem unsolvable within ﬁrst-order logic, and it spurred a greatdeal of research into nonmonotonic logics.

Token 5846:
Philosophers from Dreyfus (1972) to Crockett(1994) have cited the frame problem as one symptom of the inevitable failure of the entireAI enterprise.

Token 5847:
The solution of the frame problem with successor-state axioms is due to RayReiter (1991).

Token 5848:
Thielscher (1999) identiﬁes the inferential frame problem as a separate idea and provides a solution.

Token 5849:
In retrospect, one can see that Rosenschein’s (1985) agents were using circuits that implemented successor-state axioms, but Rosenschein did not notice thatthe frame problem was thereby largely solved.

Token 5850:
Foo (2001) explains why the discrete-eventcontrol theory models typically used by engineers do not have to explicitly deal with theframe problem: because they are dealing with prediction and control, not with explanationand reasoning about counterfactual situations.

Token 5851:
Modern propositional solvers have wide applicability in industrial applications.

Token 5852:
The ap- plication of propositional inference in the synthesis of computer hardware is now a standardtechnique having many large-scale deployments (Nowick et al.

Token 5853:
, 1993). The SATMC satisﬁ- ability checker was used to detect a previously unknown vulnerability in a Web browser usersign-on protocol (Armando et al.

Token 5854:
, 2008). The wumpus world was invented by Gregory Yob (1975).

Token 5855:
Ironically, Yob developed it because he was bored with games played on a rectangular grid: the topology of his originalwumpus world was a dodecahedron, and we put it back in the boring old grid.

Token 5856:
MichaelGenesereth was the ﬁrst to suggest that the wumpus world be used as an agent testbed.

Token 5857:
EXERCISES 7.1 Suppose the agent has progressed to the point shown in Figure 7.4(a), page 239, having perceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2], and is now concerned with the contents of [1,3], [2,2], and [3,1].

Token 5858:
Each of these can contain a pit, and at most one cancontain a wumpus. Following the example of Figure 7.5, construct the set of possible worlds.

Token 5859:
(You should ﬁnd 32 of them.) Mark the worlds in which the KB is true and those in which

Token 5860:
280 Chapter 7.

Token 5861:
Logical Agents each of the following sentences is true: α2=“There is no pit in [2,2].” α3=“There is a wumpus in [1,3].” Hence show that KB|=α2andKB|=α3.

Token 5862:
7.2 (Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove that the unicorn is mythical? How about magical? Horned?

Token 5863:
If the unicorn is mythical, then it is immortal, but if it is not mythical, then it is a mortal mammal.

Token 5864:
If the unicorn is either immortal or a mammal, then it is horned.The unicorn is magical if it is horned.

Token 5865:
7.3 Consider the problem of deciding whether a propositional logic sentence is true in a given model. a. Write a recursive algorithm PL-T RUE?

Token 5866:
(s,m) that returns true if and only if the sen- tencesis true in the model m(where massigns a truth value for every symbol in s).

Token 5867:
The algorithm should run in time linear in the size of the sentence. (Alternatively, use aversion of this function from the online code repository.)

Token 5868:
b.

Token 5869:
Give three examples of sentences that can be determined to be true or false in a partial model that does not specify a truth value for some of the symbols.

Token 5870:
c. Show that the truth value (if any) of a sentence in a partial model cannot be determined efﬁciently in general. d. Modify your PL-T RUE?

Token 5871:
algorithm so that it can sometimes judge truth from partial models, while retaining its recursive structure and linear run time.

Token 5872:
Give three examplesof sentences whose truth in a partial model is notdetected by your algorithm.

Token 5873:
e. Investigate whether the modiﬁed algorithm makes TT-E NTAILS ? more efﬁcient. 7.4 Which of the following are correct? a.False|=True .

Token 5874:
b.True|=False . c.(A∧B)|=(A⇔B). d.A⇔B|=A∨B. e.A⇔B|=¬A∨B. f.(A∧B)⇒C|=(A⇒C)∨(B⇒C). g.(C∨(¬A∧¬B))≡((A⇒C)∧(B⇒C)). h.(A∨B)∧(¬C∨¬D∨E)|=(A∨B).

Token 5875:
i.(A∨B)∧(¬C∨¬D∨E)|=(A∨B)∧(¬D∨E). j. (A∨B)∧¬(A⇒B)is satisﬁable. k.(A⇔B)∧(¬A∨B)is satisﬁable.

Token 5876:
l.(A⇔B)⇔Chas the same number of models as (A⇔B)for any ﬁxed set of proposition symbols that includes A,B,C.

Token 5877:
Exercises 281 7.5 Prove each of the following assertions: a.αis valid if and only if True|=α. b.F o r a n y α,False|=α.

Token 5878:
c.α|=βif and only if the sentence (α⇒β)is valid. d.α≡βif and only if the sentence (α⇔β)is valid.

Token 5879:
e.α|=βif and only if the sentence (α∧¬β)is unsatisﬁable.

Token 5880:
7.6 Prove, or ﬁnd a counterexample to, each of the following assertions: a.I fα|=γorβ|=γ(or both) then (α∧β)|=γ b.I fα|=(β∧γ)thenα|=βandα|=γ.

Token 5881:
c.I fα|=(β∨γ)thenα|=βorα|=γ(or both). 7.7 Consider a vocabulary with only four propositions, A,B,C,a n dD.

Token 5882:
How many models are there for the following sentences? a.B∨C. b.¬A∨¬B∨¬C∨¬D. c.(A⇒B)∧A∧¬B∧C∧D. 7.8 We have deﬁned four binary logical connectives. a.

Token 5883:
Are there any others that might be useful? b. How many binary connectives can there be? c. Why are some of them not very useful?

Token 5884:
7.9 Using a method of your choice, verify each of the equivalences in Figure 7.11 (page 249).

Token 5885:
7.10 Decide whether each of the following sentences is valid, unsatisﬁable, or neither.

Token 5886:
Ver- ify your decisions using truth tables or the equivalence rules of Figure 7.11 (page 249).

Token 5887:
a.Smoke⇒Smoke b.Smoke⇒Fire c.(Smoke⇒Fire)⇒(¬Smoke⇒¬Fire) d.Smoke∨Fire∨¬Fire e.((Smoke∧Heat)⇒Fire)⇔((Smoke⇒Fire)∨(Heat⇒Fire)) f.(Smoke⇒Fire)⇒((Smoke∧Heat)⇒Fire) g.Big∨Dumb∨(Big⇒Dumb ) 7.11 Any propositional logic sentence is logically equivalent to the assertion that each pos- sible world in which it would be false is not the case.

Token 5888:
From this observation, prove that any sentence can be written in CNF.

Token 5889:
7.12 Use resolution to prove the sentence ¬A∧¬Bfrom the clauses in Exercise 7.20.

Token 5890:
7.13 This exercise looks into the relationship between clauses and implication sentences.

Token 5891:
282 Chapter 7. Logical Agents a. Show that the clause (¬P1∨···∨¬ Pm∨Q)is logically equivalent to the implication sentence (P1∧···∧ Pm)⇒Q. b.

Token 5892:
Show that every clause (regardless of the number of positive literals) can be written in the form (P1∧···∧ Pm)⇒(Q1∨···∨ Qn),w h e r et h e Psa n dQs are proposition symbols.

Token 5893:
A knowledge base consisting of such sentences is in implicative normal form orKowalski form (Kowalski, 1979).IMPLICATIVE NORMAL FORM c. Write down the full resolution rule for sentences in implicative normal form.

Token 5894:
7.14 According to some political pundits, a person who is radical ( R) is electable ( E)i f he/she is conservative ( C), but otherwise is not electable.

Token 5895:
a. Which of the following are correct representations of this assertion? (i)(R∧E)⇐⇒C (ii)R⇒(E⇐⇒C) (iii)R⇒((C⇒E)∨¬E) b.

Token 5896:
Which of the sentences in (a) can be expressed in Horn form? 7.15 This question considers representing satisﬁability (SAT) problems as CSPs. a.

Token 5897:
Draw the constraint graph corresponding to the SAT problem (¬X1∨X2)∧(¬X2∨X3)∧...∧(¬Xn−1∨Xn) for the particular case n=5. b.

Token 5898:
How many solutions are there for this general SAT problem as a function of n?

Token 5899:
c. Suppose we apply B ACKTRACKING -SEARCH (page 215) to ﬁnd allsolutions to a SAT CSP of the type given in (a).

Token 5900:
(To ﬁnd allsolutions to a CSP, we simply modify the basic algorithm so it continues searching after each solution is found.)

Token 5901:
Assume thatvariables are ordered X 1,...,X nandfalse is ordered before true. How much time will the algorithm take to terminate?

Token 5902:
(Write an O(·)expression as a function of n.) d. We know that SAT problems in Horn form can be solved in linear time by forward chaining (unit propagation).

Token 5903:
We also know that every tree-structured binary CSP withdiscrete, ﬁnite domains can be solved in time linear in the number of variables (Sec-tion 6.5).

Token 5904:
Are these two facts connected? Discuss. 7.16 Explain why every nonempty propositional clause, by itself, is satisﬁable.

Token 5905:
Prove rig- orously that every set of ﬁve 3-SAT clauses is satisﬁable, provided that each clause mentions exactly three distinct variables.

Token 5906:
What is the smallest set of such clauses that is unsatisﬁable? Construct such a set.

Token 5907:
7.17 A propositional 2-CNF expression is a conjunction of clauses, each containing exactly 2literals, e.g., (A∨B)∧(¬A∨C)∧(¬B∨D)∧(¬C∨G)∧(¬D∨G). a.

Token 5908:
Prove using resolution that the above sentence entails G.

Token 5909:
Exercises 283 b. Two clauses are semantically distinct if they are not logically equivalent.

Token 5910:
How many semantically distinct 2-CNF clauses can be constructed from nproposition symbols?

Token 5911:
c. Using your answer to (b), prove that propositional resolution always terminates in time polynomial in ngiven a 2-CNF sentence containing no more than ndistinct symbols.

Token 5912:
d. Explain why your argument in (c) does not apply to 3-CNF.

Token 5913:
7.18 Consider the following sentence: [(Food⇒Party )∨(Drinks⇒Party )]⇒[(Food∧Drinks )⇒Party ]. a.

Token 5914:
Determine, using enumeration, whether this sentence is valid, satisﬁable (but not valid), or unsatisﬁable. b.

Token 5915:
Convert the left-hand and right-hand sides of the main implication into CNF, showing each step, and explain how the results conﬁrm your answer to (a).

Token 5916:
c. Prove your answer to (a) using resolution.

Token 5917:
7.19 A sentence is in disjunctive normal form (DNF) if it is the disjunction of conjunctionsDISJUNCTIVE NORMAL FORM of literals.

Token 5918:
For example, the sentence (A∧B∧¬C)∨(¬A∧C)∨(B∧¬C)is in DNF. a.

Token 5919:
Any propositional logic sentence is logically equivalent to the assertion that some pos- sible world in which it would be true is in fact the case.

Token 5920:
From this observation, provethat any sentence can be written in DNF.

Token 5921:
b. Construct an algorithm that converts any sentence in propositional logic into DNF.

Token 5922:
(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Sec- tion 7.5.2.)

Token 5923:
c. Construct a simple algorithm that takes as input a sentence in DNF and returns a satis- fying assignment if one exists, or reports that no satisfying assignment exists.

Token 5924:
d. Apply the algorithms in (b) and (c) to the following set of sentences: A⇒B B⇒C C⇒¬A.

Token 5925:
e. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and since the algorithm in (c) is much simpler than any algorithm for solving a set of sen-tences in CNF, why is this technique not used in automated reasoning?

Token 5926:
7.20 Convert the following set of sentences to clausal form. S1:A⇔(B∨E). S2:E⇒D. S3:C∧F⇒¬B. S4:E⇒B. S5:B⇒F.

Token 5927:
S6:B⇒C Give a trace of the execution of DPLL on the conjunction of these clauses.

Token 5928:
284 Chapter 7.

Token 5929:
Logical Agents 7.21 Is a randomly generated 4-CNF sentence with nsymbols and mclauses more or less likely to be solvable than a randomly generated 3-CNF sentence with nsymbols and m clauses?

Token 5930:
Explain. 7.22 Minesweeper, the well-known computer game, is closely related to the wumpus world.

Token 5931:
A minesweeper world is a rectangular grid of Nsquares with Minvisible mines scattered among them.

Token 5932:
Any square may be probed by the agent; instant death follows if a mine isprobed.

Token 5933:
Minesweeper indicates the presence of mines by revealing, in each probed square,thenumber of mines that are directly or diagonally adjacent.

Token 5934:
The goal is to probe every unmined square. a.L e tX i,jbe true iff square [i,j]contains a mine.

Token 5935:
Write down the assertion that exactly two mines are adjacent to [1,1] as a sentence involving some logical combination ofX i,jpropositions.

Token 5936:
b. Generalize your assertion from (a) by explaining how to construct a CNF sentence asserting that kofnneighbors contain mines.

Token 5937:
c. Explain precisely how an agent can use DPLL to prove that a given square does (or does not) contain a mine, ignoring the global constraint that there are exactly Mmines in all.

Token 5938:
d. Suppose that the global constraint is constructed from your method from part (b). How does the number of clauses depend on MandN?

Token 5939:
Suggest a way to modify DPLL so that the global constraint does not need to be represented explicitly.

Token 5940:
e. Are any conclusions derived by the method in part (c) invalidated when the global constraint is taken into account?

Token 5941:
f. Give examples of conﬁgurations of probe values that induce long-range dependencies such that the contents of a given unprobed square would give information about the contents of a far-distant square.

Token 5942:
( Hint: consider an N×1board.) 7.23 How long does it take to prove KB|=αusing DPLL when αis a literal already contained in KB? Explain.

Token 5943:
7.24 Trace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to proveQ, and compare this behavior with that of the forward-chaining algorithm.

Token 5944:
7.25 Write a successor-state axiom for the Locked predicate, which applies to doors, as- suming the only actions available are Lock andUnlock .

Token 5945:
7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus world. Write down axioms for all remaining ﬂuent symbols.

Token 5946:
7.27 Modify the H YBRID -WUMPUS -AGENT to use the 1-CNF logical state estimation method described on page 271.

Token 5947:
We noted on that page that such an agent will not be able to acquire, maintain, and use more complex beliefs such as the disjunction P3,1∨P2,2.

Token 5948:
Sug- gest a method for overcoming this problem by deﬁning additional proposition symbols, andtry it out in the wumpus world.

Token 5949:
Does it improve the performance of the agent?

Token 5950:


Token 5951:
8FIRST-ORDER LOGIC In which we notice that the world is blessed with many objects, some of which are related to other objects, and in which we endeavor to reason about them.

Token 5952:
In Chapter 7, we showed how a knowledge-based agent could represent the world in which it operates and deduce what actions to take.

Token 5953:
We used propositional logic as our representationlanguage because it sufﬁced to illustrate the basic concepts of logic and knowledge-basedagents.

Token 5954:
Unfortunately, propositional logic is too puny a language to represent knowledgeof complex environments in a concise way.

Token 5955:
In this chapter, we examine ﬁrst-order logic , 1FIRST-ORDER LOGIC which is sufﬁciently expressive to represent a good deal of our commonsense knowledge.

Token 5956:
It also either subsumes or forms the foundation of many other representation languages and has been studied intensively for many decades.

Token 5957:
We begin in Section 8.1 with a discussion of representation languages in general; Section 8.2 covers the syntax and semantics of ﬁrst-orderlogic; Sections 8.3 and 8.4 illustrate the use of ﬁrst-order logic for simple representations.

Token 5958:
8.1 R EPRESENTATION REVISITED In this section, we discuss the nature of representation languages.

Token 5959:
Our discussion motivatesthe development of ﬁrst-order logic, a much more expressive language than the propositionallogic introduced in Chapter 7.

Token 5960:
We look at propositional logic and at other kinds of languagesto understand what works and what fails.

Token 5961:
Our discussion will be cursory, compressing cen-turies of thought, trial, and error into a few paragraphs.

Token 5962:
Programming languages (such as C++ or Java or Lisp) are by far the largest class of formal languages in common use.

Token 5963:
Programs themselves represent, in a direct sense, only computational processes.

Token 5964:
Data structures within programs can represent facts; for example,a program could use a 4×4array to represent the contents of the wumpus world.

Token 5965:
Thus, the programming language statement World [2,2]←Pitis a fairly natural way to assert that there is a pit in square [2,2].

Token 5966:
(Such representations might be considered ad hoc ; database systems were developed precisely to provide a more general, domain-independent way to store and 1Also called ﬁrst-order predicate calculus , sometimes abbreviated as FOL orFOPC .

Token 5967:
285

Token 5968:
286 Chapter 8. First-Order Logic retrieve facts.)

Token 5969:
What programming languages lack is any general mechanism for deriving facts from other facts; each update to a data structure is done by a domain-speciﬁc procedurewhose details are derived by the programmer from his or her own knowledge of the domain.This procedural approach can be contrasted with the declarative nature of propositional logic, in which knowledge and inference are separate, and inference is entirely domain independent.

Token 5970:
A second drawback of data structures in programs (and of databases, for that matter) is the lack of any easy way to say, for example, “There is a pit in [2,2] or [3,1]” or “If thewumpus is in [1,1] then he is not in [2,2].” Programs can store a single value for each variable,and some systems allow the value to be “unknown,” but they lack the expressiveness requiredto handle partial information.

Token 5971:
Propositional logic is a declarative language because its semantics is based on a truth relation between sentences and possible worlds.

Token 5972:
It also has sufﬁcient expressive power todeal with partial information, using disjunction and negation.

Token 5973:
Propositional logic has a thirdproperty that is desirable in representation languages, namely, compositionality .I n a c o m - COMPOSITIONALITY positional language, the meaning of a sentence is a function of the meaning of its parts.

Token 5974:
For example, the meaning of “ S1,4∧S1,2” is related to the meanings of “ S1,4”a n d“ S1,2.” It would be very strange if “ S1,4” meant that there is a stench in square [1,4] and “ S1,2” meant that there is a stench in square [1,2], but “ S1,4∧S1,2” meant that France and Poland drew 1–1 in last week’s ice hockey qualifying match.

Token 5975:
Clearly, noncompositionality makes life muchmore difﬁcult for the reasoning system.

Token 5976:
As we saw in Chapter 7, however, propositional logic lacks the expressive power to concisely describe an environment with many objects.

Token 5977:
For example, we were forced to write a separate rule about breezes and pits for each square, such as B 1,1⇔(P1,2∨P2,1).

Token 5978:
In English, on the other hand, it seems easy enough to say, once and for all, “Squares adjacent to pits are breezy.” The syntax and semantics of English somehow make it possible to describethe environment concisely.

Token 5979:
8.1.1 The language of thought Natural languages (such as English or Spanish) are very expressive indeed.

Token 5980:
We managed towrite almost this whole book in natural language, with only occasional lapses into other lan-guages (including logic, mathematics, and the language of diagrams).

Token 5981:
There is a long tradi-tion in linguistics and the philosophy of language that views natural language as a declarativeknowledge representation language.

Token 5982:
If we could uncover the rules for natural language, wecould use it in representation and reasoning systems and gain the beneﬁt of the billions ofpages that have been written in natural language.

Token 5983:
The modern view of natural language is that it serves a as a medium for communication rather than pure representation.

Token 5984:
When a speaker points and says, “Look!” the listener comes to know that, say, Superman has ﬁnally appeared over the rooftops.

Token 5985:
Yet we would not want to say that the sentence “Look!” represents that fact.

Token 5986:
Rather, the meaning of the sentencedepends both on the sentence itself and on the context in which the sentence was spoken.

Token 5987:
Clearly, one could not store a sentence such as “Look!” in a knowledge base and expect to

Token 5988:
Section 8.1.

Token 5989:
Representation Revisited 287 recover its meaning without also storing a representation of the context—which raises the question of how the context itself can be represented.

Token 5990:
Natural languages also suffer fromambiguity , a problem for a representation language.

Token 5991:
As Pinker (1995) puts it: “When people AMBIGUITY think about spring , surely they are not confused as to whether they are thinking about a season or something that goes boing —and if one word can correspond to two thoughts, thoughts can’t be words.” The famous Sapir–Whorf hypothesis claims that our understanding of the world is strongly inﬂuenced by the language we speak.

Token 5992:
Whorf (1956) wrote “We cut nature up, orga-nize it into concepts, and ascribe signiﬁcances as we do, largely because we are parties to anagreement to organize it this way—an agreement that holds throughout our speech commu-nity and is codiﬁed in the patterns of our language.” It is certainly true that different speechcommunities divide up the world differently.

Token 5993:
The French have two words “chaise” and “fau-teuil,” for a concept that English speakers cover with one: “chair.” But English speakerscan easily recognize the category fauteuil and give it a name—roughly “open-arm chair”—so does language really make a difference?

Token 5994:
Whorf relied mainly on intuition and speculation, but in the intervening years we actually have real data from anthropological, psychologicaland neurological studies.

Token 5995:
For example, can you remember which of the following two phrases formed the opening of Section 8.1?

Token 5996:
“In this section, we discuss the nature of representation languages ...” “This section covers the topic of knowledge representation languages ...” Wanner (1974) did a similar experiment and found that subjects made the right choice at chance level—about 50% of the time—but remembered the content of what they read withbetter than 90% accuracy.

Token 5997:
This suggests that people process the words to form some kind ofnonverbal representation.

Token 5998:
More interesting is the case in which a concept is completely absent in a language.

Token 5999:
Speakers of the Australian aboriginal language Guugu Yimithirr have no words for relative directions, such as front, back, right, or left.

Token 6000:
Instead they use absolute directions, saying,for example, the equivalent of “I have a pain in my north arm.” This difference in languagemakes a difference in behavior: Guugu Yimithirr speakers are better at navigating in openterrain, while English speakers are better at placing the fork to the right of the plate.

Token 6001:
Language also seems to inﬂuence thought through seemingly arbitrary grammatical features such as the gender of nouns.

Token 6002:
For example, “bridge” is masculine in Spanish and feminine in German.

Token 6003:
Boroditsky (2003) asked subjects to choose English adjectives to de-scribe a photograph of a particular bridge.

Token 6004:
Spanish speakers chose big,dangerous ,strong , andtowering , whereas German speakers chose beautiful ,elegant ,fragile ,a n d slender .

Token 6005:
Words can serve as anchor points that affect how we perceive the world.

Token 6006:
Loftus and Palmer (1974) showed experimental subjects a movie of an auto accident.

Token 6007:
Subjects who were asked “How fast were the cars going when they contacted each other?” reported an average of 32 mph,while subjects who were asked the question with the word “smashed” instead of “contacted”reported 41mph for the same cars in the same movie.

Token 6008:
288 Chapter 8.

Token 6009:
First-Order Logic In a ﬁrst-order logic reasoning system that uses CNF, we can see that the linguistic form “¬(A∨B)”a n d“¬A∧¬B” are the same because we can look inside the system and see that the two sentences are stored as the same canonical CNF form.

Token 6010:
Can we do that with thehuman brain? Until recently the answer was “no,” but now it is “maybe.” Mitchell et al.

Token 6011:
(2008) put subjects in an fMRI (functional magnetic resonance imaging) machine, showed them words such as “celery,” and imaged their brains.

Token 6012:
The researchers were then able to train a computer program to predict, from a brain image, what word the subject had been presentedwith.

Token 6013:
Given two choices (e.g., “celery” or “airplane”), the system predicts correctly 77% ofthe time.

Token 6014:
The system can even predict at above-chance levels for words it has never seenan fMRI image of before (by considering the images of related words) and for people it hasnever seen before (proving that fMRI reveals some level of common representation acrosspeople).

Token 6015:
This type of work is still in its infancy, but fMRI (and other imaging technologysuch as intracranial electrophysiology (Sahin et al.

Token 6016:
, 2009)) promises to give us much more concrete ideas of what human knowledge representations are like.

Token 6017:
From the viewpoint of formal logic, representing the same knowledge in two different ways makes absolutely no difference; the same facts will be derivable from either represen-tation.

Token 6018:
In practice, however, one representation might require fewer steps to derive a conclu- sion, meaning that a reasoner with limited resources could get to the conclusion using one representation but not the other.

Token 6019:
For nondeductive tasks such as learning from experience, outcomes are necessarily dependent on the form of the representations used.

Token 6020:
We show in Chapter 18 that when a learning program considers two possible theories of the world, both of which are consistent with all the data, the most common way of breaking the tie is to choose the most succinct theory—and that depends on the language used to represent theories.

Token 6021:
Thus, the inﬂuence of language on thought is unavoidable for any agent that does learning.

Token 6022:
8.1.2 Combining the best of formal and natural languages We can adopt the foundation of propositional logic—a declarative, compositional semanticsthat is context-independent and unambiguous—and build a more expressive logic on that foundation, borrowing representational ideas from natural language while avoiding its draw- backs.

Token 6023:
When we look at the syntax of natural language, the most obvious elements are nounsand noun phrases that refer to objects (squares, pits, wumpuses) and verbs and verb phrases OBJECT that refer to relations among objects (is breezy, is adjacent to, shoots).

Token 6024:
Some of these rela- RELATION tions are functions —relations in which there is only one “value” for a given “input.” It is FUNCTION easy to start listing examples of objects, relations, and functions: •Objects: people, houses, numbers, theories, Ronald McDonald, colors, baseball games, wars, centuries ... •Relations: these can be unary relations or properties such as red, round, bogus, prime, PROPERTY multistoried ..., or more general n-ary relations such as brother of, bigger than, inside, part of, has color, occurred after, owns, comes between, ... •Functions: father of, best friend, third inning of, one more than, beginning of ...

Token 6025:
Indeed, almost any assertion can be thought of as referring to objects and properties or rela- tions. Some examples follow:

Token 6026:
Section 8.1. Representation Revisited 289 •“One plus two equals three.” Objects: one, two, three, one plus two; Relation: equals; Function: plus.

Token 6027:
(“One plustwo” is a name for the object that is obtained by applying the function “plus” to theobjects “one” and “two.” “Three” is another name for this object.)

Token 6028:
•“Squares neighboring the wumpus are smelly.” Objects: wumpus, squares; Property: smelly; Relation: neighboring.

Token 6029:
•“Evil King John ruled England in 1200.” Objects: John, England, 1200; Relation: ruled; Properties: evil, king.

Token 6030:
The language of ﬁrst-order logic , whose syntax and semantics we deﬁne in the next section, is built around objects and relations.

Token 6031:
It has been so important to mathematics, philosophy, andartiﬁcial intelligence precisely because those ﬁelds—and indeed, much of everyday human existence—can be usefully thought of as dealing with objects and the relations among them.

Token 6032:
First-order logic can also express facts about some orallof the objects in the universe.

Token 6033:
This enables one to represent general laws or rules, such as the statement “Squares neighboringthe wumpus are smelly.” The primary difference between propositional and ﬁrst-order logic lies in the ontologi- cal commitment made by each language—that is, what it assumes about the nature of reality .

Token 6034:
ONTOLOGICAL COMMITMENT Mathematically, this commitment is expressed through the nature of the formal models with respect to which the truth of sentences is deﬁned.

Token 6035:
For example, propositional logic assumesthat there are facts that either hold or do not hold in the world.

Token 6036:
Each fact can be in oneof two states: true or false, and each model assigns true orfalse to each proposition sym- bol (see Section 7.4.2).

Token 6037:
2First-order logic assumes more; namely, that the world consists of objects with certain relations among them that do or do not hold.

Token 6038:
The formal models are correspondingly more complicated than those for propositional logic.

Token 6039:
Special-purpose logics make still further ontological commitments; for example, temporal logic assumes that facts TEMPORAL LOGIC hold at particular times and that those times (which may be points or intervals) are ordered.

Token 6040:
Thus, special-purpose logics give certain kinds of objects (and the axioms about them) “ﬁrstclass” status within the logic, rather than simply deﬁning them within the knowledge base.Higher-order logic views the relations and functions referred to by ﬁrst-order logic as ob- HIGHER-ORDER LOGIC jects in themselves.

Token 6041:
This allows one to make assertions about allrelations—for example, one could wish to deﬁne what it means for a relation to be transitive.

Token 6042:
Unlike most special-purpose logics, higher-order logic is strictly more expressive than ﬁrst-order logic, in the sense that some sentences of higher-order logic cannot be expressed by any ﬁnite number of ﬁrst-order logic sentences.

Token 6043:
A logic can also be characterized by its epistemological commitments —the possibleEPISTEMOLOGICAL COMMITMENT states of knowledge that it allows with respect to each fact.

Token 6044:
In both propositional and ﬁrst- order logic, a sentence represents a fact and the agent either believes the sentence to be true,believes it to be false, or has no opinion.

Token 6045:
These logics therefore have three possible statesof knowledge regarding any sentence.

Token 6046:
Systems using probability theory , on the other hand, 2In contrast, facts in fuzzy logic have a degree of truth between 0 and 1.

Token 6047:
For example, the sentence “Vienna is a large city” might be true in our world only to degree 0.6 in fuzzy logic.

Token 6048:
290 Chapter 8.

Token 6049:
First-Order Logic can have any degree of belief , ranging from 0 (total disbelief) to 1 (total belief).3For ex- ample, a probabilistic wumpus-world agent might believe that the wumpus is in [1,3] withprobability 0.75.

Token 6050:
The ontological and epistemological commitments of ﬁve different logicsare summarized in Figure 8.1.

Token 6051:
Language Ontological Commitment Epistemological Commitment (What exists in the world) (What an agent believes about facts) Propositional logic facts true/false/unknown First-order logic facts, objects, relations true/false/unknown Temporal logic facts, objects, relations, times true/false/unknown Probability theory facts degree of belief ∈[0,1] Fuzzy logic facts with degree of truth ∈[0,1] known interval value Figure 8.1 Formal languages and their ontological and epistemological commitments.

Token 6052:
In the next section, we will launch into the details of ﬁrst-order logic.

Token 6053:
Just as a student of physics requires some familiarity with mathematics, a student of AI must develop a talent forworking with logical notation.

Token 6054:
On the other hand, it is also important notto get too concerned with the speciﬁcs of logical notation—after all, there are dozens of different versions.

Token 6055:
The main things to keep hold of are how the language facilitates concise representations and how its semantics leads to sound reasoning procedures.

Token 6056:
8.2 S YNTAX AND SEMANTICS OF FIRST-ORDER LOGIC We begin this section by specifying more precisely the way in which the possible worlds of ﬁrst-order logic reﬂect the ontological commitment to objects and relations.

Token 6057:
Then weintroduce the various elements of the language, explaining their semantics as we go along.

Token 6058:
8.2.1 Models for ﬁrst-order logic Recall from Chapter 7 that the models of a logical language are the formal structures thatconstitute the possible worlds under consideration.

Token 6059:
Each model links the vocabulary of thelogical sentences to elements of the possible world, so that the truth of any sentence canbe determined.

Token 6060:
Thus, models for propositional logic link proposition symbols to predeﬁnedtruth values. Models for ﬁrst-order logic are much more interesting.

Token 6061:
First, they have objectsin them! The domain of a model is the set of objects or domain elements it contains.

Token 6062:
The do- DOMAIN DOMAIN ELEMENTS main is required to be nonempty —every possible world must contain at least one object.

Token 6063:
(See Exercise 8.7 for a discussion of empty worlds.)

Token 6064:
Mathematically speaking, it doesn’t matterwhat these objects are—all that matters is how many there are in each particular model—but for pedagogical purposes we’ll use a concrete example.

Token 6065:
Figure 8.2 shows a model with ﬁve 3It is important not to confuse the degree of belief in probability theory with the degree of truth in fuzzy logic.

Token 6066:
Indeed, some fuzzy systems allow uncertainty (degree of belief) about degrees of truth.

Token 6067:
Section 8.2.

Token 6068:
Syntax and Semantics of First-Order Logic 291 objects: Richard the Lionheart, King of England from 1189 to 1199; his younger brother, the evil King John, who ruled from 1199 to 1215; the left legs of Richard and John; and a crown.

Token 6069:
The objects in the model may be related in various ways. In the ﬁgure, Richard and John are brothers.

Token 6070:
Formally speaking, a relation is just the set of tuples of objects that are TUPLE related.

Token 6071:
(A tuple is a collection of objects arranged in a ﬁxed order and is written with angle brackets surrounding the objects.)

Token 6072:
Thus, the brotherhood relation in this model is the set {/angbracketleftRichard the Lionheart ,King John/angbracketright,/angbracketleftKing John ,Richard the Lionheart /angbracketright}.

Token 6073:
(8.1) (Here we have named the objects in English, but you may, if you wish, mentally substitute the pictures for the names.)

Token 6074:
The crown is on King John’s head, so the “on head” relation containsjust one tuple, /angbracketleftthe crown ,King John/angbracketright.

Token 6075:
The “brother” and “on head” relations are binary relations—that is, they relate pairs of objects.

Token 6076:
The model also contains unary relations, orproperties: the “person” property is true of both Richard and John; the “king” property is trueonly of John (presumably because Richard is dead at this point); and the “crown” property istrue only of the crown.

Token 6077:
Certain kinds of relationships are best considered as functions, in that a given object must be related to exactly one object in this way.

Token 6078:
For example, each person has one left leg,so the model has a unary “left leg” function that includes the following mappings: /angbracketleftRichard the Lionheart /angbracketright→ Richard’s left leg /angbracketleftKing John/angbracketright→ John’s left leg .

Token 6079:
(8.2) Strictly speaking, models in ﬁrst-order logic require total functions , that is, there must be a TOTAL FUNCTIONS value for every input tuple.

Token 6080:
Thus, the crown must have a left leg and so must each of the left legs.

Token 6081:
There is a technical solution to this awkward problem involving an additional “invisible” R J $ left legon headbrother brotherperson person kingcrown left leg Figure 8.2 A model containing ﬁve objects, two binary relations, three unary relations (indicated by labels on the objects), and one unary function, left-leg.

Token 6082:
292 Chapter 8. First-Order Logic object that is the left leg of everything that has no left leg, including itself.

Token 6083:
Fortunately, as long as one makes no assertions about the left legs of things that have no left legs, thesetechnicalities are of no import.

Token 6084:
So far, we have described the elements that populate models for ﬁrst-order logic.

Token 6085:
The other essential part of a model is the link between those elements and the vocabulary of the logical sentences, which we explain next.

Token 6086:
8.2.2 Symbols and interpretations We turn now to the syntax of ﬁrst-order logic.

Token 6087:
The impatient reader can obtain a complete description from the formal grammar in Figure 8.3.

Token 6088:
The basic syntactic elements of ﬁrst-order logic are the symbols that stand for objects, relations, and functions.

Token 6089:
The symbols, therefore, come in three kinds: constant symbols , CONSTANT SYMBOL which stand for objects; predicate symbols , which stand for relations; and function sym- PREDICATE SYMBOL bols, which stand for functions.

Token 6090:
We adopt the convention that these symbols will begin with FUNCTION SYMBOL uppercase letters.

Token 6091:
For example, we might use the constant symbols Richard andJohn ;t h e predicate symbols Brother ,OnHead ,Person ,King ,a n dCrown ; and the function symbol LeftLeg .

Token 6092:
As with proposition symbols, the choice of names is entirely up to the user.

Token 6093:
Each predicate and function symbol comes with an arity that ﬁxes the number of arguments.

Token 6094:
ARITY As in propositional logic, every model must provide the information required to deter- mine if any given sentence is true or false.

Token 6095:
Thus, in addition to its objects, relations, andfunctions, each model includes an interpretation that speciﬁes exactly which objects, rela- INTERPRETATION tions and functions are referred to by the constant, predicate, and function symbols.

Token 6096:
One possible interpretation for our example—which a logician would call the intended interpre- tation —is as follows:INTENDED INTERPRETATION •Richard refers to Richard the Lionheart and John refers to the evil King John.

Token 6097:
•Brother refers to the brotherhood relation, that is, the set of tuples of objects given in Equation (8.1); OnHead refers to the “on head” relation that holds between the crown and King John; Person ,King ,a n dCrown refer to the sets of objects that are persons, kings, and crowns.

Token 6098:
•LeftLeg refers to the “left leg” function, that is, the mapping given in Equation (8.2). There are many other possible interpretations, of course.

Token 6099:
For example, one interpretation mapsRichard to the crown and John to King John’s left leg.

Token 6100:
There are ﬁve objects in the model, so there are 25 possible interpretations just for the constant symbols Richard andJohn .

Token 6101:
Notice that not all the objects need have a name—for example, the intended interpretation does not name the crown or the legs.

Token 6102:
It is also possible for an object to have several names; there is an interpretation under which both Richard andJohn refer to the crown.4If you ﬁnd this possibility confusing, remember that, in propositional logic, it is perfectly possible to have a model in which Cloudy andSunny are both true; it is the job of the knowledge base to rule out models that are inconsistent with our knowledge.

Token 6103:
4Later, in Section 8.2.8, we examine a semantics in which every object has exactly one name.

Token 6104:
Section 8.2.

Token 6105:
Syntax and Semantics of First-Order Logic 293 Sentence→AtomicSentence |ComplexSentence AtomicSentence →Predicate|Predicate (Term,...)|Term =Term ComplexSentence → (Sentence )|[Sentence ] |¬Sentence |Sentence∧Sentence |Sentence∨Sentence |Sentence⇒Sentence |Sentence⇔Sentence |Quantiﬁer Variable ,...Sentence Term→Function (Term,...) |Constant |Variable Quantiﬁer→∀ |∃ Constant→A|X1|John|··· Variable→a|x|s|··· Predicate→True|False|After|Loves|Raining|··· Function→Mother|LeftLeg|··· OPERATOR PRECEDENCE :¬,=,∧,∨,⇒,⇔ Figure 8.3 The syntax of ﬁrst-order logic with equality, speciﬁed in Backus–Naur form (see page 1060 if you are not familiar with this notation).

Token 6106:
Operator precedences are speciﬁed, from highest to lowest.

Token 6107:
The precedence of qua ntiﬁers is such that a quantiﬁer holds over everything to the right of it. RJ RJ RJ RJ RJ RJ . . . . . . . . .

Token 6108:
Figure 8.4 Some members of the set of all models for a language with two constant sym- bols,RandJ, and one binary relation symbol.

Token 6109:
The interpretation of each constant symbol is shown by a gray arrow. Within each model, the related objects are connected by arrows.

Token 6110:
294 Chapter 8.

Token 6111:
First-Order Logic In summary, a model in ﬁrst-order logic consists of a set of objects and an interpretation that maps constant symbols to objects, predicate symbols to relations on those objects, andfunction symbols to functions on those objects.

Token 6112:
Just as with propositional logic, entailment,validity, and so on are deﬁned in terms of all possible models .

Token 6113:
To get an idea of what the set of all possible models looks like, see Figure 8.4.

Token 6114:
It shows that models vary in how many objects they contain—from one up to inﬁnity—and in the way the constant symbols map to objects.

Token 6115:
If there are two constant symbols and one object, then both symbols must referto the same object; but this can still happen even with more objects.

Token 6116:
When there are moreobjects than constant symbols, some of the objects will have no names.

Token 6117:
Because the numberof possible models is unbounded, checking entailment by the enumeration of all possiblemodels is not feasible for ﬁrst-order logic (unlike propositional logic).

Token 6118:
Even if the number ofobjects is restricted, the number of combinations can be very large. (See Exercise 8.5.)

Token 6119:
Forthe example in Figure 8.4, there are 137,506,194,466 models with six or fewer objects.

Token 6120:
8.2.3 Terms Aterm is a logical expression that refers to an object.

Token 6121:
Constant symbols are therefore terms, TERM but it is not always convenient to have a distinct symbol to name every object.

Token 6122:
For example, in English we might use the expression “King John’s left leg” rather than giving a name to his leg.

Token 6123:
This is what function symbols are for: instead of using a constant symbol, weuseLeftLeg (John).

Token 6124:
In the general case, a complex term is formed by a function symbol followed by a parenthesized list of terms as arguments to the function symbol.

Token 6125:
It is importantto remember that a complex term is just a complicated kind of name.

Token 6126:
It is not a “subroutinecall” that “returns a value.” There is no LeftLeg subroutine that takes a person as input and returns a leg.

Token 6127:
We can reason about left legs (e.g., stating the general rule that everyone has oneand then deducing that John must have one) without ever providing a deﬁnition of LeftLeg .

Token 6128:
This is something that cannot be done with subroutines in programming languages. 5 The formal semantics of terms is straightforward.

Token 6129:
Consider a term f(t1,...,t n).T h e function symbol frefers to some function in the model (call it F); the argument terms refer to objects in the domain (call them d1,...,d n); and the term as a whole refers to the object that is the value of the function Fapplied to d1,...,d n. For example, suppose the LeftLeg function symbol refers to the function shown in Equation (8.2) and John refers to King John, thenLeftLeg (John)refers to King John’s left leg.

Token 6130:
In this way, the interpretation ﬁxes the referent of every term.

Token 6131:
8.2.4 Atomic sentences Now that we have both terms for referring to objects and predicate symbols for referring to relations, we can put them together to make atomic sentences that state facts.

Token 6132:
An atomic 5λ-expressions provide a useful notation in which new function symbols are constructed “on the ﬂy.” For example, the function that squares its argument can be written as (λx x×x)and can be applied to arguments just like any other function symbol.

Token 6133:
A λ-expression can also be deﬁned and used as a predicate symbol. (See Chapter 22.) The lambda operator in Lisp plays exactly the same role.

Token 6134:
Notice that the use of λin this way does notincrease the formal expressive power of ﬁrst-order logic, because any sentence that includes a λ-expression can be rewritten by “plugging in” its arguments to yield an equivalent sentence.

Token 6135:
Section 8.2.

Token 6136:
Syntax and Semantics of First-Order Logic 295 sentence (oratom for short) is formed from a predicate symbol optionally followed by a ATOMIC SENTENCE ATOM parenthesized list of terms, such as Brother (Richard ,John).

Token 6137:
This states, under the intended interpretation given earlier, that Richard the Lionheart is the brother of King John.6Atomic sentences can have complex terms as arguments.

Token 6138:
Thus, Married (Father (Richard ),Mother (John)) states that Richard the Lionheart’s father is married to King John’s mother (again, under a suitable interpretation).

Token 6139:
An atomic sentence is true in a given model if the relation referred to by the predicate symbol holds among the objects referred to by the arguments.

Token 6140:
8.2.5 Complex sentences We can use logical connectives to construct more complex sentences, with the same syntax and semantics as in propositional calculus.

Token 6141:
Here are four sentences that are true in the modelof Figure 8.2 under our intended interpretation: ¬Brother (LeftLeg (Richard ),John) Brother (Richard ,John)∧Brother (John,Richard ) King(Richard )∨King(John) ¬King(Richard )⇒King(John).

Token 6142:
8.2.6 Quantiﬁers Once we have a logic that allows objects, it is only natural to want to express properties of entire collections of objects, instead of enumerating the objects by name.

Token 6143:
Quantiﬁers let us QUANTIFIER do this. First-order logic contains two standard quantiﬁers, called universal andexistential .

Token 6144:
Universal quantiﬁcation ( ∀) Recall the difﬁculty we had in Chapter 7 with the expression of general rules in proposi- tional logic.

Token 6145:
Rules such as “Squares neighboring the wumpus are smelly” and “All kingsare persons” are the bread and butter of ﬁrst-order logic.

Token 6146:
We deal with the ﬁrst of these inSection 8.3. The second rule, “All kings are persons,” is written in ﬁrst-order logic as ∀xKing(x)⇒Person (x).

Token 6147:
∀is usually pronounced “For all ...”.

Token 6148:
(Remember that the upside-down A stands for “all.”) Thus, the sentence says, “For all x,i fxis a king, then xis a person.” The symbol xis called avariable .

Token 6149:
By convention, variables are lowercase letters.

Token 6150:
A variable is a term all by itself, VARIABLE and as such can also serve as the argument of a function—for example, LeftLeg (x).A t e r m with no variables is called a ground term .

Token 6151:
GROUND TERM Intuitively, the sentence ∀xP,w h e r e Pis any logical expression, says that Pis true for every object x.

Token 6152:
More precisely, ∀xP is true in a given model if Pis true in all possible extended interpretations constructed from the interpretation given in the model, where eachEXTENDED INTERPRETATION 6We usually follow the argument-ordering convention that P(x,y)is read as “ xis aPofy.”

Token 6153:
296 Chapter 8. First-Order Logic extended interpretation speciﬁes a domain element to which xrefers.

Token 6154:
This sounds complicated, but it is really just a careful way of stating the intuitive mean- ing of universal quantiﬁcation.

Token 6155:
Consider the model shown in Figure 8.2 and the intendedinterpretation that goes with it.

Token 6156:
We can extend the interpretation in ﬁve ways: x→Richard the Lionheart, x→King John, x→Richard’s left leg, x→John’s left leg, x→the crown.

Token 6157:
The universally quantiﬁed sentence ∀xKing(x)⇒Person (x)is true in the original model if the sentence King(x)⇒Person (x)is true under each of the ﬁve extended interpreta- tions.

Token 6158:
That is, the universally quantiﬁed sentence is equivalent to asserting the following ﬁve sentences: Richard the Lionheart is a king ⇒Richard the Lionheart is a person.

Token 6159:
King John is a king ⇒King John is a person. Richard’s left leg is a king ⇒Richard’s left leg is a person.

Token 6160:
John’s left leg is a king ⇒John’s left leg is a person. T h ec r o w ni sak i n g ⇒the crown is a person.

Token 6161:
Let us look carefully at this set of assertions.

Token 6162:
Since, in our model, King John is the only king, the second sentence asserts that he is a person, as we would hope.

Token 6163:
But what about the other four sentences, which appear to make claims about legs and crowns? Is that part of the meaning of “All kings are persons”?

Token 6164:
In fact, the other four assertions are true in the model, but make no claim whatsoever about the personhood qualiﬁcations of legs, crowns, or indeed Richard.

Token 6165:
This is because none of these objects is a king.

Token 6166:
Looking at the truth table for⇒(Figure 7.8 on page 246), we see that the implication is true whenever its premise is false— regardless of the truth of the conclusion.

Token 6167:
Thus, by asserting the universally quantiﬁed sentence, which is equivalent to asserting a whole list of individual implications, we endup asserting the conclusion of the rule just for those objects for whom the premise is trueand saying nothing at all about those individuals for whom the premise is false.

Token 6168:
Thus, thetruth-table deﬁnition of ⇒turns out to be perfect for writing general rules with universal quantiﬁers.

Token 6169:
A common mistake, made frequently even by diligent readers who have read this para- graph several times, is to use conjunction instead of implication.

Token 6170:
The sentence ∀xKing(x)∧Person (x) would be equivalent to asserting Richard the Lionheart is a king ∧Richard the Lionheart is a person, King John is a king ∧King John is a person, Richard’s left leg is a king ∧Richard’s left leg is a person, and so on.

Token 6171:
Obviously, this does not capture what we want.

Token 6172:
Section 8.2. Syntax and Semantics of First-Order Logic 297 Existential quantiﬁcation ( ∃) Universal quantiﬁcation makes statements about every object.

Token 6173:
Similarly, we can make a state- ment about some object in the universe without naming it, by using an existential quantiﬁer.

Token 6174:
To say, for example, that King John has a crown on his head, we write ∃xCrown (x)∧OnHead (x,John).

Token 6175:
∃xis pronounced “There exists an xsuch that ...”o r“ F o rs o m e x... ”.

Token 6176:
Intuitively, the sentence ∃xP says that Pis true for at least one object x.M o r e precisely,∃xP is true in a given model if Pis true in at least one extended interpretation that assigns xto a domain element.

Token 6177:
That is, at least one of the following is true: Richard the Lionheart is a crown ∧Richard the Lionheart is on John’s head; King John is a crown ∧King John is on John’s head; Richard’s left leg is a crown ∧Richard’s left leg is on John’s head; John’s left leg is a crown ∧John’s left leg is on John’s head; T h ec r o w ni sac r o w n ∧the crown is on John’s head.

Token 6178:
The ﬁfth assertion is true in the model, so the original existentially quantiﬁed sentence is true in the model.

Token 6179:
Notice that, by our deﬁnition, the sentence would also be true in a modelin which King John was wearing two crowns.

Token 6180:
This is entirely consistent with the originalsentence “King John has a crown on his head.” 7 Just as⇒appears to be the natural connective to use with ∀,∧is the natural connective to use with∃.U s i n g∧as the main connective with ∀led to an overly strong statement in the example in the previous section; using ⇒with∃usually leads to a very weak statement, indeed.

Token 6181:
Consider the following sentence: ∃xCrown (x)⇒OnHead (x,John). On the surface, this might look like a reasonable rendition of our sentence.

Token 6182:
Applying the semantics, we see that the sentence says that at least one of the following assertions is true: Richard the Lionheart is a crown ⇒Richard the Lionheart is on John’s head; King John is a crown ⇒King John is on John’s head; Richard’s left leg is a crown ⇒Richard’s left leg is on John’s head; and so on.

Token 6183:
Now an implication is true if both premise and conclusion are true, or if its premise is false .

Token 6184:
So if Richard the Lionheart is not a crown, then the ﬁrst assertion is true and the existential is satisﬁed.

Token 6185:
So, an existentially quantiﬁed implication sentence is true wheneveranyobject fails to satisfy the premise; hence such sentences really do not say much at all.

Token 6186:
Nested quantiﬁers We will often want to express more complex sentences using multiple quantiﬁers.

Token 6187:
The sim- plest case is where the quantiﬁers are of the same type.

Token 6188:
For example, “Brothers are siblings”can be written as ∀x∀yBrother (x,y)⇒Sibling (x,y).

Token 6189:
7There is a variant of the existential quantiﬁer, usually written ∃1or∃!, that means “There exists exactly one.” The same meaning can be expressed using equality statements.

Token 6190:
298 Chapter 8. First-Order Logic Consecutive quantiﬁers of the same type can be written as one quantiﬁer with several vari- ables.

Token 6191:
For example, to say that siblinghood is a symmetric relationship, we can write ∀x,ySibling (x,y)⇔Sibling (y,x). In other cases we will have mixtures.

Token 6192:
“Everybody loves somebody” means that for every person, there is someone that person loves: ∀x∃yLoves(x,y).

Token 6193:
On the other hand, to say “There is someone who is loved by everyone,” we write ∃y∀xLoves(x,y).

Token 6194:
The order of quantiﬁcation is therefore very important.

Token 6195:
It becomes clearer if we insert paren- theses.∀x(∃yLoves(x,y))says that everyone has a particular property, namely, the prop- erty that they love someone.

Token 6196:
On the other hand, ∃y(∀xLoves(x,y))says that someone in the world has a particular property, namely the property of being loved by everybody.

Token 6197:
Some confusion can arise when two quantiﬁers are used with the same variable name. Consider the sentence ∀x(Crown (x)∨(∃xBrother (Richard ,x))).

Token 6198:
Here the xinBrother (Richard ,x)isexistentially quantiﬁed.

Token 6199:
The rule is that the variable belongs to the innermost quantiﬁer that mentions it; then it will not be subject to any otherquantiﬁcation.

Token 6200:
Another way to think of it is this: ∃xBrother (Richard ,x) is a sentence about Richard (that he has a brother), not about x; so putting a∀xoutside it has no effect.

Token 6201:
It could equally well have been written ∃zBrother (Richard ,z).

Token 6202:
Because this can be a source of confusion, we will always use different variable names with nested quantiﬁers.

Token 6203:
Connections between ∀and∃ The two quantiﬁers are actually intimately connected with each other, through negation.

Token 6204:
As- serting that everyone dislikes parsnips is the same as asserting there does not exist someone who likes them, and vice versa: ∀x¬Likes(x,Parsnips )is equivalent to ¬∃xLikes(x,Parsnips ).

Token 6205:
We can go one step further: “Everyone likes ice cream” means that there is no one who does not like ice cream: ∀xLikes(x,IceCream )is equivalent to ¬∃x¬Likes(x,IceCream ).

Token 6206:
Because∀is really a conjunction over the universe of objects and ∃is a disjunction, it should not be surprising that they obey De Morgan’s rules.

Token 6207:
The De Morgan rules for quantiﬁed andunquantiﬁed sentences are as follows: ∀x¬P≡¬ ∃ xP ¬(P∨Q)≡¬P∧¬Q ¬∀xP≡∃x¬P¬(P∧Q)≡¬P∨¬Q ∀xP≡¬ ∃ x¬PP∧Q≡¬(¬P∨¬Q) ∃xP≡¬ ∀ x¬PP∨Q≡¬(¬P∧¬Q).

Token 6208:
Thus, we do not really need both ∀and∃, just as we do not really need both ∧and∨.

Token 6209:
Still, readability is more important than parsimony, so we will keep both of the quantiﬁers.

Token 6210:
Section 8.2.

Token 6211:
Syntax and Semantics of First-Order Logic 299 8.2.7 Equality First-order logic includes one more way to make atomic sentences, other than using a predi- cate and terms as described earlier.

Token 6212:
We can use the equality symbol to signify that two terms EQUALITY SYMBOL refer to the same object.

Token 6213:
For example, Father (John)=Henry says that the object referred to by Father (John)and the object referred to by Henry are the same.

Token 6214:
Because an interpretation ﬁxes the referent of any term, determining the truth of an equality sentence is simply a matter of seeing that the referents of the two terms are the sameobject.

Token 6215:
The equality symbol can be used to state facts about a given function, as we just did for theFather symbol.

Token 6216:
It can also be used with negation to insist that two terms are not the same object.

Token 6217:
To say that Richard has at least two brothers, we would write ∃x,yBrother (x,Richard )∧Brother (y,Richard )∧¬(x=y).

Token 6218:
The sentence ∃x,yBrother (x,Richard )∧Brother (y,Richard ) does not have the intended meaning.

Token 6219:
In particular, it is true in the model of Figure 8.2, where Richard has only one brother.

Token 6220:
To see this, consider the extended interpretation in which bothxandyare assigned to King John. The addition of ¬(x=y)rules out such models.

Token 6221:
The notation x/negationslash=yis sometimes used as an abbreviation for ¬(x=y). 8.2.8 An alternative semantics?

Token 6222:
Continuing the example from the previous section, suppose that we believe that Richard has two brothers, John and Geoffrey.8Can we capture this state of affairs by asserting Brother (John,Richard )∧Brother (Geoﬀrey ,Richard )?

Token 6223:
(8.3) Not quite. First, this assertion is true in a model where Richard has only one brother— we need to add John/negationslash=Geoﬀrey .

Token 6224:
Second, the sentence doesn’t rule out models in which Richard has many more brothers besides John and Geoffrey.

Token 6225:
Thus, the correct translation of“Richard’s brothers are John and Geoffrey” is as follows: Brother (John,Richard )∧Brother (Geoﬀrey ,Richard )∧John/negationslash=Geoﬀrey ∧∀xBrother (x,Richard )⇒(x=John∨x=Geoﬀrey ).

Token 6226:
For many purposes, this seems much more cumbersome than the corresponding natural- language expression.

Token 6227:
As a consequence, humans may make mistakes in translating theirknowledge into ﬁrst-order logic, resulting in unintuitive behaviors from logical reasoningsystems that use the knowledge.

Token 6228:
Can we devise a semantics that allows a more straightfor-ward logical expression?

Token 6229:
One proposal that is very popular in database systems works as follows.

Token 6230:
First, we insist that every constant symbol refer to a distinct object—the so-called unique-names assump- tion.

Token 6231:
Second, we assume that atomic sentences not known to be true are in fact false—the UNIQUE-NAMES ASSUMPTION closed-world assumption .

Token 6232:
Finally, we invoke domain closure , meaning that each modelCLOSED-WORLD ASSUMPTION DOMAIN CLOSURE 8Actually he had four, the others being William and Henry.

Token 6233:
300 Chapter 8. First-Order Logic . .

Token 6234:
.RJ R JRJ R JRJ R JRJ R JRJ R J Figure 8.5 Some members of the set of all models for a language with two constant sym- bols,RandJ, and one binary relation symbol, under database semantics.

Token 6235:
The interpretation of the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.

Token 6236:
contains no more domain elements than those named by the constant symbols.

Token 6237:
Under the resulting semantics, which we call database semantics to distinguish it from the standardDATABASE SEMANTICS semantics of ﬁrst-order logic, the sentence Equation (8.3) does indeed state that Richard’s two brothers are John and Geoffrey.

Token 6238:
Database semantics is also used in logic programmingsystems, as explained in Section 9.4.5.

Token 6239:
It is instructive to consider the set of all possible models under database semantics for the same case as shown in Figure 8.4.

Token 6240:
Figure 8.5 shows some of the models, ranging from the model with no tuples satisfying the relation to the model with all tuples satisfying therelation.

Token 6241:
With two objects, there are four possible two-element tuples, so there are 2 4=1 6 different subsets of tuples that can satisfy the relation.

Token 6242:
Thus, there are 16 possible models inall—a lot fewer than the inﬁnitely many models for the standard ﬁrst-order semantics.

Token 6243:
On theother hand, the database semantics requires deﬁnite knowledge of what the world contains.

Token 6244:
This example brings up an important point: there is no one “correct” semantics for logic.

Token 6245:
The usefulness of any proposed semantics depends on how concise and intuitive itmakes the expression of the kinds of knowledge we want to write down, and on how easyand natural it is to develop the corresponding rules of inference.

Token 6246:
Database semantics is mostuseful when we are certain about the identity of all the objects described in the knowledge base and when we have all the facts at hand; in other cases, it is quite awkward.

Token 6247:
For the rest of this chapter, we assume the standard semantics while noting instances in which this choiceleads to cumbersome expressions.

Token 6248:
8.3 U SING FIRST-ORDER LOGIC Now that we have deﬁned an expressive logical language, it is time to learn how to use it.

Token 6249:
Thebest way to do this is through examples.

Token 6250:
We have seen some simple sentences illustrating thevarious aspects of logical syntax; in this section, we provide more systematic representations of some simple domains .

Token 6251:
In knowledge representation, a domain is just some part of the DOMAIN world about which we wish to express some knowledge.

Token 6252:
We begin with a brief description of the T ELL/ASKinterface for ﬁrst-order knowledge bases.

Token 6253:
Then we look at the domains of family relationships, numbers, sets, and lists, and at

Token 6254:
Section 8.3. Using First-Order Logic 301 the wumpus world.

Token 6255:
The next section contains a more substantial example (electronic circuits) and Chapter 12 covers everything in the universe.

Token 6256:
8.3.1 Assertions and queries in ﬁrst-order logic Sentences are added to a knowledge base using T ELL, exactly as in propositional logic.

Token 6257:
Such sentences are called assertions .

Token 6258:
For example, we can assert that John is a king, Richard is a ASSERTION person, and all kings are persons: TELL(KB,King(John)).

Token 6259:
TELL(KB,Person (Richard )). TELL(KB,∀xKing(x)⇒Person (x)). We can ask questions of the knowledge base using A SK.

Token 6260:
For example, ASK(KB,King(John)) returns true. Questions asked with A SKare called queries orgoals .

Token 6261:
Generally speaking, any QUERY GOAL query that is logically entailed by the knowledge base should be answered afﬁrmatively.

Token 6262:
For example, given the two preceding assertions, the query ASK(KB,Person (John)) should also return true.

Token 6263:
We can ask quantiﬁed queries, such as ASK(KB,∃xPerson (x)). The answer is true, but this is perhaps not as helpful as we would like.

Token 6264:
It is rather like answering “Can you tell me the time?” with “Yes.” If we want to know what value of x makes the sentence true, we will need a different function, A SKVARS, which we call with ASKVARS(KB,Person (x)) and which yields a stream of answers.

Token 6265:
In this case there will be two answers: {x/John}and {x/Richard}.

Token 6266:
Such an answer is called a substitution orbinding list .ASKVARS is usually SUBSTITUTION BINDINGLIST reserved for knowledge bases consisting solely of Horn clauses, because in such knowledge bases every way of making the query true will bind the variables to speciﬁc values.

Token 6267:
That is not the case with ﬁrst-order logic; if KBhas been told King(John)∨King(Richard ),t h e n there is no binding to xfor the query∃xKing(x), even though the query is true.

Token 6268:
8.3.2 The kinship domain The ﬁrst example we consider is the domain of family relationships, or kinship.

Token 6269:
This domain includes facts such as “Elizabeth is the mother of Charles” and “Charles is the father ofWilliam” and rules such as “One’s grandmother is the mother of one’s parent.” Clearly, the objects in our domain are people.

Token 6270:
We have two unary predicates, Male and Female .

Token 6271:
Kinship relations—parenthood, brotherhood, marriage, and so on—are represented by binary predicates: Parent ,Sibling ,Brother ,Sister ,Child ,Daughter ,Son,Spouse , Wife ,Husband ,Grandparent ,Grandchild ,Cousin ,Aunt ,a n dUncle .

Token 6272:
We use functions forMother andFather , because every person has exactly one of each of these (at least according to nature’s design).

Token 6273:
302 Chapter 8. First-Order Logic We can go through each function and predicate, writing down what we know in terms of the other symbols.

Token 6274:
For example, one’s mother is one’s female parent: ∀m,cMother (c)=m⇔Female (m)∧Parent (m,c).

Token 6275:
One’s husband is one’s male spouse: ∀w,hHusband (h, w)⇔Male(h)∧Spouse (h, w). Male and female are disjoint categories: ∀xMale(x)⇔¬Female (x).

Token 6276:
Parent and child are inverse relations: ∀p,cParent (p,c)⇔Child(c,p).

Token 6277:
A grandparent is a parent of one’s parent: ∀g,cGrandparent (g,c)⇔∃pParent (g,p)∧Parent (p,c).

Token 6278:
A sibling is another child of one’s parents: ∀x,ySibling (x,y)⇔x/negationslash=y∧∃pParent (p,x)∧Parent (p,y).

Token 6279:
We could go on for several more pages like this, and Exercise 8.14 asks you to do just that.

Token 6280:
Each of these sentences can be viewed as an axiom of the kinship domain, as explained in Section 7.1.

Token 6281:
Axioms are commonly associated with purely mathematical domains—we will see some axioms for numbers shortly—but they are needed in all domains.

Token 6282:
They provide the basic factual information from which useful conclusions can be derived.

Token 6283:
Our kinship axioms are also deﬁnitions ; they have the form ∀x,y P (x,y)⇔....

Token 6284:
The axioms deﬁne DEFINITION theMother function and the Husband ,Male ,Parent ,Grandparent ,a n dSibling predicates in terms of other predicates.

Token 6285:
Our deﬁnitions “bottom out” at a basic set of predicates ( Child , Spouse ,a n dFemale ) in terms of which the others are ultimately deﬁned.

Token 6286:
This is a natural way in which to build up the representation of a domain, and it is analogous to the way inwhich software packages are built up by successive deﬁnitions of subroutines from primitivelibrary functions.

Token 6287:
Notice that there is not necessarily a unique set of primitive predicates;we could equally well have used Parent ,Spouse ,a n dMale .

Token 6288:
In some domains, as we show, there is no clearly identiﬁable basic set. Not all logical sentences about a domain are axioms.

Token 6289:
Some are theorems —that is, they THEOREM are entailed by the axioms.

Token 6290:
For example, consider the assertion that siblinghood is symmetric: ∀x,ySibling (x,y)⇔Sibling (y,x). Is this an axiom or a theorem?

Token 6291:
In fact, it is a theorem that follows logically from the axiom that deﬁnes siblinghood.

Token 6292:
If we A SKthe knowledge base this sentence, it should return true.

Token 6293:
From a purely logical point of view, a knowledge base need contain only axioms and no theorems, because the theorems do not increase the set of conclusions that follow from the knowledge base.

Token 6294:
From a practical point of view, theorems are essential to reduce the computational cost of deriving new sentences.

Token 6295:
Without them, a reasoning system has to startfrom ﬁrst principles every time, rather like a physicist having to rederive the rules of calculusfor every new problem.

Token 6296:
Section 8.3. Using First-Order Logic 303 Not all axioms are deﬁnitions.

Token 6297:
Some provide more general information about certain predicates without constituting a deﬁnition.

Token 6298:
Indeed, some predicates have no complete deﬁ-nition because we do not know enough to characterize them fully.

Token 6299:
For example, there is noobvious deﬁnitive way to complete the sentence ∀xPerson (x)⇔... Fortunately, ﬁrst-order logic allows us to make use of the Person predicate without com- pletely deﬁning it.

Token 6300:
Instead, we can write partial speciﬁcations of properties that every personhas and properties that make something a person: ∀xPerson (x)⇒... ∀x ...⇒Person (x).

Token 6301:
Axioms can also be “just plain facts,” such as Male(Jim)andSpouse (Jim,Laura ).

Token 6302:
Such facts form the descriptions of speciﬁc problem instances, enabling speciﬁc questionsto be answered.

Token 6303:
The answers to these questions will then be theorems that follow fromthe axioms.

Token 6304:
Often, one ﬁnds that the expected answers are not forthcoming—for example,fromSpouse (Jim,Laura )one expects (under the laws of many countries) to be able to infer ¬Spouse (George ,Laura ); but this does not follow from the axioms given earlier—even after we add Jim/negationslash=George as suggested in Section 8.2.8.

Token 6305:
This is a sign that an axiom is missing. Exercise 8.8 asks the reader to supply it.

Token 6306:
8.3.3 Numbers, sets, and lists Numbers are perhaps the most vivid example of how a large theory can be built up froma tiny kernel of axioms.

Token 6307:
We describe here the theory of natural numbers or non-negative NATURAL NUMBERS integers.

Token 6308:
We need a predicate NatNum that will be true of natural numbers; we need one constant symbol, 0; and we need one function symbol, S(successor).

Token 6309:
The Peano axioms PEANO AXIOMS deﬁne natural numbers and addition.9Natural numbers are deﬁned recursively: NatNum (0). ∀nNatNum (n)⇒NatNum (S(n)).

Token 6310:
That is, 0 is a natural number, and for every object n,i fnis a natural number, then S(n)is a natural number.

Token 6311:
So the natural numbers are 0,S(0),S(S(0)), and so on.

Token 6312:
(After reading Section 8.2.8, you will notice that these axioms allow for other natural numbers besides the usual ones; see Exercise 8.12.)

Token 6313:
We also need axioms to constrain the successor function: ∀n0/negationslash=S(n). ∀m,n m/negationslash=n⇒S(m)/negationslash=S(n).

Token 6314:
Now we can deﬁne addition in terms of the successor function: ∀mNatNum (m)⇒+( 0,m)=m. ∀m,nNatNum (m)∧NatNum (n)⇒+(S(m),n)=S(+(m,n)).

Token 6315:
The ﬁrst of these axioms says that adding 0 to any natural number mgivesmitself.

Token 6316:
Notice the use of the binary function symbol “ +”i nt h et e r m +(m,0); in ordinary mathematics, the term would be written m+0using inﬁx notation.

Token 6317:
(The notation we have used for ﬁrst-order INFIX 9The Peano axioms also include the principle of induction, which is a sentence of second-order logic rather than of ﬁrst-order logic.

Token 6318:
The importance of this distinction is explained in Chapter 9.

Token 6319:
304 Chapter 8. First-Order Logic logic is called preﬁx .)

Token 6320:
To make our sentences about numbers easier to read, we allow the use PREFIX of inﬁx notation.

Token 6321:
We can also write S(n)asn+1, so the second axiom becomes ∀m,nNatNum (m)∧NatNum (n)⇒(m+1 )+ n=(m+n)+1.

Token 6322:
This axiom reduces addition to repeated application of the successor function.

Token 6323:
The use of inﬁx notation is an example of syntactic sugar , that is, an extension to or SYNTACTIC SUGAR abbreviation of the standard syntax that does not change the semantics.

Token 6324:
Any sentence that uses sugar can be “desugared” to produce an equivalent sentence in ordinary ﬁrst-order logic.

Token 6325:
Once we have addition, it is straightforward to deﬁne multiplication as repeated addi- tion, exponentiation as repeated multiplication, integer division and remainders, prime num- bers, and so on.

Token 6326:
Thus, the whole of number theory (including cryptography) can be built up from one constant, one function, one predicate and four axioms.

Token 6327:
The domain of sets is also fundamental to mathematics as well as to commonsense SET reasoning.

Token 6328:
(In fact, it is possible to deﬁne number theory in terms of set theory.) We want to be able to represent individual sets, including the empty set.

Token 6329:
We need a way to build up setsby adding an element to a set or taking the union or intersection of two sets.

Token 6330:
We will wantto know whether an element is a member of a set and we will want to distinguish sets fromobjects that are not sets.

Token 6331:
We will use the normal vocabulary of set theory as syntactic sugar. The empty set is a constant written as {}.

Token 6332:
There is one unary predicate, Set, which is true of sets.

Token 6333:
The binary predicates are x∈s(xis a member of set s)a n ds 1⊆s2(sets1is a subset, not necessarily proper, of set s2).

Token 6334:
The binary functions are s1∩s2(the intersection of two sets), s1∪s2 (the union of two sets), and {x|s}(the set resulting from adjoining element xto sets).

Token 6335:
One possible set of axioms is as follows: 1.

Token 6336:
The only sets are the empty set and those made by adjoining something to a set: ∀sSet(s)⇔(s={})∨(∃x,s2Set(s2)∧s={x|s2}). 2.

Token 6337:
The empty set has no elements adjoined into it. In other words, there is no way to decompose{}into a smaller set and an element: ¬∃x,s{x|s}={}. 3.

Token 6338:
Adjoining an element already in the set has no effect: ∀x,s x∈s⇔s={x|s}. 4. The only members of a set are the elements that were adjoined into it.

Token 6339:
We express this recursively, saying that xi sam e m b e ro f sif and only if sis equal to some set s2 adjoined with some element y, where either yis the same as xorxis a member of s2: ∀x,s x∈s⇔∃y,s2(s={y|s2}∧(x=y∨x∈s2)).

Token 6340:
5. A set is a subset of another set if and only if all of the ﬁrst set’s members are members of the second set: ∀s1,s2s1⊆s2⇔(∀xx∈s1⇒x∈s2). 6.

Token 6341:
Two sets are equal if and only if each is a subset of the other: ∀s1,s2(s1=s2)⇔(s1⊆s2∧s2⊆s1).

Token 6342:
Section 8.3. Using First-Order Logic 305 7.

Token 6343:
An object is in the intersection of two sets if and only if it is a member of both sets: ∀x,s1,s2x∈(s1∩s2)⇔(x∈s1∧x∈s2). 8.

Token 6344:
An object is in the union of two sets if and only if it is a member of either set: ∀x,s1,s2x∈(s1∪s2)⇔(x∈s1∨x∈s2). Lists are similar to sets.

Token 6345:
The differences are that lists are ordered and the same element can LIST appear more than once in a list.

Token 6346:
We can use the vocabulary of Lisp for lists: Nilis the constant list with no elements; Cons ,Append ,First ,a n dRest are functions; and Find is the pred- icate that does for lists what Member does for sets.

Token 6347:
List?is a predicate that is true only of lists. As with sets, it is common to use syntactic sugar in logical sentences involving lists.

Token 6348:
Theempty list is [].T h et e r m Cons(x,y),w h e r e yis a nonempty list, is written [x|y].T h et e r m Cons(x,Nil)(i.e., the list containing the element x) is written as [x].

Token 6349:
A list of several ele- ments, such as [A,B,C ], corresponds to the nested term Cons(A,Cons(B,Cons(C,Nil))).

Token 6350:
Exercise 8.16 asks you to write out the axioms for lists.

Token 6351:
8.3.4 The wumpus world Some propositional logic axioms for the wumpus world were given in Chapter 7.

Token 6352:
The ﬁrst-order axioms in this section are much more concise, capturing in a natural way exactly whatwe want to say.

Token 6353:
Recall that the wumpus agent receives a percept vector with ﬁve elements.

Token 6354:
The corre- sponding ﬁrst-order sentence stored in the knowledge base must include both the percept andthe time at which it occurred; otherwise, the agent will get confused about when it saw what.We use integers for time steps.

Token 6355:
A typical percept sentence would be Percept ([Stench ,Breeze ,Glitter ,None,None],5).

Token 6356:
Here,Percept is a binary predicate, and Stench and so on are constants placed in a list.

Token 6357:
The actions in the wumpus world can be represented by logical terms: Turn(Right),Turn(Left),Forward ,Shoot,Grab,Climb .

Token 6358:
To determine which is best, the agent program executes the query A SKVARS(∃aBestAction (a,5)), which returns a binding list such as {a/Grab}.

Token 6359:
The agent program can then return Grab as the action to take. The raw percept data implies certain facts about the current state.

Token 6360:
Forexample: ∀t,s,g,m,c Percept ([s,Breeze ,g,m,c ],t)⇒Breeze (t), ∀t,s,b,m,c Percept ([s,b,Glitter ,m,c],t)⇒Glitter (t), and so on.

Token 6361:
These rules exhibit a trivial form of the reasoning process called perception ,w h i c h we study in depth in Chapter 24.

Token 6362:
Notice the quantiﬁcation over time t. In propositional logic, we would need copies of each sentence for each time step.

Token 6363:
Simple “reﬂex” behavior can also be implemented by quantiﬁed implication sentences. For example, we have ∀tGlitter (t)⇒BestAction (Grab,t).

Token 6364:
306 Chapter 8.

Token 6365:
First-Order Logic Given the percept and rules from the preceding paragraphs, this would yield the desired con- clusion BestAction (Grab,5)—that is, Grab is the right thing to do.

Token 6366:
We have represented the agent’s inputs and outputs; now it is time to represent the environment itself. Let us begin with objects.

Token 6367:
Obvious candidates are squares, pits, and thewumpus.

Token 6368:
We could name each square— Square 1,2and so on—but then the fact that Square 1,2 andSquare 1,3are adjacent would have to be an “extra” fact, and we would need one such fact for each pair of squares.

Token 6369:
It is better to use a complex term in which the row and columnappear as integers; for example, we can simply use the list term [1,2].

Token 6370:
Adjacency of any two squares can be deﬁned as ∀x,y,a,b Adjacent ([x,y],[a,b])⇔ (x=a∧(y=b−1∨y=b+1 ) )∨(y=b∧(x=a−1∨x=a+1 ) ) .

Token 6371:
We could name each pit, but this would be inappropriate for a different reason: there is no reason to distinguish among pits.

Token 6372:
10It is simpler to use a unary predicate Pitthat is true of squares containing pits.

Token 6373:
Finally, since there is exactly one wumpus, a constant Wumpus is just as good as a unary predicate (and perhaps more digniﬁed from the wumpus’s viewpoint).

Token 6374:
The agent’s location changes over time, so we write At(Agent ,s,t)to mean that the agent is at square sat time t. We can ﬁx the wumpus’s location with ∀tAt(Wumpus ,[2,2],t).

Token 6375:
We can then say that objects can only be at one location at a time: ∀x,s1,s2,tAt(x,s1,t)∧At(x,s2,t)⇒s1=s2.

Token 6376:
Given its current location, the agent can infer properties of the square from properties of its current percept.

Token 6377:
For example, if the agent is at a square and perceives a breeze, then thatsquare is breezy: ∀s,tAt(Agent ,s,t)∧Breeze (t)⇒Breezy (s).

Token 6378:
It is useful to know that a square is breezy because we know that the pits cannot move about. Notice that Breezy has no time argument.

Token 6379:
Having discovered which places are breezy (or smelly) and, very important, notbreezy (ornotsmelly), the agent can deduce where the pits are (and where the wumpus is).

Token 6380:
Whereas propositional logic necessitates a separate axiom for each square (see R 2andR3on page 247) and would need a different set of axioms for each geographical layout of the world, ﬁrst-orderlogic just needs one axiom: ∀sBreezy (s)⇔∃rAdjacent (r, s)∧Pit(r).

Token 6381:
(8.4) Similarly, in ﬁrst-order logic we can quantify over time, so we need just one successor-state axiom for each predicate, rather than a different copy for each time step.

Token 6382:
For example, theaxiom for the arrow (Equation (7.2) on page 267) becomes ∀tHaveArrow (t+1 )⇔(HaveArrow (t)∧¬Action (Shoot,t)).

Token 6383:
From these two example sentences, we can see that the ﬁrst-order logic formulation is no less concise than the original English-language description given in Chapter 7.

Token 6384:
The reader 10Similarly, most of us do not name each bird that ﬂies overhead as it migrates to warmer regions in winter.

Token 6385:
An ornithologist wishing to study migration patterns, survival rates, and so on does name each bird, by means of a ring on its leg, because individual birds must be tracked.

Token 6386:
Section 8.4.

Token 6387:
Knowledge Engineering in First-Order Logic 307 is invited to construct analogous axioms for the agent’s location and orientation; in these cases, the axioms quantify over both space and time.

Token 6388:
As in the case of propositional stateestimation, an agent can use logical inference with axioms of this kind to keep track of aspectsof the world that are not directly observed.

Token 6389:
Chapter 10 goes into more depth on the subject ofﬁrst-order successor-state axioms and their uses for constructing plans.

Token 6390:
8.4 K NOWLEDGE ENGINEERING IN FIRST-ORDER LOGIC The preceding section illustrated the use of ﬁrst-order logic to represent knowledge in threesimple domains.

Token 6391:
This section describes the general process of knowledge-base construction—a process called knowledge engineering .

Token 6392:
A knowledge engineer is someone who investigates KNOWLEDGE ENGINEERING a particular domain, learns what concepts are important in that domain, and creates a formal representation of the objects and relations in the domain.

Token 6393:
We illustrate the knowledge engi-neering process in an electronic circuit domain that should already be fairly familiar, so thatwe can concentrate on the representational issues involved.

Token 6394:
The approach we take is suitablefor developing special-purpose knowledge bases whose domain is carefully circumscribed and whose range of queries is known in advance.

Token 6395:
General-purpose knowledge bases, which cover a broad range of human knowledge and are intended to support tasks such as naturallanguage understanding, are discussed in Chapter 12.

Token 6396:
8.4.1 The knowledge-engineering process Knowledge engineering projects vary widely in content, scope, and difﬁculty, but all suchprojects include the following steps: 1.Identify the task.

Token 6397:
The knowledge engineer must delineate the range of questions that the knowledge base will support and the kinds of facts that will be available for eachspeciﬁc problem instance.

Token 6398:
For example, does the wumpus knowledge base need to beable to choose actions or is it required to answer questions only about the contents of the environment?

Token 6399:
Will the sensor facts include the current location?

Token 6400:
The task will determine what knowledge must be represented in order to connect problem instances toanswers.

Token 6401:
This step is analogous to the PEAS process for designing agents in Chapter 2. 2.Assemble the relevant knowledge.

Token 6402:
The knowledge engineer might already be an expert in the domain, or might need to work with real experts to extract what they know—a process called knowledge acquisition .

Token 6403:
At this stage, the knowledge is not represented KNOWLEDGE ACQUISITION formally.

Token 6404:
The idea is to understand the scope of the knowledge base, as determined by the task, and to understand how the domain actually works.

Token 6405:
For the wumpus world, which is deﬁned by an artiﬁcial set of rules, the relevant knowledge is easy to identify.

Token 6406:
(Notice, however, that the deﬁnition of adjacency was not supplied explicitly in the wumpus-world rules.)

Token 6407:
For real domains, the issue ofrelevance can be quite difﬁcult—for example, a system for simulating VLSI designsmight or might not need to take into account stray capacitances and skin effects.

Token 6408:
308 Chapter 8. First-Order Logic 3.Decide on a vocabulary of predicates, functions, and constants.

Token 6409:
That is, translate the important domain-level concepts into logic-level names. This involves many questionsof knowledge-engineering style.

Token 6410:
Like programming style, this can have a signiﬁcant impact on the eventual success of the project.

Token 6411:
For example, should pits be representedby objects or by a unary predicate on squares? Should the agent’s orientation be a function or a predicate?

Token 6412:
Should the wumpus’s location depend on time?

Token 6413:
Once the choices have been made, the result is a vocabulary that is known as the ontology of ONTOLOGY the domain.

Token 6414:
The word ontology means a particular theory of the nature of being or existence.

Token 6415:
The ontology determines what kinds of things exist, but does not determinetheir speciﬁc properties and interrelationships.

Token 6416:
4.Encode general knowledge about the domain. The knowledge engineer writes down the axioms for all the vocabulary terms.

Token 6417:
This pins down (to the extent possible) themeaning of the terms, enabling the expert to check the content.

Token 6418:
Often, this step revealsmisconceptions or gaps in the vocabulary that must be ﬁxed by returning to step 3 anditerating through the process.

Token 6419:
5.Encode a description of the speciﬁc problem instance. If the ontology is well thought out, this step will be easy.

Token 6420:
It will involve writing simple atomic sentences about in-stances of concepts that are already part of the ontology.

Token 6421:
For a logical agent, probleminstances are supplied by the sensors, whereas a “disembodied” knowledge base is sup-plied with additional sentences in the same way that traditional programs are suppliedwith input data.

Token 6422:
6.Pose queries to the inference procedure and get answers.

Token 6423:
This is where the reward is: we can let the inference procedure operate on the axioms and problem-speciﬁc facts to derive the facts we are interested in knowing.

Token 6424:
Thus, we avoid the need for writing an application-speciﬁc solution algorithm. 7.Debug the knowledge base.

Token 6425:
Alas, the answers to queries will seldom be correct on the ﬁrst try.

Token 6426:
More precisely, the answers will be correct for the knowledge base as written , assuming that the inference procedure is sound, but they will not be the ones that the user is expecting.

Token 6427:
For example, if an axiom is missing, some queries will not be answerable from the knowledge base.

Token 6428:
A considerable debugging process could ensue.Missing axioms or axioms that are too weak can be easily identiﬁed by noticing placeswhere the chain of reasoning stops unexpectedly.

Token 6429:
For example, if the knowledge base includes a diagnostic rule (see Exercise 8.13) for ﬁnding the wumpus, ∀sSmelly (s)⇒Adjacent (Home (Wumpus ),s), instead of the biconditional, then the agent will never be able to prove the absence of wumpuses.

Token 6430:
Incorrect axioms can be identiﬁed because they are false statements aboutthe world.

Token 6431:
For example, the sentence ∀xNumOfLegs (x,4)⇒Mammal (x) is false for reptiles, amphibians, and, more importantly, tables.

Token 6432:
The falsehood of this sentence can be determined independently of the rest of the knowledge base. In contrast,

Token 6433:
Section 8.4. Knowledge Engineering in First-Order Logic 309 a typical error in a program looks like this: offset = position + 1 .

Token 6434:
It is impossible to tell whether this statement is correct without looking at the rest of the program to see whether, for example, offset is used to refer to the current position, or to one beyond the current position, or whether the value of position is changed by another statement and so offset should also be changed again.

Token 6435:
To understand this seven-step process better, we now apply it to an extended example—the domain of electronic circuits.

Token 6436:
8.4.2 The electronic circuits domain We will develop an ontology and knowledge base that allow us to reason about digital circuitsof the kind shown in Figure 8.6.

Token 6437:
We follow the seven-step process for knowledge engineering. Identify the task There are many reasoning tasks associated with digital circuits.

Token 6438:
At the highest level, one analyzes the circuit’s functionality. For example, does the circuit in Figure 8.6 actually addproperly?

Token 6439:
If all the inputs are high, what is the output of gate A2? Questions about thecircuit’s structure are also interesting.

Token 6440:
For example, what are all the gates connected to theﬁrst input terminal? Does the circuit contain feedback loops?

Token 6441:
These will be our tasks in thissection.

Token 6442:
There are more detailed levels of analysis, including those related to timing delays,circuit area, power consumption, production cost, and so on.

Token 6443:
Each of these levels wouldrequire additional knowledge. Assemble the relevant knowledge What do we know about digital circuits?

Token 6444:
For our purposes, they are composed of wires and gates.

Token 6445:
Signals ﬂow along wires to the input terminals of gates, and each gate produces a 1 2 31 2X1 X2 A1A2 O1C1 Figure 8.6 A digital circuit C1, purporting to be a one-bit full adder.

Token 6446:
The ﬁrst two inputs are the two bits to be added, and the third input is a carry bit.

Token 6447:
The ﬁrst output is the sum, and the second output is a carry bit for the next adder.

Token 6448:
The circuit contains two XOR gates, two AND gates, and one OR gate.

Token 6449:
310 Chapter 8. First-Order Logic signal on the output terminal that ﬂows along another wire.

Token 6450:
To determine what these signals will be, we need to know how the gates transform their input signals.

Token 6451:
There are four typesof gates: AND, OR, and XOR gates have two input terminals, and NOT gates have one. Allgates have one output terminal.

Token 6452:
Circuits, like gates, have input and output terminals.

Token 6453:
To reason about functionality and connectivity, we do not need to talk about the wires themselves, the paths they take, or the junctions where they come together.

Token 6454:
All that matters is the connections between terminals—we can say that one output terminal is connected toanother input terminal without having to say what actually connects them.

Token 6455:
Other factors suchas the size, shape, color, or cost of the various components are irrelevant to our analysis.

Token 6456:
If our purpose were something other than verifying designs at the gate level, the ontol- ogy would be different.

Token 6457:
For example, if we were interested in debugging faulty circuits, thenit would probably be a good idea to include the wires in the ontology, because a faulty wirecan corrupt the signal ﬂowing along it.

Token 6458:
For resolving timing faults, we would need to includegate delays.

Token 6459:
If we were interested in designing a product that would be proﬁtable, then thecost of the circuit and its speed relative to other products on the market would be important.

Token 6460:
Decide on a vocabulary We now know that we want to talk about circuits, terminals, signals, and gates.

Token 6461:
The next step is to choose functions, predicates, and constants to represent them.

Token 6462:
First, we need to be able to distinguish gates from each other and from other objects.

Token 6463:
Each gate is represented as an object named by a constant, about which we assert that it is a gate with, say, Gate(X 1).T h e behavior of each gate is determined by its type: one of the constants AND,OR ,XOR ,o r NOT .

Token 6464:
Because a gate has exactly one type, a function is appropriate: Type(X1)=XOR . Circuits, like gates, are identiﬁed by a predicate: Circuit (C1).

Token 6465:
Next we consider terminals, which are identiﬁed by the predicate Terminal (x).Ag a t e or circuit can have one or more input terminals and one or more output terminals.

Token 6466:
We use thefunction In(1,X 1)to denote the ﬁrst input terminal for gate X1. A similar function Out is used for output terminals.

Token 6467:
The function Arity(c,i,j)says that circuit chasiinput and jout- put terminals.

Token 6468:
The connectivity between gates can be represented by a predicate, Connected , which takes two terminals as arguments, as in Connected (Out(1,X1),In(1,X2)).

Token 6469:
Finally, we need to know whether a signal is on or off.

Token 6470:
One possibility is to use a unary predicate, On(t), which is true when the signal at a terminal is on.

Token 6471:
This makes it a little difﬁcult, however, to pose questions such as “What are all the possible values of the signalsat the output terminals of circuit C 1?” We therefore introduce as objects two signal values, 1 and0, and a function Signal (t)that denotes the signal value for the terminal t. Encode general knowledge of the domain One sign that we have a good ontology is that we require only a few general rules, which can be stated clearly and concisely.

Token 6472:
These are all the axioms we will need: 1.

Token 6473:
If two terminals are connected, then they have the same signal: ∀t1,t2Terminal (t1)∧Terminal (t2)∧Connected (t1,t2)⇒ Signal (t1)=Signal (t2).

Token 6474:
Section 8.4. Knowledge Engineering in First-Order Logic 311 2.

Token 6475:
The signal at every terminal is either 1 or 0: ∀tTerminal (t)⇒Signal (t)=1∨Signal (t)=0 . 3.

Token 6476:
Connected is commutative: ∀t1,t2Connected (t1,t2)⇔Connected (t2,t1). 4. There are four types of gates: ∀gGate(g)∧k=Type(g)⇒k=AND∨k=OR∨k=XOR∨k=NOT . 5.

Token 6477:
An AND gate’s output is 0 if and only if any of its inputs is 0: ∀gGate(g)∧Type(g)=AND⇒ Signal (Out(1,g))= 0⇔∃nSignal (In(n,g))= 0 . 6.

Token 6478:
An OR gate’s output is 1 if and only if any of its inputs is 1: ∀gGate(g)∧Type(g)=OR⇒ Signal (Out(1,g))= 1⇔∃nSignal (In(n,g))= 1 . 7.

Token 6479:
An XOR gate’s output is 1 if and only if its inputs are different: ∀gGate(g)∧Type(g)=XOR⇒ Signal (Out(1,g))= 1⇔Signal (In(1,g))/negationslash=Signal (In(2,g)).

Token 6480:
8. A NOT gate’s output is different from its input: ∀gGate(g)∧(Type(g)=NOT)⇒ Signal (Out(1,g))/negationslash=Signal (In(1,g)). 9.

Token 6481:
The gates (except for NOT) have two inputs and one output. ∀gGate(g)∧Type(g)=NOT⇒Arity(g,1,1).

Token 6482:
∀gGate(g)∧k=Type(g)∧(k=AND∨k=OR∨k=XOR)⇒ Arity(g,2,1) 10.

Token 6483:
A circuit has terminals, up to its input and output arity, and nothing beyond its arity: ∀c,i,jCircuit (c)∧Arity(c,i,j)⇒ ∀n(n≤i⇒Terminal (In(c,n)))∧(n>i⇒In(c,n)=Nothing )∧ ∀n(n≤j⇒Terminal (Out(c,n)))∧(n>j⇒Out(c,n)=Nothing ) 11.

Token 6484:
Gates, terminals, signals, gate types, and Nothing are all distinct.

Token 6485:
∀g,tGate(g)∧Terminal (t)⇒ g/negationslash=t/negationslash=1/negationslash=0/negationslash=OR/negationslash=AND/negationslash=XOR/negationslash=NOT/negationslash=Nothing .

Token 6486:
12. Gates are circuits.

Token 6487:
∀gGate(g)⇒Circuit (g) Encode the speciﬁc problem instance The circuit shown in Figure 8.6 is encoded as circuit C1with the following description.

Token 6488:
First, we categorize the circuit and its component gates: Circuit (C1)∧Arity(C1,3,2) Gate(X1)∧Type(X1)=XOR Gate(X2)∧Type(X2)=XOR Gate(A1)∧Type(A1)=AND Gate(A2)∧Type(A2)=AND Gate(O1)∧Type(O1)=OR.

Token 6489:
312 Chapter 8.

Token 6490:
First-Order Logic Then, we show the connections between them: Connected (Out(1,X1),In(1,X2))Connected (In(1,C1),In(1,X1)) Connected (Out(1,X1),In(2,A2))Connected (In(1,C1),In(1,A1)) Connected (Out(1,A2),In(1,O1))Connected (In(2,C1),In(2,X1)) Connected (Out(1,A1),In(2,O1))Connected (In(2,C1),In(2,A1)) Connected (Out(1,X2),Out(1,C1))Connected (In(3,C1),In(2,X2)) Connected (Out(1,O1),Out(2,C1))Connected (In(3,C1),In(1,A2)).

Token 6491:
Pose queries to the inference procedure What combinations of inputs would cause the ﬁrst output of C1(the sum bit) to be 0 and the second output of C1(the carry bit) to be 1?

Token 6492:
∃i1,i2,i3Signal (In(1,C1)) =i1∧Signal (In(2,C1)) =i2∧Signal (In(3,C1)) =i3 ∧Signal (Out(1,C1)) =0∧Signal (Out(2,C1)) =1 .

Token 6493:
The answers are substitutions for the variables i1,i2,a n di3such that the resulting sentence is entailed by the knowledge base.

Token 6494:
A SKVARS will give us three such substitutions: {i1/1,i2/1,i3/0}{i1/1,i2/0,i3/1}{i1/0,i2/1,i3/1}.

Token 6495:
What are the possible sets of values of all the terminals for the adder circuit?

Token 6496:
∃i1,i2,i3,o1,o2Signal (In(1,C1)) =i1∧Signal (In(2,C1)) =i2 ∧Signal (In(3,C1)) =i3∧Signal (Out(1,C1)) =o1∧Signal (Out(2,C1)) =o2.

Token 6497:
This ﬁnal query will return a complete input–output table for the device, which can be used to check that it does in fact add its inputs correctly.

Token 6498:
This is a simple example of circuit veriﬁcation .

Token 6499:
We can also use the deﬁnition of the circuit to build larger digital systems, forCIRCUIT VERIFICATION which the same kind of veriﬁcation procedure can be carried out.

Token 6500:
(See Exercise 8.26.)

Token 6501:
Many domains are amenable to the same kind of structured knowledge-base development, in whichmore complex concepts are deﬁned on top of simpler concepts.

Token 6502:
Debug the knowledge base We can perturb the knowledge base in various ways to see what kinds of erroneous behaviors emerge.

Token 6503:
For example, suppose we fail to read Section 8.2.8 and hence forget to assert that1/negationslash=0.

Token 6504:
Suddenly, the system will be unable to prove any outputs for the circuit, except for the input cases 000 and 110.

Token 6505:
We can pinpoint the problem by asking for the outputs of each gate.

Token 6506:
For example, we can ask ∃i 1,i2,oSignal (In(1,C1))=i1∧Signal (In(2,C1))=i2∧Signal (Out(1,X1)), which reveals that no outputs are known at X1for the input cases 10 and 01.

Token 6507:
Then, we look at the axiom for XOR gates, as applied to X1: Signal (Out(1,X1))= 1⇔Signal (In(1,X1))/negationslash=Signal (In(2,X1)).

Token 6508:
If the inputs are known to be, say, 1 and 0, then this reduces to Signal (Out(1,X1))= 1⇔1/negationslash=0.

Token 6509:
Now the problem is apparent: the system is unable to infer that Signal (Out(1,X1))= 1 ,s o we need to tell it that 1/negationslash=0.

Token 6510:
Section 8.5.

Token 6511:
Summary 313 8.5 S UMMARY This chapter has introduced ﬁrst-order logic , a representation language that is far more pow- erful than propositional logic.

Token 6512:
The important points are as follows: •Knowledge representation languages should be declarative, compositional, expressive, context independent, and unambiguous.

Token 6513:
•Logics differ in their ontological commitments andepistemological commitments .

Token 6514:
While propositional logic commits only to the existence of facts, ﬁrst-order logic com-mits to the existence of objects and relations and thereby gains expressive power.

Token 6515:
•The syntax of ﬁrst-order logic builds on that of propositional logic.

Token 6516:
It adds terms to represent objects, and has universal and existential quantiﬁers to construct assertionsabout all or some of the possible values of the quantiﬁed variables.

Token 6517:
•Apossible world ,o rmodel , for ﬁrst-order logic includes a set of objects and an inter- pretation that maps constant symbols to objects, predicate symbols to relations among objects, and function symbols to functions on objects.

Token 6518:
•An atomic sentence is true just when the relation named by the predicate holds between the objects named by the terms.

Token 6519:
Extended interpretations , which map quantiﬁer vari- ables to objects in the model, deﬁne the truth of quantiﬁed sentences.

Token 6520:
•Developing a knowledge base in ﬁrst-order logic requires a careful process of analyzing the domain, choosing a vocabulary, and encoding the axioms required to support thedesired inferences.

Token 6521:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Although Aristotle’s logic deals with generalizations over objects, it fell far short of the ex- pressive power of ﬁrst-order logic.

Token 6522:
A major barrier to its further development was its concen- tration on one-place predicates to the exclusion of many-place relational predicates.

Token 6523:
The ﬁrstsystematic treatment of relations was given by Augustus De Morgan (1864), who cited thefollowing example to show the sorts of inferences that Aristotle’s logic could not handle: “Allhorses are animals; therefore, the head of a horse is the head of an animal.” This inferenceis inaccessible to Aristotle because any valid rule that can support this inference must ﬁrstanalyze the sentence using the two-place predicate “ xis the head of y.” The logic of relations was studied in depth by Charles Sanders Peirce (1870, 2004).

Token 6524:
True ﬁrst-order logic dates from the introduction of quantiﬁers in Gottlob Frege’s (1879) Begriffschrift (“Concept Writing” or “Conceptual Notation”).

Token 6525:
Peirce (1883) also developed ﬁrst-order logic independently of Frege, although slightly later.

Token 6526:
Frege’s ability to nest quan- tiﬁers was a big step forward, but he used an awkward notation.

Token 6527:
The present notation for ﬁrst-order logic is due substantially to Giuseppe Peano (1889), but the semantics is virtuallyidentical to Frege’s.

Token 6528:
Oddly enough, Peano’s axioms were due in large measure to Grassmann(1861) and Dedekind (1888).

Token 6529:
314 Chapter 8.

Token 6530:
First-Order Logic Leopold L¨ owenheim (1915) gave a systematic treatment of model theory for ﬁrst-order logic, including the ﬁrst proper treatment of the equality symbol.

Token 6531:
L¨ owenheim’s results were further extended by Thoralf Skolem (1920).

Token 6532:
Alfred Tarski (1935, 1956) gave an explicitdeﬁnition of truth and model-theoretic satisfaction in ﬁrst-order logic, using set theory.

Token 6533:
McCarthy (1958) was primarily responsible for the introduction of ﬁrst-order logic as a tool for building AI systems.

Token 6534:
The prospects for logic-based AI were advanced signiﬁcantly by Robinson’s (1965) development of resolution, a complete procedure for ﬁrst-order inferencedescribed in Chapter 9.

Token 6535:
The logicist approach took root at Stanford University.

Token 6536:
Cordell Green(1969a, 1969b) developed a ﬁrst-order reasoning system, QA3, leading to the ﬁrst attempts tobuild a logical robot at SRI (Fikes and Nilsson, 1971).

Token 6537:
First-order logic was applied by ZoharManna and Richard Waldinger (1971) for reasoning about programs and later by MichaelGenesereth (1984) for reasoning about circuits.

Token 6538:
In Europe, logic programming (a restrictedform of ﬁrst-order reasoning) was developed for linguistic analysis (Colmerauer et al.

Token 6539:
, 1973) and for general declarative systems (Kowalski, 1974).

Token 6540:
Computational logic was also wellentrenched at Edinburgh through the LCF (Logic for Computable Functions) project (Gordonet al. , 1979).

Token 6541:
These developments are chronicled further in Chapters 9 and 12.

Token 6542:
Practical applications built with ﬁrst-order logic include a system for evaluating the manufacturing requirements for electronic products (Mannion, 2002), a system for reasoning about policies for ﬁle access and digital rights management (Halpern and Weissman, 2008),and a system for the automated composition of Web services (McIlraith and Zeng, 2001).

Token 6543:
Reactions to the Whorf hypothesis (Whorf, 1956) and the problem of language and thought in general, appear in several recent books (Gumperz and Levinson, 1996; Bowerman and Levinson, 2001; Pinker, 2003; Gentner and Goldin-Meadow, 2003).

Token 6544:
The “theory” theory (Gopnik and Glymour, 2002; Tenenbaum et al.

Token 6545:
, 2007) views children’s learning about the world as analogous to the construction of scientiﬁc theories.

Token 6546:
Just as the predictions of amachine learning algorithm depend strongly on the vocabulary supplied to it, so will thechild’s formulation of theories depend on the linguistic environment in which learning occurs.

Token 6547:
There are a number of good introductory texts on ﬁrst-order logic, including some by leading ﬁgures in the history of logic: Alfred Tarski (1941), Alonzo Church (1956), and W.V.

Token 6548:
Quine (1982) (which is one of the most readable). Enderton (1972) gives a more math-ematically oriented perspective.

Token 6549:
A highly formal treatment of ﬁrst-order logic, along withmany more advanced topics in logic, is provided by Bell and Machover (1977).

Token 6550:
Manna andWaldinger (1985) give a readable introduction to logic from a computer science perspec-tive, as do Huth and Ryan (2004), who concentrate on program veriﬁcation.

Token 6551:
Barwise andEtchemendy (2002) take an approach similar to the one used here. Smullyan (1995) presentsresults concisely, using the tableau format.

Token 6552:
Gallier (1986) provides an extremely rigorousmathematical exposition of ﬁrst-order logic, along with a great deal of material on its use inautomated reasoning.

Token 6553:
Logical Foundations of Artiﬁcial Intelligence (Genesereth and Nilsson, 1987) is both a solid introduction to logic and the ﬁrst systematic treatment of logical agents with percepts and actions, and there are two good handbooks: van Bentham and ter Meulen (1997) and Robinson and Voronkov (2001).

Token 6554:
The journal of record for the ﬁeld of pure math-ematical logic is the Journal of Symbolic Logic , whereas the Journal of Applied Logic deals with concerns closer to those of artiﬁcial intelligence.

Token 6555:
Exercises 315 EXERCISES 8.1 A logical knowledge base represents the world using a set of sentences with no explicit structure.

Token 6556:
An analogical representation, on the other hand, has physical structure that corre- sponds directly to the structure of the thing represented.

Token 6557:
Consider a road map of your country as an analogical representation of facts about the country—it represents facts with a map lan- guage.

Token 6558:
The two-dimensional structure of the map corresponds to the two-dimensional surfaceof the area. a. Give ﬁve examples of symbols in the map language.

Token 6559:
b.A n explicit sentence is a sentence that the creator of the representation actually writes down.

Token 6560:
An implicit sentence is a sentence that results from explicit sentences because of properties of the analogical representation.

Token 6561:
Give three examples each of implicit and explicit sentences in the map language.

Token 6562:
c. Give three examples of facts about the physical structure of your country that cannot be represented in the map language.

Token 6563:
d. Give two examples of facts that are much easier to express in the map language than in ﬁrst-order logic.

Token 6564:
e. Give two other examples of useful analogical representations. What are the advantages and disadvantages of each of these languages?

Token 6565:
8.2 Consider a knowledge base containing just two sentences: P(a)andP(b). Does this knowledge base entail ∀xP(x)?

Token 6566:
Explain your answer in terms of models. 8.3 Is the sentence ∃x,y x =yvalid? Explain.

Token 6567:
8.4 Write down a logical sentence such that every world in which it is true contains exactly one object.

Token 6568:
8.5 Consider a symbol vocabulary that contains cconstant symbols, p kpredicate symbols of each arity k,a n dfkfunction symbols of each arity k,w h e r e 1≤k≤A.

Token 6569:
Let the domain size be ﬁxed at D. For any given model, each predicate or function symbol is mapped onto a relation or function, respectively, of the same arity.

Token 6570:
You may assume that the functions in the model allow some input tuples to have no value for the function (i.e., the value is the invisible object).

Token 6571:
Derive a formula for the number of possible models for a domain with Delements. Don’t worry about eliminating redundant combinations.

Token 6572:
8.6 Which of the following are valid (necessarily true) sentences? a.(∃xx=x)⇒(∀y∃zy=z). b.∀xP(x)∨¬P(x). c.∀xSmart (x)∨(x=x).

Token 6573:
8.7 Consider a version of the semantics for ﬁrst-order logic in which models with empty domains are allowed.

Token 6574:
Give at least two examples of sentences that are valid according to the

Token 6575:
316 Chapter 8. First-Order Logic standard semantics but not according to the new semantics.

Token 6576:
Discuss which outcome makes more intuitive sense for your examples.

Token 6577:
8.8 Does the fact ¬Spouse (George ,Laura )follow from the facts Jim/negationslash=George and Spouse (Jim,Laura )?

Token 6578:
If so, give a proof; if not, supply additional axioms as needed.

Token 6579:
What happens if we use Spouse as a unary function symbol instead of a binary predicate?

Token 6580:
8.9 This exercise uses the function MapColor and predicates In(x,y),Borders (x,y),a n d Country (x), whose arguments are geographical regions, along with constant symbols for various regions.

Token 6581:
In each of the following we give an English sentence and a number of can-didate logical expressions.

Token 6582:
For each of the logical expressions, state whether it (1) correctlyexpresses the English sentence; (2) is syntactically invalid and therefore meaningless; or (3)is syntactically valid but does not express the meaning of the English sentence.

Token 6583:
a. Paris and Marseilles are both in France. (i)In(Paris∧Marseilles ,France ). (ii)In(Paris,France )∧In(Marseilles ,France ).

Token 6584:
(iii)In(Paris,France )∨In(Marseilles ,France ). b. There is a country that borders both Iraq and Pakistan.

Token 6585:
(i)∃cCountry (c)∧Border (c, Iraq)∧Border (c,Pakistan ). (ii)∃cCountry (c)⇒[Border (c,Iraq)∧Border (c,Pakistan )].

Token 6586:
(iii)[∃cCountry (c)]⇒[Border (c,Iraq)∧Border (c,Pakistan )]. (iv)∃cBorder (Country (c),Iraq∧Pakistan ).

Token 6587:
c. All countries that border Ecuador are in South America. (i)∀cC o u n t r y (c)∧Border (c,Ecuador )⇒In(c,SouthAmerica ).

Token 6588:
(ii)∀cCountry (c)⇒[Border (c,Ecuador )⇒In(c,SouthAmerica )]. (iii)∀c[Country (c)⇒Border (c,Ecuador )]⇒In(c,SouthAmerica ).

Token 6589:
(iv)∀cC o u n t r y (c)∧Border (c,Ecuador )∧In(c,SouthAmerica ). d. No region in South America borders any region in Europe.

Token 6590:
(i)¬[∃c,dIn(c,SouthAmerica )∧In(d,Europe )∧Borders (c,d)]. (ii)∀c,d[In(c,SouthAmerica )∧In(d,Europe )]⇒¬Borders (c,d)].

Token 6591:
(iii)¬∀cIn(c,SouthAmerica )⇒∃dIn(d,Europe )∧¬Borders (c,d). (iv)∀cIn(c,SouthAmerica )⇒∀dIn(d,Europe )⇒¬Borders (c,d).

Token 6592:
e. No two adjacent countries have the same map color. (i)∀x,y¬Country (x)∨¬Country (y)∨¬Borders (x,y)∨ ¬(MapColor (x)=MapColor (y)).

Token 6593:
(ii)∀x,y(Country (x)∧Country (y)∧Borders (x,y)∧¬(x=y))⇒ ¬(MapColor (x)=MapColor (y)).

Token 6594:
(iii)∀x,yCountry (x)∧Country (y)∧Borders (x,y)∧ ¬(MapColor (x)=MapColor (y)).

Token 6595:
(iv)∀x,y(Country (x)∧Country (y)∧Borders (x,y))⇒MapColor (x/negationslash=y).

Token 6596:
Exercises 317 8.10 Consider a vocabulary with the following symbols: Occupation (p,o): Predicate. Person phas occupation o.

Token 6597:
Customer (p1,p2): Predicate. Person p1is a customer of person p2. Boss(p1,p2): Predicate. Person p1is a boss of person p2.

Token 6598:
Doctor ,Surgeon ,Lawyer ,Actor : Constants denoting occupations. Emily ,Joe: Constants denoting people.

Token 6599:
Use these symbols to write the following assertions in ﬁrst-order logic: a. Emily is either a surgeon or a lawyer.

Token 6600:
b. Joe is an actor, but he also holds another job. c. All surgeons are doctors. d. Joe does not have a lawyer (i.e., is not a customer of any lawyer).

Token 6601:
e. Emily has a boss who is a lawyer. f. There exists a lawyer all of whose customers are doctors. g. Every surgeon has a lawyer.

Token 6602:
8.11 Complete the following exercises about logical senntences: a. Translate into good, natural English (no xso rys!

Token 6603:
): ∀x,y,l SpeaksLanguage (x,l)∧SpeaksLanguage (y,l) ⇒Understands (x,y)∧Understands (y,x). b.

Token 6604:
Explain why this sentence is entailed by the sentence ∀x,y,l SpeaksLanguage (x,l)∧SpeaksLanguage (y,l) ⇒Understands (x,y).

Token 6605:
c. Translate into ﬁrst-order logic the following sentences: (i) Understanding leads to friendship. (ii) Friendship is transitive.

Token 6606:
Remember to deﬁne all predicates, functions, and constants you use.

Token 6607:
8.12 Rewrite the ﬁrst two Peano axioms in Section 8.3.3 as a single axiom that deﬁnes NatNum (x)so as to exclude the possibility of natural numbers except for those generated by the successor function.

Token 6608:
8.13 Equation (8.4) on page 306 deﬁnes the conditions under which a square is breezy.

Token 6609:
Here we consider two other ways to describe this aspect of the wumpus world. a.

Token 6610:
We can write diagnostic rules leading from observed effects to hidden causes.

Token 6611:
For ﬁnd- DIAGNOSTIC RULE ing pits, the obvious diagnostic rules say that if a square is breezy, some adjacent square must contain a pit; and if a square is not breezy, then no adjacent square contains a pit.Write these two rules in ﬁrst-order logic and show that their conjunction is logicallyequivalent to Equation (8.4).

Token 6612:
b. We can write causal rules leading from cause to effect. One obvious causal rule is that CAUSAL RULE a pit causes all adjacent squares to be breezy.

Token 6613:
Write this rule in ﬁrst-order logic, explain why it is incomplete compared to Equation (8.4), and supply the missing axiom.

Token 6614:
318 Chapter 8.

Token 6615:
First-Order Logic BeatriceAndrew Eugenie William HarryCharles DianaMum George Philip Elizabeth Margaret Kydd Spencer PeterMark ZaraAnne Sarah Edward Sophie Louise James Figure 8.7 A typical family tree.

Token 6616:
The symbol “ ⊿/triangleleft” connects spouses and arrows point to children.

Token 6617:
8.14 Write axioms describing the predicates Grandchild ,Greatgrandparent ,Ancestor , Brother ,Sister ,Daughter ,Son,FirstCousin ,BrotherInLaw ,SisterInLaw ,Aunt ,a n d Uncle .

Token 6618:
Find out the proper deﬁnition of mth cousin ntimes removed, and write the def- inition in ﬁrst-order logic.

Token 6619:
Now write down the basic facts depicted in the family tree in Figure 8.7.

Token 6620:
Using a suitable logical reasoning system, T ELL it all the sentences you have written down, and A SKit who are Elizabeth’s grandchildren, Diana’s brothers-in-law, Zara’s great-grandparents, and Eugenie’s ancestors.

Token 6621:
8.15 Explain what is wrong with the following proposed deﬁnition of the set membership predicate∈: ∀x,s x∈{x|s} ∀x,s x∈s⇒∀yx∈{y|s}.

Token 6622:
8.16 Using the set axioms as examples, write axioms for the list domain, including all the constants, functions, and predicates mentioned in the chapter.

Token 6623:
8.17 Explain what is wrong with the following proposed deﬁnition of adjacent squares in the wumpus world: ∀x,yAdjacent ([x,y],[x+1,y])∧Adjacent ([x,y],[x,y+1 ] ).

Token 6624:
8.18 Write out the axioms required for reasoning about the wumpus’s location, using a constant symbol Wumpus and a binary predicate At(Wumpus ,Location ).

Token 6625:
Remember that there is only one wumpus.

Token 6626:
8.19 Assuming predicates Parent (p,q)andFemale (p)and constants Joan andKevin , with the obvious meanings, express each of the following sentences in ﬁrst-order logic.

Token 6627:
(Youmay use the abbreviation ∃ 1to mean “there exists exactly one.”) a. Joan has a daughter (possibly more than one, and possibly sons as well).

Token 6628:
b. Joan has exactly one daughter (but may have sons as well). c. Joan has exactly one child, a daughter.

Token 6629:
d. Joan and Kevin have exactly one child together. e. Joan has at least one child with Kevin, and no children with anyone else.

Token 6630:


Token 6631:
Exercises 319 8.20 Arithmetic assertions can be written in ﬁrst-order logic with the predicate symbol <, the function symbols +and×, and the constant symbols 0 and 1.

Token 6632:
Additional predicates can also be deﬁned with biconditionals. a. Represent the property “ xis an even number.” b.

Token 6633:
Represent the property “ xis prime.” c. Goldbach’s conjecture is the conjecture (unproven as yet) that every even number is equal to the sum of two primes.

Token 6634:
Represent this conjecture as a logical sentence. 8.21 In Chapter 6, we used equality to indicate the relation between a variable and its value.

Token 6635:
For instance, we wrote WA=redto mean that Western Australia is colored red.

Token 6636:
Repre- senting this in ﬁrst-order logic, we must write more verbosely ColorOf (WA)=red.W h a t incorrect inference could be drawn if we wrote sentences such as WA=reddirectly as logical assertions?

Token 6637:
8.22 Write in ﬁrst-order logic the assertion that every key and at least one of every pair of socks will eventually be lost forever, using only the following vocabulary: Key(x),xis a key; Sock(x),xis a sock; Pair(x,y),xandyare a pair; Now , the current time; Before (t1,t2), timet1comes before time t2;Lost(x,t), object xis lost at time t. 8.23 For each of the following sentences in English, decide if the accompanying ﬁrst-order logic sentence is a good translation.

Token 6638:
If not, explain why not and correct it. (Some sentencesmay have more than one error!) a. No two people have the same social security number.

Token 6639:
¬∃x,y,n Person (x)∧Person (y)⇒[HasSS #(x,n)∧HasSS #(y,n)]. b. John’s social security number is the same as Mary’s. ∃nHasSS #(John,n)∧HasSS #(Mary,n).

Token 6640:
c. Everyone’s social security number has nine digits. ∀x,nPerson (x)⇒[HasSS #(x,n)∧Digits (n,9)].

Token 6641:
d. Rewrite each of the above (uncorrected) sentences using a function symbol SS#instead of the predicate HasSS #.

Token 6642:
8.24 Represent the following sentences in ﬁrst-order logic, using a consistent vocabulary (which you must deﬁne): a.

Token 6643:
Some students took French in spring 2001. b. Every student who takes French passes it.

Token 6644:
c. Only one student took Greek in spring 2001. d. The best score in Greek is always higher than the best score in French.

Token 6645:
e. Every person who buys a policy is smart. f. No person buys an expensive policy.

Token 6646:
g. There is an agent who sells policies only to people who are not insured.

Token 6647:
320 Chapter 8. First-Order Logic Z0 Z1 Z2 Z3 Z4X0 Y0 X1 Y1 X2 Y2 X3 Y3Ad0 Ad1 Ad2 Ad3X0X1X2X3 Z0Z1Z2Z3Z4Y0Y1Y2Y3 + Figure 8.8 A four-bit adder.

Token 6648:
Each Adiis a one-bit adder, as in Figure 8.6 on page 309. h. There is a barber who shaves all men in town who do not shave themselves. i.

Token 6649:
A person born in the UK, each of whose parents is a UK citizen or a UK resident, is a UK citizen by birth. j.

Token 6650:
A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK citizen by descent.

Token 6651:
k. Politicians can fool some of the people all of the time, and they can fool all of the people some of the time, but they can’t fool all of the people all of the time.

Token 6652:
l. All Greeks speak the same language.

Token 6653:
(Use Speaks (x,l)to mean that person xspeaks language l.) 8.25 Write a general set of facts and axioms to represent the assertion “Wellington heard about Napoleon’s death” and to correctly answer the question “Did Napoleon hear aboutWellington’s death?” 8.26 Extend the vocabulary from Section 8.4 to deﬁne addition for n-bit binary numbers.

Token 6654:
Then encode the description of the four-bit adder in Figure 8.8, and pose the queries needed to verify that it is in fact correct.

Token 6655:
8.27 Obtain a passport application for your country, identify the rules determining eligi- bility for a passport, and translate them into ﬁrst-order logic, following the steps outlined in Section 8.4.

Token 6656:
8.28 Consider a ﬁrst-order logical knowledge base that describes worlds containing people, songs, albums (e.g., “Meet the Beatles”) and disks (i.e., particular physical instances of CDs).The vocabulary contains the following symbols: CopyOf (d,a): Predicate.

Token 6657:
Disk dis a copy of album a. Owns(p,d): Predicate.

Token 6658:
Person powns disk d. Sings(p,s,a):A l b u m aincludes a recording of song ssung by person p. Wrote (p,s): Person pwrote song s. McCartney ,Gershwin ,BHoliday ,Joe,EleanorRigby ,TheManILove ,Revolver : Constants with the obvious meanings.

Token 6659:


Token 6660:
Exercises 321 Express the following statements in ﬁrst-order logic: a. Gershwin wrote “The Man I Love.” b. Gershwin did not write “Eleanor Rigby.” c. Either Gershwin or McCartney wrote “The Man I Love.” d. Joe has written at least one song.

Token 6661:
e. Joe owns a copy of Revolver . f. Every song that McCartney sings on Revolver was written by McCartney.

Token 6662:
g. Gershwin did not write any of the songs on Revolver . h. Every song that Gershwin wrote has been recorded on some album.

Token 6663:
(Possibly different songs are recorded on different albums.) i. There is a single album that contains every song that Joe has written.

Token 6664:
j. Joe owns a copy of an album that has Billie Holiday singing “The Man I Love.” k. Joe owns a copy of every album that has a song sung by McCartney.

Token 6665:
(Of course, each different album is instantiated in a different physical CD.)

Token 6666:
l. Joe owns a copy of every album on which all the songs are sung by Billie Holiday.

Token 6667:
9INFERENCE IN FIRST-ORDER LOGIC In which we deﬁne effective procedures for answering questions posed in ﬁrst- order logic.

Token 6668:
Chapter 7 showed how sound and complete inference can be achieved for propositional logic.

Token 6669:
In this chapter, we extend those results to obtain algorithms that can answer any answer-able question stated in ﬁrst-order logic.

Token 6670:
Section 9.1 introduces inference rules for quantiﬁersand shows how to reduce ﬁrst-order inference to propositional inference, albeit at potentially great expense.

Token 6671:
Section 9.2 describes the idea of uniﬁcation , showing how it can be used to construct inference rules that work directly with ﬁrst-order sentences.

Token 6672:
We then discussthree major families of ﬁrst-order inference algorithms.

Token 6673:
Forward chaining and its applica- tions to deductive databases andproduction systems are covered in Section 9.3; backward chaining andlogic programming systems are developed in Section 9.4.

Token 6674:
Forward and back- ward chaining can be very efﬁcient, but are applicable only to knowledge bases that canbe expressed as sets of Horn clauses.

Token 6675:
General ﬁrst-order sentences require resolution-basedtheorem proving , which is described in Section 9.5.

Token 6676:
9.1 P ROPOSITIONAL VS .FIRST-ORDER INFERENCE This section and the next introduce the ideas underlying modern logical inference systems.

Token 6677:
We begin with some simple inference rules that can be applied to sentences with quantiﬁersto obtain sentences without quantiﬁers.

Token 6678:
These rules lead naturally to the idea that ﬁrst-order inference can be done by converting the knowledge base to propositional logic and using propositional inference, which we already know how to do.

Token 6679:
The next section points out an obvious shortcut, leading to inference methods that manipulate ﬁrst-order sentences directly.

Token 6680:
9.1.1 Inference rules for quantiﬁers Let us begin with universal quantiﬁers.

Token 6681:
Suppose our knowledge base contains the standard folkloric axiom stating that all greedy kings are evil: ∀xKing(x)∧Greedy (x)⇒Evil(x). 322

Token 6682:
Section 9.1.

Token 6683:
Propositional vs. First-Order Inference 323 Then it seems quite permissible to infer any of the following sentences: King(John)∧Greedy (John)⇒Evil(John) King(Richard )∧Greedy (Richard )⇒Evil(Richard ) King(Father (John))∧Greedy (Father (John))⇒Evil(Father (John)).

Token 6684:
... T h er u l eo f Universal Instantiation (UIfor short) says that we can infer any sentence ob- UNIVERSAL INSTANTIATION tained by substituting a ground term (a term without variables) for the variable.1To write GROUND TERM out the inference rule formally, we use the notion of substitutions introduced in Section 8.3.

Token 6685:
Let S UBST(θ,α)denote the result of applying the substitution θto the sentence α.

Token 6686:
Then the rule is written ∀vα SUBST({v/g},α) for any variable vand ground term g. For example, the three sentences given earlier are obtained with the substitutions {x/John},{x/Richard},a n d{x/Father (John)}.

Token 6687:
I nt h er u l ef o r Existential Instantiation , the variable is replaced by a single new con-EXISTENTIAL INSTANTIATION stant symbol .

Token 6688:
The formal statement is as follows: for any sentence α,v a r i a b l e v, and constant symbol kthat does not appear elsewhere in the knowledge base, ∃vα SUBST({v/k},α).

Token 6689:
For example, from the sentence ∃xCrown (x)∧OnHead (x,John) we can infer the sentence Crown (C1)∧OnHead (C1,John) as long as C1does not appear elsewhere in the knowledge base.

Token 6690:
Basically, the existential sentence says there is some object satisfying a condition, and applying the existential instan-tiation rule just gives a name to that object.

Token 6691:
Of course, that name must not already belongto another object.

Token 6692:
Mathematics provides a nice example: suppose we discover that there is anumber that is a little bigger than 2.71828 and that satisﬁes the equation d(x y)/dy=xyforx.

Token 6693:
We can give this number a name, such as e, but it would be a mistake to give it the name of an existing object, such as π.

Token 6694:
In logic, the new name is called a Skolem constant .

Token 6695:
Existen- SKOLEM CONSTANT tial Instantiation is a special case of a more general process called skolemization ,w h i c hw e cover in Section 9.5.

Token 6696:
Whereas Universal Instantiation can be applied many times to produce many different consequences, Existential Instantiation can be applied once, and then the existentially quan-tiﬁed sentence can be discarded.

Token 6697:
For example, we no longer need ∃xKill(x,Victim )once we have added the sentence Kill(Murderer ,Victim ).

Token 6698:
Strictly speaking, the new knowledge base is not logically equivalent to the old, but it can be shown to be inferentially equivalent INFERENTIAL EQUIVALENCE in the sense that it is satisﬁable exactly when the original knowledge base is satisﬁable.

Token 6699:
1Do not confuse these substitutions with the extended inter pretations used to deﬁne the semantics of quantiﬁers.

Token 6700:
The substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas aninterpretation maps a variable to an object in the domain.

Token 6701:
324 Chapter 9.

Token 6702:
Inference in First-Order Logic 9.1.2 Reduction to propositional inference Once we have rules for inferring nonquantiﬁed sentences from quantiﬁed sentences, it be- comes possible to reduce ﬁrst-order inference to propositional inference.

Token 6703:
In this section wegive the main ideas; the details are given in Section 9.5.

Token 6704:
The ﬁrst idea is that, just as an existentially quantiﬁed sentence can be replaced by one instantiation, a universally quantiﬁed sentence can be replaced by the set of all possible instantiations.

Token 6705:
For example, suppose our knowledge base contains just the sentences ∀xKing(x)∧Greedy (x)⇒Evil(x) King(John) Greedy (John) Brother (Richard ,John).

Token 6706:
(9.1) Then we apply UI to the ﬁrst sentence using all possible ground-term substitutions from the vocabulary of the knowledge base—in this case, {x/John}and{x/Richard}.

Token 6707:
We obtain King(John)∧Greedy (John)⇒Evil(John) King(Richard )∧Greedy (Richard )⇒Evil(Richard ), and we discard the universally quantiﬁed sentence.

Token 6708:
Now, the knowledge base is essentially propositional if we view the ground atomic sentences— King(John),Greedy (John),a n d so on—as proposition symbols.

Token 6709:
Therefore, we can apply any of the complete propositionalalgorithms in Chapter 7 to obtain conclusions such as Evil(John).

Token 6710:
This technique of propositionalization can be made completely general, as we show in Section 9.5; that is, every ﬁrst-order knowledge base and query can be propositionalized in such a way that entailment is preserved.

Token 6711:
Thus, we have a complete decision procedurefor entailment ...or perhaps not.

Token 6712:
There is a problem: when the knowledge base includes a function symbol, the set of possible ground-term substitutions is inﬁnite!

Token 6713:
For example, ifthe knowledge base mentions the Father symbol, then inﬁnitely many nested terms such as Father (Father (Father (John)))can be constructed.

Token 6714:
Our propositional algorithms will have difﬁculty with an inﬁnitely large set of sentences.

Token 6715:
Fortunately, there is a famous theorem due to Jacques Herbrand (1930) to the effect that if a sentence is entailed by the original, ﬁrst-order knowledge base, then there is a proofinvolving just a ﬁnite subset of the propositionalized knowledge base.

Token 6716:
Since any such subset has a maximum depth of nesting among its ground terms, we can ﬁnd the subset by ﬁrstgenerating all the instantiations with constant symbols ( Richard andJohn ), then all terms of depth 1 ( Father (Richard )andFather (John)), then all terms of depth 2, and so on, until we are able to construct a propositional proof of the entailed sentence.

Token 6717:
We have sketched an approach to ﬁrst-order inference via propositionalization that is complete —that is, any entailed sentence can be proved.

Token 6718:
This is a major achievement, given that the space of possible models is inﬁnite.

Token 6719:
On the other hand, we do not know until the proof is done that the sentence isentailed! What happens when the sentence is notentailed? Can we tell?

Token 6720:
Well, for ﬁrst-order logic, it turns out that we cannot.

Token 6721:
Our proof procedure cango on and on, generating more and more deeply nested terms, but we will not know whetherit is stuck in a hopeless loop or whether the proof is just about to pop out.

Token 6722:
This is very much

Token 6723:
Section 9.2. Uniﬁcation and Lifting 325 like the halting problem for Turing machines.

Token 6724:
Alan Turing (1936) and Alonzo Church (1936) both proved, in rather different ways, the inevitability of this state of affairs.

Token 6725:
The question of entailment for ﬁrst-order logic is semidecidable —that is, algorithms exist that say yes to every entailed sentence, but no algorithm exists that also says no to every nonentailed sentence.

Token 6726:
9.2 U NIFICATION AND LIFTING The preceding section described the understanding of ﬁrst-order inference that existed upto the early 1960s.

Token 6727:
The sharp-eyed reader (and certainly the computational logicians of theearly 1960s) will have noticed that the propositionalization approach is rather inefﬁcient.

Token 6728:
Forexample, given the query Evil(x)and the knowledge base in Equation (9.1), it seems per- verse to generate sentences such as King(Richard )∧Greedy (Richard )⇒Evil(Richard ).

Token 6729:
Indeed, the inference of Evil(John)from the sentences ∀xKing(x)∧Greedy (x)⇒Evil(x) King(John) Greedy (John) seems completely obvious to a human being.

Token 6730:
We now show how to make it completely obvious to a computer.

Token 6731:
9.2.1 A ﬁrst-order inference rule The inference that John is evil—that is, that {x/John}solves the query Evil(x)—works like this: to use the rule that greedy kings are evil, ﬁnd some xsuch that xis a king and xis greedy, and then infer that this xis evil.

Token 6732:
More generally, if there is some substitution θthat makes each of the conjuncts of the premise of the implication identical to sentences already in the knowledge base, then we can assert the conclusion of the implication, after applying θ.

Token 6733:
In this case, the substitution θ={x/John}achieves that aim. We can actually make the inference step do even more work.

Token 6734:
Suppose that instead of knowing Greedy (John), we know that everyone is greedy: ∀yGreedy (y).

Token 6735:
(9.2) Then we would still like to be able to conclude that Evil(John), because we know that John is a king (given) and John is greedy (because everyone is greedy).

Token 6736:
What we need forthis to work is to ﬁnd a substitution both for the variables in the implication sentence andfor the variables in the sentences that are in the knowledge base.

Token 6737:
In this case, applying thesubstitution{x/John,y/John}to the implication premises King(x)andGreedy (x)and the knowledge-base sentences King(John)andGreedy (y)will make them identical.

Token 6738:
Thus, we can infer the conclusion of the implication.

Token 6739:
This inference process can be captured as a single inference rule that we call Gener- alized Modus Ponens : 2For atomic sentences pi,pi/prime,a n dq, where there is a substitution θGENERALIZED MODUS PONENS

Token 6740:
326 Chapter 9.

Token 6741:
Inference in First-Order Logic such that S UBST(θ,pi/prime)=SUBST(θ,pi),f o ra l l i, p1/prime,p2/prime, ..., p n/prime,(p1∧p2∧...∧pn⇒q) SUBST(θ,q).

Token 6742:
There are n+1premises to this rule: the natomic sentences pi/primeand the one implication.

Token 6743:
The conclusion is the result of applying the substitution θto the consequent q.

Token 6744:
For our example: p1/primeisKing(John) p1isKing(x) p2/primeisGreedy (y) p2isGreedy (x) θis{x/John,y/John} qisEvil(x) SUBST(θ,q)isEvil(John).

Token 6745:
It is easy to show that Generalized Modus Ponens is a sound inference rule.

Token 6746:
First, we observe that, for any sentence p(whose variables are assumed to be universally quantiﬁed) and for any substitution θ, p|=SUBST(θ,p) holds by Universal Instantiation.

Token 6747:
It holds in particular for a θthat satisﬁes the conditions of the Generalized Modus Ponens rule.

Token 6748:
Thus, from p1/prime,...,p n/primewe can infer SUBST(θ,p1/prime)∧...∧SUBST(θ,pn/prime) and from the implication p1∧...∧pn⇒qwe can infer SUBST(θ,p1)∧...∧SUBST(θ,pn)⇒SUBST(θ,q).

Token 6749:
Now,θin Generalized Modus Ponens is deﬁned so that S UBST(θ,pi/prime)=SUBST(θ,pi),f o r alli; therefore the ﬁrst of these two sentences matches the premise of the second exactly.

Token 6750:
Hence, S UBST(θ,q)follows by Modus Ponens.

Token 6751:
Generalized Modus Ponens is a lifted version of Modus Ponens—it raises Modus Po- LIFTING nens from ground (variable-free) propositional logic to ﬁrst-order logic.

Token 6752:
We will see in the rest of this chapter that we can develop lifted versions of the forward chaining, backwardchaining, and resolution algorithms introduced in Chapter 7.

Token 6753:
The key advantage of lifted inference rules over propositionalization is that they make only those substitutions that are required to allow particular inferences to proceed.

Token 6754:
9.2.2 Uniﬁcation Lifted inference rules require ﬁnding substitutions that make different logical expressionslook identical.

Token 6755:
This process is called uniﬁcation and is a key component of all ﬁrst-order UNIFICATION inference algorithms.

Token 6756:
The U NIFY algorithm takes two sentences and returns a uniﬁer for UNIFIER them if one exists: UNIFY(p,q)=θwhere S UBST(θ,p)=SUBST(θ,q).

Token 6757:
Let us look at some examples of how U NIFY should behave. Suppose we have a query AskVars (Knows (John,x)): whom does John know?

Token 6758:
Answers to this query can be found 2Generalized Modus Ponens is more general than Modus Ponens (page 249) in the sense that the known facts and the premise of the implication need match only up to a substitution, rather than exactly.

Token 6759:
On the other hand,Modus Ponens allows any sentence αas the premise, rather than just a conjunction of atomic sentences.

Token 6760:
Section 9.2.

Token 6761:
Uniﬁcation and Lifting 327 by ﬁnding all sentences in the knowledge base that unify with Knows (John,x).H e r ea r et h e results of uniﬁcation with four different sentences that might be in the knowledge base: UNIFY(Knows (John,x),Knows (John,Jane)) ={x/Jane} UNIFY(Knows (John,x),Knows (y,Bill)) ={x/Bill,y/John} UNIFY(Knows (John,x),Knows (y,Mother (y))) ={y/John,x/Mother (John)} UNIFY(Knows (John,x),Knows (x,Elizabeth )) =fail.

Token 6762:
The last uniﬁcation fails because xcannot take on the values John andElizabeth at the same time.

Token 6763:
Now, remember that Knows (x,Elizabeth )means “Everyone knows Elizabeth,” so we should be able to infer that John knows Elizabeth.

Token 6764:
The problem arises only because the two sentences happen to use the same variable name, x.

Token 6765:
The problem can be avoided bystandardizing apart one of the two sentences being uniﬁed, which means renaming itsSTANDARDIZING APART variables to avoid name clashes.

Token 6766:
For example, we can rename xinKnows (x,Elizabeth )to x17(a new variable name) without changing its meaning.

Token 6767:
Now the uniﬁcation will work: UNIFY(Knows (John,x),Knows (x17,Elizabeth )) ={x/Elizabeth ,x17/John}.

Token 6768:
Exercise 9.12 delves further into the need for standardizing apart.

Token 6769:
There is one more complication: we said that U NIFY should return a substitution that makes the two arguments look the same.

Token 6770:
But there could be more than one such uni-ﬁer. For example, U NIFY(Knows (John,x),Knows (y,z))could return{y/John,x/ z}or {y/John,x/John,z/John}.

Token 6771:
The ﬁrst uniﬁer gives Knows (John,z)as the result of uniﬁ- cation, whereas the second gives Knows (John,John).

Token 6772:
The second result could be obtained from the ﬁrst by an additional substitution {z/John}; we say that the ﬁrst uniﬁer is more general than the second, because it places fewer restrictions on the values of the variables.

Token 6773:
It turns out that, for every uniﬁable pair of expressions, there is a single most general uniﬁer (orMOST GENERAL UNIFIER MGU) that is unique up to renaming and substitution of variables.

Token 6774:
(For example, {x/John} and{y/John}are considered equivalent, as are {x/John,y/John}and{x/John,y/ x}.) In this case it is{y/John,x/ z}.

Token 6775:
An algorithm for computing most general uniﬁers is shown in Figure 9.1.

Token 6776:
The process is simple: recursively explore the two expressions simultaneously “side by side,” building upa uniﬁer along the way, but failing if two corresponding points in the structures do not match.

Token 6777:
There is one expensive step: when matching a variable against a complex term, one must check whether the variable itself occurs inside the term; if it does, the match fails because noconsistent uniﬁer can be constructed.

Token 6778:
For example, S(x)can’t unify with S(S(x)).

Token 6779:
This so- called occur check makes the complexity of the entire algorithm quadratic in the size of the OCCUR CHECK expressions being uniﬁed.

Token 6780:
Some systems, including all logic programming systems, simply omit the occur check and sometimes make unsound inferences as a result; other systems usemore complex algorithms with linear-time complexity.

Token 6781:
9.2.3 Storage and retrieval Underlying the T ELL and A SKfunctions used to inform and interrogate a knowledge base are the more primitive S TORE and F ETCH functions.

Token 6782:
S TORE (s) stores a sentence sinto the knowledge base and F ETCH (q) returns all uniﬁers such that the query quniﬁes with some

Token 6783:
328 Chapter 9.

Token 6784:
Inference in First-Order Logic function UNIFY (x,y,θ)returns a substitution to make xandyidentical inputs :x, a variable, constant, list, or compound expression y, a variable, constant, list, or compound expression θ, the substitution built up so far (optional, defaults to empty) ifθ= failure then return failure else ifx=ythen return θ else if VARIABLE ?

Token 6785:
(x)then return UNIFY -VAR(x,y,θ) else if VARIABLE ? (y)then return UNIFY -VAR(y,x,θ) else if COMPOUND ? (x)and COMPOUND ?

Token 6786:
(y)then return UNIFY (x.ARGS,y.ARGS,UNIFY (x.OP,y.OP,θ)) else if LIST? (x)and LIST?

Token 6787:
(y)then return UNIFY (x.REST,y.REST,UNIFY (x.FIRST ,y.FIRST ,θ)) else return failure function UNIFY -VAR(var,x,θ)returns a substitution if{var/val}∈θthen return UNIFY (val,x,θ) else if{x/val}∈θthen return UNIFY (var,val,θ) else if OCCUR -CHECK ?

Token 6788:
(var,x)then return failure else return add{var/x}toθ Figure 9.1 The uniﬁcation algorithm.

Token 6789:
The algorithm works by comparing the structures of the inputs, element by element.

Token 6790:
The substitution θthat is the argument to U NIFY is built up along the way and is used to make sure that later comparisons are consistent with bindings that were established earlier.

Token 6791:
In a compound expression such as F(A, B),t h eO Pﬁeld picks out the function symbol Fand the A RGS ﬁeld picks out the argument list (A, B).

Token 6792:
sentence in the knowledge base.

Token 6793:
The problem we used to illustrate uniﬁcation—ﬁnding all facts that unify with Knows (John,x)—is an instance of F ETCH ing.

Token 6794:
The simplest way to implement S TORE and F ETCH is to keep all the facts in one long list and unify each query against every element of the list.

Token 6795:
Such a process is inefﬁcient, butit works, and it’s all you need to understand the rest of the chapter.

Token 6796:
The remainder of thissection outlines ways to make retrieval more efﬁcient; it can be skipped on ﬁrst reading.

Token 6797:
We can make F ETCH more efﬁcient by ensuring that uniﬁcations are attempted only with sentences that have some chance of unifying.

Token 6798:
For example, there is no point in trying to unify Knows (John,x)withBrother (Richard ,John).

Token 6799:
We can avoid such uniﬁcations by indexing the facts in the knowledge base.

Token 6800:
A simple scheme called predicate indexing puts INDEXING PREDICATE INDEXING all the Knows facts in one bucket and all the Brother facts in another.

Token 6801:
The buckets can be stored in a hash table for efﬁcient access.

Token 6802:
Predicate indexing is useful when there are many predicate symbols but only a few clauses for each symbol.

Token 6803:
Sometimes, however, a predicate has many clauses.

Token 6804:
For example, suppose that the tax authorities want to keep track of who employs whom, using a predi- cateEmploys (x,y).

Token 6805:
This would be a very large bucket with perhaps millions of employers

Token 6806:
Section 9.2.

Token 6807:
Uniﬁcation and Lifting 329 Employs (x,y) Employs (x,Richard ) Employs (IBM,y ) Employs (IBM,Richard )Employs (x,y) Employs (John,John )Employs (x,x) Employs (x,John ) Employs (John,y ) (a) (b) Figure 9.2 (a) The subsumption lattice whose lowest node is Employs (IBM,Richard ).

Token 6808:
(b) The subsumption lattice for the sentence Employs (John,John). and tens of millions of employees.

Token 6809:
Answering a query such as Employs (x,Richard )with predicate indexing would require scanning the entire bucket.

Token 6810:
For this particular query, it would help if facts were indexed both by predicate and by second argument, perhaps using a combined hash table key.

Token 6811:
Then we could simply constructthe key from the query and retrieve exactly those facts that unify with the query.

Token 6812:
For otherqueries, such as Employs (IBM,y), we would need to have indexed the facts by combining the predicate with the ﬁrst argument.

Token 6813:
Therefore, facts can be stored under multiple indexkeys, rendering them instantly accessible to various queries that they might unify with.

Token 6814:
Given a sentence to be stored, it is possible to construct indices for all possible queries that unify with it.

Token 6815:
For the fact Employs (IBM,Richard ), the queries are Employs (IBM,Richard )Does IBM employ Richard? Employs (x,Richard ) Who employs Richard?

Token 6816:
Employs (IBM,y) Whom does IBM employ? Employs (x,y) Who employs whom? These queries form a subsumption lattice , as shown in Figure 9.2(a).

Token 6817:
The lattice has some SUBSUMPTION LATTICE interesting properties.

Token 6818:
For example, the child of any node in the lattice is obtained from its parent by a single substitution; and the “highest” common descendant of any two nodes is the result of applying their most general uniﬁer.

Token 6819:
The portion of the lattice above any ground fact can be constructed systematically (Exercise 9.5).

Token 6820:
A sentence with repeated constants has a slightly different lattice, as shown in Figure 9.2(b).

Token 6821:
Function symbols and variables in thesentences to be stored introduce still more interesting lattice structures.

Token 6822:
The scheme we have described works very well whenever the lattice contains a small number of nodes.

Token 6823:
For a predicate with narguments, however, the lattice contains O(2 n) nodes.

Token 6824:
If function symbols are allowed, the number of nodes is also exponential in the sizeof the terms in the sentence to be stored.

Token 6825:
This can lead to a huge number of indices.

Token 6826:
At somepoint, the beneﬁts of indexing are outweighed by the costs of storing and maintaining allthe indices.

Token 6827:
We can respond by adopting a ﬁxed policy, such as maintaining indices only onkeys composed of a predicate plus each argument, or by using an adaptive policy that creates indices to meet the demands of the kinds of queries being asked.

Token 6828:
For most AI systems, the number of facts to be stored is small enough that efﬁcient indexing is considered a solvedproblem.

Token 6829:
For commercial databases, where facts number in the billions, the problem hasbeen the subject of intensive study and technology development..

Token 6830:
330 Chapter 9.

Token 6831:
Inference in First-Order Logic 9.3 F ORW ARD CHAINING A forward-chaining algorithm for propositional deﬁnite clauses was given in Section 7.5.

Token 6832:
The idea is simple: start with the atomic sentences in the knowledge base and apply ModusPonens in the forward direction, adding new atomic sentences, until no further inferencescan be made.

Token 6833:
Here, we explain how the algorithm is applied to ﬁrst-order deﬁnite clauses.Deﬁnite clauses such as Situation⇒Response are especially useful for systems that make inferences in response to newly arrived information.

Token 6834:
Many systems can be deﬁned this way,and forward chaining can be implemented very efﬁciently.

Token 6835:
9.3.1 First-order deﬁnite clauses First-order deﬁnite clauses closely resemble propositional deﬁnite clauses (page 256): theyare disjunctions of literals of which exactly one is positive .

Token 6836:
A deﬁnite clause either is atomic or is an implication whose antecedent is a conjunction of positive literals and whose conse- quent is a single positive literal.

Token 6837:
The following are ﬁrst-order deﬁnite clauses: King(x)∧Greedy (x)⇒Evil(x). King(John). Greedy (y).

Token 6838:
Unlike propositional literals, ﬁrst-order literals can include variables, in which case those variables are assumed to be universally quantiﬁed.

Token 6839:
(Typically, we omit universal quantiﬁerswhen writing deﬁnite clauses.)

Token 6840:
Not every knowledge base can be converted into a set ofdeﬁnite clauses because of the single-positive-literal restriction, but many can.

Token 6841:
Consider thefollowing problem: The law says that it is a crime for an American to sell weapons to hostile nations.

Token 6842:
The country Nono, an enemy of America, has some missiles, and all of its missiles were sold to it by Colonel West, who is American.

Token 6843:
We will prove that West is a criminal. First, we will represent these facts as ﬁrst-order deﬁnite clauses.

Token 6844:
The next section shows how the forward-chaining algorithm solves the problem.

Token 6845:
“...it is a crime for an American to sell weapons to hostile nations”: American (x)∧Weapon (y)∧Sells(x, y, z )∧Hostile (z)⇒Criminal (x).

Token 6846:
(9.3) “Nono ...has some missiles.” The sentence ∃xOwns(Nono,x)∧Missile (x)is transformed into two deﬁnite clauses by Existential Instantiation, introducing a new constant M1: Owns(Nono,M1) (9.4) Missile (M1) .

Token 6847:
(9.5) “All of its missiles were sold to it by Colonel West”: Missile (x)∧Owns(Nono,x)⇒Sells(West,x,Nono).

Token 6848:
(9.6) We will also need to know that missiles are weapons: Missile (x)⇒Weapon (x) (9.7)

Token 6849:
Section 9.3. Forward Chaining 331 and we must know that an enemy of America counts as “hostile”: Enemy (x,America )⇒Hostile (x).

Token 6850:
(9.8) “West, who is American ...”: American (West). (9.9) “The country Nono, an enemy of America ...”: Enemy (Nono,America ).

Token 6851:
(9.10) This knowledge base contains no function symbols and is therefore an instance of the class ofDatalog knowledge bases.

Token 6852:
Datalog is a language that is restricted to ﬁrst-order deﬁnite DATALOG clauses with no function symbols.

Token 6853:
Datalog gets its name because it can represent the type of statements typically made in relational databases.

Token 6854:
We will see that the absence of functionsymbols makes inference much easier.

Token 6855:
9.3.2 A simple forward-chaining algorithm The ﬁrst forward-chaining algorithm we consider is a simple one, shown in Figure 9.3.

Token 6856:
Start-ing from the known facts, it triggers all the rules whose premises are satisﬁed, adding theirconclusions to the known facts.

Token 6857:
The process repeats until the query is answered (assumingthat just one answer is required) or no new facts are added.

Token 6858:
Notice that a fact is not “new”if it is just a renaming of a known fact.

Token 6859:
One sentence is a renaming of another if they RENAMING are identical except for the names of the variables.

Token 6860:
For example, Likes(x,IceCream )and Likes(y,IceCream )are renamings of each other because they differ only in the choice of x ory; their meanings are identical: everyone likes ice cream.

Token 6861:
We use our crime problem to illustrate how FOL-FC-A SKworks. The implication sentences are (9.3), (9.6), (9.7), and (9.8).

Token 6862:
Two iterations are required: •On the ﬁrst iteration, rule (9.3) has unsatisﬁed premises.

Token 6863:
Rule (9.6) is satisﬁed with {x/M 1},a n dSells(West,M1,Nono)is added. Rule (9.7) is satisﬁed with {x/M 1},a n dWeapon (M1)is added.

Token 6864:
Rule (9.8) is satisﬁed with {x/Nono},a n dHostile (Nono)is added.

Token 6865:
•On the second iteration, rule (9.3) is satisﬁed with {x/West,y/ M 1,z/Nono},a n d Criminal (West)is added.

Token 6866:
Figure 9.4 shows the proof tree that is generated.

Token 6867:
Notice that no new inferences are possible at this point because every sentence that could be concluded by forward chaining is already contained explicitly in the KB.

Token 6868:
Such a knowledge base is called a ﬁxed point of the inference process.

Token 6869:
Fixed points reached by forward chaining with ﬁrst-order deﬁnite clauses are similarto those for propositional forward chaining (page 258); the principal difference is that a ﬁrst-order ﬁxed point can include universally quantiﬁed atomic sentences.

Token 6870:
FOL-FC-A SKis easy to analyze. First, it is sound , because every inference is just an application of Generalized Modus Ponens, which is sound.

Token 6871:
Second, it is complete for deﬁnite clause knowledge bases; that is, it answers every query whose answers are entailed by anyknowledge base of deﬁnite clauses.

Token 6872:
For Datalog knowledge bases, which contain no functionsymbols, the proof of completeness is fairly easy. We begin by counting the number of

Token 6873:
332 Chapter 9.

Token 6874:
Inference in First-Order Logic function FOL-FC-A SK(KB,α)returns a substitution or false inputs :KB, the knowledge base, a set of ﬁrst-order deﬁnite clauses α, the query, an atomic sentence local variables :new, the new sentences inferred on each iteration repeat until new is empty new←{} for each rule inKB do (p1∧...∧pn⇒q)←STANDARDIZE -VARIABLES (rule) for each θsuch that S UBST (θ,p1∧...∧pn)=S UBST (θ,p/prime 1∧...∧p/prime n) for some p/prime 1,...,p/prime ninKB q/prime←SUBST (θ,q) ifq/primedoes not unify with some sentence already in KBornew then addq/primetonew φ←UNIFY (q/prime,α) ifφis notfailthen return φ addnew toKB return false Figure 9.3 A conceptually straightforward, but very inefﬁcient, forward-chaining algo- rithm.

Token 6875:
On each iteration, it adds to KBall the atomic sentences that can be inferred in one step from the implication sentences and the atomic sentences already in KB.

Token 6876:
The function STANDARDIZE -VARIABLES replaces all variables in its arguments with new ones that have not been used before.

Token 6877:
Hostile (Nono ) Enemy (Nono,America ) Owns (Nono,M1) Missile (M1) American (West )Weapon (M1)Criminal (West ) Sells (West,M1,Nono ) Figure 9.4 The proof tree generated by forward chaining on the crime example.

Token 6878:
The initial facts appear at the bottom level, facts inferred on the ﬁrst iteration in the middle level, and facts inferred on the second iteration at the top level.

Token 6879:
possible facts that can be added, which determines the maximum number of iterations.

Token 6880:
Let k be the maximum arity (number of arguments) of any predicate, pbe the number of predicates, andnbe the number of constant symbols.

Token 6881:
Clearly, there can be no more than pnkdistinct ground facts, so after this many iterations the algorithm must have reached a ﬁxed point.

Token 6882:
Thenwe can make an argument very similar to the proof of completeness for propositional forward

Token 6883:
Section 9.3. Forward Chaining 333 chaining. (See page 258.)

Token 6884:
The details of how to make the transition from propositional to ﬁrst-order completeness are given for the resolution algorithm in Section 9.5.

Token 6885:
For general deﬁnite clauses with function symbols, FOL-FC-A SKcan generate in- ﬁnitely many new facts, so we need to be more careful.

Token 6886:
For the case in which an answer tothe query sentence qis entailed, we must appeal to Herbrand’s theorem to establish that the algorithm will ﬁnd a proof.

Token 6887:
(See Section 9.5 for the resolution case.) If the query has no answer, the algorithm could fail to terminate in some cases.

Token 6888:
For example, if the knowledgebase includes the Peano axioms NatNum (0) ∀nNatNum (n)⇒NatNum (S(n)), then forward chaining adds NatNum (S(0)),NatNum (S(S(0))) ,NatNum (S(S(S(0)))) , and so on.

Token 6889:
This problem is unavoidable in general. As with general ﬁrst-order logic, entail-ment with deﬁnite clauses is semidecidable.

Token 6890:
9.3.3 Efﬁcient forward chaining The forward-chaining algorithm in Figure 9.3 is designed for ease of understanding ratherthan for efﬁciency of operation.

Token 6891:
There are three possible sources of inefﬁciency.

Token 6892:
First, the“inner loop” of the algorithm involves ﬁnding all possible uniﬁers such that the premise ofa rule uniﬁes with a suitable set of facts in the knowledge base.

Token 6893:
This is often called pattern matching and can be very expensive.

Token 6894:
Second, the algorithm rechecks every rule on every PATTERN MATCHING iteration to see whether its premises are satisﬁed, even if very few additions are made to the knowledge base on each iteration.

Token 6895:
Finally, the algorithm might generate many facts that areirrelevant to the goal. We address each of these issues in turn.

Token 6896:
Matching rules against known facts The problem of matching the premise of a rule against the facts in the knowledge base might seem simple enough.

Token 6897:
For example, suppose we want to apply the rule Missile (x)⇒Weapon (x).

Token 6898:
Then we need to ﬁnd all the facts that unify with Missile (x); in a suitably indexed knowledge base, this can be done in constant time per fact.

Token 6899:
Now consider a rule such as Missile (x)∧Owns(Nono,x)⇒Sells(West,x,Nono).

Token 6900:
Again, we can ﬁnd all the objects owned by Nono in constant time per object; then, for each object, we could check whether it is a missile.

Token 6901:
If the knowledge base contains many objectsowned by Nono and very few missiles, however, it would be better to ﬁnd all the missiles ﬁrstand then check whether they are owned by Nono.

Token 6902:
This is the conjunct ordering problem: CONJUNCT ORDERING ﬁnd an ordering to solve the conjuncts of the rule premise so that the total cost is minimized.

Token 6903:
It turns out that ﬁnding the optimal ordering is NP-hard, but good heuristics are available.

Token 6904:
For example, the minimum-remaining-values (MRV) heuristic used for CSPs in Chapter 6 would suggest ordering the conjuncts to look for missiles ﬁrst if fewer missiles than objects are owned by Nono.

Token 6905:
334 Chapter 9.

Token 6906:
Inference in First-Order Logic WANT SAQ NSW V TDiﬀ(wa,nt)∧Diﬀ(wa,sa)∧ Diﬀ(nt,q)∧Diﬀ(nt,sa)∧ Diﬀ(q,nsw)∧Diﬀ(q,sa)∧ Diﬀ(nsw,v)∧Diﬀ(nsw,sa)∧ Diﬀ(v,sa)⇒Colorable () Diﬀ(Red,Blue)Diﬀ(Red,Green ) Diﬀ(Green ,Red)Diﬀ(Green ,Blue) Diﬀ(Blue,Red)Diﬀ(Blue,Green ) (a) (b) Figure 9.5 (a) Constraint graph for coloring the map of Australia.

Token 6907:
(b) The map-coloring CSP expressed as a single deﬁnite clause.

Token 6908:
Each map region is represented as a variable whosevalue can be one of the constants Red,Green orBlue .

Token 6909:
The connection between pattern matching and constraint satisfaction is actually very close.

Token 6910:
We can view each conjunct as a constraint on the variables that it contains—for ex-ample, Missile (x)is a unary constraint on x.

Token 6911:
Extending this idea, we can express every ﬁnite-domain CSP as a single deﬁnite clause together with some associated ground facts.

Token 6912:
Consider the map-coloring problem from Figure 6.1, shown again in Figure 9.5(a).

Token 6913:
An equiv- alent formulation as a single deﬁnite clause is given in Figure 9.5(b).

Token 6914:
Clearly, the conclusionColorable ()can be inferred only if the CSP has a solution.

Token 6915:
Because CSPs in general include 3-SAT problems as special cases, we can conclude that matching a deﬁnite clause against a set of facts is NP-hard.

Token 6916:
It might seem rather depressing that forward chaining has an NP-hard matching problem in its inner loop.

Token 6917:
There are three ways to cheer ourselves up: •We can remind ourselves that most rules in real-world knowledge bases are small and simple (like the rules in our crime example) rather than large and complex (like theCSP formulation in Figure 9.5).

Token 6918:
It is common in the database world to assume thatboth the sizes of rules and the arities of predicates are bounded by a constant and toworry only about data complexity —that is, the complexity of inference as a function DATA COMPLEXITY of the number of ground facts in the knowledge base.

Token 6919:
It is easy to show that the data complexity of forward chaining is polynomial. •We can consider subclasses of rules for which matching is efﬁcient.

Token 6920:
Essentially every Datalog clause can be viewed as deﬁning a CSP, so matching will be tractable just when the corresponding CSP is tractable.

Token 6921:
Chapter 6 describes several tractable families of CSPs.

Token 6922:
For example, if the constraint graph (the graph whose nodes are variablesand whose links are constraints) forms a tree, then the CSP can be solved in lineartime.

Token 6923:
Exactly the same result holds for rule matching. For instance, if we remove South

Token 6924:
Section 9.3.

Token 6925:
Forward Chaining 335 Australia from the map in Figure 9.5, the resulting clause is Diﬀ(wa,nt)∧Diﬀ(nt,q)∧Diﬀ(q,nsw)∧Diﬀ(nsw,v)⇒Colorable () which corresponds to the reduced CSP shown in Figure 6.12 on page 224.

Token 6926:
Algorithms for solving tree-structured CSPs can be applied directly to the problem of rule matching.

Token 6927:
•We can try to to eliminate redundant rule-matching attempts in the forward-chaining algorithm, as described next.

Token 6928:
Incremental forward chaining When we showed how forward chaining works on the crime example, we cheated; in partic- ular, we omitted some of the rule matching done by the algorithm shown in Figure 9.3.

Token 6929:
For example, on the second iteration, the rule Missile (x)⇒Weapon (x) matches against Missile (M1)(again), and of course the conclusion Weapon (M1)is already known so nothing happens.

Token 6930:
Such redundant rule matching can be avoided if we make thefollowing observation: Every new fact inferred on iteration tmust be derived from at least one new fact inferred on iteration t−1.This is true because any inference that does not require a new fact from iteration t−1could have been done at iteration t−1already.

Token 6931:
This observation leads naturally to an incremental forward-chaining algorithm where, at iteration t, we check a rule only if its premise includes a conjunct pithat uniﬁes with a fact p/prime inewly inferred at iteration t−1.

Token 6932:
The rule-matching step then ﬁxes pito match with p/prime i,b u t allows the other conjuncts of the rule to match with facts from any previous iteration.

Token 6933:
Thisalgorithm generates exactly the same facts at each iteration as the algorithm in Figure 9.3, butis much more efﬁcient.

Token 6934:
With suitable indexing, it is easy to identify all the rules that can be triggered by any given fact, and indeed many real systems operate in an “update” mode wherein forward chain- ing occurs in response to each new fact that is T ELLed to the system.

Token 6935:
Inferences cascade through the set of rules until the ﬁxed point is reached, and then the process begins again forthe next new fact.

Token 6936:
Typically, only a small fraction of the rules in the knowledge base are actually triggered by the addition of a given fact.

Token 6937:
This means that a great deal of redundant work is done inrepeatedly constructing partial matches that have some unsatisﬁed premises.

Token 6938:
Our crime ex-ample is rather too small to show this effectively, but notice that a partial match is constructedon the ﬁrst iteration between the rule American (x)∧Weapon (y)∧Sells(x, y, z )∧Hostile (z)⇒Criminal (x) and the fact American (West).

Token 6939:
This partial match is then discarded and rebuilt on the second iteration (when the rule succeeds).

Token 6940:
It would be better to retain and gradually complete thepartial matches as new facts arrive, rather than discarding them.

Token 6941:
The rete algorithm 3was the ﬁrst to address this problem.

Token 6942:
The algorithm preprocesses RETE the set of rules in the knowledge base to construct a sort of dataﬂow network in which each 3Rete is Latin for net.

Token 6943:
The English pronunciation rhymes with treaty.

Token 6944:
336 Chapter 9. Inference in First-Order Logic node is a literal from a rule premise.

Token 6945:
Variable bindings ﬂow through the network and are ﬁltered out when they fail to match a literal.

Token 6946:
If two literals in a rule share a variable—forexample, Sells(x,y,z)∧Hostile (z)in the crime example—then the bindings from each literal are ﬁltered through an equality node.

Token 6947:
A variable binding reaching a node for an n- ary literal such as Sells(x,y,z)might have to wait for bindings for the other variables to be established before the process can continue.

Token 6948:
At any given point, the state of a rete network captures all the partial matches of the rules, avoiding a great deal of recomputation.

Token 6949:
Rete networks, and various improvements thereon, have been a key component of so- called production systems , which were among the earliest forward-chaining systems in PRODUCTION SYSTEM widespread use.4The X CON system (originally called R1; McDermott, 1982) was built with a production-system architecture.

Token 6950:
X CON contained several thousand rules for designing conﬁgurations of computer components for customers of the Digital Equipment Corporation.It was one of the ﬁrst clear commercial successes in the emerging ﬁeld of expert systems.Many other similar systems have been built with the same underlying technology, which hasbeen implemented in the general-purpose language O PS-5.

Token 6951:
Production systems are also popular in cognitive architectures —that is, models of hu-COGNITIVE ARCHITECTURES man reasoning—such as ACT (Anderson, 1983) and S OAR (Laird et al.

Token 6952:
, 1987). In such sys- tems, the “working memory” of the system models human short-term memory, and the pro- ductions are part of long-term memory.

Token 6953:
On each cycle of operation, productions are matchedagainst the working memory of facts.

Token 6954:
A production whose conditions are satisﬁed can add ordelete facts in working memory.

Token 6955:
In contrast to the typical situation in databases, productionsystems often have many rules and relatively few facts.

Token 6956:
With suitably optimized matchingtechnology, some modern systems can operate in real time with tens of millions of rules.

Token 6957:
Irrelevant facts The ﬁnal source of inefﬁciency in forward chaining appears to be intrinsic to the approach and also arises in the propositional context.

Token 6958:
Forward chaining makes all allowable inferencesbased on the known facts, even if they are irrelevant to the goal at hand .

Token 6959:
In our crime example, there were no rules capable of drawing irrelevant conclusions, so the lack of directedness wasnot a problem.

Token 6960:
In other cases (e.g., if many rules describe the eating habits of Americans andthe prices of missiles), FOL-FC-A SKwill generate many irrelevant conclusions.

Token 6961:
One way to avoid drawing irrelevant conclusions is to use backward chaining, as de- scribed in Section 9.4.

Token 6962:
Another solution is to restrict forward chaining to a selected subset ofrules, as in PL-FC-E NTAILS ? (page 258).

Token 6963:
A third approach has emerged in the ﬁeld of de- ductive databases , which are large-scale databases, like relational databases, but which useDEDUCTIVE DATABASES forward chaining as the standard inference tool rather than SQL queries.

Token 6964:
The idea is to rewrite the rule set, using information from the goal, so that only relevant variable bindings—thosebelonging to a so-called magic set —are considered during forward inference.

Token 6965:
For example, if MAGIC SET the goal is Criminal (West), the rule that concludes Criminal (x)will be rewritten to include an extra conjunct that constrains the value of x: Magic (x)∧American (x)∧Weapon (y)∧Sells(x, y, z )∧Hostile (z)⇒Criminal (x).

Token 6966:
4The word production inproduction systems denotes a condition–action rule.

Token 6967:
Section 9.4. Backward Chaining 337 The fact Magic (West)is also added to the KB.

Token 6968:
In this way, even if the knowledge base contains facts about millions of Americans, only Colonel West will be considered during theforward inference process.

Token 6969:
The complete process for deﬁning magic sets and rewriting theknowledge base is too complex to go into here, but the basic idea is to perform a sort of“generic” backward inference from the goal in order to work out which variable bindings need to be constrained.

Token 6970:
The magic sets approach can therefore be thought of as a kind of hybrid between forward inference and backward preprocessing.

Token 6971:
9.4 B ACKWARD CHAINING The second major family of logical inference algorithms uses the backward chaining ap- proach introduced in Section 7.5 for deﬁnite clauses.

Token 6972:
These algorithms work backward fromthe goal, chaining through rules to ﬁnd known facts that support the proof.

Token 6973:
We describethe basic algorithm, and then we describe how it is used in logic programming , which is the most widely used form of automated reasoning.

Token 6974:
We also see that backward chaining has somedisadvantages compared with forward chaining, and we look at ways to overcome them.

Token 6975:
Fi-nally, we look at the close connection between logic programming and constraint satisfactionproblems.

Token 6976:
9.4.1 A backward-chaining algorithm Figure 9.6 shows a backward-chaining algorithm for deﬁnite clauses.

Token 6977:
FOL-BC-A SK(KB, goal) will be proved if the knowledge base contains a clause of the form lhs⇒goal,w h e r e lhs(left-hand side) is a list of conjuncts.

Token 6978:
An atomic fact like American (West)is considered as a clause whose lhsis the empty list.

Token 6979:
Now a query that contains variables might be proved in multiple ways.

Token 6980:
For example, the query Person (x)could be proved with the substitution {x/John}as well as with {x/Richard}.

Token 6981:
So we implement FOL-BC-A SKas agenerator — GENERATOR a function that returns multiple times, each time giving one possible result.

Token 6982:
Backward chaining is a kind of AND /ORsearch—the ORpart because the goal query can be proved by any rule in the knowledge base, and the AND part because all the conjuncts in thelhsof a clause must be proved.

Token 6983:
FOL-BC-O Rworks by fetching all clauses that might unify with the goal, standardizing the variables in the clause to be brand-new variables, andthen, if the rhsof the clause does indeed unify with the goal, proving every conjunct in the lhs, using FOL-BC-A ND.

Token 6984:
That function in turn works by proving each of the conjuncts in turn, keeping track of the accumulated substitution as we go.

Token 6985:
Figure 9.7 is the proof tree forderiving Criminal (West)from sentences (9.3) through (9.10).

Token 6986:
Backward chaining, as we have written it, is clearly a depth-ﬁrst search algorithm.

Token 6987:
This means that its space requirements are linear in the size of the proof (neglecting, for now, the space required to accumulate the solutions).

Token 6988:
It also means that backward chaining (unlike forward chaining) suffers from problems with repeated states and incompleteness.

Token 6989:
Wewill discuss these problems and some potential solutions, but ﬁrst we show how backwardchaining is used in logic programming systems.

Token 6990:
338 Chapter 9.

Token 6991:
Inference in First-Order Logic function FOL-BC-A SK(KB,query )returns a generator of substitutions return FOL-BC-O R(KB,query ,{}) generator FOL-BC-O R(KB,goal,θ)yields a substitution for each rule (lhs⇒rhs)i nF ETCH -RULES -FOR-GOAL(KB,goal)do (lhs,rhs)←STANDARDIZE -VARIABLES ((lhs,rhs)) for each θ/primeinFOL-BC-A ND(KB,lhs,UNIFY (rhs,goal,θ))do yieldθ/prime generator FOL-BC-A ND(KB,goals ,θ)yields a substitution ifθ=failure then return else if LENGTH (goals )=0 then yield θ else do ﬁrst,rest←FIRST (goals ), R EST(goals ) for each θ/primeinFOL-BC-O R(KB,SUBST (θ,ﬁrst),θ)do for each θ/prime/primeinFOL-BC-A ND(KB,rest,θ/prime)do yieldθ/prime/prime Figure 9.6 A simple backward-chaining algorithm for ﬁrst-order knowledge bases.

Token 6992:
Hostile (Nono ) Enemy (Nono,America ) Owns (Nono,M 1) Missile (M1)Criminal (West ) Missile (y)Weapon (y)Sells (West,M 1,z) American (West ) {y/M1} { } { } { } {z/Nono } { } Figure 9.7 Proof tree constructed by backward chaining to prove that West is a criminal.

Token 6993:
The tree should be read depth ﬁrst, left to right. To prove Criminal (West), we have to prove the four conjuncts below it.

Token 6994:
Some of these are in the knowledge base, and others require further backward chaining.

Token 6995:
Bindings for each successful uniﬁcation are shown next to thecorresponding subgoal.

Token 6996:
Note that once one subgoal in a conjunction succeeds, its substitution is applied to subsequent subgoals.

Token 6997:
Thus, by the time FOL-BC-A SKgets to the last conjunct, originally Hostile (z),zis already bound to Nono .

Token 6998:
Section 9.4.

Token 6999:
Backward Chaining 339 9.4.2 Logic programming Logic programming is a technology that comes fairly close to embodying the declarative ideal described in Chapter 7: that systems should be constructed by expressing knowledge ina formal language and that problems should be solved by running inference processes on thatknowledge.

Token 7000:
The ideal is summed up in Robert Kowalski’s equation, Algorithm =Logic +Control . Prolog is the most widely used logic programming language.

Token 7001:
It is used primarily as a rapid- PROLOG prototyping language and for symbol-manipulation tasks such as writing compilers (Van Roy, 1990) and parsing natural language (Pereira and Warren, 1980).

Token 7002:
Many expert systems have been written in Prolog for legal, medical, ﬁnancial, and other domains.

Token 7003:
Prolog programs are sets of deﬁnite clauses written in a notation somewhat different from standard ﬁrst-order logic.

Token 7004:
Prolog uses uppercase letters for variables and lowercase for constants—the opposite of our convention for logic.

Token 7005:
Commas separate conjuncts in a clause, and the clause is written “backwards” from what we are used to; instead of A∧B⇒Cin Prolog we have C: -A ,B .

Token 7006:
Here is a typical example: criminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).

Token 7007:
The notation [E|L] denotes a list whose ﬁrst element is Eand whose rest is L.H e r ei sa Prolog program for append(X,Y,Z) , which succeeds if list Zis the result of appending listsXandY: append([],Y,Y).

Token 7008:
append([A|X],Y,[A|Z]) :- append(X,Y,Z).

Token 7009:
In English, we can read these clauses as (1) appending an empty list with a list Yproduces the same list Yand (2) [A|Z] is the result of appending [A|X] ontoY, provided that Zis the result of appending XontoY.

Token 7010:
In most high-level languages we can write a similar recur- sive function that describes how to append two lists.

Token 7011:
The Prolog deﬁnition is actually muchmore powerful, however, because it describes a relation that holds among three arguments, rather than a function computed from two arguments.

Token 7012:
For example, we can ask the query append(X,Y,[1,2]) : what two lists can be appended to give [1,2] ?

Token 7013:
We get back the solutions X=[] Y=[1,2]; X=[1] Y=[2];X=[1,2] Y=[] The execution of Prolog programs is done through depth-ﬁrst backward chaining, where clauses are tried in the order in which they are written in the knowledge base.

Token 7014:
Some aspects of Prolog fall outside standard logical inference: •Prolog uses the database semantics of Section 8.2.8 rather than ﬁrst-order semantics, and this is apparent in its treatment of equality and negation (see Section 9.4.5).

Token 7015:
•There is a set of built-in functions for arithmetic.

Token 7016:
Literals using these function symbols are “proved” by executing code rather than doing further inference. For example, the

Token 7017:
340 Chapter 9. Inference in First-Order Logic goal “ X is 4+3 ” succeeds with Xbound to 7.

Token 7018:
On the other hand, the goal “ 5 is X+Y ” fails, because the built-in functions do not do arbitrary equation solving.5 •There are built-in predicates that have side effects when executed.

Token 7019:
These include input– output predicates and the assert /retract predicates for modifying the knowledge base.

Token 7020:
Such predicates have no counterpart in logic and can produce confusing results—for example, if facts are asserted in a branch of the proof tree that eventually fails.

Token 7021:
•Theoccur check is omitted from Prolog’s uniﬁcation algorithm.

Token 7022:
This means that some unsound inferences can be made; these are almost never a problem in practice.

Token 7023:
•Prolog uses depth-ﬁrst backward-chaining search with no checks for inﬁnite recursion.

Token 7024:
This makes it very fast when given the right set of axioms, but incomplete when giventhe wrong ones.

Token 7025:
Prolog’s design represents a compromise between declarativeness and execution efﬁciency— inasmuch as efﬁciency was understood at the time Prolog was designed.

Token 7026:
9.4.3 Efﬁcient implementation of logic programs The execution of a Prolog program can happen in two modes: interpreted and compiled.Interpretation essentially amounts to running the FOL-BC-A SKalgorithm from Figure 9.6, with the program as the knowledge base.

Token 7027:
We say “essentially” because Prolog interpreterscontain a variety of improvements designed to maximize speed. Here we consider only two.

Token 7028:
First, our implementation had to explicitly manage the iteration over possible results generated by each of the subfunctions.

Token 7029:
Prolog interpreters have a global data structure,a stack of choice points , to keep track of the multiple possibilities that we considered in CHOICEPOINT FOL-BC-O R. This global stack is more efﬁcient, and it makes debugging easier, because the debugger can move up and down the stack.

Token 7030:
Second, our simple implementation of FOL-BC-A SKspends a good deal of time gener- ating substitutions.

Token 7031:
Instead of explicitly constructing substitutions, Prolog has logic variables that remember their current binding.

Token 7032:
At any point in time, every variable in the program ei- ther is unbound or is bound to some value.

Token 7033:
Together, these variables and values implicitlydeﬁne the substitution for the current branch of the proof.

Token 7034:
Extending the path can only addnew variable bindings, because an attempt to add a different binding for an already boundvariable results in a failure of uniﬁcation.

Token 7035:
When a path in the search fails, Prolog will backup to a previous choice point, and then it might have to unbind some variables.

Token 7036:
This is doneby keeping track of all the variables that have been bound in a stack called the trail.

Token 7037:
As each TRAIL new variable is bound by U NIFY -VAR, the variable is pushed onto the trail.

Token 7038:
When a goal fails and it is time to back up to a previous choice point, each of the variables is unbound as it is removed from the trail.

Token 7039:
Even the most efﬁcient Prolog interpreters require several thousand machine instruc- tions per inference step because of the cost of index lookup, uniﬁcation, and building therecursive call stack.

Token 7040:
In effect, the interpreter always behaves as if it has never seen the pro-gram before; for example, it has to ﬁndclauses that match the goal.

Token 7041:
A compiled Prolog 5Note that if the Peano axioms are provided, such goals can be solved by inference within a Prolog program.

Token 7042:
Section 9.4.

Token 7043:
Backward Chaining 341 procedure APPEND (ax,y,az,continuation ) trail←GLOBAL -TRAIL -POINTER () ifax=[]and U NIFY (y,az)then CALL(continuation ) RESET -TRAIL (trail) a,x,z←NEW-VARIABLE (), N EW-VARIABLE (), N EW-VARIABLE () ifUNIFY (ax,[a|x]) and U NIFY (az,[a|z])then APPEND (x,y,z,continuation ) Figure 9.8 Pseudocode representing the result of compiling the Append predicate.

Token 7044:
The function N EW-VARIABLE returns a new variable, distinct from all other variables used so far.

Token 7045:
The procedure C ALL(continuation ) continues execution with the speciﬁed continuation.

Token 7046:
program, on the other hand, is an inference procedure for a speciﬁc set of clauses, so it knows what clauses match the goal.

Token 7047:
Prolog basically generates a miniature theorem prover for eachdifferent predicate, thereby eliminating much of the overhead of interpretation.

Token 7048:
It is also pos-sible to open-code the uniﬁcation routine for each different call, thereby avoiding explicit OPEN-CODE analysis of term structure.

Token 7049:
(For details of open-coded uniﬁcation, see Warren et al. (1977).)

Token 7050:
The instruction sets of today’s computers give a poor match with Prolog’s semantics, so most Prolog compilers compile into an intermediate language rather than directly into ma-chine language.

Token 7051:
The most popular intermediate language is the Warren Abstract Machine,or WAM, named after David H. D. Warren, one of the implementers of the ﬁrst Prolog com-piler.

Token 7052:
The WAM is an abstract instruction set that is suitable for Prolog and can be eitherinterpreted or translated into machine language.

Token 7053:
Other compilers translate Prolog into a high-level language such as Lisp or C and then use that language’s compiler to translate to machinelanguage.

Token 7054:
For example, the deﬁnition of the Append predicate can be compiled into the code shown in Figure 9.8.

Token 7055:
Several points are worth mentioning: •Rather than having to search the knowledge base for Append clauses, the clauses be- come a procedure and the inferences are carried out simply by calling the procedure.

Token 7056:
•As described earlier, the current variable bindings are kept on a trail.

Token 7057:
The ﬁrst step of the procedure saves the current state of the trail, so that it can be restored by R ESET -TRAIL if the ﬁrst clause fails.

Token 7058:
This will undo any bindings generated by the ﬁrst call to U NIFY . •The trickiest part is the use of continuations to implement choice points.

Token 7059:
You can think CONTINUATION of a continuation as packaging up a procedure and a list of arguments that together deﬁne what should be done next whenever the current goal succeeds.

Token 7060:
It would notdo just to return from a procedure like A PPEND when the goal succeeds, because it could succeed in several ways, and each of them has to be explored.

Token 7061:
The continuationargument solves this problem because it can be called each time the goal succeeds.

Token 7062:
Inthe A PPEND code, if the ﬁrst argument is empty and the second argument uniﬁes with the third, then the A PPEND predicate has succeeded.

Token 7063:
We then C ALL the continuation, with the appropriate bindings on the trail, to do whatever should be done next.

Token 7064:
Forexample, if the call to A PPEND were at the top level, the continuation would print the bindings of the variables.

Token 7065:
342 Chapter 9.

Token 7066:
Inference in First-Order Logic Before Warren’s work on the compilation of inference in Prolog, logic programming was too slow for general use.

Token 7067:
Compilers by Warren and others allowed Prolog code to achievespeeds that are competitive with C on a variety of standard benchmarks (Van Roy, 1990).Of course, the fact that one can write a planner or natural language parser in a few dozenlines of Prolog makes it somewhat more desirable than C for prototyping most small-scale AI research projects.

Token 7068:
Parallelization can also provide substantial speedup. There are two principal sources of parallelism.

Token 7069:
The ﬁrst, called OR-parallelism , comes from the possibility of a goal unifying OR-PARALLELISM with many different clauses in the knowledge base.

Token 7070:
Each gives rise to an independent branch in the search space that can lead to a potential solution, and all such branches can be solvedin parallel.

Token 7071:
The second, called AND-parallelism , comes from the possibility of solving AND-PARALLELISM each conjunct in the body of an implication in parallel.

Token 7072:
AND-parallelism is more difﬁcult to achieve, because solutions for the whole conjunction require consistent bindings for all thevariables.

Token 7073:
Each conjunctive branch must communicate with the other branches to ensure aglobal solution.

Token 7074:
9.4.4 Redundant inference and inﬁnite loops We now turn to the Achilles heel of Prolog: the mismatch between depth-ﬁrst search andsearch trees that include repeated states and inﬁnite paths.

Token 7075:
Consider the following logic pro-gram that decides if a path exists between two points on a directed graph: path(X,Z) :- link(X,Z).

Token 7076:
path(X,Z) :- path(X,Y), link(Y,Z). A simple three-node graph, described by the facts link(a,b) andlink(b,c) ,i ss h o w n in Figure 9.9(a).

Token 7077:
With this program, the query path(a,c) generates the proof tree shown in Figure 9.10(a).

Token 7078:
On the other hand, if we put the two clauses in the order path(X,Z) :- path(X,Y), link(Y,Z). path(X,Z) :- link(X,Z).

Token 7079:
then Prolog follows the inﬁnite path shown in Figure 9.10(b).

Token 7080:
Prolog is therefore incomplete as a theorem prover for deﬁnite clauses—even for Datalog programs, as this example shows—because, for some knowledge bases, it fails to prove sentences that are entailed.

Token 7081:
Notice thatforward chaining does not suffer from this problem: once path(a,b) ,path(b,c) ,a n d path(a,c) are inferred, forward chaining halts.

Token 7082:
Depth-ﬁrst backward chaining also has problems with redundant computations.

Token 7083:
For example, when ﬁnding a path from A 1toJ4in Figure 9.9(b), Prolog performs 877 inferences, most of which involve ﬁnding all possible paths to nodes from which the goal is unreachable.This is similar to the repeated-state problem discussed in Chapter 3.

Token 7084:
The total amount ofinference can be exponential in the number of ground facts that are generated.

Token 7085:
If we apply forward chaining instead, at most n 2path(X,Y) facts can be generated linking nnodes.

Token 7086:
For the problem in Figure 9.9(b), only 62 inferences are needed.

Token 7087:
Forward chaining on graph search problems is an example of dynamic programming ,DYNAMIC PROGRAMMING in which the solutions to subproblems are constructed incrementally from those of smaller

Token 7088:
Section 9.4. Backward Chaining 343 (a) (b)ABCA1 J4 Figure 9.9 (a) Finding a path from AtoCcan lead Prolog into an inﬁnite loop.

Token 7089:
(b) A graph in which each node is connected to two random successors in the next layer. Finding a path from A1toJ4requires 877 inferences.

Token 7090:
path(a,c) fail {}/Y b{ }link(a,c) path(a,Y) link(a,Y)link(b,c)path(a,c) path(a,Y) link(Y,c) path(a,Y’) link(Y’,Y) (a) (b) Figure 9.10 (a) Proof that a path exists from AtoC.

Token 7091:
(b) Inﬁnite proof tree generated when the clauses are in the “wrong” order. subproblems and are cached to avoid recomputation.

Token 7092:
We can obtain a similar effect in a backward chaining system using memoization —that is, caching solutions to subgoals as they are found and then reusing those solutions when the subgoal recurs, rather than repeat-ing the previous computation.

Token 7093:
This is the approach taken by tabled logic programming sys- TABLED LOGIC PROGRAMMING tems, which use efﬁcient storage and retrieval mechanisms to perform memoization.

Token 7094:
Tabled logic programming combines the goal-directedness of backward chaining with the dynamic- programming efﬁciency of forward chaining.

Token 7095:
It is also complete for Datalog knowledge bases, which means that the programmer need worry less about inﬁnite loops.

Token 7096:
(It is still pos-sible to get an inﬁnite loop with predicates like father(X,Y) that refer to a potentially unbounded number of objects.)

Token 7097:
9.4.5 Database semantics of Prolog Prolog uses database semantics, as discussed in Section 8.2.8.

Token 7098:
The unique names assumptionsays that every Prolog constant and every ground term refers to a distinct object, and theclosed world assumption says that the only sentences that are true are those that are entailed

Token 7099:
344 Chapter 9. Inference in First-Order Logic by the knowledge base. There is no way to assert that a sentence is false in Prolog.

Token 7100:
This makes Prolog less expressive than ﬁrst-order logic, but it is part of what makes Prolog more efﬁcientand more concise.

Token 7101:
Consider the following Prolog assertions about some course offerings: Course (CS,101),Course (CS,102),Course (CS,106),Course (EE,101).

Token 7102:
(9.11) Under the unique names assumption, CSand EEare different (as are 101, 102, and 106), so this means that there are four distinct courses.

Token 7103:
Under the closed-world assumption thereare no other courses, so there are exactly four courses.

Token 7104:
But if these were assertions in FOLrather than in Prolog, then all we could say is that there are somewhere between one andinﬁnity courses.

Token 7105:
That’s because the assertions (in FOL) do not deny the possibility that otherunmentioned courses are also offered, nor do they say that the courses mentioned are different from each other.

Token 7106:
If we wanted to translate Equation (9.11) into FOL, we would get this: Course (d,n)⇔(d=CS∧n= 101)∨(d=CS∧n= 102) ∨(d=CS∧n= 106)∨(d=EE∧n= 101) .

Token 7107:
(9.12) This is called the completion of Equation (9.11). It expresses in FOL the idea that there are COMPLETION at most four courses.

Token 7108:
To express in FOL the idea that there are at least four courses, we need to write the completion of the equality predicate: x=y⇔(x=CS∧y=CS)∨(x=EE∧y=EE)∨(x= 101∧y= 101) ∨(x= 102∧y= 102)∨(x= 106∧y= 106) .

Token 7109:
The completion is useful for understanding database semantics, but for practical purposes, if your problem can be described with database semantics, it is more efﬁcient to reason withProlog or some other database semantics system, rather than translating into FOL and rea-soning with a full FOL theorem prover.

Token 7110:
9.4.6 Constraint logic programming In our discussion of forward chaining (Section 9.3), we showed how constraint satisfactionproblems (CSPs) can be encoded as deﬁnite clauses.

Token 7111:
Standard Prolog solves such problemsin exactly the same way as the backtracking algorithm given in Figure 6.5.

Token 7112:
Because backtracking enumerates the domains of the variables, it works only for ﬁnite- domain CSPs.

Token 7113:
In Prolog terms, there must be a ﬁnite number of solutions for any goal with unbound variables.

Token 7114:
(For example, the goal diff(Q,SA) , which says that Queensland and South Australia must be different colors, has six solutions if three colors are allowed.)

Token 7115:
Inﬁnite-domain CSPs—for example, with integer or real-valued variables—require quite dif- ferent algorithms, such as bounds propagation or linear programming.

Token 7116:
Consider the following example.

Token 7117:
We deﬁne triangle(X,Y,Z) as a predicate that holds if the three arguments are numbers that satisfy the triangle inequality: triangle(X,Y,Z) :- X>0, Y>0, Z>0, X+Y>=Z, Y+Z>=X, X+Z>=Y.

Token 7118:
If we ask Prolog the query triangle(3,4,5) , it succeeds.

Token 7119:
On the other hand, if we asktriangle(3,4,Z) , no solution will be found, because the subgoal Z>=0 cannot be handled by Prolog; we can’t compare an unbound value to 0.

Token 7120:
Section 9.5. Resolution 345 Constraint logic programming (CLP) allows variables to be constrained rather thanCONSTRAINT LOGIC PROGRAMMING bound .

Token 7121:
A CLP solution is the most speciﬁc set of constraints on the query variables that can be derived from the knowledge base.

Token 7122:
For example, the solution to the triangle(3,4,Z) query is the constraint 7> =Z> =1 .

Token 7123:
Standard logic programs are just a special case of CLP in which the solution constraints must be equality constraints—that is, bindings.

Token 7124:
CLP systems incorporate various constraint-solving algorithms for the constraints al- lowed in the language.

Token 7125:
For example, a system that allows linear inequalities on real-valuedvariables might include a linear programming algorithm for solving those constraints.

Token 7126:
CLPsystems also adopt a much more ﬂexible approach to solving standard logic programmingqueries.

Token 7127:
For example, instead of depth-ﬁrst, left-to-right backtracking, they might use any ofthe more efﬁcient algorithms discussed in Chapter 6, including heuristic conjunct ordering,backjumping, cutset conditioning, and so on.

Token 7128:
CLP systems therefore combine elements ofconstraint satisfaction algorithms, logic programming, and deductive databases.

Token 7129:
Several systems that allow the programmer more control over the search order for in- ference have been deﬁned.

Token 7130:
The MRS language (Genesereth and Smith, 1981; Russell, 1985)allows the programmer to write metarules to determine which conjuncts are tried ﬁrst.

Token 7131:
The METARULE user could write a rule saying that the goal with the fewest variables should be tried ﬁrst or could write domain-speciﬁc rules for particular predicates.

Token 7132:
9.5 R ESOLUTION The last of our three families of logical systems is based on resolution .

Token 7133:
We saw on page 250 that propositional resolution using refutation is a complete inference procedure for proposi-tional logic.

Token 7134:
In this section, we describe how to extend resolution to ﬁrst-order logic.

Token 7135:
9.5.1 Conjunctive normal form for ﬁrst-order logic As in the propositional case, ﬁrst-order resolution requires that sentences be in conjunctive normal form (CNF)—that is, a conjunction of clauses, where each clause is a disjunction of literals.6Literals can contain variables, which are assumed to be universally quantiﬁed.

Token 7136:
For example, the sentence ∀xAmerican (x)∧Weapon (y)∧Sells(x,y,z)∧Hostile (z)⇒Criminal (x) becomes, in CNF, ¬American (x)∨¬Weapon (y)∨¬Sells(x,y,z)∨¬Hostile (z)∨Criminal (x).

Token 7137:
Every sentence of ﬁrst-order logic can be converted into an inferentially equivalent CNF sentence.

Token 7138:
In particular, the CNF sentence will be unsatisﬁable just when the original sentence is unsatisﬁable, so we have a basis for doing proofs by contradiction on the CNF sentences.

Token 7139:
6A clause can also be represented as an implication with a conjunction of atoms in the premise and a disjunction of atoms in the conclusion (Exercise 7.13).

Token 7140:
This is called implicative normal form orKowalski form (especially when written with a right-to-left implication symbol (Kowalski, 1979)) and is often much easier to read.

Token 7141:
346 Chapter 9. Inference in First-Order Logic The procedure for conversion to CNF is similar to the propositional case, which we saw on page 253.

Token 7142:
The principal difference arises from the need to eliminate existential quantiﬁers.We illustrate the procedure by translating the sentence “Everyone who loves all animals isloved by someone,” or ∀x[∀yAnimal (y)⇒Loves(x,y)]⇒[∃yLoves(y,x)].

Token 7143:
The steps are as follows: •Eliminate implications : ∀x[¬∀y¬Animal (y)∨Loves(x,y)]∨[∃yLoves(y,x)].

Token 7144:
•Move¬inwards : In addition to the usual rules for negated connectives, we need rules for negated quantiﬁers.

Token 7145:
Thus, we have ¬∀xp becomes∃x¬p ¬∃xp becomes∀x¬p.

Token 7146:
Our sentence goes through the following transformations: ∀x[∃y¬(¬Animal (y)∨Loves(x,y))]∨[∃yLoves(y,x)].

Token 7147:
∀ x[∃y¬¬Animal (y)∧¬Loves(x,y)]∨[∃yLoves(y,x)]. ∀x[∃yAnimal (y)∧¬Loves(x,y)]∨[∃yLoves(y,x)].

Token 7148:
Notice how a universal quantiﬁer ( ∀y) in the premise of the implication has become an existential quantiﬁer.

Token 7149:
The sentence now reads “Either there is some animal that x doesn’t love, or (if this is not the case) someone loves x.” Clearly, the meaning of the original sentence has been preserved.

Token 7150:
•Standardize variables : For sentences like (∃xP(x))∨(∃xQ(x))which use the same variable name twice, change the name of one of the variables.

Token 7151:
This avoids confusionlater when we drop the quantiﬁers. Thus, we have ∀x[∃yAnimal (y)∧¬Loves(x,y)]∨[∃zLoves(z,x)].

Token 7152:
•Skolemize :Skolemization is the process of removing existential quantiﬁers by elimi- SKOLEMIZATION nation.

Token 7153:
In the simple case, it is just like the Existential Instantiation rule of Section 9.1: translate∃xP(x)intoP(A),w h e r e Ais a new constant.

Token 7154:
However, we can’t apply Ex- istential Instantiation to our sentence above because it doesn’t match the pattern ∃vα; only parts of the sentence match the pattern.

Token 7155:
If we blindly apply the rule to the twomatching parts we get ∀x[Animal (A)∧¬Loves(x,A)]∨Loves(B,x), which has the wrong meaning entirely: it says that everyone either fails to love a par- ticular animal Aor is loved by some particular entity B.

Token 7156:
In fact, our original sentence allows each person to fail to love a different animal or to be loved by a different person.Thus, we want the Skolem entities to depend on xandz: ∀x[Animal (F(x))∧¬Loves(x,F(x))]∨Loves(G(z),x).

Token 7157:
HereFandGareSkolem functions .

Token 7158:
The general rule is that the arguments of the SKOLEM FUNCTION Skolem function are all the universally quantiﬁed variables in whose scope the exis- tential quantiﬁer appears.

Token 7159:
As with Existential Instantiation, the Skolemized sentence issatisﬁable exactly when the original sentence is satisﬁable.

Token 7160:
Section 9.5. Resolution 347 •Drop universal quantiﬁers : At this point, all remaining variables must be universally quantiﬁed.

Token 7161:
Moreover, the sentence is equivalent to one in which all the universal quan-tiﬁers have been moved to the left.

Token 7162:
We can therefore drop the universal quantiﬁers: [Animal (F(x))∧¬Loves(x,F(x))]∨Loves(G(z),x).

Token 7163:
•Distribute∨over∧: [Animal (F(x))∨Loves(G(z),x)]∧[¬Loves(x,F(x))∨Loves(G(z),x)].

Token 7164:
This step may also require ﬂattening out nested conjunctions and disjunctions. The sentence is now in CNF and consists of two clauses.

Token 7165:
It is quite unreadable.

Token 7166:
(It may help to explain that the Skolem function F(x)refers to the animal potentially unloved by x, whereas G(z)refers to someone who might love x.)

Token 7167:
Fortunately, humans seldom need look at CNF sentences—the translation process is easily automated.

Token 7168:
9.5.2 The resolution inference rule The resolution rule for ﬁrst-order clauses is simply a lifted version of the propositional reso-lution rule given on page 253.

Token 7169:
Two clauses, which are assumed to be standardized apart sothat they share no variables, can be resolved if they contain complementary literals.

Token 7170:
Propo-sitional literals are complementary if one is the negation of the other; ﬁrst-order literals are complementary if one uniﬁes with the negation of the other.

Token 7171:
Thus, we have /lscript 1∨···∨ /lscriptk,m 1∨···∨ mn SUBST(θ,/lscript1∨···∨ /lscripti−1∨/lscripti+1∨···∨ /lscriptk∨m1∨···∨ mj−1∨mj+1∨···∨ mn) where U NIFY(/lscripti,¬mj)=θ.

Token 7172:
For example, we can resolve the two clauses [Animal (F(x))∨Loves(G(x),x)]and[¬Loves(u,v)∨¬Kills(u,v)] by eliminating the complementary literals Loves(G(x),x)and¬Loves(u,v), with uniﬁer θ={u/G(x),v/ x}, to produce the resolvent clause [Animal (F(x))∨¬Kills(G(x),x)].

Token 7173:
This rule is called the binary resolution rule because it resolves exactly two literals.

Token 7174:
The BINARY RESOLUTION binary resolution rule by itself does not yield a complete inference procedure.

Token 7175:
The full reso- lution rule resolves subsets of literals in each clause that are uniﬁable.

Token 7176:
An alternative approachis to extend factoring —the removal of redundant literals—to the ﬁrst-order case.

Token 7177:
Proposi- tional factoring reduces two literals to one if they are identical ; ﬁrst-order factoring reduces two literals to one if they are uniﬁable .

Token 7178:
The uniﬁer must be applied to the entire clause. The combination of binary resolution and factoring is complete.

Token 7179:
9.5.3 Example proofs Resolution proves that KB|=αby proving KB∧¬αunsatisﬁable, that is, by deriving the empty clause.

Token 7180:
The algorithmic approach is identical to the propositional case, described in

Token 7181:
348 Chapter 9.

Token 7182:
Inference in First-Order Logic ¬American(x) ¬Weapon(y) ¬Sells(x,y,z) ¬Hostile(z) Criminal(x) ¬Criminal(West) ¬Enemy(Nono, America) Enemy(Nono,America)¬Missile(x) Weapon(x) ¬Weapon(y) ¬Sells(West,y,z) ¬Hostile(z) Missile(M1) ¬Missile(y) ¬Sells(West,y,z) ¬Hostile(z) ¬Missile(x) ¬Owns(Nono,x) Sells(West,x,Nono) ¬Sells(West,M1,z) ¬Hostile(z)¬American(West) ¬Weapon(y) ¬Sells(West,y,z) ¬Hostile(z) American(West) ¬Missile(M1) ¬Owns(Nono,M1) ¬Hostile(Nono) Missile(M1) ¬Owns(Nono,M1) ¬Hostile(Nono) Owns(Nono,M1) ¬Enemy(x,America) Hostile(x) ¬Hostile(Nono) ^ ^^ ^^ ^ ^^ ^ ^^ ^^ ^ ^^ ^^^ Figure 9.11 A resolution proof that West is a criminal.

Token 7183:
At each step, the literals that unify are in bold. Figure 7.12, so we need not repeat it here. Instead, we give two example proofs.

Token 7184:
The ﬁrst is the crime example from Section 9.3.

Token 7185:
The sentences in CNF are ¬American (x)∨¬Weapon (y)∨¬Sells(x,y,z)∨¬Hostile (z)∨Criminal (x) ¬Missile (x)∨¬Owns(Nono,x)∨Sells(West,x,Nono) ¬Enemy (x,America )∨Hostile (x) ¬Missile (x)∨Weapon (x) Owns(Nono,M1) Missile (M1) American (West) Enemy (Nono,America ).

Token 7186:
We also include the negated goal ¬Criminal (West). The resolution proof is shown in Fig- ure 9.11.

Token 7187:
Notice the structure: single “spine” beginning with the goal clause, resolving againstclauses from the knowledge base until the empty clause is generated.

Token 7188:
This is characteristicof resolution on Horn clause knowledge bases.

Token 7189:
In fact, the clauses along the main spinecorrespond exactly to the consecutive values of the goals variable in the backward-chaining algorithm of Figure 9.6.

Token 7190:
This is because we always choose to resolve with a clause whose positive literal uniﬁed with the leftmost literal of the “current” clause on the spine; this is exactly what happens in backward chaining.

Token 7191:
Thus, backward chaining is just a special case of resolution with a particular control strategy to decide which resolution to perform next.

Token 7192:
Our second example makes use of Skolemization and involves clauses that are not def- inite clauses.

Token 7193:
This results in a somewhat more complex proof structure. In English, the problem is as follows: Everyone who loves all animals is loved by someone.

Token 7194:
Anyone who kills an animal is loved by no one. Jack loves all animals.Either Jack or Curiosity killed the cat, who is named Tuna.

Token 7195:
Did Curiosity kill the cat?

Token 7196:
Section 9.5.

Token 7197:
Resolution 349 First, we express the original sentences, some background knowledge, and the negated goal G in ﬁrst-order logic: A.∀x[∀yAnimal (y)⇒Loves(x,y)]⇒[∃yLoves(y,x)] B.∀x[∃zAnimal (z)∧Kills(x,z)]⇒[∀y¬Loves(y,x)] C.∀xAnimal (x)⇒Loves(Jack,x) D.Kills(Jack,Tuna)∨Kills(Curiosity ,Tuna) E.Cat(Tuna) F.∀xCat(x)⇒Animal (x) ¬G.¬Kills(Curiosity ,Tuna) Now we apply the conversion procedure to convert each sentence to CNF: A1.

Token 7198:
Animal (F(x))∨Loves(G(x),x) A2.¬Loves(x,F(x))∨Loves(G(x),x) B.¬Loves(y,x)∨¬Animal (z)∨¬Kills(x,z) C.¬Animal (x)∨Loves(Jack,x) D.Kills(Jack,Tuna)∨Kills(Curiosity ,Tuna) E.Cat(Tuna) F.¬Cat(x)∨Animal (x) ¬G.¬Kills(Curiosity ,Tuna) The resolution proof that Curiosity killed the cat is given in Figure 9.12.

Token 7199:
In English, the proof could be paraphrased as follows: Suppose Curiosity did not kill Tuna.

Token 7200:
We know that either Jack or Curiosity did; thus Jack must have. Now, Tuna is a cat and cats a re animals, so Tuna is an animal.

Token 7201:
Because anyone who kills an animal is loved by no one, we know that no one loves Jack.

Token 7202:
On the other hand, Jack loves all animals, so someone loves him; so we have a contradiction.Therefore, Curiosity killed the cat.

Token 7203:
¬Loves (y, Jack ) Loves (G(Jack ),Jack )¬Kills (Curiosity , Tuna ) Kills (Jack, Tuna ) Kills (Curiosity , Tuna ) ¬Cat(x) Animal (x) Cat(Tuna ) ¬Animal (F(Jack )) Loves (G(Jack), Jack)Animal (F(x)) Loves (G(x), x) ¬Loves (y, x) ¬Kills (x, Tuna )Kills (Jack , Tuna ) ¬Loves (y, x) ¬Animal (z) ¬Kills (x, z) Animal (Tuna ) ¬Loves (x,F(x)) Loves (G(x), x) ¬Animal (x) Loves (Jack , x) ^ ^^ ^ ^ ^^^ ^ Figure 9.12 A resolution proof that Curiosity killed the cat.

Token 7204:
Notice the use of factoring in the derivation of the clause Loves (G(Jack),Jack).

Token 7205:
Notice also in the upper right, the uniﬁcation of Loves (x, F(x))andLoves(Jack,x )can only succeed after the variables have been standardized apart.

Token 7206:
350 Chapter 9.

Token 7207:
Inference in First-Order Logic The proof answers the question “Did Curiosity kill the cat?” but often we want to pose more general questions, such as “Who killed the cat?” Resolution can do this, but it takes a littlemore work to obtain the answer.

Token 7208:
The goal is ∃wKills(w,Tuna), which, when negated, becomes¬Kills(w,Tuna)in CNF.

Token 7209:
Repeating the proof in Figure 9.12 with the new negated goal, we obtain a similar proof tree, but with the substitution {w/Curiosity}in one of the steps.

Token 7210:
So, in this case, ﬁnding out who killed the cat is just a matter of keeping track of the bindings for the query variables in the proof.

Token 7211:
Unfortunately, resolution can produce nonconstructive proofs for existential goals.

Token 7212:
NONCONSTRUCTIVE PROOF For example,¬Kills(w,Tuna)resolves with Kills(Jack,Tuna)∨Kills(Curiosity ,Tuna) to give Kills(Jack,Tuna), which resolves again with ¬Kills(w,Tuna)to yield the empty clause.

Token 7213:
Notice that whas two different bindings in this proof; resolution is telling us that, yes, someone killed Tuna—either Jack or Curiosity.

Token 7214:
This is no great surprise!

Token 7215:
One so-lution is to restrict the allowed resolution steps so that the query variables can be boundonly once in a given proof; then we need to be able to backtrack over the possible bind-ings.

Token 7216:
Another solution is to add a special answer literal to the negated goal, which be- ANSWER LITERAL comes¬Kills(w,Tuna)∨Answer (w).

Token 7217:
Now, the resolution process generates an answer whenever a clause is generated containing just a single answer literal.

Token 7218:
For the proof in Fig- ure 9.12, this is Answer (Curiosity ).

Token 7219:
The nonconstructive proof would generate the clause Answer (Curiosity )∨Answer (Jack), which does not constitute an answer.

Token 7220:
9.5.4 Completeness of resolution This section gives a completeness proof of resolution.

Token 7221:
It can be safely skipped by those who are willing to take it on faith.

Token 7222:
We show that resolution is refutation-complete , which means that ifa set of sentencesREFUTATION COMPLETENESS is unsatisﬁable, then resolution will always be able to derive a contradiction.

Token 7223:
Resolution cannot be used to generate all logical consequences of a set of sentences, but it can be usedto establish that a given sentence is entailed by the set of sentences.

Token 7224:
Hence, it can be used toﬁnd all answers to a given question, Q(x), by proving that KB∧¬Q(x)is unsatisﬁable.

Token 7225:
We take it as given that any sentence in ﬁrst-order logic (without equality) can be rewrit- ten as a set of clauses in CNF.

Token 7226:
This can be proved by induction on the form of the sentence, using atomic sentences as the base case (Davis and Putnam, 1960).

Token 7227:
Our goal therefore is to prove the following: ifSis an unsatisﬁable set of clauses, then the application of a ﬁnite number of resolution steps to Swill yield a contradiction.

Token 7228:
Our proof sketch follows Robinson’s original proof with some simpliﬁcations from Genesereth and Nilsson (1987).

Token 7229:
The basic structure of the proof (Figure 9.13) is as follows: 1.

Token 7230:
First, we observe that if Sis unsatisﬁable, then there exists a particular set of ground instances of the clauses of Ssuch that this set is also unsatisﬁable (Herbrand’s theorem).

Token 7231:
2. We then appeal to the ground resolution theorem given in Chapter 7, which states that propositional resolution is complete for ground sentences. 3.

Token 7232:
We then use a lifting lemma to show that, for any propositional resolution proof using the set of ground sentences, there is a corresponding ﬁrst-order resolution proof usingthe ﬁrst-order sentences from which the ground sentences were obtained.

Token 7233:
Section 9.5.

Token 7234:
Resolution 351 Resolution can find a contradiction in S' There is a resolution proof for the contradiction in S'Herbrand’s theorem Some set S' of ground instances is unsatisfiableAny set of sentences S is representable in clausal form Assume S is unsatisfiable, and in clausal form Lifting lemmaGround resolution theorem Figure 9.13 Structure of a completeness proof for resolution.

Token 7235:
To carry out the ﬁrst step, we need three new concepts: •Herbrand universe :I fSis a set of clauses, then HS, the Herbrand universe of S,i sHERBRAND UNIVERSE the set of all ground terms constructable from the following: a.

Token 7236:
The function symbols in S,i fa n y . b. The constant symbols in S, if any; if none, then the constant symbol A.

Token 7237:
For example, if Scontains just the clause ¬P(x,F(x,A))∨¬Q(x,A)∨R(x,B),t h e n HSis the following inﬁnite set of ground terms: {A,B,F (A,A),F(A,B),F(B,A),F(B,B),F(A,F(A,A)),...}.

Token 7238:
•Saturation :I fSis a set of clauses and Pis a set of ground terms, then P(S),t h e SATURATION saturation of Swith respect to P, is the set of all ground clauses obtained by applying all possible consistent substitutions of ground terms in Pwith variables in S. •Herbrand base : The saturation of a set Sof clauses with respect to its Herbrand uni- HERBRAND BASE verse is called the Herbrand base of S, written as HS(S).

Token 7239:
For example, if Scontains solely the clause just given, then HS(S)is the inﬁnite set of clauses {¬P(A,F(A,A))∨¬Q(A,A)∨R(A,B), ¬P(B,F(B,A))∨¬Q(B,A)∨R(B,B), ¬P(F(A,A),F(F(A,A),A))∨¬Q(F(A,A),A)∨R(F(A,A),B), ¬P(F(A,B),F(F(A,B),A))∨¬Q(F(A,B),A)∨R(F(A,B),B),...} These deﬁnitions allow us to state a form of Herbrand’s theorem (Herbrand, 1930):HERBRAND’S THEOREM If a set Sof clauses is unsatisﬁable, then there exists a ﬁnite subset of HS(S)that is also unsatisﬁable.

Token 7240:
LetS/primebe this ﬁnite subset of ground sentences.

Token 7241:
Now, we can appeal to the ground resolution theorem (page 255) to show that the resolution closure RC(S/prime)contains the empty clause.

Token 7242:
That is, running propositional resolution to completion on S/primewill derive a contradiction.

Token 7243:
Now that we have established that there is always a resolution proof involving some ﬁnite subset of the Herbrand base of S, the next step is to show that there is a resolution

Token 7244:
352 Chapter 9.

Token 7245:
Inference in First-Order Logic G¨ODEL ’SINCOMPLETENESS THEOREM By slightly extending the language of ﬁrst-order logic to allow for the mathemat- ical induction schema in arithmetic, Kurt G¨ odel was able to show, in his incom- pleteness theorem , that there are true arithmetic sentences that cannot be proved.

Token 7246:
The proof of the incompleteness theorem is somewhat beyond the scope of this book, occupying, as it does, at least 30 pages, but we can give a hint here.

Token 7247:
Webegin with the logical theory of numbers. In this theory, there is a single constant,0, and a single function, S(the successor function).

Token 7248:
In the intended model, S(0) denotes 1, S(S(0)) denotes 2, and so on; the language therefore has names for all the natural numbers.

Token 7249:
The vocabulary also includes the function symbols +,×,a n d Expt (exponentiation) and the usual set of logical connectives and quantiﬁers.

Token 7250:
The ﬁrst step is to notice that the set of sentences that we can write in this language can be enumerated.

Token 7251:
(Imagine deﬁning an alphabetical order on the symbols and thenarranging, in alphabetical order, each of the sets of sentences of length 1, 2, andso on.)

Token 7252:
We can then number each sentence αwith a unique natural number #α (the G¨odel number ).

Token 7253:
This is crucial: number theory contains a name for each of its own sentences.

Token 7254:
Similarly, we can number each possible proof Pwith a G¨ odel number G(P), because a proof is simply a ﬁnite sequence of sentences.

Token 7255:
Now suppose we have a recursively enumerable set Aof sentences that are true statements about the natural numbers.

Token 7256:
Recalling that Acan be named by a given set of integers, we can imagine writing in our language a sentence α(j, A)of the following sort: ∀iiis not the G¨ odel number of a proof of the sentence whose G¨ odel number is j, where the proof uses only premises in A.

Token 7257:
Then let σbe the sentence α(#σ, A), that is, a sentence that states its own unprov- ability from A.

Token 7258:
(That this sentence always exists is true but not entirely obvious.)

Token 7259:
Now we make the following ingenious argument: Suppose that σisprovable fromA;t h e n σis false (because σsays it cannot be proved).

Token 7260:
But then we have a false sentence that is provable from A,s oAcannot consist of only true sentences— a violation of our premise.

Token 7261:
Therefore, σis notprovable from A. But this is exactly whatσitself claims; hence σis a true sentence.

Token 7262:
So, we have shown (barring 291 2pages) that for any set of true sentences of number theory, and in particular any set of basic axioms, there are other true sen-tences that cannot be proved from those axioms.

Token 7263:
This establishes, among other things, that we can never prove all the theorems of mathematics within any given system of axioms .

Token 7264:
Clearly, this was an important discovery for mathematics.

Token 7265:
Its signiﬁcance for AI has been widely debated, beginning with speculations by G¨ odel himself. We take up the debate in Chapter 26.

Token 7266:
Section 9.5. Resolution 353 proof using the clauses of Sitself, which are not necessarily ground clauses.

Token 7267:
We start by considering a single application of the resolution rule.

Token 7268:
Robinson stated this lemma: LetC1andC2be two clauses with no shared variables, and let C/prime 1andC/prime 2be ground instances of C1andC2.I fC/primeis a resolvent of C/prime 1andC/prime 2, then there exists ac l a u s e Csuch that (1) Cis a resolvent of C1andC2and (2) C/primeis a ground instance of C. This is called a lifting lemma , because it lifts a proof step from ground clauses up to general LIFTINGLEMMA ﬁrst-order clauses.

Token 7269:
In order to prove his basic lifting lemma, Robinson had to invent uniﬁ- cation and derive all of the properties of most general uniﬁers.

Token 7270:
Rather than repeat the proofhere, we simply illustrate the lemma: C 1=¬P(x,F(x,A))∨¬Q(x,A)∨R(x,B) C2=¬N(G(y),z)∨P(H(y),z) C/prime 1=¬P(H(B),F(H(B),A))∨¬Q(H(B),A)∨R(H(B),B) C/prime 2=¬N(G(B),F(H(B),A))∨P(H(B),F(H(B),A)) C/prime=¬N(G(B),F(H(B),A))∨¬Q(H(B),A)∨R(H(B),B) C=¬N(G(y),F(H(y),A))∨¬Q(H(y),A)∨R(H(y),B).

Token 7271:
We see that indeed C/primeis a ground instance of C. In general, for C/prime 1andC/prime 2to have any resolvents, they must be constructed by ﬁrst applying to C1andC2the most general uniﬁer of a pair of complementary literals in C1andC2.

Token 7272:
From the lifting lemma, it is easy to derive a similar statement about any sequence of applications of the resolution rule: For any clause C/primein the resolution closure of S/primethere is a clause Cin the resolu- tion closure of Ssuch that C/primeis a ground instance of Cand the derivation of Cis the same length as the derivation of C/prime.

Token 7273:
From this fact, it follows that if the empty clause appears in the resolution closure of S/prime,i t must also appear in the resolution closure of S. This is because the empty clause cannot be a ground instance of any other clause.

Token 7274:
To recap: we have shown that if Sis unsatisﬁable, then there is a ﬁnite derivation of the empty clause using the resolution rule.

Token 7275:
The lifting of theorem proving from ground clauses to ﬁrst-order clauses provides a vast increase in power.

Token 7276:
This increase comes from the fact that the ﬁrst-order proof need instantiatevariables only as far as necessary for the proof, whereas the ground-clause methods wererequired to examine a huge number of arbitrary instantiations.

Token 7277:
9.5.5 Equality None of the inference methods described so far in this chapter handle an assertion of the form x=y.

Token 7278:
Three distinct approaches can be taken.

Token 7279:
The ﬁrst approach is to axiomatize equality— to write down sentences about the equality relation in the knowledge base.

Token 7280:
We need to say thatequality is reﬂexive, symmetric, and transitive, and we also have to say that we can substituteequals for equals in any predicate or function.

Token 7281:
So we need three basic axioms, and then one

Token 7282:
354 Chapter 9.

Token 7283:
Inference in First-Order Logic for each predicate and function: ∀xx=x ∀x,y x =y⇒y=x ∀x,y,z x =y∧y=z⇒x=z ∀x,y x =y⇒(P1(x)⇔P1(y)) ∀x,y x =y⇒(P2(x)⇔P2(y)) ... ∀w,x,y,z w =y∧x=z⇒(F1(w,x)=F1(y,z)) ∀w,x,y,z w =y∧x=z⇒(F2(w,x)=F2(y,z)) ...

Token 7284:
Given these sentences, a standard inference procedure such as resolution can perform tasks requiring equality reasoning, such as solving mathematical equations.

Token 7285:
However, these axiomswill generate a lot of conclusions, most of them not helpful to a proof.

Token 7286:
So there has been asearch for more efﬁcient ways of handling equality. One alternative is to add inference rulesrather than axioms.

Token 7287:
The simplest rule, demodulation , takes a unit clause x=yand some clause αthat contains the term x, and yields a new clause formed by substituting yforx within α.

Token 7288:
It works if the term within αuniﬁes with x; it need not be exactly equal to x.

Token 7289:
Note that demodulation is directional; given x=y,t h exalways gets replaced with y,n e v e r vice versa.

Token 7290:
That means that demodulation can be used for simplifying expressions usingdemodulators such as x+0= xorx 1=x.

Token 7291:
As another example, given Father (Father (x)) =PaternalGrandfather (x) Birthdate (Father (Father (Bella)),1926) we can conclude by demodulation Birthdate (PaternalGrandfather (Bella),1926) .

Token 7292:
More formally, we have •Demodulation : For any terms x,y,a n dz,w h e r e zappears somewhere in literal miDEMODULATION and where U NIFY(x,z)=θ, x=y, m 1∨···∨ mn SUB(SUBST(θ,x),SUBST(θ,y),m1∨···∨ mn).

Token 7293:
where S UBST is the usual substitution of a binding list, and S UB(x,y,m )means to replace xwithyeverywhere that xoccurs within m. The rule can also be extended to handle non-unit clauses in which an equality literal appears: •Paramodulation : For any terms x,y,a n dz,w h e r e zappears somewhere in literal mi, PARAMODULATION and where U NIFY(x,z)=θ, /lscript1∨···∨ /lscriptk∨x=y, m 1∨···∨ mn SUB(SUBST(θ,x),SUBST(θ,y),SUBST(θ,/lscript1∨···∨ /lscriptk∨m1∨···∨ mn).

Token 7294:
For example, from P(F(x,B),x)∨Q(x) and F(A,y)=y∨R(y)

Token 7295:
Section 9.5. Resolution 355 we have θ=UNIFY(F(A,y),F(x,B))={x/A,y/B}, and we can conclude by paramodu- lation the sentence P(B,A)∨Q(A)∨R(B).

Token 7296:
Paramodulation yields a complete inference procedure for ﬁrst-order logic with equality.

Token 7297:
A third approach handles equality reasoning entirely within an extended uniﬁcation algorithm.

Token 7298:
That is, terms are uniﬁable if they are provably equal under some substitution, where “provably” allows for equality reasoning.

Token 7299:
For example, the terms 1+2 and2+1 normally are not uniﬁable, but a uniﬁcation algorithm that knows that x+y=y+xcould unify them with the empty substitution.

Token 7300:
Equational uniﬁcation of this kind can be done withEQUATIONAL UNIFICATION efﬁcient algorithms designed for the particular axioms used (commutativity, associativity, and so on) rather than through explicit inference with those axioms.

Token 7301:
Theorem provers using this technique are closely related to the CLP systems described in Section 9.4.

Token 7302:
9.5.6 Resolution strategies We know that repeated applications of the resolution inference rule will eventually ﬁnd aproof if one exists.

Token 7303:
In this subsection, we examine strategies that help ﬁnd proofs efﬁciently .

Token 7304:
Unit preference : This strategy prefers to do resolutions where one of the sentences is a single UNIT PREFERENCE literal (also known as a unit clause ).

Token 7305:
The idea behind the strategy is that we are trying to produce an empty clause, so it might be a good idea to prefer inferences that produce shorter clauses.

Token 7306:
Resolving a unit sentence (such as P) with any other sentence (such as ¬P∨¬Q∨R) always yields a clause (in this case, ¬Q∨R) that is shorter than the other clause.

Token 7307:
When the unit preference strategy was ﬁrst tried for propositional inference in 1964, it led to adramatic speedup, making it feasible to prove theorems that could not be handled without thepreference.

Token 7308:
Unit resolution is a restricted form of resolution in which every resolution step must involve a unit clause.

Token 7309:
Unit resolution is incomplete in general, but complete for Hornclauses. Unit resolution proofs on Horn clauses resemble forward chaining.

Token 7310:
The O TTER theorem prover (Organized Techniques for Theorem-proving and Effective Research, McCune, 1992), uses a form of best-ﬁrst search.

Token 7311:
Its heuristic function measures the “weight” of each clause, where lighter clauses are preferred.

Token 7312:
The exact choice of heuristic is up to the user, but generally, the weight of a clause should be correlated with its size or difﬁculty.

Token 7313:
Unit clauses are treated as light; the search can thus be seen as a generalization of the unit preference strategy.

Token 7314:
Set of support : Preferences that try certain resolutions ﬁrst are helpful, but in general it is SET OF SUPPORT more effective to try to eliminate some potential resolutions altogether.

Token 7315:
For example, we can insist that every resolution step involve at least one element of a special set of clauses—theset of support .

Token 7316:
The resolvent is then added into the set of support.

Token 7317:
If the set of support is small relative to the whole knowledge base, the search space will be reduced dramatically.

Token 7318:
We have to be careful with this approach because a bad choice for the set of support will make the algorithm incomplete.

Token 7319:
However, if we choose the set of support Sso that the remainder of the sentences are jointly satisﬁable, then set-of-support resolution is complete.For example, one can use the negated query as the set of support, on the assumption that the

Token 7320:
356 Chapter 9. Inference in First-Order Logic original knowledge base is consistent.

Token 7321:
(After all, if it is not consistent, then the fact that the query follows from it is vacuous.)

Token 7322:
The set-of-support strategy has the additional advantage ofgenerating goal-directed proof trees that are often easy for humans to understand.

Token 7323:
Input resolution : In this strategy, every resolution combines one of the input sentences (from INPUT RESOLUTION the KB or the query) with some other sentence.

Token 7324:
The proof in Figure 9.11 on page 348 uses only input resolutions and has the characteristic shape of a single “spine” with single sen-tences combining onto the spine.

Token 7325:
Clearly, the space of proof trees of this shape is smallerthan the space of all proof graphs.

Token 7326:
In Horn knowledge bases, Modus Ponens is a kind ofinput resolution strategy, because it combines an implication from the original KB with someother sentences.

Token 7327:
Thus, it is no surprise that input resolution is complete for knowledge basesthat are in Horn form, but incomplete in the general case.

Token 7328:
The linear resolution strategy is a LINEAR RESOLUTION slight generalization that allows PandQto be resolved together either if Pis in the original KBor ifPis an ancestor of Qin the proof tree.

Token 7329:
Linear resolution is complete.

Token 7330:
Subsumption : The subsumption method eliminates all sentences that are subsumed by (that SUBSUMPTION is, more speciﬁc than) an existing sentence in the KB.

Token 7331:
For example, if P(x)is in the KB, then there is no sense in adding P(A)and even less sense in adding P(A)∨Q(B).

Token 7332:
Subsumption helps keep the KB small and thus helps keep the search space small.

Token 7333:
Practical uses of resolution theorem provers Theorem provers can be applied to the problems involved in the synthesis andveriﬁcation SYNTHESIS VERIFICATION of both hardware and software.

Token 7334:
Thus, theorem-proving research is carried out in the ﬁelds of hardware design, programming languages, and software engineering—not just in AI.

Token 7335:
In the case of hardware, the axioms describe the interactions between signals and cir- cuit elements. (See Section 8.4.2 on page 309 for an example.)

Token 7336:
Logical reasoners designed specially for veriﬁcation have been able to verify entire CPUs, including their timing prop-erties (Srivas and Bickford, 1990).

Token 7337:
The A URA theorem prover has been applied to design circuits that are more compact than any previous design (Wojciechowski and Wojcik, 1983).

Token 7338:
In the case of software, reasoning about programs is quite similar to reasoning about actions, as in Chapter 7: axioms describe the preconditions and effects of each statement.The formal synthesis of algorithms was one of the ﬁrst uses of theorem provers, as outlinedby Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963).

Token 7339:
The ideais to constructively prove a theorem to the effect that “there exists a program psatisfying a certain speciﬁcation.” Although fully automated deductive synthesis , as it is called, has not DEDUCTIVE SYNTHESIS yet become feasible for general-purpose programming, hand-guided deductive synthesis has been successful in designing several novel and sophisticated algorithms.

Token 7340:
Synthesis of special-purpose programs, such as scientiﬁc computing code, is also an active area of research.

Token 7341:
Similar techniques are now being applied to software veriﬁcation by systems such as the S PINmodel checker (Holzmann, 1997).

Token 7342:
For example, the Remote Agent spacecraft control program was veriﬁed before and after ﬂight (Havelund et al. , 2000).

Token 7343:
The RSA public key encryption algorithm and the Boyer–Moore string-matching algorithm have been veriﬁed thisway (Boyer and Moore, 1984).

Token 7344:
Section 9.6.

Token 7345:
Summary 357 9.6 S UMMARY We have presented an analysis of logical inference in ﬁrst-order logic and a number of algo- rithms for doing it.

Token 7346:
•A ﬁrst approach uses inference rules ( universal instantiation andexistential instan- tiation )t opropositionalize the inference problem.

Token 7347:
Typically, this approach is slow, unless the domain is small.

Token 7348:
•The use of uniﬁcation to identify appropriate substitutions for variables eliminates the instantiation step in ﬁrst-order proofs, making the process more efﬁcient in many cases.

Token 7349:
•A lifted version of Modus Ponens uses uniﬁcation to provide a natural and powerful inference rule, generalized Modus Ponens .T h e forward-chaining andbackward- chaining algorithms apply this rule to sets of deﬁnite clauses.

Token 7350:
•Generalized Modus Ponens is complete for deﬁnite clauses, although the entailment problem is semidecidable .F o r Datalog knowledge bases consisting of function-free deﬁnite clauses, entailment is decidable.

Token 7351:
•Forward chaining is used in deductive databases , where it can be combined with re- lational database operations.

Token 7352:
It is also used in production systems , which perform efﬁcient updates with very large rule sets.

Token 7353:
Forward chaining is complete for Datalog and runs in polynomial time.

Token 7354:
•Backward chaining is used in logic programming systems , which employ sophisti- cated compiler technology to provide very fast inference.

Token 7355:
Backward chaining suffersfrom redundant inferences and inﬁnite loops; these can be alleviated by memoization .

Token 7356:
•Prolog, unlike ﬁrst-order logic, uses a closed world with the unique names assumption and negation as failure.

Token 7357:
These make Prolog a more practical programming language, but bring it further from pure logic.

Token 7358:
•The generalized resolution inference rule provides a complete proof system for ﬁrst- order logic, using knowledge bases in conjunctive normal form.

Token 7359:
•Several strategies exist for reducing the search space of a resolution system without compromising completeness.

Token 7360:
One of the most important issues is dealing with equality;we showed how demodulation andparamodulation can be used.

Token 7361:
•Efﬁcient resolution-based theorem provers have been used to prove interesting mathe- matical theorems and to verify and synthesize software and hardware.

Token 7362:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Gottlob Frege, who developed full ﬁrst-order logic in 1879, based his system of inferenceon a collection of valid schemas plus a single inference rule, Modus Ponens.

Token 7363:
Whiteheadand Russell (1910) expounded the so-called rules of passage (the actual term is from Her- brand (1930)) that are used to move quantiﬁers to the front of formulas.

Token 7364:
Skolem constants

Token 7365:
358 Chapter 9. Inference in First-Order Logic and Skolem functions were introduced, appropriately enough, by Thoralf Skolem (1920).

Token 7366:
Oddly enough, it was Skolem who introduced the Herbrand universe (Skolem, 1928).

Token 7367:
Herbrand’s theorem (Herbrand, 1930) has played a vital role in the development of automated reasoning. Herbrand is also the inventor of uniﬁcation.

Token 7368:
G¨ odel (1930) built on the ideas of Skolem and Herbrand to show that ﬁrst-order logic has a complete proof pro- cedure.

Token 7369:
Alan Turing (1936) and Alonzo Church (1936) simultaneously showed, using very different proofs, that validity in ﬁrst-order logic was not decidable.

Token 7370:
The excellent text byEnderton (1972) explains all of these results in a rigorous yet understandable fashion.

Token 7371:
Abraham Robinson proposed that an automated reasoner could be built using proposi- tionalization and Herbrand’s theorem, and Paul Gilmore (1960) wrote the ﬁrst program.

Token 7372:
Davisand Putnam (1960) introduced the propositionalization method of Section 9.1.

Token 7373:
Prawitz (1960)developed the key idea of letting the quest for propositional inconsistency drive the search,and generating terms from the Herbrand universe only when they were necessary to estab-lish propositional inconsistency.

Token 7374:
After further development by other researchers, this idea ledJ. A. Robinson (no relation) to develop resolution (Robinson, 1965).

Token 7375:
In AI, resolution was adopted for question-answering systems by Cordell Green and Bertram Raphael (1968).

Token 7376:
Early AI implementations put a good deal of effort into data struc- tures that would allow efﬁcient retrieval of facts; this work is covered in AI programming texts (Charniak et al.

Token 7377:
, 1987; Norvig, 1992; Forbus and de Kleer, 1993).

Token 7378:
By the early 1970s, forward chaining was well established in AI as an easily understandable alternative to res- olution.

Token 7379:
AI applications typically involved large numbers of rules, so it was important to develop efﬁcient rule-matching technology, particularly for incremental updates.

Token 7380:
The tech- nology for production systems was developed to support such applications.

Token 7381:
The production system language O PS-5 (Forgy, 1981; Brownston et al.

Token 7382:
, 1985), incorporating the efﬁcient rete match process (Forgy, 1982), was used for applications such as the R1 expert system for RETE minicomputer conﬁguration (McDermott, 1982).

Token 7383:
The S OAR cognitive architecture (Laird et al.

Token 7384:
, 1987; Laird, 2008) was designed to han- dle very large rule sets—up to a million rules (Doorenbos, 1994).

Token 7385:
Example applications of SOAR include controlling simulated ﬁghter aircraft (Jones et al. , 1998), airspace manage- ment (Taylor et al.

Token 7386:
, 2007), AI characters for computer games (Wintermute et al. , 2007), and training tools for soldiers (Wray and Jones, 2005).

Token 7387:
The ﬁeld of deductive databases began with a workshop in Toulouse in 1977 that brought together experts in logical inference and database systems (Gallaire and Minker,1978).

Token 7388:
Inﬂuential work by Chandra and Harel (1980) and Ullman (1985) led to the adoptionof Datalog as a standard language for deductive databases.

Token 7389:
The development of the magic sets technique for rule rewriting by Bancilhon et al.

Token 7390:
(1986) allowed forward chaining to borrow the advantage of goal-directedness from backward chaining.

Token 7391:
Current work includes the ideaof integrating multiple databases into a consistent dataspace (Halevy, 2007).

Token 7392:
Backward chaining for logical inference appeared ﬁrst in Hewitt’s P LANNER lan- guage (1969).

Token 7393:
Meanwhile, in 1972, Alain Colmerauer had developed and implemented Pro- logfor the purpose of parsing natural language—Prolog’s clauses were intended initially as context-free grammar rules (Roussel, 1975; Colmerauer et al.

Token 7394:
, 1973). Much of the the- oretical background for logic programming was developed by Robert Kowalski, working

Token 7395:
Bibliographical and Historical Notes 359 with Colmerauer; see Kowalski (1988) and Colmerauer and Roussel (1993) for a historical overview.

Token 7396:
Efﬁcient Prolog compilers are generally based on the Warren Abstract Machine(WAM) model of computation developed by David H. D. Warren (1983).

Token 7397:
Van Roy (1990)showed that Prolog programs can be competitive with C programs in terms of speed.

Token 7398:
Methods for avoiding unnecessary looping in recursive logic programs were developed independently by Smith et al. (1986) and Tamaki and Sato (1986).

Token 7399:
The latter paper also included memoization for logic programs, a method developed extensively as tabled logic programming by David S. Warren.

Token 7400:
Swift and Warren (1994) show how to extend the WAM to handle tabling, enabling Datalog programs to execute an order of magnitude faster thanforward-chaining deductive database systems.

Token 7401:
Early work on constraint logic programming was done by Jaffar and Lassez (1987). Jaffar et al.

Token 7402:
(1992) developed the CLP(R) system for handling real-valued constraints.

Token 7403:
There are now commercial products for solving large-scale conﬁguration and optimization problemswith constraint programming; one of the best known is ILOG (Junker, 2003).

Token 7404:
Answer setprogramming (Gelfond, 2008) extends Prolog, allowing disjunction and negation.

Token 7405:
Texts on logic programming and Prolog, including Shoham (1994), Bratko (2001), Clocksin (2003), and Clocksin and Mellish (2003).

Token 7406:
Prior to 2000, the Journal of Logic Pro- gramming was the journal of record; it has now been replaced by Theory and Practice of Logic Programming .

Token 7407:
Logic programming conferences include the International Conference on Logic Programming (ICLP) and the International Logic Programming Symposium (ILPS).

Token 7408:
Research into mathematical theorem proving began even before the ﬁrst complete ﬁrst-order systems were developed.

Token 7409:
Herbert Gelernter’s Geometry Theorem Prover (Gelern- ter, 1959) used heuristic search methods combined with diagrams for pruning false subgoals and was able to prove some quite intricate results in Euclidean geometry.

Token 7410:
The demodula-tion and paramodulation rules for equality reasoning were introduced by Wos et al. (1967) and Wos and Robinson (1968), respectively.

Token 7411:
These rules were also developed independentlyin the context of term-rewriting systems (Knuth and Bendix, 1970).

Token 7412:
The incorporation ofequality reasoning into the uniﬁcation algorithm is due to Gordon Plotkin (1972).

Token 7413:
Jouannaud and Kirchner (1991) survey equational uniﬁcation from a term-rewriting perspective.

Token 7414:
An overview of uniﬁcation is given by Baader and Snyder (2001).

Token 7415:
A number of control strategies have been proposed for resolution, beginning with the unit preference strategy (Wos et al. , 1964).

Token 7416:
The set-of-support strategy was proposed by Wos et al. (1965) to provide a degree of goal-directedness in resolution.

Token 7417:
Linear resolution ﬁrst appeared in Loveland (1970).

Token 7418:
Genesereth and Nilsson (1987, Chapter 5) provide a short butthorough analysis of a wide variety of control strategies.

Token 7419:
A Computational Logic (Boyer and Moore, 1979) is the basic reference on the Boyer- Moore theorem prover.

Token 7420:
Stickel (1992) covers the Prolog Technology Theorem Prover (PTTP),which combines the advantages of Prolog compilation with the completeness of model elimi-nation.

Token 7421:
SETHEO (Letz et al. , 1992) is another widely used theorem prover based on this ap- proach.

Token 7422:
L EANTAP (Beckert and Posegga, 1995) is an efﬁcient theorem prover implemented in only 25 lines of Prolog.

Token 7423:
Weidenbach (2001) describes S PASS , one of the strongest current theorem provers.

Token 7424:
The most successful theorem prover in recent annual competitions has been VAMPIRE (Riazanov and Voronkov, 2002). The C OQsystem (Bertot et al.

Token 7425:
, 2004) and the E

Token 7426:
360 Chapter 9. Inference in First-Order Logic equational solver (Schulz, 2004) have also proven to be valuable tools for proving correct- ness.

Token 7427:
Theorem provers have been used to automatically synthesize and verify software forcontrolling spacecraft (Denney et al.

Token 7428:
, 2006), including NASA’s new Orion capsule (Lowry, 2008).

Token 7429:
The design of the FM9001 32-bit microprocessor was proved correct by the N QTHM system (Hunt and Brock, 1992).

Token 7430:
The Conference on Automated Deduction (CADE) runs an annual contest for automated theorem provers.

Token 7431:
From 2002 through 2008, the most successful system has been V AMPIRE (Riazanov and Voronkov, 2002).

Token 7432:
Wiedijk (2003) compares the strength of 15 mathematical provers.

Token 7433:
TPTP (Thousands of Problems for Theorem Provers)is a library of theorem-proving problems, useful for comparing the performance of systems(Sutcliffe and Suttner, 1998; Sutcliffe et al.

Token 7434:
, 2006).

Token 7435:
Theorem provers have come up with novel mathematical results that eluded human mathematicians for decades, as detailed in the book Automated Reasoning and the Discov- ery of Missing Elegant Proofs (Wos and Pieper, 2003).

Token 7436:
The S AM(Semi-Automated Math- ematics) program was the ﬁrst, proving a lemma in lattice theory (Guard et al. , 1969).

Token 7437:
The AURA program has also answered open questions in several areas of mathematics (Wos and Winker, 1983).

Token 7438:
The Boyer–Moore theorem prover (Boyer and Moore, 1979) was used byNatarajan Shankar to give the ﬁrst fully rigorous formal proof of G¨ odel’s Incompleteness Theorem (Shankar, 1986).

Token 7439:
The N UPRL system proved Girard’s paradox (Howe, 1987) and Higman’s Lemma (Murthy and Russell, 1990).

Token 7440:
In 1933, Herbert Robbins proposed a simpleset of axioms—the Robbins algebra —that appeared to deﬁne Boolean algebra, but no proof ROBBINS ALGEBRA could be found (despite serious work by Alfred Tarski and others).

Token 7441:
On October 10, 1996, after eight days of computation, EQP (a version of O TTER ) found a proof (McCune, 1997).

Token 7442:
Many early papers in mathematical logic are to be found in From Frege to G ¨odel: A Source Book in Mathematical Logic (van Heijenoort, 1967).

Token 7443:
Textbooks geared toward automated deduction include the classic Symbolic Logic and Mechanical Theorem Prov- ing(Chang and Lee, 1973), as well as more recent works by Duffy (1991), Wos et al.

Token 7444:
(1992), Bibel (1993), and Kaufmann et al. (2000).

Token 7445:
The principal journal for theorem proving is the Journal of Automated Reasoning ; the main conferences are the annual Conference on Auto- mated Deduction (CADE) and the International Joint Conference on Automated Reasoning (IJCAR).

Token 7446:
The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects papers in the ﬁeld.

Token 7447:
MacKenzie’s Mechanizing Proof (2004) covers the history and technology of theorem proving for the popular audience.

Token 7448:
EXERCISES 9.1 Prove that Universal Instantiation is sound and that Existential Instantiation produces an inferentially equivalent knowledge base.

Token 7449:
9.2 FromLikes(Jerry,IceCream )it seems reasonable to infer ∃xLikes(x,IceCream ).

Token 7450:
Write down a general inference rule, Existential Introduction , that sanctions this inference.EXISTENTIAL INTRODUCTION State carefully the conditions that must be satisﬁed by the variables and terms involved.

Token 7451:
Exercises 361 9.3 Suppose a knowledge base contains just one sentence, ∃xAsHighAs (x,Everest ).

Token 7452:
Which of the following are legitimate results of applying Existential Instantiation? a.AsHighAs (Everest ,Everest ).

Token 7453:
b.AsHighAs (Kilimanjaro ,Everest ). c.AsHighAs (Kilimanjaro ,Everest )∧AsHighAs (BenNevis ,Everest ) (after two applications).

Token 7454:
9.4 For each pair of atomic sentences, give the most general uniﬁer if it exists: a.P(A,B,B ),P(x,y,z). b.Q(y,G(A,B)),Q(G(x,x),y).

Token 7455:
c.Older (Father (y),y),Older (Father (x),John). d.Knows (Father (y),y),Knows (x,x).

Token 7456:
9.5 Consider the subsumption lattices shown in Figure 9.2 (page 329).

Token 7457:
a. Construct the lattice for the sentence Employs (Mother (John),Father (Richard )).

Token 7458:
b. Construct the lattice for the sentence Employs (IBM,y)(“Everyone works for IBM”).

Token 7459:
Remember to include every kind of query that uniﬁes with the sentence.

Token 7460:
c. Assume that S TORE indexes each sentence under every node in its subsumption lattice.

Token 7461:
Explain how F ETCH should work when some of these sentences contain variables; use as examples the sentences in (a) and (b) and the query Employs (x,Father (x)).

Token 7462:
9.6 Write down logical representations for the following sentences, suitable for use with Generalized Modus Ponens: a.

Token 7463:
Horses, cows, and pigs are mammals. b. An offspring of a horse is a horse. c. Bluebeard is a horse. d. Bluebeard is Charlie’s parent.

Token 7464:
e. Offspring and parent are inverse relations. f. Every mammal has a parent.

Token 7465:
9.7 These questions concern concern issues with substitution and Skolemization. a.

Token 7466:
Given the premise ∀x∃yP(x,y), it is not valid to conclude that ∃qP(q,q).G i v e an example of a predicate Pwhere the ﬁrst is true but the second is false.

Token 7467:
b.

Token 7468:
Suppose that an inference engine is incorrectly written with the occurs check omitted, so that it allows a literal like P(x,F(x))to be uniﬁed with P(q,q).

Token 7469:
(As mentioned, most standard implementations of Prolog actually do allow this.)

Token 7470:
Show that such an inference engine will allow the conclusion ∃yP(q,q)to be inferred from the premise ∀x∃yP(x,y).

Token 7471:
362 Chapter 9.

Token 7472:
Inference in First-Order Logic c. Suppose that a procedure that converts ﬁrst-order logic to clausal form incorrectly Skolemizes∀x∃yP(x,y)toP(x,Sk0)—that is, it replaces yby a Skolem con- stant rather than by a Skolem function of x.

Token 7473:
Show that an inference engine that uses such a procedure will likewise allow ∃qP(q,q)to be inferred from the premise ∀x∃yP(x,y).

Token 7474:
d. A common error among students is to suppose that, in uniﬁcation, one is allowed to substitute a term for a Skolem constant instead of for a variable.

Token 7475:
For instance, they willsay that the formulas P(Sk1)andP(A)can be uniﬁed under the substitution {Sk1/A}.

Token 7476:
Give an example where this leads to an invalid inference.

Token 7477:
9.8 Explain how to write any given 3-SAT problem of arbitrary size using a single ﬁrst-order deﬁnite clause and no more than 30 ground facts.

Token 7478:
9.9 Suppose you are given the following axioms: 1.0≤3. 2.7≤9. 3.∀xx≤x. 4.∀xx≤x+0. 5.∀xx+0≤x. 6.∀x,y x +y≤y+x. 7.∀w,x,y,z w≤y∧x≤z⇒w+x≤y+z.

Token 7479:
8.∀x,y,z x≤y∧y≤z⇒x≤z a. Give a backward-chaining proof of the sentence 7≤3+9 .

Token 7480:
(Be sure, of course, to use only the axioms given here, not anything else you may know about arithmetic.)

Token 7481:
Showonly the steps that leads to success, not the irrelevant steps. b. Give a forward-chaining proof of the sentence 7≤3+9 .

Token 7482:
Again, show only the steps that lead to success.

Token 7483:
9.10 A popular children’s riddle is “Brothers and sisters have I none, but that man’s father is my father’s son.” Use the rules of the family domain (Section 8.3.2 on page 301) to show who that man is.

Token 7484:
You may apply any of the inference methods described in this chapter. Why do you think that this riddle is difﬁcult?

Token 7485:
9.11 Suppose we put into a logical knowledge base a segment of the U.S. census data list- ing the age, city of residence, date of birth, and mother of every person, using social se-curity numbers as identifying constants for each person.

Token 7486:
Thus, George’s age is given by Age(443-65-1282 ,56).

Token 7487:
Which of the following indexing schemes S1–S5 enable an efﬁcient solution for which of the queries Q1–Q4 (assuming normal backward chaining)?

Token 7488:
•S1: an index for each atom in each position. •S2: an index for each ﬁrst argument. •S3: an index for each predicate atom.

Token 7489:
•S4: an index for each combination of predicate and ﬁrst argument.

Token 7490:
Exercises 363 •S5: an index for each combination of predicate and second argument and an index for each ﬁrst argument.

Token 7491:
•Q1:Age(443-44-4321 ,x) •Q2:ResidesIn (x,Houston ) •Q3:Mother (x,y) •Q4:Age(x,34)∧ResidesIn (x,TinyTownUSA ) 9.12 One might suppose that we can avoid the problem of variable conﬂict in uniﬁcation during backward chaining by standardizing apart all of the sentences in the knowledge base once and for all.

Token 7492:
Show that, for some sentences, this approach cannot work. ( Hint: Consider a sentence in which one part uniﬁes with another.)

Token 7493:
9.13 In this exercise, use the sentences you wrote in Exercise 9.6 to answer a question by using a backward-chaining algorithm. a.

Token 7494:
Draw the proof tree generated by an exhaustive backward-chaining algorithm for the query∃hHorse (h), where clauses are matched in the order given. b.

Token 7495:
What do you notice about this domain? c. How many solutions for hactually follow from your sentences? d. Can you think of a way to ﬁnd all of them?

Token 7496:
( Hint: See Smith et al. (1986).)

Token 7497:
9.14 Trace the execution of the backward-chaining algorithm in Figure 9.6 (page 338) when it is applied to solve the crime problem (page 330).

Token 7498:
Show the sequence of values taken on by thegoals variable, and arrange them into a tree.

Token 7499:
9.15 The following Prolog code deﬁnes a predicate P. (Remember that uppercase terms are variables, not constants, in Prolog.) P(X,[X|Y]).

Token 7500:
P(X,[Y|Z]) :- P(X,Z). a. Show proof trees and solutions for the queries P(A,[2,1,3]) andP(2,[1,A,3]) . b.

Token 7501:
What standard list operation does Prepresent? 9.16 This exercise looks at sorting in Prolog. a.

Token 7502:
Write Prolog clauses that deﬁne the predicate sorted(L) , which is true if and only if listLis sorted in ascending order. b.

Token 7503:
Write a Prolog deﬁnition for the predicate perm(L,M) , which is true if and only if L is a permutation of M. c.D e ﬁ n e sort(L,M) (Mis a sorted version of L)u s i n g perm andsorted .

Token 7504:
d.R u n sort on longer and longer lists until you lose patience. What is the time complex- ity of your program?

Token 7505:
e. Write a faster sorting algorithm, such as insertion sort or quicksort, in Prolog.

Token 7506:
364 Chapter 9. Inference in First-Order Logic 9.17 This exercise looks at the recursive application of rewrite rules, using logic program- ming.

Token 7507:
A rewrite rule (or demodulator in O TTER terminology) is an equation with a speciﬁed direction.

Token 7508:
For example, the rewrite rule x+0→xsuggests replacing any expression that matches x+0with the expression x. Rewrite rules are a key component of equational reason- ing systems.

Token 7509:
Use the predicate rewrite(X,Y) to represent rewrite rules. For example, the earlier rewrite rule is written as rewrite(X+0,X) .

Token 7510:
Some terms are primitive and cannot be further simpliﬁed; thus, we write primitive(0) to say that 0 is a primitive term. a.

Token 7511:
Write a deﬁnition of a predicate simplify(X,Y) ,t h a ti st r u ew h e n Yis a simpliﬁed version of X—that is, when no further rewrite rules apply to any subexpression of Y. b.

Token 7512:
Write a collection of rules for the simpliﬁcation of expressions involving arithmetic operators, and apply your simpliﬁcation algorithm to some sample expressions.

Token 7513:
c. Write a collection of rewrite rules for symbolic differentiation, and use them along with your simpliﬁcation rules to differentiate and simplify expressions involving arithmeticexpressions, including exponentiation.

Token 7514:
9.18 This exercise considers the implementation of search algorithms in Prolog.

Token 7515:
Suppose thatsuccessor(X,Y) is true when state Yis a successor of state X;a n dt h a t goal(X) is true when Xis a goal state.

Token 7516:
Write a deﬁnition for solve(X,P) , which means that Pis a path (list of states) beginning with X, ending in a goal state, and consisting of a sequence of legal steps as deﬁned by successor .

Token 7517:
You will ﬁnd that depth-ﬁrst search is the easiest way to do this. How easy would it be to add heuristic search control?

Token 7518:
9.19 Suppose a knowledge base contains just the following ﬁrst-order Horn clauses: Ancestor (Mother (x),x) Ancestor (x,y)∧Ancestor (y,z)⇒Ancestor (x,z) Consider a forward chaining algorithm that, on the jth iteration, terminates if the KB contains a sentence that uniﬁes with the query, else adds to the KB every atomic sentence that can beinferred from the sentences already in the KB after iteration j−1.

Token 7519:
a.

Token 7520:
For each of the following queries, say whether the algorithm will (1) give an answer (if so, write down that answer); or (2) terminate with no answer; or (3) never terminate.

Token 7521:
(i) Ancestor (Mother (y),Joh n ) (ii)Ancestor (Mother (Mother (y)),Joh n ) (iii)Ancestor (Mother (Mother (Mother (y))),Moth er (y)) (iv)Ancestor (Mother (John),Moth er (Mother (John))) b.

Token 7522:
Can a resolution algorithm prove the sentence ¬Ancestor (John,John )from the orig- inal knowledge base? Explain how, or why not.

Token 7523:
c. Suppose we add the assertion that ¬(Mother (x)=x)and augment the resolution al- gorithm with inference rules for equality.

Token 7524:
Now what is the answer to (b)? 9.20 LetLbe the ﬁrst-order language with a single predicate S(p,q), meaning “ pshaves q.” Assume a domain of people.

Token 7525:
Exercises 365 a.

Token 7526:
Consider the sentence “There exists a person Pwho shaves every one who does not shave themselves, and only people that do not shave themselves.” Express this in L. b.

Token 7527:
Convert the sentence in (a) to clausal form. c. Construct a resolution proof to show that the clauses in (b) are inherently inconsistent.

Token 7528:
(Note: you do not need any additional axioms.) 9.21 How can resolution be used to show that a sentence is valid? Unsatisﬁable?

Token 7529:
9.22 Construct an example of two clauses that can be resolved together in two different ways giving two different outcomes.

Token 7530:
9.23 From “Horses are animals,” it follows that “The head of a horse is the head of an animal.” Demonstrate that this inference is valid by carrying out the following steps: a.

Token 7531:
Translate the premise and the conclusion into the language of ﬁrst-order logic.

Token 7532:
Use three predicates: HeadOf (h, x)(meaning “ his the head of x”),Horse (x),a n dAnimal (x).

Token 7533:
b. Negate the conclusion, and convert the premise and the negated conclusion into con- junctive normal form.

Token 7534:
c. Use resolution to show that the conclusion follows from the premise.

Token 7535:
9.24 Here are two sentences in the language of ﬁrst-order logic: (A)∀x∃y(x≥y) (B)∃y∀x(x≥y) a.

Token 7536:
Assume that the variables range over all the natural numbers 0,1,2,...,∞and that the “≥” predicate means “is greater than or equal to.” Under this interpretation, translate (A) and (B) into English.

Token 7537:
b. Is (A) true under this interpretation? c. Is (B) true under this interpretation? d. Does (A) logically entail (B)?

Token 7538:
e. Does (B) logically entail (A)? f. Using resolution, try to prove that (A) follows from (B).

Token 7539:
Do this even if you think that (B) does not logically entail (A); continue until the proof breaks down and you cannotproceed (if it does break down).

Token 7540:
Show the unifying substitution for each resolution step.If the proof fails, explain exactly where, how, and why it breaks down.

Token 7541:
g. Now try to prove that (B) follows from (A).

Token 7542:
9.25 Resolution can produce nonconstructive proofs for queries with variables, so we had to introduce special mechanisms to extract deﬁnite answers.

Token 7543:
Explain why this issue does notarise with knowledge bases containing only deﬁnite clauses.

Token 7544:
9.26 We said in this chapter that resolution cannot be used to generate all logical conse- quences of a set of sentences. Can any algorithm do this?

Token 7545:
10CLASSICAL PLANNING In which we see how an agent can take advantage of the structure of a problem to construct complex plans of action.

Token 7546:
We have deﬁned AI as the study of rational action, which means that planning —devising a plan of action to achieve one’s goals—is a critical part of AI.

Token 7547:
We have seen two examplesof planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-brid logical agent of Chapter 7.

Token 7548:
In this chapter we introduce a representation for planningproblems that scales up to problems that could not be handled by those earlier approaches.

Token 7549:
Section 10.1 develops an expressive yet carefully constrained language for representing planning problems.

Token 7550:
Section 10.2 shows how forward and backward search algorithms can take advantage of this representation, primarily through accurate heuristics that can be derived automatically from the structure of the representation.

Token 7551:
(This is analogous to the way in whicheffective domain-independent heuristics were constructed for constraint satisfaction problemsin Chapter 6.)

Token 7552:
Section 10.3 shows how a data structure called the planning graph can make thesearch for a plan more efﬁcient.

Token 7553:
We then describe a few of the other approaches to planning,and conclude by comparing the various approaches.

Token 7554:
This chapter covers fully observable, deterministic, static environments with single agents.

Token 7555:
Chapters 11 and 17 cover partially observable, stochastic, dynamic environmentswith multiple agents.

Token 7556:
10.1 D EFINITION OF CLASSICAL PLANNING The problem-solving agent of Chapter 3 can ﬁnd sequences of actions that result in a goalstate.

Token 7557:
But it deals with atomic representations of states and thus needs good domain-speciﬁcheuristics to perform well.

Token 7558:
The hybrid propositional logical agent of Chapter 7 can ﬁnd planswithout domain-speciﬁc heuristics because it uses domain-independent heuristics based on the logical structure of the problem.

Token 7559:
But it relies on ground (variable-free) propositional inference, which means that it may be swamped when there are many actions and states.

Token 7560:
Forexample, in the wumpus world, the simple action of moving a step forward had to be repeatedfor all four agent orientations, Ttime steps, and n 2current locations.

Token 7561:
366

Token 7562:
Section 10.1.

Token 7563:
Deﬁnition of Classical Planning 367 In response to this, planning researchers have settled on a factored representation — one in which a state of the world is represented by a collection of variables.

Token 7564:
We use a languagecalled PDDL , the Planning Domain Deﬁnition Language, that allows us to express all 4Tn 2PDDL actions with one action schema.

Token 7565:
There have been several versions of PDDL; we select a simple version and alter its syntax to be consistent with the rest of the book.1We now show how PDDL describes the four things we need to deﬁne a search problem: the initial state, the actions that are available in a state, the result of applying an action, and the goal test.

Token 7566:
Each state is represented as a conjunction of ﬂuents that are ground, functionless atoms.

Token 7567:
For example, Poor∧Unknown might represent the state of a hapless agent, and a state in a package delivery problem might be At(Truck 1,Melbourne )∧At(Truck 2,Sydney ).

Token 7568:
Database semantics is used: the closed-world assumption means that any ﬂuents that are not mentioned are false, and the unique names assumption means that Truck 1andTruck 2are distinct.

Token 7569:
The following ﬂuents are notallowed in a state: At(x,y)(because it is non-ground), ¬Poor (because it is a negation), and At(Father (Fred),Sydney )(because it uses a function symbol).

Token 7570:
The representation of states is carefully designed so that a state can be treatedeither as a conjunction of ﬂuents, which can be manipulated by logical inference, or as a set of ﬂuents, which can be manipulated with set operations.

Token 7571:
The set semantics is sometimes SET SEMANTICS easier to deal with.

Token 7572:
Actions are described by a set of action schemas that implicitly deﬁne the A CTIONS (s) and R ESULT (s,a)functions needed to do a problem-solving search.

Token 7573:
We saw in Chapter 7 that any system for action description needs to solve the frame problem—to say what changes and what stays the same as the result of the action.

Token 7574:
Classical planning concentrates on problems where most actions leave most things unchanged.

Token 7575:
Think of a world consisting of a bunch of objects on a ﬂat surface.

Token 7576:
The action of nudging an object causes that object to change its lo-cation by a vector Δ.

Token 7577:
A concise description of the action should mention only Δ; it shouldn’t have to mention all the objects that stay in place.

Token 7578:
PDDL does that by specifying the result ofan action in terms of what changes; everything that stays the same is left unmentioned.

Token 7579:
A set of ground (variable-free) actions can be represented by a single action schema .

Token 7580:
ACTION SCHEMA The schema is a lifted representation—it lifts the level of reasoning from propositional logic to a restricted subset of ﬁrst-order logic.

Token 7581:
For example, here is an action schema for ﬂying aplane from one location to another: Action (Fly(p,from,to), P RECOND :At(p,from)∧Plane (p)∧Airport (from)∧Airport (to) EFFECT :¬At(p,from)∧At(p,to)) The schema consists of the action name, a list of all the variables used in the schema, a precondition and an effect .

Token 7582:
Although we haven’t said yet how the action schema converts PRECONDITION EFFECT into logical sentences, think of the variables as being universally quantiﬁed.

Token 7583:
We are free to choose whatever values we want to instantiate the variables.

Token 7584:
For example, here is one ground 1PDDL was derived from the original S TRIPS planning language(Fikes and Nilsson, 1971).

Token 7585:
which is slightly more restricted than PDDL: S TRIPS preconditions and goals cannot contain negative literals.

Token 7586:
368 Chapter 10.

Token 7587:
Classical Planning action that results from substituting values for all the variables: Action (Fly(P1,SFO,JFK), PRECOND :At(P1,SFO)∧Plane (P1)∧Airport (SFO)∧Airport (JFK) EFFECT :¬At(P1,SFO)∧At(P1,JFK)) The precondition and effect of an action are each conjunctions of literals (positive or negated atomic sentences).

Token 7588:
The precondition deﬁnes the states in which the action can be executed,and the effect deﬁnes the result of executing the action.

Token 7589:
An action acan be executed in state sifsentails the precondition of a. Entailment can also be expressed with the set semantics: s|=qiff every positive literal in qis insand every negated literal in qis not.

Token 7590:
In formal notation we say (a∈A CTIONS (s))⇔s|=PRECOND (a), where any variables in aare universally quantiﬁed.

Token 7591:
For example, ∀p,from,to(Fly(p,from,to)∈ACTIONS (s))⇔ s|=(At(p,from)∧Plane (p)∧Airport (from)∧Airport (to)) We say that action aisapplicable in state sif the preconditions are satisﬁed by s.W h e n APPLICABLE an action schema acontains variables, it may have multiple applicable instantiations.

Token 7592:
For example, with the initial state deﬁned in Figure 10.1, the Flyaction can be instantiated as Fly(P1,SFO,JFK)or asFly(P2,JFK,SFO), both of which are applicable in the initial state.

Token 7593:
If an action ahasvvariables, then, in a domain with kunique names of objects, it takes O(vk)time in the worst case to ﬁnd the applicable ground actions.

Token 7594:
Sometimes we want to propositionalize a PDDL problem—replace each action schema PROPOSITIONALIZE with a set of ground actions and then use a propositional solver such as SATP LAN to ﬁnd a solution.

Token 7595:
However, this is impractical when vandkare large.

Token 7596:
The result of executing action ain state sis deﬁned as a state s/primewhich is represented by the set of ﬂuents formed by starting with s, removing the ﬂuents that appear as negative literals in the action’s effects (what we call the delete list or D EL(a)), and adding the ﬂuents DELETE LIST that are positive literals in the action’s effects (what we call the add list or A DD(a)): ADD LIST RESULT (s,a)=(s−DEL(a))∪ADD(a).

Token 7597:
(10.1) For example, with the action Fly(P1,SFO,JFK), we would remove At(P1,SFO)and add At(P1,JFK).

Token 7598:
It is a requirement of action schemas that any variable in the effect must also appear in the precondition.

Token 7599:
That way, when the precondition is matched against the state s, all the variables will be bound, and R ESULT (s,a)will therefore have only ground atoms.

Token 7600:
In other words, ground states are closed under the R ESULT operation.

Token 7601:
Also note that the ﬂuents do not explicitly refer to time, as they did in Chapter 7.

Token 7602:
There we needed superscripts for time, and successor-state axioms of the form Ft+1⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt).

Token 7603:
In PDDL the times and states are implicit in the action schemas: the precondition always refers to time tand the effect to time t+1.

Token 7604:
A set of action schemas serves as a deﬁnition of a planning domain .

Token 7605:
A speciﬁc problem within the domain is deﬁned with the addition of an initial state and a goal. The initial

Token 7606:
Section 10.1.

Token 7607:
Deﬁnition of Classical Planning 369 Init(At(C1,SFO)∧At(C2,JFK)∧At(P1,SFO)∧At(P2,JFK) ∧Cargo (C1)∧Cargo (C2)∧Plane (P1)∧Plane (P2) ∧Airport (JFK)∧Airport (SFO)) Goal(At(C1,JFK)∧At(C2,SFO)) Action (Load(c, p, a ), PRECOND :At(c, a)∧At(p, a)∧Cargo (c)∧Plane (p)∧Airport (a) EFFECT :¬At(c, a)∧In(c, p)) Action (Unload (c, p, a ), PRECOND :In(c, p)∧At(p, a)∧Cargo (c)∧Plane (p)∧Airport (a) EFFECT :At(c, a)∧¬In(c, p)) Action (Fly(p,from,to), PRECOND :At(p,from)∧Plane (p)∧Airport (from)∧Airport (to) EFFECT :¬At(p,from)∧At(p,to)) Figure 10.1 A PDDL description of an air cargo transportation planning problem.

Token 7608:
state is a conjunction of ground atoms.

Token 7609:
(As with all states, the closed-world assumption is INITIAL STATE used, which means that any atoms that are not mentioned are false.)

Token 7610:
The goal is just like a GOAL precondition: a conjunction of literals (positive or negative) that may contain variables, such asAt(p,SFO)∧Plane (p).

Token 7611:
Any variables are treated as existentially quantiﬁed, so this goal is to have anyplane at SFO.

Token 7612:
The problem is solved when we can ﬁnd a sequence of actions that end in a state sthat entails the goal.

Token 7613:
For example, the state Rich∧Famous∧Miserable entails the goal Rich∧Famous , and the state Plane (Plane 1)∧At(Plane 1,SFO)entails the goal At(p,SFO)∧Plane (p).

Token 7614:
Now we have deﬁned planning as a search problem: we have an initial state, an A CTIONS function, a R ESULT function, and a goal test.

Token 7615:
We’ll look at some example problems before investigating efﬁcient search algorithms.

Token 7616:
10.1.1 Example: Air cargo transport Figure 10.1 shows an air cargo transport problem involving loading and unloading cargo andﬂying it from place to place.

Token 7617:
The problem can be deﬁned with three actions: Load ,Unload , andFly.

Token 7618:
The actions affect two predicates: In(c,p)means that cargo cis inside plane p,a n d At(x,a)means that object x(either plane or cargo) is at airport a.

Token 7619:
Note that some care must be taken to make sure the Atpredicates are maintained properly.

Token 7620:
When a plane ﬂies from one airport to another, all the cargo inside the plane goes with it.

Token 7621:
In ﬁrst-order logic it would be easy to quantify over all objects that are inside the plane.

Token 7622:
But basic PDDL does not have a universal quantiﬁer, so we need a different solution.

Token 7623:
The approach we use is to say that a piece of cargo ceases to be Atanywhere when it is Ina plane; the cargo only becomes Atthe new airport when it is unloaded.

Token 7624:
So Atreally means “available for use at a given location.” The following plan is a solution to the problem: [Load(C 1,P1,SFO),Fly(P1,SFO,JFK),Unload (C1,P1,JFK), Load(C2,P2,JFK),Fly(P2,JFK,SFO),Unload (C2,P2,SFO)].

Token 7625:
370 Chapter 10.

Token 7626:
Classical Planning Finally, there is the problem of spurious actions such as Fly(P1,JFK,JFK), which should be a no-op, but which has contradictory effects (according to the deﬁnition, the effect wouldinclude At(P 1,JFK)∧¬At(P1,JFK)).

Token 7627:
It is common to ignore such problems, because they seldom cause incorrect plans to be produced.

Token 7628:
The correct approach is to add inequalitypreconditions saying that the from andtoairports must be different; see another example of this in Figure 10.3.

Token 7629:
10.1.2 Example: The spare tire problem Consider the problem of changing a ﬂat tire (Figure 10.2).

Token 7630:
The goal is to have a good spare tire properly mounted onto the car’s axle, where the initial state has a ﬂat tire on the axle anda good spare tire in the trunk.

Token 7631:
To keep it simple, our version of the problem is an abstractone, with no sticky lug nuts or other complications.

Token 7632:
There are just four actions: removing thespare from the trunk, removing the ﬂat tire from the axle, putting the spare on the axle, andleaving the car unattended overnight.

Token 7633:
We assume that the car is parked in a particularly badneighborhood, so that the effect of leaving it overnight is that the tires disappear.

Token 7634:
A solutionto the problem is [Remove (Flat,Axle),Remove (Spare,Trunk ),PutOn (Spare,Axle)].

Token 7635:
Init(Tire(Flat)∧Tire(Spare)∧At(Flat,Axle)∧At(Spare,Trunk )) Goal(At(Spare,Axle)) Action (Remove (obj,loc), PRECOND :At(obj,loc) EFFECT :¬At(obj,loc)∧At(obj,Ground )) Action (PutOn (t,Axle), PRECOND :Tire(t)∧At(t,Ground )∧¬At(Flat,Axle) EFFECT :¬At(t,Ground )∧At(t,Axle)) Action (LeaveOvernight , PRECOND : EFFECT :¬At(Spare,Ground )∧¬At(Spare,Axle)∧¬At(Spare,Trunk ) ∧¬At(Flat,Ground )∧¬At(Flat,Axle)∧¬At(Flat,Trunk )) Figure 10.2 The simple spare tire problem.

Token 7636:
10.1.3 Example: The blocks world One of the most famous planning domains is known as the blocks world .

Token 7637:
This domain BLOCKS WORLD consists of a set of cube-shaped blocks sitting on a table.2The blocks can be stacked, but only one block can ﬁt directly on top of another.

Token 7638:
A robot arm can pick up a block and moveit to another position, either on the table or on top of another block.

Token 7639:
The arm can pick uponly one block at a time, so it cannot pick up a block that has another one on it.

Token 7640:
The goal willalways be to build one or more stacks of blocks, speciﬁed in terms of what blocks are on top 2The blocks world used in planning research is much simpler than S HRDLU ’s version, shown on page 20.

Token 7641:
Section 10.1.

Token 7642:
Deﬁnition of Classical Planning 371 Init(On(A,Table)∧On(B,Table)∧On(C,A) ∧Block(A)∧Block(B)∧Block(C)∧Clear(B)∧Clear(C)) Goal(On(A, B)∧On(B,C)) Action (Move(b,x,y), PRECOND :On(b,x)∧Clear(b)∧Clear(y)∧Block(b)∧Block(y)∧ (b/negationslash=x)∧(b/negationslash=y)∧(x/negationslash=y), EFFECT :On(b,y)∧Clear(x)∧¬On(b,x)∧¬Clear(y)) Action (MoveToTable (b,x), PRECOND :On(b,x)∧Clear(b)∧Block(b)∧(b/negationslash=x), EFFECT :On(b,Table)∧Clear(x)∧¬On(b,x)) Figure 10.3 A planning problem in the blocks world: building a three-block tower.

Token 7643:
One solution is the sequence [MoveToTable (C,A),Move(B,Table,C),Move(A,Table,B)].

Token 7644:
Start State Goal StateB ACA B C Figure 10.4 Diagram of the blocks-world problem in Figure 10.3. of what other blocks.

Token 7645:
For example, a goal might be to get block AonBand block BonC (see Figure 10.4).

Token 7646:
We use On(b,x)to indicate that block bis onx,w h e r e xis either another block or the table.

Token 7647:
The action for moving block bfrom the top of xto the top of ywill be Move(b,x,y).

Token 7648:
Now, one of the preconditions on moving bis that no other block be on it. In ﬁrst-order logic, this would be¬∃xOn(x,b)or, alternatively, ∀x¬On(x,b).

Token 7649:
Basic PDDL does not allow quantiﬁers, so instead we introduce a predicate Clear(x)that is true when nothing is on x.

Token 7650:
(The complete problem description is in Figure 10.3.) The action Move moves a block bfromxtoyif both bandyare clear.

Token 7651:
After the move is made, bis still clear but yis not.

Token 7652:
A ﬁrst attempt at the Move schema is Action (Move(b,x,y), PRECOND :On(b,x)∧Clear(b)∧Clear(y), EFFECT :On(b,y)∧Clear(x)∧¬On(b,x)∧¬Clear(y)).

Token 7653:
Unfortunately, this does not maintain Clear properly when xoryis the table.

Token 7654:
When xis the Table , this action has the effect Clear(Table), but the table should not become clear; and wheny=Table , it has the precondition Clear(Table), but the table does not have to be clear

Token 7655:
372 Chapter 10. Classical Planning for us to move a block onto it. To ﬁx this, we do two things.

Token 7656:
First, we introduce another action to move a block bfromxto the table: Action (MoveToTable (b,x), PRECOND :On(b,x)∧Clear(b), EFFECT :On(b,Table)∧Clear(x)∧¬On(b,x)).

Token 7657:
Second, we take the interpretation of Clear(x)to be “there is a clear space on xto hold a block.” Under this interpretation, Clear(Table)will always be true.

Token 7658:
The only problem is that nothing prevents the planner from using Move(b,x,Table)instead of MoveToTable (b,x).

Token 7659:
We could live with this problem—it will lead to a larger-than-necessary search space, but will not lead to incorrect answers—or we could introduce the predicate Block and add Block(b)∧ Block(y)to the precondition of Move .

Token 7660:
10.1.4 The complexity of classical planning In this subsection we consider the theoretical complexity of planning and distinguish two decision problems.

Token 7661:
PlanSAT is the question of whether there exists any plan that solves a PLANSAT planning problem.

Token 7662:
Bounded PlanSAT asks whether there is a solution of length kor less; BOUNDED PLANSAT this can be used to ﬁnd an optimal plan.

Token 7663:
The ﬁrst result is that both decision problems are decidable for classical planning.

Token 7664:
The proof follows from the fact that the number of states is ﬁnite.

Token 7665:
But if we add function symbolsto the language, then the number of states becomes inﬁnite, and PlanSAT becomes onlysemidecidable: an algorithm exists that will terminate with the correct answer for any solvableproblem, but may not terminate on unsolvable problems.

Token 7666:
The Bounded PlanSAT problemremains decidable even in the presence of function symbols. For proofs of the assertions inthis section, see Ghallab et al.

Token 7667:
(2004).

Token 7668:
Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that is larger (and hence more difﬁcult) than NP and refers to problems that can be solved by a deterministic Turing machine with a polynomial amount of space.

Token 7669:
Even if we make somerather severe restrictions, the problems remain quite difﬁcult.

Token 7670:
For example, if we disallownegative effects, both problems are still NP-hard.

Token 7671:
However, if we also disallow negativepreconditions, PlanSAT reduces to the class P. These worst-case results may seem discouraging.

Token 7672:
We can take solace in the fact that agents are usually not asked to ﬁnd plans for arbitrary worst-case problem instances, butrather are asked for plans in speciﬁc domains (such as blocks-world problems with nblocks), which can be much easier than the theoretical worst case.

Token 7673:
For many domains (including theblocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT isin P; in other words, optimal planning is usually hard, but sub-optimal planning is sometimes easy.

Token 7674:
To do well on easier-than-worst-case problems, we will need good search heuristics.

Token 7675:
That’s the true advantage of the classical planning formalism: it has facilitated the develop-ment of very accurate domain-independent heuristics, whereas systems based on successor-state axioms in ﬁrst-order logic have had less success in coming up with good heuristics.

Token 7676:
Section 10.2.

Token 7677:
Algorithms for Planning as State-Space Search 373 10.2 A LGORITHMS FOR PLANNING AS STATE -SPACE SEARCH Now we turn our attention to planning algorithms.

Token 7678:
We saw how the description of a planning problem deﬁnes a search problem: we can search from the initial state through the spaceof states, looking for a goal.

Token 7679:
One of the nice advantages of the declarative representation of action schemas is that we can also search backward from the goal, looking for the initial state.

Token 7680:
Figure 10.5 compares forward and backward searches.

Token 7681:
10.2.1 Forward (progression) state-space search Now that we have shown how a planning problem maps into a search problem, we can solveplanning problems with any of the heuristic search algorithms from Chapter 3 or a localsearch algorithm from Chapter 4 (provided we keep track of the actions used to reach thegoal).

Token 7682:
From the earliest days of planning research (around 1961) until around 1998 it wasassumed that forward state-space search was too inefﬁcient to be practical.

Token 7683:
It is not hard tocome up with reasons why. First, forward search is prone to exploring irrelevant actions.

Token 7684:
Consider the noble task of buying a copy of AI: A Modern Approach from an online bookseller.

Token 7685:
Suppose there is an (a) (b)At(P1, A)Fly(P1, A, B ) Fly(P2, A, B ) Fly(P1, A, B ) Fly(P2, A, B )At(P2, A) At(P1, B) At(P2, A)At(P1, A) At(P2, B) At(P1, B) At(P2, B)At(P1, B) At(P2, A) At(P1, A) At(P2, B) Figure 10.5 Two approaches to searching for a plan.

Token 7686:
(a) Forward (progression) search through the space of states, starting in the initial state and using the problem’s actions to search forward for a member of the set of goal states.

Token 7687:
(b) Backward (regression) searchthrough sets of relevant states, starting at the set of states representing the goal and using the inverse of the actions to search backward for the initial state.

Token 7688:
374 Chapter 10. Classical Planning action schema Buy(isbn)with effect Own(isbn).

Token 7689:
ISBNs are 10 digits, so this action schema represents 10 billion ground actions.

Token 7690:
An uninformed forward-search algorithm would haveto start enumerating these 10 billion actions to ﬁnd one that leads to the goal.

Token 7691:
Second, planning problems often have large state spaces.

Token 7692:
Consider an air cargo problem with 10 airports, where each airport has 5 planes and 20 pieces of cargo.

Token 7693:
The goal is to move all the cargo at airport Ato airport B.

Token 7694:
There is a simple solution to the problem: load the 20 pieces of cargo into one of the planes at A, ﬂy the plane to B, and unload the cargo.

Token 7695:
Finding the solution can be difﬁcult because the average branching factor is huge: each of the 50planes can ﬂy to 9 other airports, and each of the 200 packages can be either unloaded (ifit is loaded) or loaded into any plane at its airport (if it is unloaded).

Token 7696:
So in any state thereis a minimum of 450 actions (when all the packages are at airports with no planes) and amaximum of 10,450 (when all packages and planes are at the same airport).

Token 7697:
On average, let’ssay there are about 2000 possible actions per state, so the search graph up to the depth of theobvious solution has about 2000 41nodes.

Token 7698:
Clearly, even this relatively small problem instance is hopeless without an accurate heuristic.

Token 7699:
Although many real-world applications of planning have relied on domain-speciﬁcheuristics, it turns out (as we see in Section 10.2.3) that strong domain-independent heuristics can be derived automatically; that is what makes forward search feasible.

Token 7700:
10.2.2 Backward (regression) relevant-states search In regression search we start at the goal and apply the actions backward until we ﬁnd a sequence of steps that reaches the initial state.

Token 7701:
It is called relevant-states search because we RELEVANT-STATES only consider actions that are relevant to the goal (or current state).

Token 7702:
As in belief-state search (Section 4.4), there is a setof relevant states to consider at each step, not just a single state.

Token 7703:
We start with the goal, which is a conjunction of literals forming a description of a set of states—for example, the goal ¬Poor∧Famous describes those states in which Poor is false, Famous is true, and any other ﬂuent can have any value.

Token 7704:
If there are nground ﬂuents in a domain, then there are 2nground states (each ﬂuent can be true or false), but 3ndescriptions of sets of goal states (each ﬂuent can be positive, negative, or not mentioned).

Token 7705:
In general, backward search works only when we know how to regress from a state description to the predecessor state description.

Token 7706:
For example, it is hard to search backwardsfor a solution to the n-queens problem because there is no easy way to describe the states that are one move away from the goal.

Token 7707:
Happily, the PDDL representation was designed to makeit easy to regress actions—if a domain can be expressed in PDDL, then we can do regression search on it.

Token 7708:
Given a ground goal description gand a ground action a, the regression from g overagives us a state description g /primedeﬁned by g/prime=(g−ADD(a))∪Precond (a).

Token 7709:
That is, the effects that were added by the action need not have been true before, and also the preconditions must have held before, or else the action could not have been executed.

Token 7710:
Note that D EL(a)does not appear in the formula; that’s because while we know the ﬂuents in D EL(a)are no longer true after the action, we don’t know whether or not they were true before, so there’s nothing to be said about them.

Token 7711:
Section 10.2.

Token 7712:
Algorithms for Planning as State-Space Search 375 To get the full advantage of backward search, we need to deal with partially uninstanti- ated actions and states, not just ground ones.

Token 7713:
For example, suppose the goal is to deliver a spe-ciﬁc piece of cargo to SFO: At(C 2,SFO).

Token 7714:
That suggests the action Unload (C2,p/prime,SFO): Action (Unload (C2,p/prime,SFO), PRECOND :In(C2,p/prime)∧At(p/prime,SFO)∧Cargo (C2)∧Plane (p/prime)∧Airport (SFO) EFFECT :At(C2,SFO)∧¬In(C2,p/prime).

Token 7715:
(Note that we have standardized variable names (changing ptop/primein this case) so that there will be no confusion between variable names if we happen to use the same action schematwice in a plan.

Token 7716:
The same approach was used in Chapter 9 for ﬁrst-order logical inference.

Token 7717:
)This represents unloading the package from an unspeciﬁed plane at SFO; any plane will do, but we need not say which one now.

Token 7718:
We can take advantage of the power of ﬁrst-order representations: a single description summarizes the possibility of using anyof the planes by implicitly quantifying over p /prime.

Token 7719:
The regressed state description is g/prime=In(C2,p/prime)∧At(p/prime,SFO)∧Cargo (C2)∧Plane (p/prime)∧Airport (SFO).

Token 7720:
The ﬁnal issue is deciding which actions are candidates to regress over.

Token 7721:
In the forward direc- tion we chose actions that were applicable —those actions that could be the next step in the plan.

Token 7722:
In backward search we want actions that are relevant —those actions that could be the RELEVANCE laststep in a plan leading up to the current goal state.

Token 7723:
For an action to be relevant to a goal it obviously must contribute to the goal: at least one of the action’s effects (either positive or negative) must unify with an element of the goal.What is less obvious is that the action must not have any effect (positive or negative) thatnegates an element of the goal.

Token 7724:
Now, if the goal is A∧B∧Cand an action has the effect A∧B∧¬Cthen there is a colloquial sense in which that action is very relevant to the goal—it gets us two-thirds of the way there.

Token 7725:
But it is not relevant in the technical sense deﬁned here, because this action could not be the ﬁnal step of a solution—we would always need at least one more step to achieve C. Given the goal At(C 2,SFO), several instantiations of Unload are relevant: we could chose any speciﬁc plane to unload from, or we could leave the plane unspeciﬁed by using the action Unload (C2,p/prime,SFO).

Token 7726:
We can reduce the branching factor without ruling out any solutions by always using the action formed by substituting the most general uniﬁer into the (standardized) action schema.

Token 7727:
As another example, consider the goal Own(0136042597) , given an initial state with 10 billion ISBNs, and the single action schema A=Action (Buy(i),PRECOND :ISBN (i),EFFECT :Own(i)).

Token 7728:
As we mentioned before, forward search without a heuristic would have to start enumer- ating the 10 billion ground Buy actions.

Token 7729:
But with backward search, we would unify the goalOwn(0136042597) with the (standardized) effect Own(i/prime), yielding the substitution θ={i/prime/0136042597}.

Token 7730:
Then we would regress over the action Subst(θ,A/prime)to yield the predecessor state description ISBN (0136042597) .

Token 7731:
This is part of, and thus entailed by, the initial state, so we are done.

Token 7732:
376 Chapter 10. Classical Planning We can make this more formal.

Token 7733:
Assume a goal description gwhich contains a goal literal giand an action schema Athat is standardized to produce A/prime.I fA/primehas an effect literal e/prime jwhere Unify(gi,e/prime j)=θand where we deﬁne a/prime=SUBST(θ,A/prime)and if there is no effect ina/primethat is the negation of a literal in g,t h e na/primeis a relevant action towards g. Backward search keeps the branching factor lower than forward search, for most prob- lem domains.

Token 7734:
However, the fact that backward search uses state sets rather than individual states makes it harder to come up with good heuristics.

Token 7735:
That is the main reason why themajority of current systems favor forward search.

Token 7736:
10.2.3 Heuristics for planning Neither forward nor backward search is efﬁcient without a good heuristic function.

Token 7737:
Recallfrom Chapter 3 that a heuristic function h(s)estimates the distance from a state sto the goal and that if we can derive an admissible heuristic for this distance—one that does not overestimate—then we can use A ∗search to ﬁnd optimal solutions.

Token 7738:
An admissible heuristic can be derived by deﬁning a relaxed problem that is easier to solve.

Token 7739:
The exact cost of a solution to this easier problem then becomes the heuristic for the original problem.

Token 7740:
By deﬁnition, there is no way to analyze an atomic state, and thus it it requires some ingenuity by a human analyst to deﬁne good domain-speciﬁc heuristics for search problemswith atomic states.

Token 7741:
Planning uses a factored representation for states and action schemas.That makes it possible to deﬁne good domain-independent heuristics and for programs toautomatically apply a good domain-independent heuristic for a given problem.

Token 7742:
Think of a search problem as a graph where the nodes are states and the edges are actions.

Token 7743:
The problem is to ﬁnd a path connecting the initial state to a goal state.

Token 7744:
There aretwo ways we can relax this problem to make it easier: by adding more edges to the graph,making it strictly easier to ﬁnd a path, or by grouping multiple nodes together, forming anabstraction of the state space that has fewer states, and thus is easier to search.

Token 7745:
We look ﬁrst at heuristics that add edges to the graph. For example, the ignore pre- conditions heuristic drops all preconditions from actions.

Token 7746:
Every action becomes applicable IGNORE PRECONDITIONSHEURISTIC in every state, and any single goal ﬂuent can be achieved in one step (if there is an applica- ble action—if not, the problem is impossible).

Token 7747:
This almost implies that the number of stepsrequired to solve the relaxed problem is the number of unsatisﬁed goals—almost but notquite, because (1) some action may achieve multiple goals and (2) some actions may undothe effects of others.

Token 7748:
For many problems an accurate heuristic is obtained by considering (1)and ignoring (2).

Token 7749:
First, we relax the actions by removing all preconditions and all effectsexcept those that are literals in the goal.

Token 7750:
Then, we count the minimum number of actionsrequired such that the union of those actions’ effects satisﬁes the goal.

Token 7751:
This is an instanceof the set-cover problem . There is one minor irritation: the set-cover problem is NP-hard.

Token 7752:
SET-COVER PROBLEM Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is within a factor of lognof the true minimum covering, where nis the number of literals in the goal.

Token 7753:
Unfortunately, the greedy algorithm loses the guarantee of admissibility. It is also possible to ignore only selected preconditions of actions.

Token 7754:
Consider the sliding- block puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning

Token 7755:
Section 10.2.

Token 7756:
Algorithms for Planning as State-Space Search 377 problem involving tiles with a single schema Slide : Action (Slide(t,s1,s2), PRECOND :On(t,s1)∧Tile(t)∧Blank (s2)∧Adjacent (s1,s2) EFFECT :On(t,s2)∧Blank (s1)∧¬On(t,s1)∧¬Blank (s2)) As we saw in Section 3.6, if we remove the preconditions Blank (s2)∧Adjacent (s1,s2) then any tile can move in one action to any space and we get the number-of-misplaced-tilesheuristic.

Token 7757:
If we remove Blank (s 2)then we get the Manhattan-distance heuristic.

Token 7758:
It is easy to see how these heuristics could be derived automatically from the action schema description.The ease of manipulating the schemas is the great advantage of the factored representation ofplanning problems, as compared with the atomic representation of search problems.

Token 7759:
Another possibility is the ignore delete lists heuristic.

Token 7760:
Assume for a moment that all IGNORE DELETE LISTS goals and preconditions contain only positive literals3We want to create a relaxed version of the original problem that will be easier to solve, and where the length of the solution will serve as a good heuristic.

Token 7761:
We can do that by removing the delete lists from all actions (i.e., removingall negative literals from effects).

Token 7762:
That makes it possible to make monotonic progress towardsthe goal—no action will ever undo progress made by another action.

Token 7763:
It turns out it is still NP-hard to ﬁnd the optimal solution to this relaxed problem, but an approximate solution can befound in polynomial time by hill-climbing.

Token 7764:
Figure 10.6 diagrams part of the state space fortwo planning problems using the ignore-delete-lists heuristic.

Token 7765:
The dots represent states andthe edges actions, and the height of each dot above the bottom plane represents the heuristicvalue.

Token 7766:
States on the bottom plane are solutions. In both these problems, there is a wide pathto the goal.

Token 7767:
There are no dead ends, so no need for backtracking; a simple hillclimbing searchwill easily ﬁnd a solution to these problems (although it may not be an optimal solution).

Token 7768:
The relaxed problems leave us with a simpliﬁed—but still expensive—planning prob- lem just to calculate the value of the heuristic function.

Token 7769:
Many planning problems have 10 100 states or more, and relaxing the actions does nothing to reduce the number of states.

Token 7770:
There- fore, we now look at relaxations that decrease the number of states by forming a state ab- straction —a many-to-one mapping from states in the ground representation of the problem STATE ABSTRACTION to the abstract representation.

Token 7771:
The easiest form of state abstraction is to ignore some ﬂuents.

Token 7772:
For example, consider an air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo.

Token 7773:
Each plane canbe at one of 10 airports and each package can be either in one of the planes or unloaded atone of the airports.

Token 7774:
So there are 50 10×20050+10≈10155states.

Token 7775:
Now consider a particular problem in that domain in which it happens that all the packages are at just 5 of the airports, and all packages at a given airport have the same destination.

Token 7776:
Then a useful abstraction of theproblem is to drop all the Atﬂuents except for the ones involving one plane and one package at each of the 5 airports.

Token 7777:
Now there are only 5 10×55+10≈1017states.

Token 7778:
A solution in this abstract state space will be shorter than a solution in the original space (and thus will be anadmissible heuristic), and the abstract solution is easy to extend to a solution to the originalproblem (by adding additional Load andUnload actions).

Token 7779:
3Many problems are written with this convention.

Token 7780:
For problems that aren’t, replace every negative literal ¬P in a goal or precondition with a new positive literal, P/prime.

Token 7781:
378 Chapter 10. Classical Planning Figure 10.6 Two state spaces from planning problems with the ignore-delete-lists heuris- tic.

Token 7782:
The height above the bottom plane is the heuristic score of a state; states on the bottom plane are goals.

Token 7783:
There are no local minima, so search for the goal is straightforward. FromHoffmann (2005).

Token 7784:
A key idea in deﬁning heuristics is decomposition : dividing a problem into parts, solv- DECOMPOSITION ing each part independently, and then combining the parts.

Token 7785:
The subgoal independence as-SUBGOAL INDEPENDENCE sumption is that the cost of solving a conjunction of subgoals is approximated by the sum of the costs of solving each subgoal independently .

Token 7786:
The subgoal independence assumption can be optimistic or pessimistic.

Token 7787:
It is optimistic when there are negative interactions betweenthe subplans for each subgoal—for example, when an action in one subplan deletes a goalachieved by another subplan.

Token 7788:
It is pessimistic, and therefore inadmissible, when subplanscontain redundant actions—for instance, two actions that could be replaced by a single actionin the merged plan.

Token 7789:
Suppose the goal is a set of ﬂuents G, which we divide into disjoint subsets G 1,...,G n. We then ﬁnd plans P1,...,P nthat solve the respective subgoals.

Token 7790:
What is an estimate of the cost of the plan for achieving all of G?

Token 7791:
We can think of each Cost(Pi)as a heuristic estimate, and we know that if we combine estimates by taking their maximum value, we always get anadmissible heuristic.

Token 7792:
So max iCOST(Pi)is admissible, and sometimes it is exactly correct: it could be that P1serendipitously achieves all the Gi.

Token 7793:
But in most cases, in practice the estimate is too low. Could we sum the costs instead?

Token 7794:
For many problems that is a reasonableestimate, but it is not admissible. The best case is when we can determine that G iandGjare independent .

Token 7795:
If the effects of Pileave all the preconditions and goals of Pjunchanged, then the estimate C OST(Pi)+COST(Pj)is admissible, and more accurate than the max estimate.

Token 7796:
We show in Section 10.3.1 that planning graphs can help provide better heuristic estimates.

Token 7797:
It is clear that there is great potential for cutting down the search space by forming ab- stractions.

Token 7798:
The trick is choosing the right abstractions and using them in a way that makes the total cost—deﬁning an abstraction, doing an abstract search, and mapping the abstraction back to the original problem—less than the cost of solving the original problem.

Token 7799:
The tech-

Token 7800:
Section 10.3.

Token 7801:
Planning Graphs 379 niques of pattern databases from Section 3.6.3 can be useful, because the cost of creating the pattern database can be amortized over multiple problem instances.

Token 7802:
An example of a system that makes use of effective heuristics is FF, or F ASTFORW ARD (Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,estimating the heuristic with the help of a planning graph (see Section 10.3).

Token 7803:
FF then uses hill-climbing search (modiﬁed to keep track of the plan) with the heuristic to ﬁnd a solution.

Token 7804:
When it hits a plateau or local maximum—when no action leads to a state with better heuristicscore—then FF uses iterative deepening search until it ﬁnds a state that is better, or it givesup and restarts hill-climbing.

Token 7805:
10.3 P LANNING GRAPHS All of the heuristics we have suggested can suffer from inaccuracies.

Token 7806:
This section showshow a special data structure called a planning graph can be used to give better heuristic PLANNING GRAPH estimates.

Token 7807:
These heuristics can be applied to any of the search techniques we have seen so far.

Token 7808:
Alternatively, we can search for a solution over the space formed by the planning graph, using an algorithm called G RAPHPLAN .

Token 7809:
A planning problem asks if we can reach a goal state from the initial state.

Token 7810:
Suppose we are given a tree of all possible actions from the initial state to successor states, and their suc-cessors, and so on.

Token 7811:
If we indexed this tree appropriately, we could answer the planning ques-tion “can we reach state Gfrom state S 0” immediately, just by looking it up.

Token 7812:
Of course, the tree is of exponential size, so this approach is impractical.

Token 7813:
A planning graph is polynomial-size approximation to this tree that can be constructed quickly.

Token 7814:
The planning graph can’tanswer deﬁnitively whether Gis reachable from S 0, but it can estimate how many steps it takes to reach G. The estimate is always correct when it reports the goal is not reachable, and it never overestimates the number of steps, so it is an admissible heuristic.

Token 7815:
A planning graph is a directed graph organized into levels :ﬁ r s tal e v e l S0for the initial LEVEL state, consisting of nodes representing each ﬂuent that holds in S0;t h e nal e v e l A0consisting of nodes for each ground action that might be applicable in S0; then alternating levels Si followed by Ai; until we reach a termination condition (to be discussed later).

Token 7816:
Roughly speaking, Sicontains all the literals that could hold at time i, depending on the actions executed at preceding time steps.

Token 7817:
If it is possible that either Por¬Pcould hold, then both will be represented in Si.

Token 7818:
Also roughly speaking, Aicontains all the actions that could have their preconditions satisﬁed at time i.

Token 7819:
We say “roughly speaking” because the planning graph records only a restricted subset of the possible negative interactions amongactions; therefore, a literal might show up at level S jwhen actually it could not be true until a later level, if at all.

Token 7820:
(A literal will never show up too late.)

Token 7821:
Despite the possible error, the leveljat which a literal ﬁrst appears is a good estimate of how difﬁcult it is to achieve the literal from the initial state.

Token 7822:
Planning graphs work only for propositional planning problems—ones with no vari- ables.

Token 7823:
As we mentioned on page 368, it is straightforward to propositionalize a set of ac-

Token 7824:
380 Chapter 10.

Token 7825:
Classical Planning Init(Have(Cake)) Goal(Have(Cake)∧Eaten (Cake)) Action (Eat(Cake) PRECOND :Have(Cake) EFFECT :¬Have(Cake)∧Eaten (Cake)) Action (Bake(Cake) PRECOND :¬Have(Cake) EFFECT :Have(Cake)) Figure 10.7 The “have cake and eat cake too” problem.

Token 7826:
Bake(Cake) Eat(Cake)Have (Cake )S0 A0S1 A1S2 Have (Cake ) Have (Cake ) Have (Cake ) Have (Cake ) Eaten (Cake ) Eaten (Cake ) Eaten (Cake ) Eaten (Cake )Eaten (Cake )Eat(Cake)¬ ¬ ¬¬ ¬ Figure 10.8 The planning graph for the “have cake and eat cake too” problem up to level S2.

Token 7827:
Rectangles indicate actions (small squares i ndicate persistence actions), and straight lines indicate preconditions and effects.

Token 7828:
Mutex links are shown as curved gray lines. Not all mutex links are shown, because the graph would be too cluttered.

Token 7829:
In general, if two literals are mutex at Si, then the persistence actions for those literals will be mutex at Aiand we need not draw that mutex link.

Token 7830:
tion schemas.

Token 7831:
Despite the resulting increase in the size of the problem description, planning graphs have proved to be effective tools for solving hard planning problems.

Token 7832:
Figure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning graph.

Token 7833:
Each action at level Aiis connected to its preconditions at Siand its effects at Si+1.

Token 7834:
So a literal appears because an action caused it, but we also want to say that a literal canpersist if no action negates it.

Token 7835:
This is represented by a persistence action (sometimes called PERSISTENCE ACTION ano-op ).

Token 7836:
For every literal C, we add to the problem a persistence action with precondition C and effect C.L e v e l A0in Figure 10.8 shows one “real” action, Eat(Cake), along with two persistence actions drawn as small square boxes.

Token 7837:
Level A0contains all the actions that could occur in state S0, but just as important it records conﬂicts between actions that would prevent them from occurring together.

Token 7838:
The graylines in Figure 10.8 indicate mutual exclusion (ormutex ) links.

Token 7839:
For example, Eat(Cake)is MUTUAL EXCLUSION MUTEX mutually exclusive with the persistence of either Have(Cake)or¬Eaten (Cake).W e s h a l l see shortly how mutex links are computed.

Token 7840:
LevelS1contains all the literals that could result from picking any subset of the actions inA0, as well as mutex links (gray lines) indicating literals that could not appear together, regardless of the choice of actions.

Token 7841:
For example, Have(Cake)andEaten (Cake)are mutex:

Token 7842:
Section 10.3. Planning Graphs 381 depending on the choice of actions in A0, either, but not both, could be the result.

Token 7843:
In other words, S1represents a belief state: a set of possible states.

Token 7844:
The members of this set are all subsets of the literals such that there is no mutex link between any members of the subset.

Token 7845:
We continue in this way, alternating between state level Siand action level Aiuntil we reach a point where two consecutive levels are identical.

Token 7846:
At this point, we say that the graph hasleveled off . The graph in Figure 10.8 levels off at S2.

Token 7847:
LEVELED OFF What we end up with is a structure where every Ailevel contains all the actions that are applicable in Si, along with constraints saying that two actions cannot both be executed at the same level.

Token 7848:
Every Silevel contains all the literals that could result from any possible choice of actions in Ai−1, along with constraints saying which pairs of literals are not possible.

Token 7849:
It is important to note that the process of constructing the planning graph does notrequire choosing among actions, which would entail combinatorial search.

Token 7850:
Instead, it just records theimpossibility of certain choices using mutex links. We now deﬁne mutex links for both actions and literals.

Token 7851:
A mutex relation holds between two actions at a given level if any of the following three conditions holds: •Inconsistent effects: one action negates an effect of the other.

Token 7852:
For example, Eat(Cake) and the persistence of Have(Cake)have inconsistent effects because they disagree on the effect Have(Cake).

Token 7853:
•Interference: one of the effects of one action is the negation of a precondition of the other.

Token 7854:
For example Eat(Cake)interferes with the persistence of Have(Cake)by negat- ing its precondition.

Token 7855:
•Competing needs: one of the preconditions of one action is mutually exclusive with a precondition of the other.

Token 7856:
For example, Bake(Cake)andEat(Cake)are mutex because they compete on the value of the Have(Cake)precondition.

Token 7857:
A mutex relation holds between two literals at the same level if one is the negation of the other or if each possible pair of actions that could achieve the two literals is mutually exclusive.

Token 7858:
This condition is called inconsistent support .

Token 7859:
For example, Have(Cake)andEaten (Cake) are mutex in S 1because the only way of achieving Have(Cake), the persistence action, is mutex with the only way of achieving Eaten (Cake), namely Eat(Cake).I nS2the two literals are not mutex, because there are new ways of achieving them, such as Bake(Cake) and the persistence of Eaten (Cake), that are not mutex.

Token 7860:
A planning graph is polynomial in the size of the planning problem.

Token 7861:
For a planning problem with lliterals and aactions, each Sihas no more than lnodes and l2mutex links, and each Aihas no more than a+lnodes (including the no-ops), (a+l)2mutex links, and 2(al+l)precondition and effect links.

Token 7862:
Thus, an entire graph with nlevels has a size of O(n(a+l)2). The time to build the graph has the same complexity.

Token 7863:
10.3.1 Planning graphs for heuristic estimation A planning graph, once constructed, is a rich source of information about the problem.

Token 7864:
First, if any goal literal fails to appear in the ﬁnal level of the graph, then the problem is unsolvable.Second, we can estimate the cost of achieving any goal literal g ifrom state sas the level at which giﬁrst appears in the planning graph constructed from initial state s. We call this the

Token 7865:
382 Chapter 10. Classical Planning level cost ofgi. In Figure 10.8, Have(Cake)has level cost 0 and Eaten (Cake)has level cost LEVEL COST 1.

Token 7866:
It is easy to show (Exercise 10.10) that these estimates are admissible for the individual goals.

Token 7867:
The estimate might not always be accurate, however, because planning graphs allowseveral actions at each level, whereas the heuristic counts just the level and not the numberof actions.

Token 7868:
For this reason, it is common to use a serial planning graph for computing SERIAL PLANNING GRAPH heuristics.

Token 7869:
A serial graph insists that only one action can actually occur at any given time step; this is done by adding mutex links between every pair of nonpersistence actions.

Token 7870:
Levelcosts extracted from serial graphs are often quite reasonable estimates of actual costs.

Token 7871:
To estimate the cost of a conjunction of goals, there are three simple approaches.

Token 7872:
The max-level heuristic simply takes the maximum level cost of any of the goals; this is admissi- MAX-LEVEL ble, but not necessarily accurate.

Token 7873:
The level sum heuristic, following the subgoal independence assumption, returns the LEVEL SUM sum of the level costs of the goals; this can be inadmissible but works well in practice for problems that are largely decomposable.

Token 7874:
It is much more accurate than the number-of-unsatisﬁed-goals heuristic from Section 10.2.

Token 7875:
For our problem, the level-sum heuristicestimate for the conjunctive goal Have(Cake)∧Eaten (Cake)will be 0+1=1 , whereas the correct answer is 2, achieved by the plan [Eat(Cake),Bake(Cake)].

Token 7876:
That doesn’t seem so bad.

Token 7877:
A more serious error is that if Bake(Cake)were not in the set of actions, then the estimate would still be 1, when in fact the conjunctive goal would be impossible.

Token 7878:
Finally, the set-level heuristic ﬁnds the level at which all the literals in the conjunctive SET-LEVEL goal appear in the planning graph without any pair of them being mutually exclusive.

Token 7879:
This heuristic gives the correct values of 2 for our original problem and inﬁnity for the problemwithout Bake(Cake).

Token 7880:
It is admissible, it dominates the max-level heuristic, and it works extremely well on tasks in which there is a good deal of interaction among subplans.

Token 7881:
It is notperfect, of course; for example, it ignores interactions among three or more literals.

Token 7882:
As a tool for generating accurate heuristics, we can view the planning graph as a relaxed problem that is efﬁciently solvable.

Token 7883:
To understand the nature of the relaxed problem, we need to understand exactly what it means for a literal gto appear at level S iin the planning graph.

Token 7884:
Ideally, we would like it to be a guarantee that there exists a plan with iaction levels that achieves g, and also that if gdoes not appear, there is no such plan.

Token 7885:
Unfortunately, making that guarantee is as difﬁcult as solving the original planning problem.

Token 7886:
So the planninggraph makes the second half of the guarantee (if gdoes not appear, there is no plan), but ifgdoes appear, then all the planning graph promises is that there is a plan that possibly achieves gand has no “obvious” ﬂaws.

Token 7887:
An obvious ﬂaw is deﬁned as a ﬂaw that can be detected by considering two actions or two literals at a time—in other words, by looking atthe mutex relations.

Token 7888:
There could be more subtle ﬂaws involving three, four, or more actions,but experience has shown that it is not worth the computational effort to keep track of thesepossible ﬂaws.

Token 7889:
This is similar to a lesson learned from constraint satisfaction problems—thatit is often worthwhile to compute 2-consistency before searching for a solution, but less often worthwhile to compute 3-consistency or higher.

Token 7890:
(See page 211.)

Token 7891:
One example of an unsolvable problem that cannot be recognized as such by a planning graph is the blocks-world problem where the goal is to get block AonB,BonC,a n dCon A.

Token 7892:
This is an impossible goal; a tower with the bottom on top of the top. But a planning graph

Token 7893:
Section 10.3. Planning Graphs 383 cannot detect the impossibility, because any two of the three subgoals are achievable.

Token 7894:
There are no mutexes between any pair of literals, only between the three as a whole.

Token 7895:
To detect thatthis problem is impossible, we would have to search over the planning graph.

Token 7896:
10.3.2 The GRAPHPLAN algorithm This subsection shows how to extract a plan directly from the planning graph, rather than just using the graph to provide a heuristic.

Token 7897:
The G RAPHPLAN algorithm (Figure 10.9) repeatedly adds a level to a planning graph with E XPAND -GRAPH .

Token 7898:
Once all the goals show up as non- mutex in the graph, G RAPHPLAN calls E XTRACT -SOLUTION to search for a plan that solves the problem.

Token 7899:
If that fails, it expands another level and tries again, terminating with failurewhen there is no reason to go on.

Token 7900:
function GRAPHPLAN (problem )returns solution or failure graph←INITIAL -PLANNING -GRAPH (problem ) goals←CONJUNCTS (problem .GOAL) nogoods←an empty hash table fortl=0to∞do ifgoals all non-mutex in Stofgraph then solution←EXTRACT -SOLUTION (graph ,goals ,NUMLEVELS (graph ),nogoods ) ifsolution/negationslash=failure then return solution ifgraph andnogoods have both leveled off then return failure graph←EXPAND -GRAPH (graph ,problem ) Figure 10.9 The G RAPHPLAN algorithm.

Token 7901:
G RAPHPLAN calls E XPAND -GRAPH to add a level until either a solution is found by E XTRACT -SOLUTION , or no solution is possible.

Token 7902:
Let us now trace the operation of G RAPHPLAN on the spare tire problem from page 370. The graph is shown in Figure 10.10.

Token 7903:
The ﬁrst line of G RAPHPLAN initializes the planning graph to a one-level ( S0) graph representing the initial state.

Token 7904:
The positive ﬂuents from the problem description’s initial state are shown, as are the relevant negative ﬂuents.

Token 7905:
Not shownare the unchanging positive literals (such as Tire(Spare)) and the irrelevant negative literals.

Token 7906:
The goal At(Spare,Axle)is not present in S 0, so we need not call E XTRACT -SOLUTION — we are certain that there is no solution yet.

Token 7907:
Instead, E XPAND -GRAPH adds into A0the three actions whose preconditions exist at level S0(i.e., all the actions except PutOn (Spare,Axle)), along with persistence actions for all the literals in S0.

Token 7908:
The effects of the actions are added at levelS1.EXPAND -GRAPH then looks for mutex relations and adds them to the graph.

Token 7909:
At(Spare,Axle)is still not present in S1, so again we do not call E XTRACT -SOLUTION .

Token 7910:
We call E XPAND -GRAPH again, adding A1andS1and giving us the planning graph shown in Figure 10.10.

Token 7911:
Now that we have the full complement of actions, it is worthwhile to look at some of the examples of mutex relations and their causes: •Inconsistent effects: Remove (Spare,Trunk )is mutex with LeaveOvernight because one has the effect At(Spare,Ground )and the other has its negation.

Token 7912:
384 Chapter 10.

Token 7913:
Classical Planning S0A1S2 At(Spare,Trunk) At(Spare,Trunk) At(Flat,Axle) At(Flat,Axle) At(Spare,Axle) At(Flat,Ground) At(Flat,Ground)At(Spare,Ground)At(Spare,Ground)At(Spare,Trunk) At(Spare,Trunk) At(Flat,Axle)At(Flat,Axle) At(Spare,Axle) At(Flat,Ground) At(Flat,Ground) At(Spare,Ground) At(Spare,Ground)At(Spare,Axle)At(Spare,Trunk) At(Flat,Axle) At(Spare,Axle) At(Flat,Ground) At(Spare,Ground)PutOn(Spare,Axle)LeaveOvernightRemove(Flat,Axle)Remove(Spare,Trunk)Remove(Spare,Trunk) Remove(Flat,Axle) LeaveOvernight ¬ ¬ ¬¬ ¬ ¬ ¬ ¬¬ ¬ ¬ ¬ ¬A0S1 Figure 10.10 The planning graph for the spare tire problem after expansion to level S2.

Token 7914:
Mutex links are shown as gray lines. Not all links are shown, because the graph would be too cluttered if we showed them all.

Token 7915:
The solution is indicated by bold lines and outlines.

Token 7916:
•Interference: Remove (Flat,Axle)is mutex with LeaveOvernight because one has the precondition At(Flat,Axle)and the other has its negation as an effect.

Token 7917:
•Competing needs: PutOn (Spare,Axle)is mutex with Remove (Flat,Axle)because one has At(Flat,Axle)as a precondition and the other has its negation.

Token 7918:
•Inconsistent support: At(Spare,Axle)is mutex with At(Flat,Axle)inS2because the only way of achieving At(Spare,Axle)is byPutOn (Spare,Axle), and that is mutex with the persistence action that is the only way of achieving At(Flat,Axle).

Token 7919:
Thus, the mutex relations detect the immediate conﬂict that arises from trying to put two objectsin the same place at the same time.

Token 7920:
This time, when we go back to the start of the loop, all the literals from the goal are present inS 2, and none of them is mutex with any other.

Token 7921:
That means that a solution might exist, and E XTRACT -SOLUTION will try to ﬁnd it.

Token 7922:
We can formulate E XTRACT -SOLUTION as a Boolean constraint satisfaction problem (CSP) where the variables are the actions at eachlevel, the values for each variable are inoroutof the plan, and the constraints are the mutexes and the need to satisfy each goal and precondition.

Token 7923:
Alternatively, we can deﬁne E XTRACT -SOLUTION as a backward search problem, where each state in the search contains a pointer to a level in the planning graph and a set of unsat- isﬁed goals.

Token 7924:
We deﬁne this search problem as follows: •The initial state is the last level of the planning graph, Sn, along with the set of goals from the planning problem.

Token 7925:
•The actions available in a state at level Siare to select any conﬂict-free subset of the actions in Ai−1whose effects cover the goals in the state.

Token 7926:
The resulting state has level Si−1and has as its set of goals the preconditions for the selected set of actions.

Token 7927:
By “conﬂict free,” we mean a set of actions such that no two of them are mutex and no twoof their preconditions are mutex.

Token 7928:
Section 10.3. Planning Graphs 385 •The goal is to reach a state at level S0such that all the goals are satisﬁed. •The cost of each action is 1.

Token 7929:
For this particular problem, we start at S2with the goal At(Spare,Axle). The only choice we have for achieving the goal set is PutOn (Spare,Axle).

Token 7930:
That brings us to a search state at S1 with goals At(Spare,Ground )and¬At(Flat,Axle).

Token 7931:
The former can be achieved only by Remove (Spare,Trunk ), and the latter by either Remove (Flat,Axle)orLeaveOvernight .

Token 7932:
ButLeaveOvernight is mutex with Remove (Spare,Trunk ), so the only solution is to choose Remove (Spare,Trunk )andRemove (Flat,Axle).

Token 7933:
That brings us to a search state at S0with the goals At(Spare,Trunk )andAt(Flat,Axle).

Token 7934:
Both of these are present in the state, so we have a solution: the actions Remove (Spare,Trunk )andRemove (Flat,Axle)in level A0, followed by PutOn (Spare,Axle)inA1.

Token 7935:
In the case where E XTRACT -SOLUTION fails to ﬁnd a solution for a set of goals at a given level, we record the (level,goals)pair as a no-good , just as we did in constraint learning for CSPs (page 220).

Token 7936:
Whenever E XTRACT -SOLUTION is called again with the same level and goals, we can ﬁnd the recorded no-good and immediately return failure rather thansearching again.

Token 7937:
We see shortly that no-goods are also used in the termination test.

Token 7938:
We know that planning is PSPACE-complete and that constructing the planning graph takes polynomial time, so it must be the case that solution extraction is intractable in the worstcase.

Token 7939:
Therefore, we will need some heuristic guidance for choosing among actions during thebackward search.

Token 7940:
One approach that works well in practice is a greedy algorithm based onthe level cost of the literals.

Token 7941:
For any set of goals, we proceed in the following order: 1. Pick ﬁrst the literal with the highest level cost. 2.

Token 7942:
To achieve that literal, prefer actions with easier preconditions.

Token 7943:
That is, choose an action such that the sum (or maximum) of the level costs of its preconditions is smallest.

Token 7944:
10.3.3 Termination of GRAPHPLAN So far, we have skated over the question of termination.

Token 7945:
Here we show that G RAPHPLAN will in fact terminate and return failure when there is no solution.

Token 7946:
The ﬁrst thing to understand is why we can’t stop expanding the graph as soon as it has leveled off.

Token 7947:
Consider an air cargo domain with one plane and npieces of cargo at airport A, all of which have airport Bas their destination.

Token 7948:
In this version of the problem, only one piece of cargo can ﬁt in the plane at a time.

Token 7949:
The graph will level off at level 4, reﬂecting thefact that for any single piece of cargo, we can load it, ﬂy it, and unload it at the destination inthree steps.

Token 7950:
But that does not mean that a solution can be extracted from the graph at level 4;in fact a solution will require 4n−1steps: for each piece of cargo we load, ﬂy, and unload, and for all but the last piece we need to ﬂy back to airport Ato get the next piece.

Token 7951:
How long do we have to keep expanding after the graph has leveled off?

Token 7952:
If the function E XTRACT -SOLUTION fails to ﬁnd a solution, then there must have been at least one set of goals that were not achievable and were marked as a no-good.

Token 7953:
So if it is possible that there might be fewer no-goods in the next level, then we should continue.

Token 7954:
As soon as the graphitself and the no-goods have both leveled off, with no solution found, we can terminate withfailure because there is no possibility of a subsequent change that could add a solution.

Token 7955:
386 Chapter 10. Classical Planning Now all we have to do is prove that the graph and the no-goods will always level off.

Token 7956:
The key to this proof is that certain properties of planning graphs are monotonically increasing ordecreasing.

Token 7957:
“X increases monotonically” means that the set of Xs at level i+1is a superset (not necessarily proper) of the set at level i.

Token 7958:
The properties are as follows: •Literals increase monotonically: Once a literal appears at a given level, it will appear at all subsequent levels.

Token 7959:
This is because of the persistence actions; once a literal showsup, persistence actions cause it to stay forever.

Token 7960:
•Actions increase monotonically: Once an action appears at a given level, it will appear at all subsequent levels.

Token 7961:
This is a consequence of the monotonic increase of literals; ifthe preconditions of an action appear at one level, they will appear at subsequent levels,and thus so will the action.

Token 7962:
•Mutexes decrease monotonically: If two actions are mutex at a given level A i, then they will also be mutex for all previous levels at which they both appear.

Token 7963:
The same holds for mutexes between literals.

Token 7964:
It might not always appear that way in the ﬁgures, becausethe ﬁgures have a simpliﬁcation: they display neither literals that cannot hold at levelS inor actions that cannot be executed at level Ai.

Token 7965:
We can see that “mutexes decrease monotonically” is true if you consider that these invisible literals and actions are mutexwith everything.

Token 7966:
The proof can be handled by cases: if actions AandBare mutex at level A i,i t must be because of one of the three types of mutex.

Token 7967:
The ﬁrst two, inconsistent effectsand interference, are properties of the actions themselves, so if the actions are mutexatA i, they will be mutex at every level.

Token 7968:
The third case, competing needs, depends on conditions at level Si: that level must contain a precondition of Athat is mutex with a precondition of B.

Token 7969:
Now, these two preconditions can be mutex if they are negations of each other (in which case they would be mutex in every level) or if all actions for achieving one are mutex with all actions for achieving the other.

Token 7970:
But we already know that the available actions are increasing monotonically, so, by induction, the mutexesmust be decreasing.

Token 7971:
•No-goods decrease monotonically: If a set of goals is not achievable at a given level, then they are not achievable in any previous level.

Token 7972:
The proof is by contradiction: if they were achievable at some previous level, then we could just add persistence actions to make them achievable at a subsequent level.

Token 7973:
Because the actions and literals increase monotonically and because there are only a ﬁnite number of actions and literals, there must come a level that has the same number of actions and literals as the previous level.

Token 7974:
Because mutexes and no-goods decrease, and because there can never be fewer than zero mutexes or no-goods, there must come a level that has thesame number of mutexes and no-goods as the previous level.

Token 7975:
Once a graph has reached thisstate, then if one of the goals is missing or is mutex with another goal, then we can stop the G RAPHPLAN algorithm and return failure.

Token 7976:
That concludes a sketch of the proof; for more details see Ghallab et al. (2004).

Token 7977:
Section 10.4.

Token 7978:
Other Classical Planning Approaches 387 Year Track Winning Systems (approaches) 2008 Optimal GAMER (model checking, bidirectional search) 2008 Satisﬁcing LAMA (fast downward search with FF heuristic) 2006 Optimal SATP LAN,MAXPLAN (Boolean satisﬁability) 2006 Satisﬁcing SGPLAN (forward search; partiti ons into independent subproblems) 2004 Optimal SATP LAN (Boolean satisﬁability) 2004 Satisﬁcing FAST DIAGONALLY DOWNWARD (forward search with causal graph) 2002 Automated LPG (local search, planning graphs converted to CSPs) 2002 Hand-coded TLPLAN (temporal action logic with control rules for forward search) 2000 Automated FF (forward search) 2000 Hand-coded TALPLANNER (temporal action logic with control rules for forward search) 1998 Automated IPP (planning graphs); HSP (forward search) Figure 10.11 Some of the top-performing systems in the International Planning Compe- tition.

Token 7979:
Each year there are various tracks: “ Optimal” means the planners must produce the shortest possible plan, while “Satisﬁcing” means nonoptimal solutions are accepted.

Token 7980:
“Hand- coded” means domain-speciﬁc heuristics are allowed; “Automated” means they are not.

Token 7981:
10.4 O THER CLASSICAL PLANNING APPROACHES Currently the most popular and effective approaches to fully automated planning are: •Translating to a Boolean satisﬁability (SAT) problem •Forward state-space search with carefully crafted heuristics (Section 10.2) •Search using a planning graph (Section 10.3) These three approaches are not the only ones tried in the 40-year history of automated plan- ning.

Token 7982:
Figure 10.11 shows some of the top systems in the International Planning Competitions,which have been held every even year since 1998.

Token 7983:
In this section we ﬁrst describe the transla-tion to a satisﬁability problem and then describe three other inﬂuential approaches: planningas ﬁrst-order logical deduction; as constraint satisfaction; and as plan reﬁnement.

Token 7984:
10.4.1 Classical planning as Boolean satisﬁability In Section 7.7.4 we saw how SATP LAN solves planning problems that are expressed in propo- sitional logic.

Token 7985:
Here we show how to translate a PDDL description into a form that can beprocessed by SATP LAN.

Token 7986:
The translation is a series of straightforward steps: •Propositionalize the actions: replace each action schema with a set of ground actions formed by substituting constants for each of the variables.

Token 7987:
These ground actions are notpart of the translation, but will be used in subsequent steps.

Token 7988:
•Deﬁne the initial state: assert F 0for every ﬂuent Fin the problem’s initial state, and ¬Ffor every ﬂuent not mentioned in the initial state.

Token 7989:
•Propositionalize the goal: for every variable in the goal, replace the literals that contain the variable with a disjunction over constants.

Token 7990:
For example, the goal of having block A

Token 7991:
388 Chapter 10.

Token 7992:
Classical Planning on another block, On(A,x)∧Block(x)in a world with objects A,B andC, would be replaced by the goal (On(A,A)∧Block(A))∨(On(A,B)∧Block(B))∨(On(A,C)∧Block(C)).

Token 7993:
•Add successor-state axioms: For each ﬂuent F, add an axiom of the form Ft+1⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt), where ActionCausesF is a disjunction of all the ground actions that have Fin their add list, and ActionCausesNotF is a disjunction of all the ground actions that have F in their delete list.

Token 7994:
•Add precondition axioms: For each ground action A, add the axiom At⇒PRE(A)t, that is, if an action is taken at time t, then the preconditions must have been true.

Token 7995:
•Add action exclusion axioms: say that every action is distinct from every other action.

Token 7996:
The resulting translation is in the form that we can hand to SATP LAN to ﬁnd a solution.

Token 7997:
10.4.2 Planning as ﬁrst-order logical deduction: Situation calculus PDDL is a language that carefully balances the expressiveness of the language with the com- plexity of the algorithms that operate on it.

Token 7998:
But some problems remain difﬁcult to express inPDDL.

Token 7999:
For example, we can’t express the goal “move all the cargo from AtoBregardless of how many pieces of cargo there are” in PDDL, but we can do it in ﬁrst-order logic, using auniversal quantiﬁer.

Token 8000:
Likewise, ﬁrst-order logic can concisely express global constraints suchas “no more than four robots can be in the same place at the same time.” PDDL can only say this with repetitious preconditions on every possible action that involves a move.

Token 8001:
The propositional logic representation of planning problems also has limitations, such as the fact that the notion of time is tied directly to ﬂuents.

Token 8002:
For example, South 2means “the agent is facing south at time 2.” With that representation, there is no way to say “theagent would be facing south at time 2 if it executed a right turn at time 1; otherwise it wouldbe facing east.” First-order logic lets us get around this limitation by replacing the notionof linear time with a notion of branching situations , using a representation called situation calculus that works like this: SITUATION CALCULUS •The initial state is called a situation .I fsis a situation and ais an action, then SITUATION RESULT (s,a)is also a situation.

Token 8003:
There are no other situations. Thus, a situation cor- responds to a sequence, or history, of actions.

Token 8004:
You can also think of a situation as theresult of applying the actions, but note that two situations are the same only if their startand actions are the same: (R ESULT (s,a)= RESULT (s/prime,a/prime))⇔(s=s/prime∧a=a/prime).

Token 8005:
Some examples of actions and situations are shown in Figure 10.12. •A function or relation that can vary from one situation to the next is a ﬂuent .

Token 8006:
By conven- tion, the situation sis always the last argument to the ﬂuent, for example At(x,l,s)is a relational ﬂuent that is true when object xis at location lin situation s,a n dLocation is a functional ﬂuent such that Location (x,s)=lholds in the same situations as At(x,l,s).

Token 8007:
•Each action’s preconditions are described with a possibility axiom that says when the POSSIBILITY AXIOM action can be taken.

Token 8008:
It has the form Φ(s)⇒Poss(a,s)where Φ(s)is some formula

Token 8009:
Section 10.4.

Token 8010:
Other Classical Planning Approaches 389 PIT PIT PITGoldPIT PIT PITGoldPIT PIT PITGold S0ForwardResult (S0, Forward )Result (Result (S0, Forward ), Turn (Right )) Turn (Right ) Figure 10.12 Situations as the results of actions in the wumpus world.

Token 8011:
involving sthat describes the preconditions.

Token 8012:
An example from the wumpus world says that it is possible to shoot if the agent is alive and has an arrow: Alive(Agent ,s)∧Have(Agent ,Arrow ,s)⇒Poss(Shoot,s) •Each ﬂuent is described with a successor-state axiom that says what happens to the ﬂuent, depending on what action is taken.

Token 8013:
This is similar to the approach we took forpropositional logic.

Token 8014:
The axiom has the form Action is possible ⇒ (Fluent is true in result state ⇔Action’s effect made it true ∨It was true before and action left it alone ).

Token 8015:
For example, the axiom for the relational ﬂuent Holding says that the agent is holding some gold gafter executing a possible action if and only if the action was a Grab ofg or if the agent was already holding gand the action was not releasing it: Poss(a,s)⇒ (Holding (Agent ,g,Result (a,s))⇔ a=Grab(g)∨(Holding (Agent ,g,s)∧a/negationslash=Release (g))).

Token 8016:
•We need unique action axioms so that the agent can deduce that, for example, a/negationslash= UNIQUEACTION AXIOMS Release (g).

Token 8017:
For each distinct pair of action names AiandAjwe have an axiom that says the actions are different: Ai(x,...)/negationslash=Aj(y,...)

Token 8018:
390 Chapter 10.

Token 8019:
Classical Planning and for each action name Aiwe have an axiom that says two uses of that action name are equal if and only if all their arguments are equal: Ai(x1,...,x n)=Ai(y1,...,y n)⇔x1=y1∧...∧xn=yn.

Token 8020:
•A solution is a situation (and hence a sequence of actions) that satisﬁes the goal.

Token 8021:
Work in situation calculus has done a lot to deﬁne the formal semantics of planning and to open up new areas of investigation.

Token 8022:
But so far there have not been any practical large-scaleplanning programs based on logical deduction over the situation calculus.

Token 8023:
This is in partbecause of the difﬁculty of doing efﬁcient inference in FOL, but is mainly because the ﬁeldhas not yet developed effective heuristics for planning with situation calculus.

Token 8024:
10.4.3 Planning as constraint satisfaction We have seen that constraint satisfaction has a lot in common with Boolean satisﬁability, andwe have seen that CSP techniques are effective for scheduling problems, so it is not surprising that it is possible to encode a bounded planning problem (i.e., the problem of ﬁnding a plan of length k) as a constraint satisfaction problem (CSP).

Token 8025:
The encoding is similar to the encoding to a SAT problem (Section 10.4.1), with one important simpliﬁcation: at each time step weneed only a single variable, Action t, whose domain is the set of possible actions.

Token 8026:
We no longer need one variable for every action, and we don’t need the action exclusion axioms.

Token 8027:
Itis also possible to encode a planning graph into a CSP. This is the approach taken by GP-CSP(Do and Kambhampati, 2003).

Token 8028:
10.4.4 Planning as reﬁnement of partially ordered plans All the approaches we have seen so far construct totally ordered plans consisting of a strictly linear sequences of actions.

Token 8029:
This representation ignores the fact that many subproblems are independent.

Token 8030:
A solution to an air cargo problem consists of a totally ordered sequence of actions, yet if 30 packages are being loaded onto one plane in one airport and 50 packages are being loaded onto another at another airport, it seems pointless to come up with a strict linear ordering of 80 load actions; the two subsets of actions should be thought of independently.

Token 8031:
An alternative is to represent plans as partially ordered structures: a plan is a set of actions and a set of constraints of the form Before (ai,aj)saying that one action occurs before another.

Token 8032:
In the bottom of Figure 10.13, we see a partially ordered plan that is a solutionto the spare tire problem.

Token 8033:
Actions are boxes and ordering constraints are arrows.

Token 8034:
Note thatRemove (Spare,Trunk )andRemove (Flat,Axle)can be done in either order as long as they are both completed before the PutOn (Spare,Axle)action.

Token 8035:
Partially ordered plans are created by a search through the space of plans rather than through the state space.

Token 8036:
We start with the empty plan consisting of just the initial state andthe goal, with no actions in between, as in the top of Figure 10.13.

Token 8037:
The search procedure then looks for a ﬂaw in the plan, and makes an addition to the plan to correct the ﬂaw (or if no FLAW correction can be made, the search backtracks and tries something else).

Token 8038:
A ﬂaw is anything that keeps the partial plan from being a solution. For example, one ﬂaw in the empty plan isthat no action achieves At(Spare,Axle).

Token 8039:
One way to correct the ﬂaw is to insert into the plan

Token 8040:
Section 10.4.

Token 8041:
Other Classical Planning Approaches 391 Finish At(Spare,Axle ) Start At(Flat,Axle )At(Spare,Trunk ) (a) Remove(Spare,Trunk) At(Spare,Trunk ) PutOn(Spare,Axle)At(Spare,Ground ) At(Flat,Axle )Finish At(Spare,Axle ) Start At(Flat,Axle )At(Spare,Trunk ) ¬ (b) StartRemove(Spare,Trunk) At(Spare,Trunk ) Remove(Flat,Axle) At(Flat,Axle )PutOn(Spare,Axle)At(Spare,Ground ) At(Flat,Axle )Finish At(Spare,Axle ) At(Flat,Axle )At(Spare,Trunk ) ¬ (c) Figure 10.13 (a) the tire problem expressed as an empty plan.

Token 8042:
(b) an incomplete partially ordered plan for the tire problem. Boxes represent actions and arrows indicate that one action must occur before another.

Token 8043:
(c) a complete partially-ordered solution. the action PutOn (Spare,Axle).

Token 8044:
Of course that introduces some new ﬂaws: the preconditions of the new action are not achieved.

Token 8045:
The search keeps adding to the plan (backtracking ifnecessary) until all ﬂaws are resolved, as in the bottom of Figure 10.13.

Token 8046:
At every step, wemake the least commitment possible to ﬁx the ﬂaw.

Token 8047:
For example, in adding the action LEAST COMMITMENT Remove (Spare,Trunk )we need to commit to having it occur before PutOn (Spare,Axle), but we make no other commitment that places it before or after other actions.

Token 8048:
If there were avariable in the action schema that could be left unbound, we would do so.

Token 8049:
In the 1980s and 90s, partial-order planning was seen as the best way to handle plan- ning problems with independent subproblems—after all, it was the only approach that ex- plicitly represents independent branches of a plan.

Token 8050:
On the other hand, it has the disadvantageof not having an explicit representation of states in the state-transition model.

Token 8051:
That makessome computations cumbersome.

Token 8052:
By 2000, forward-search planners had developed excellentheuristics that allowed them to efﬁciently discover the independent subproblems that partial-order planning was designed for.

Token 8053:
As a result, partial-order planners are not competitive onfully automated classical planning problems.

Token 8054:
However, partial-order planning remains an important part of the ﬁeld.

Token 8055:
For some spe- ciﬁc tasks, such as operations scheduling, partial-order planning with domain speciﬁc heuris-tics is the technology of choice.

Token 8056:
Many of these systems use libraries of high-level plans, asdescribed in Section 11.2.

Token 8057:
Partial-order planning is also often used in domains where it is im- portant for humans to understand the plans.

Token 8058:
Operational plans for spacecraft and Mars rovers are generated by partial-order planners and are then checked by human operators before beinguploaded to the vehicles for execution.

Token 8059:
The plan reﬁnement approach makes it easier for thehumans to understand what the planning algorithms are doing and verify that they are correct.

Token 8060:
392 Chapter 10.

Token 8061:
Classical Planning 10.5 A NALYSIS OF PLANNING APPROACHES Planning combines the two major areas of AI we have covered so far: search and logic .A planner can be seen either as a program that searches for a solution or as one that (construc-tively) proves the existence of a solution.

Token 8062:
The cross-fertilization of ideas from the two areas has led both to improvements in performance amounting to several orders of magnitude in the last decade and to an increased use of planners in industrial applications.

Token 8063:
Unfortunately,we do not yet have a clear understanding of which techniques work best on which kinds ofproblems.

Token 8064:
Quite possibly, new techniques will emerge that dominate existing methods. Planning is foremost an exercise in controlling combinatorial explosion.

Token 8065:
If there are n propositions in a domain, then there are 2 nstates. As we have seen, planning is PSPACE- hard.

Token 8066:
Against such pessimism, the identiﬁcation of independent subproblems can be a pow- erful weapon.

Token 8067:
In the best case—full decomposability of the problem—we get an exponential speedup.

Token 8068:
Decomposability is destroyed, however, by negative interactions between actions.

Token 8069:
GRAPHPLAN records mutexes to point out where the difﬁcult interactions are.

Token 8070:
SATP LAN rep- resents a similar range of mutex relations, but does so by using the general CNF form rather than a speciﬁc data structure.

Token 8071:
Forward search addresses the problem heuristically by trying to ﬁnd patterns (subsets of propositions) that cover the independent subproblems.

Token 8072:
Since thisapproach is heuristic, it can work even when the subproblems are not completely independent.

Token 8073:
Sometimes it is possible to solve a problem efﬁciently by recognizing that negative interactions can be ruled out.

Token 8074:
We say that a problem has serializable subgoals if there exists SERIALIZABLE SUBGOAL an order of subgoals such that the planner can achieve them in that order without having to undo any of the previously achieved subgoals.

Token 8075:
For example, in the blocks world, if the goalis to build a tower (e.g., AonB, which in turn is on C, which in turn is on the Table ,a si n Figure 10.4 on page 371), then the subgoals are serializable bottom to top: if we ﬁrst achieveConTable , we will never have to undo it while we are achieving the other subgoals.

Token 8076:
A planner that uses the bottom-to-top trick can solve any problem in the blocks world without backtracking (although it might not always ﬁnd the shortest plan).

Token 8077:
As a more complex example, for the Remote Agent planner that commanded NASA’s Deep Space One spacecraft, it was determined that the propositions involved in command-ing a spacecraft are serializable.

Token 8078:
This is perhaps not too surprising, because a spacecraft isdesigned by its engineers to be as easy as possible to control (subject to other constraints).

Token 8079:
Taking advantage of the serialized ordering of goals, the Remote Agent planner was able toeliminate most of the search.

Token 8080:
This meant that it was fast enough to control the spacecraft inreal time, something previously considered impossible.

Token 8081:
Planners such as G RAPHPLAN ,S A T P LAN, and FF have moved the ﬁeld of planning forward, by raising the level of performance of planning systems, by clarifying the repre-sentational and combinatorial issues involved, and by the development of useful heuristics.

Token 8082:
However, there is a question of how far these techniques will scale.

Token 8083:
It seems likely that further progress on larger problems cannot rely only on factored and propositional representations,and will require some kind of synthesis of ﬁrst-order and hierarchical representations withthe efﬁcient heuristics currently in use.

Token 8084:
Section 10.6. Summary 393 10.6 S UMMARY In this chapter, we deﬁned the problem of planning in deterministic, fully observable, static environments.

Token 8085:
We described the PDDL representation for planning problems and severalalgorithmic approaches for solving them.

Token 8086:
The points to remember: •Planning systems are problem-solving algorithms that operate on explicit propositional or relational representations of states and actions.

Token 8087:
These representations make possi-ble the derivation of effective heuristics and the development of powerful and ﬂexiblealgorithms for solving problems.

Token 8088:
•PDDL, the Planning Domain Deﬁnition Language, describes the initial and goal states as conjunctions of literals, and actions in terms of their preconditions and effects.

Token 8089:
•State-space search can operate in the forward direction ( progression ) or the backward direction ( regression ).

Token 8090:
Effective heuristics can be derived by subgoal independence assumptions and by various relaxations of the planning problem.

Token 8091:
•Aplanning graph can be constructed incrementally, starting from the initial state.

Token 8092:
Each layer contains a superset of all the literals or actions that could occur at that time step and encodes mutual exclusion (mutex) relations among literals or actions that cannot co- occur.

Token 8093:
Planning graphs yield useful heuristics for state-space and partial-order planners and can be used directly in the G RAPHPLAN algorithm.

Token 8094:
•Other approaches include ﬁrst-order deduction over situation calculus axioms; encoding a planning problem as a Boolean satisﬁability problem or as a constraint satisfactionproblem; and explicitly searching through the space of partially ordered plans.

Token 8095:
•Each of the major approaches to planning has its adherents, and there is as yet no con- sensus on which is best.

Token 8096:
Competition and cross-fertilization among the approaches haveresulted in signiﬁcant gains in efﬁciency for planning systems.

Token 8097:
BIBLIOGRAPHICAL AND HISTORICAL NOTES AI planning arose from investigations into state-space search, theorem proving, and controltheory and from the practical needs of robotics, scheduling, and other domains.

Token 8098:
S TRIPS (Fikes and Nilsson, 1971), the ﬁrst major planning system, illustrates the interaction of these inﬂu-ences.

Token 8099:
S TRIPS was designed as the planning component of the software for the Shakey robot project at SRI.

Token 8100:
Its overall control structure was modeled on that of GPS, the General ProblemSolver (Newell and Simon, 1961), a state-space search system that used means–ends anal-ysis.

Token 8101:
Bylander (1992) shows simple S TRIPS planning to be PSPACE-complete.

Token 8102:
Fikes and Nilsson (1993) give a historical retrospective on the S TRIPS project and its relationship to more recent planning efforts.

Token 8103:
The representation language used by S TRIPS has been far more inﬂuential than its al- gorithmic approach; what we call the “classical” language is close to what S TRIPS used.

Token 8104:
394 Chapter 10.

Token 8105:
Classical Planning The Action Description Language, or ADL (Pednault, 1986), relaxed some of the S TRIPS restrictions and made it possible to encode more realistic problems.

Token 8106:
Nebel (2000) explores schemes for compiling ADL into S TRIPS . The Problem Domain Description Language, or PDDL (Ghallab et al.

Token 8107:
, 1998), was introduced as a computer-parsable, standardized syntax for representing planning problems and has been used as the standard language for the Interna- tional Planning Competition since 1998.

Token 8108:
There have been several extensions; the most recent version, PDDL 3.0, includes plan constraints and preferences (Gerevini and Long, 2005).

Token 8109:
Planners in the early 1970s generally considered totally ordered action sequences.

Token 8110:
Prob- lem decomposition was achieved by computing a subplan for each subgoal and then stringingthe subplans together in some order.

Token 8111:
This approach, called linear planning by Sacerdoti LINEAR PLANNING (1975), was soon discovered to be incomplete.

Token 8112:
It cannot solve some very simple problems, such as the Sussman anomaly (see Exercise 10.7), found by Allen Brown during experimen-tation with the H ACKER system (Sussman, 1975).

Token 8113:
A complete planner must allow for inter- leaving of actions from different subplans within a single sequence.

Token 8114:
The notion of serializable INTERLEAVING subgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved planners are complete.

Token 8115:
One solution to the interleaving problem was goal-regression planning, a technique in which steps in a totally ordered plan are reordered so as to avoid conﬂict between subgoals.

Token 8116:
This was introduced by Waldinger (1975) and also used by Warren’s (1974) W ARPLAN .

Token 8117:
WARPLAN is also notable in that it was the ﬁrst planner to be written in a logic program- ming language (Prolog) and is one of the best examples of the remarkable economy that cansometimes be gained with logic programming: W ARPLAN is only 100 lines of code, a small fraction of the size of comparable planners of the time.

Token 8118:
The ideas underlying partial-order planning include the detection of conﬂicts (Tate, 1975a) and the protection of achieved conditions from interference (Sussman, 1975).

Token 8119:
The construction of partially ordered plans (then called task networks ) was pioneered by the NOAH planner (Sacerdoti, 1975, 1977) and by Tate’s (1975b, 1977) N ONLIN system.

Token 8120:
Partial-order planning dominated the next 20 years of research, yet the ﬁrst clear for- mal exposition was T WEAK (Chapman, 1987), a planner that was simple enough to allow proofs of completeness and intractability (NP-hardness and undecidability) of various plan- ning problems.

Token 8121:
Chapman’s work led to a straightforward description of a complete partial-order planner (McAllester and Rosenblitt, 1991), then to the widely distributed implementa-tions SNLP (Soderland and Weld, 1991) and UCPOP (Penberthy and Weld, 1992).

Token 8122:
Partial-order planning fell out of favor in the late 1990s as faster methods emerged.

Token 8123:
Nguyen andKambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics de-rived from a planning graph, their R EPOP planner scales up much better than G RAPHPLAN in parallelizable domains and is competitive with the fastest state-space planners.

Token 8124:
The resurgence of interest in state-space planning was pioneered by Drew McDer- mott’s U NPOP program (1996), which was the ﬁrst to suggest the ignore-delete-list heuristic, The name U NPOP was a reaction to the overwhelming concentration on partial-order plan- ning at the time; McDermott suspected that other approaches were not getting the attentionthey deserved.

Token 8125:
Bonet and Geffner’s Heuristic Search Planner (HSP) and its later deriva-tives (Bonet and Geffner, 1999; Haslum et al.

Token 8126:
, 2005; Haslum, 2006) were the ﬁrst to make

Token 8127:
Bibliographical and Historical Notes 395 state-space search practical for large planning problems.

Token 8128:
HSP searches in the forward di- rection while HSP R(Bonet and Geffner, 1999) searches backward.

Token 8129:
The most successful state-space searcher to date is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,2005), winner of the AIPS 2000 planning competition.

Token 8130:
F ASTDOWNW ARD (Helmert, 2006) is a forward state-space search planner that preprocesses the action schemas into an alter- native representation which makes some of the constraints more explicit.

Token 8131:
F ASTDOWNW ARD (Helmert and Richter, 2004; Helmert, 2006) won the 2004 planning competition, and LAMA (Richter and Westphal, 2008), a planner based on F ASTDOWNW ARD with improved heuris- tics, won the 2008 competition.

Token 8132:
Bylander (1994) and Ghallab et al. (2004) discuss the computational complexity of several variants of the planning problem.

Token 8133:
Helmert (2003) proves complexity bounds for manyof the standard benchmark problems, and Hoffmann (2005) analyzes the search space of theignore-delete-list heuristic.

Token 8134:
Heuristics for the set-covering problem are discussed by Capraraet al. (1995) for scheduling operations of the Italian railway.

Token 8135:
Edelkamp (2009) and Haslum et al. (2007) describe how to construct pattern databases for planning heuristics.

Token 8136:
As we mentioned in Chapter 3, Felner et al.

Token 8137:
(2004) show encouraging results using pattern databases for sliding blocks puzzles, which can be thought of as a planning domain, but Hoffmann et al.

Token 8138:
(2006) show some limitations of abstraction for classical planning problems.

Token 8139:
Avrim Blum and Merrick Furst (1995, 1997) revitalized the ﬁeld of planning with their G RAPHPLAN system, which was orders of magnitude faster than the partial-order planners of the time.

Token 8140:
Other graph-planning systems, such as IPP (Koehler et al. , 1997), S TAN (Fox and Long, 1998), and SGP (Weld et al. , 1998), soon followed.

Token 8141:
A data structure closely resembling the planning graph had been developed slightly earlier by Ghallab and Laruelle (1994), whose IXTET partial-order planner used it to derive accurate heuristics to guide searches.

Token 8142:
Nguyen et al. (2001) thoroughly analyze heuristics derived from planning graphs.

Token 8143:
Our discussion of planning graphs is based partly on this work and on lecture notes and articles by SubbaraoKambhampati (Bryce and Kambhampati, 2007).

Token 8144:
As mentioned in the chapter, a planninggraph can be used in many different ways to guide the search for a solution.

Token 8145:
The winner of the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched planning graphs using a local search technique inspired by W ALKSAT.

Token 8146:
The situation calculus approach to planning was introduced by John McCarthy (1963). The version we show here was proposed by Ray Reiter (1991, 2001).

Token 8147:
Kautz et al.

Token 8148:
(1996) investigated various ways to propositionalize action schemas, ﬁnd- ing that the most compact forms did not necessarily lead to the fastest solution times.

Token 8149:
Asystematic analysis was carried out by Ernst et al.

Token 8150:
(1997), who also developed an auto- matic “compiler” for generating propositional representations from PDDL problems.

Token 8151:
The B LACKBOX planner, which combines ideas from G RAPHPLAN and SATP LAN,w a sd e v e l - oped by Kautz and Selman (1998).

Token 8152:
CP LAN, a planner based on constraint satisfaction, was described by van Beek and Chen (1999).

Token 8153:
Most recently, there has been interest in the representation of plans as binary decision diagrams , compact data structures for Boolean expressions widely studied in the hardwareBINARY DECISION DIAGRAM veriﬁcation community (Clarke and Grumberg, 1987; McMillan, 1993).

Token 8154:
There are techniques for proving properties of binary decision diagrams, including the property of being a solution

Token 8155:
396 Chapter 10. Classical Planning to a planning problem. Cimatti et al. (1998) present a planner based on this approach.

Token 8156:
Other representations have also been used; for example, Vossen et al. (2001) survey the use of integer programming for planning.

Token 8157:
The jury is still out, but there are now some interesting comparisons of the various approaches to planning.

Token 8158:
Helmert (2001) analyzes several classes of planning problems, and shows that constraint-based approaches such as G RAPHPLAN and SATP LAN are best for NP- hard domains, while search-based approaches do better in domains where feasible solutionscan be found without backtracking.

Token 8159:
G RAPHPLAN and SATP LAN have trouble in domains with many objects because that means they must create many actions.

Token 8160:
In some cases theproblem can be delayed or avoided by generating the propositionalized actions dynamically,only as needed, rather than instantiating them all before the search begins.

Token 8161:
Readings in Planning (Allen et al. , 1990) is a comprehensive anthology of early work in the ﬁeld.

Token 8162:
Weld (1994, 1999) provides two excellent surveys of planning algorithms ofthe 1990s.

Token 8163:
It is interesting to see the change in the ﬁve years between the two surveys:the ﬁrst concentrates on partial-order planning, and the second introduces G RAPHPLAN and SATP LAN.Automated Planning (Ghallab et al.

Token 8164:
, 2004) is an excellent textbook on all aspects of planning.

Token 8165:
LaValle’s text Planning Algorithms (2006) covers both classical and stochastic planning, with extensive coverage of robot motion planning.

Token 8166:
Planning research has been central to AI since its inception, and papers on planning are a staple of mainstream AI journals and conferences.

Token 8167:
There are also specialized conferences such as the International Conference on AI Planning Systems, the International Workshop on Planning and Scheduling for Space, and the European Conference on Planning.

Token 8168:
EXERCISES 10.1 Describe the differences and similarities between problem solving and planning.

Token 8169:
10.2 Given the action schemas and initial state from Figure 10.1, what are all the applicable concrete instances of Fly(p,from,to)in the state described by At(P1,JFK)∧At(P2,SFO)∧Plane (P1)∧Plane (P2) ∧Airport (JFK)∧Airport (SFO)?

Token 8170:
10.3 The monkey-and-bananas problem is faced by a monkey in a laboratory with some bananas hanging out of reach from the ceiling.

Token 8171:
A box is available that will enable the monkeyto reach the bananas if he climbs on it.

Token 8172:
Initially, the monkey is at A, the bananas at B,a n dt h e box at C. The monkey and box have height Low, but if the monkey climbs onto the box he will have height High , the same as the bananas.

Token 8173:
The actions available to the monkey include Gofrom one place to another, Push an object from one place to another, ClimbUp onto or ClimbDown from an object, and Grasp orUngrasp an object.

Token 8174:
The result of a Grasp is that the monkey holds the object if the monkey and object are in the same place at the same height. a.

Token 8175:
Write down the initial state description.

Token 8176:


Token 8177:
Exercises 397 Room 4 Room 3 Room 2 Room 1Door 1Door 2Door 3Door 4 Box 1Box 2Box 3Shakey Switch 1Switch 2Switch 3Switch 4 Box 4Corridor Figure 10.14 Shakey’s world.

Token 8178:
Shakey can move between landmarks within a room, can pass through the door between rooms, can climb climbable objects and push pushable objects, and can ﬂip light switches.

Token 8179:
b. Write the six action schemas.

Token 8180:
c. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the bananas, but leaving the box in its original place.

Token 8181:
Write this as a general goal (i.e., notassuming that the box is necessarily at C) in the language of situation calculus.

Token 8182:
Can thisgoal be solved by a classical planning system?

Token 8183:
d. Your schema for pushing is probably incorrect, because if the object is too heavy, its position will remain the same when the Push schema is applied.

Token 8184:
Fix your action schema to account for heavy objects. 10.4 The original S TRIPS planner was designed to control Shakey the robot.

Token 8185:
Figure 10.14 shows a version of Shakey’s world consisting of four rooms lined up along a corridor, whereeach room has a door and a light switch.

Token 8186:
The actions in Shakey’s world include moving fromplace to place, pushing movable objects (such as boxes), climbing onto and down from rigid

Token 8187:
398 Chapter 10. Classical Planning objects (such as boxes), and turning light switches on and off.

Token 8188:
The robot itself could not climb on a box or toggle a switch, but the planner was capable of ﬁnding and printing out plans thatwere beyond the robot’s abilities.

Token 8189:
Shakey’s six actions are the following: •Go(x,y,r), which requires that Shakey be Atxand that xandyare locations Inthe same room r. By convention a door between two rooms is in both of them.

Token 8190:
•Push a box bfrom location xto location ywithin the same room: Push(b,x,y,r ).Y o u will need the predicate Box and constants for the boxes.

Token 8191:
•Climb onto a box from position x:ClimbUp (x,b); climb down from a box to position x:ClimbDown (b,x).

Token 8192:
We will need the predicate Onand the constant Floor . •Turn a light switch on or off: TurnOn (s,b);TurnOﬀ (s,b).

Token 8193:
To turn a light on or off, Shakey must be on top of a box at the light switch’s location.

Token 8194:
Write PDDL sentences for Shakey’s six actions and the initial state from Figure 10.14. Con- struct a plan for Shakey to get Box 2intoRoom 2.

Token 8195:
10.5 A ﬁnite Turing machine has a ﬁnite one-dimensional tape of cells, each cell containing one of a ﬁnite number of symbols.

Token 8196:
One cell has a read and write head above it. There is aﬁnite set of states the machine can be in, one of which is the accept state.

Token 8197:
At each time step,depending on the symbol on the cell under the head and the machine’s current state, there area set of actions we can choose from.

Token 8198:
Each action involves writing a symbol to the cell under the head, transitioning the machine to a state, and optionally moving the head left or right.

Token 8199:
The mapping that determines which actions are allowed is the Turing machine’s program.Your goal is to control the machine into the accept state.

Token 8200:
Represent the Turing machine acceptance problem as a planning problem.

Token 8201:
If you can do this, it demonstrates that determining whether a planning problem has a solution is at leastas hard as the Turing acceptance problem, which is PSPACE-hard.

Token 8202:
10.6 Explain why dropping negative effects from every action schema in a planning prob- lem results in a relaxed problem.

Token 8203:
10.7 Figure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman anomaly .

Token 8204:
The problem was considered anomalous because the noninterleaved planners of SUSSMAN ANOMALY the early 1970s could not solve it.

Token 8205:
Write a deﬁnition of the problem and solve it, either by hand or with a planning program.

Token 8206:
A noninterleaved planner is a planner that, when given two subgoals G1andG2, produces either a plan for G1concatenated with a plan for G2,o rv i c e versa.

Token 8207:
Explain why a noninterleaved planner cannot solve this problem. 10.8 Prove that backward search with PDDL problems is complete.

Token 8208:
10.9 Construct levels 0, 1, and 2 of the planning graph for the problem in Figure 10.1. 10.10 Prove the following assertions about planning graphs: a.

Token 8209:
A literal that does not appear in the ﬁnal level of the graph cannot be achieved.

Token 8210:
Exercises 399 b. The level cost of a literal in a serial graph is no greater than the actual cost of an optimal plan for achieving it.

Token 8211:
10.11 The set-level heuristic (see page 382) uses a planning graph to estimate the cost of achieving a conjunctive goal from the current state.

Token 8212:
What relaxed problem is the set-levelheuristic the solution to? 10.12 Examine the deﬁnition of bidirectional search in Chapter 3. a.

Token 8213:
Would bidirectional state-space search be a good idea for planning? b. What about bidirectional search in the space of partial-order plans?

Token 8214:
c. Devise a version of partial-order planning in which an action can be added to a plan if its preconditions can be achieved by the effects of actions already in the plan.

Token 8215:
Explain howto deal with conﬂicts and ordering constraints. Is the algorithm essentially identical toforward state-space search?

Token 8216:
10.13 We contrasted forward and backward state-space searchers with partial-order plan- ners, saying that the latter is a plan-space searcher.

Token 8217:
Explain how forward and backward state-space search can also be considered plan-space searchers, and say what the plan reﬁnementoperators are.

Token 8218:
10.14 Up to now we have assumed that the plans we create always make sure that an action’s preconditions are satisﬁed.

Token 8219:
Let us now investigate what propositional successor-state axiomssuch as HaveArrow t+1⇔(HaveArrowt∧¬Shoott)have to say about actions whose preconditions are not satisﬁed.

Token 8220:
a. Show that the axioms predict that nothing will happen when an action is executed in a state where its preconditions are not satisﬁed. b.

Token 8221:
Consider a plan pthat contains the actions required to achieve a goal but also includes illegal actions.

Token 8222:
Is it the case that initial state∧successor-state axioms ∧p|=goal ?

Token 8223:
c. With ﬁrst-order successor-state axioms in situation calculus, is it possible to prove that a plan containing illegal actions will achieve the goal?

Token 8224:
10.15 Consider how to translate a set of action schemas into the successor-state axioms of situation calculus. a.

Token 8225:
Consider the schema for Fly(p,from,to).

Token 8226:
Write a logical deﬁnition for the predicate Poss(Fly(p,from,to),s), which is true if the preconditions for Fly(p,from,to)are satisﬁed in situation s. b.

Token 8227:
Next, assuming that Fly(p,from,to)is the only action schema available to the agent, write down a successor-state axiom for At(p,x,s)that captures the same information as the action schema.

Token 8228:
400 Chapter 10.

Token 8229:
Classical Planning c. Now suppose there is an additional method of travel: Teleport (p,from,to).I t h a s the additional precondition ¬Warped (p)and the additional effect Warped (p).

Token 8230:
Explain how the situation calculus knowledge base must be modiﬁed.

Token 8231:
d. Finally, develop a general and precisely speciﬁed procedure for carrying out the trans- lation from a set of action schemas to a set of successor-state axioms.

Token 8232:
10.16 In the SATP LAN algorithm in Figure 7.22 (page 272), each call to the satisﬁabil- ity algorithm asserts a goal gT,w h e r e Tranges from 0 to Tmax.

Token 8233:
Suppose instead that the satisﬁability algorithm is called only once, with the goal g0∨g1∨···∨ gTmax. a.

Token 8234:
Will this always return a plan if one exists with length less than or equal to Tmax? b. Does this approach introduce any new spurious “solutions”?

Token 8235:
c. Discuss how one might modify a satisﬁability algorithm such as W ALKSAT so that it ﬁnds short solutions (if they exist) when given a disjunctive goal of this form.

Token 8236:


Token 8237:
11PLANNING AND ACTING IN THE REAL WORLD In which we see how more expressive representations and more interactive agent architectures lead to planners that are useful in the real world.

Token 8238:
The previous chapter introduced the most basic concepts, representations, and algorithms for planning.

Token 8239:
Planners that are are used in the real world for planning and scheduling the oper-ations of spacecraft, factories, and military campaigns are more complex; they extend boththe representation language and the way the planner interacts with the environment.

Token 8240:
Thischapter shows how. Section 11.1 extends the classical language for planning to talk aboutactions with durations and resource constraints.

Token 8241:
Section 11.2 describes methods for con-structing plans that are organized hierarchically.

Token 8242:
This allows human experts to communicate to the planner what they know about how to solve the problem.

Token 8243:
Hierarchy also lends itself to efﬁcient plan construction because the planner can solve a problem at an abstract level beforedelving into details.

Token 8244:
Section 11.3 presents agent architectures that can handle uncertain envi-ronments and interleave deliberation with execution, and gives some examples of real-worldsystems.

Token 8245:
Section 11.4 shows how to plan when the environment contains other agents.

Token 8246:
11.1 T IME,SCHEDULES ,AND RESOURCES The classical planning representation talks about what to do ,a n di n what order ,b u tt h er e p r e - sentation cannot talk about time: how long an action takes and when it occurs.

Token 8247:
For example, the planners of Chapter 10 could produce a schedule for an airline that says which planes areassigned to which ﬂights, but we really need to know departure and arrival times as well.

Token 8248:
Thisis the subject matter of scheduling .

Token 8249:
The real world also imposes many resource constraints ; for example, an airline has a limited number of staff—and staff who are on one ﬂight cannotbe on another at the same time.

Token 8250:
This section covers methods for representing and solvingplanning problems that include temporal and resource constraints.

Token 8251:
The approach we take in this section is “plan ﬁrst, schedule later”: that is, we divide the overall problem into a planning phase in which actions are selected, with some ordering constraints, to meet the goals of the problem, and a later scheduling phase, in which tempo- ral information is added to the plan to ensure that it meets resource and deadline constraints.

Token 8252:
401

Token 8253:
402 Chapter 11.

Token 8254:
Planning and Acting in the Real World Jobs({AddEngine1≺AddWheels1≺Inspect1}, {AddEngine2≺AddWheels2≺Inspect2}) Resources (EngineHoists (1),WheelStations (1),Inspectors (2),LugNuts (500)) Action (AddEngine1 ,DURATION :30, USE:EngineHoists (1)) Action (AddEngine2 ,DURATION :60, USE:EngineHoists (1)) Action (AddWheels1 ,DURATION :30, CONSUME :LugNuts (20),USE:WheelStations (1)) Action (AddWheels2 ,DURATION :15, CONSUME :LugNuts (20),USE:WheelStations (1)) Action (Inspecti,DURATION :10, USE:Inspectors (1)) Figure 11.1 A job-shop scheduling problem for assembling two cars, with resource con- straints.

Token 8255:
The notation A≺Bmeans that action Amust precede action B.

Token 8256:
This approach is common in real-world manufacturing and logistical settings, where the plan- ning phase is often performed by human experts.

Token 8257:
The automated methods of Chapter 10 canalso be used for the planning phase, provided that they produce plans with just the minimalordering constraints required for correctness.

Token 8258:
G RAPHPLAN (Section 10.3), SATP LAN (Sec- tion 10.4.1), and partial-order planners (Section 10.4.4) can do this; search-based methods(Section 10.2) produce totally ordered plans, but these can easily be converted to plans with minimal ordering constraints.

Token 8259:
11.1.1 Representing temporal and resource constraints A typical job-shop scheduling problem , as ﬁrst introduced in Section 6.1.2, consists of a set of jobs, each of which consists a collection of actions with ordering constraints among JOB them.

Token 8260:
Each action has a duration and a set of resource constraints required by the action.

Token 8261:
DURATION Each constraint speciﬁes a type of resource (e.g., bolts, wrenches, or pilots), the number of that resource required, and whether that resource is consumable (e.g., the bolts are no CONSUMABLE longer available for use) or reusable (e.g., a pilot is occupied during a ﬂight but is available REUSABLE again when the ﬂight is over).

Token 8262:
Resources can also be produced by actions with negative con- sumption, including manufacturing, growing, and resupply actions.

Token 8263:
A solution to a job-shopscheduling problem must specify the start times for each action and must satisfy all the tem-poral ordering constraints and resource constraints.

Token 8264:
As with search and planning problems,solutions can be evaluated according to a cost function; this can be quite complicated, with nonlinear resource costs, time-dependent delay costs, and so on.

Token 8265:
For simplicity, we assume that the cost function is just the total duration of the plan, which is called the makespan .

Token 8266:
MAKESPAN Figure 11.1 shows a simple example: a problem involving the assembly of two cars.

Token 8267:
The problem consists of two jobs, each of the form [AddEngine ,AddWheels ,Inspect ]. Then the

Token 8268:
Section 11.1.

Token 8269:
Time, Schedules, and Resources 403 Resources statement declares that there are four types of resources, and gives the number of each type available at the start: 1 engine hoist, 1 wheel station, 2 inspectors, and 500 lugnuts.

Token 8270:
The action schemas give the duration and resource needs of each action.

Token 8271:
The lug nutsareconsumed as wheels are added to the car, whereas the other resources are “borrowed” at the start of an action and released at the action’s end.

Token 8272:
The representation of resources as numerical quantities, such as Inspectors (2),r a t h e r than as named entities, such as Inspector (I 1)andInspector (I2), is an example of a very general technique called aggregation .

Token 8273:
The central idea of aggregation is to group individual AGGREGATION objects into quantities when the objects are all indistinguishable with respect to the purpose at hand.

Token 8274:
In our assembly problem, it does not matter which inspector inspects the car, so there is no need to make the distinction.

Token 8275:
(The same idea works in the missionaries-and-cannibalsproblem in Exercise 3.9.) Aggregation is essential for reducing complexity.

Token 8276:
Consider whathappens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors are available.

Token 8277:
With inspectors represented as quantities, a failure is detected immediately andthe algorithm backtracks to try another schedule.

Token 8278:
With inspectors represented as individuals,the algorithm backtracks to try all 10!ways of assigning inspectors to actions.

Token 8279:
11.1.2 Solving scheduling problems We begin by considering just the temporal scheduling problem, ignoring resource constraints.

Token 8280:
To minimize makespan (plan duration), we must ﬁnd the earliest start times for all the actionsconsistent with the ordering constraints supplied with the problem.

Token 8281:
It is helpful to view theseordering constraints as a directed graph relating the actions, as shown in Figure 11.2.

Token 8282:
We canapply the critical path method (CPM) to this graph to determine the possible start and end CRITICAL PATH METHOD times of each action.

Token 8283:
A path through a graph representing a partial-order plan is a linearly ordered sequence of actions beginning with Start and ending with Finish .

Token 8284:
( F o r e x a m p l e , there are two paths in the partial-order plan in Figure 11.2.)

Token 8285:
The critical path is that path whose total duration is longest; the path is “critical” CRITICAL PATH because it determines the duration of the entire plan—shortening other paths doesn’t shorten the plan as a whole, but delaying the start of any action on the critical path slows down the whole plan.

Token 8286:
Actions that are off the critical path have a window of time in which they can be executed.

Token 8287:
The window is speciﬁed in terms of an earliest possible start time, ES, and a latest possible start time, LS.

Token 8288:
The quantity LS–ESis known as the slack of an action.

Token 8289:
We can SLACK see in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job has 15 minutes of slack, and that each action on the critical path has no slack (by deﬁnition).Together the ESandLStimes for all the actions constitute a schedule for the problem.

Token 8290:
SCHEDULE The following formulas serve as a deﬁnition for ESandLSand also as the outline of a dynamic-programming algorithm to compute them.

Token 8291:
AandBare actions, and A≺Bmeans thatAcomes before B: ES(Start)=0 ES(B)=m a x A≺BES(A)+Duration (A) LS(Finish )=ES(Finish ) LS(A)=m i n B/followsALS(B)−Duration (A).

Token 8292:
404 Chapter 11.

Token 8293:
Planning and Acting in the Real World Start [0,0]AddEngine1 30 [0,15] AddWheels1 30 [30,45] 10Inspect1 [60,75] Finish [85,85] 10Inspect2 [75,75] 15AddWheels2 [60,60] 60AddEngine2 [0,0] AddEngine1AddWheels1 Inspect1 AddWheels2Inspect2 AddEngine2 0 1 02 03 04 05 06 07 08 09 0 Figure 11.2 Top: a representation of the temporal constraints for the job-shop scheduling problem of Figure 11.1.

Token 8294:
The duration of each action is given at the bottom of each rectangle.

Token 8295:
In solving the problem, we compute the earliest and latest start times as the pair [ES,LS], displayed in the upper left.

Token 8296:
The difference between these two numbers is the slack of an action; actions with zero slack are on the critical path, shown with bold arrows.

Token 8297:
Bottom: the same solution shown as a timeline.

Token 8298:
Grey rectangles represent time intervals during which anaction may be executed, provided that the orderi ng constraints are respected.

Token 8299:
The unoccupied portion of a gray rectangle indicates the slack. The idea is that we start by assigning ES(Start)to be0.

Token 8300:
Then, as soon as we get an action Bsuch that all the actions that come immediately before BhaveESvalues assigned, we setES(B)to be the maximum of the earliest ﬁnish times of those immediately preceding actions, where the earliest ﬁnish time of an action is deﬁned as the earliest start time plus theduration.

Token 8301:
This process repeats until every action has been assigned an ESvalue.

Token 8302:
The LS values are computed in a similar manner, working backward from the Finish action.

Token 8303:
The complexity of the critical path algorithm is just O(Nb),w h e r e Nis the number of actions and bis the maximum branching factor into or out of an action.

Token 8304:
(To see this, note that theLSandEScomputations are done once for each action, and each computation iterates over at most bother actions.)

Token 8305:
Therefore, ﬁnding a minimum-duration schedule, given a partial ordering on the actions and no resource constraints, is quite easy.

Token 8306:
Mathematically speaking, critical-path problems are easy to solve because they are de- ﬁned as a conjunction oflinear inequalities on the start and end times.

Token 8307:
When we introduce resource constraints, the resulting constraints on start and end times become more compli-cated.

Token 8308:
For example, the AddEngine actions, which begin at the same time in Figure 11.2,

Token 8309:
Section 11.1.

Token 8310:
Time, Schedules, and Resources 405 AddEngine1 AddWheels1 Inspect1AddWheels2 Inspect2AddEngine2 0 1 02 03 04 05 06 07 08 09 0 100 110 120EngineHoists(1) WheelStations(1) Inspectors(2) Figure 11.3 A solution to the job-shop scheduling problem from Figure 11.1, taking into account resource constraints.

Token 8311:
The left-hand margin lists the three reusable resources, and actions are shown aligned horizontally with the resources they use.

Token 8312:
There are two possi- ble schedules, depending on which assembly uses the engine hoist ﬁrst; we’ve shown theshortest-duration solution, which takes 115 minutes.

Token 8313:
require the same EngineHoist and so cannot overlap.

Token 8314:
The “cannot overlap” constraint is a disjunction of two linear inequalities, one for each possible ordering.

Token 8315:
The introduction of disjunctions turns out to make scheduling with resource constraints NP-hard.

Token 8316:
Figure 11.3 shows the solution with the fastest completion time, 115 minutes.

Token 8317:
This is 30 minutes longer than the 85 minutes required for a schedule without resource constraints.Notice that there is no time at which both inspectors are required, so we can immediatelymove one of our two inspectors to a more productive position.

Token 8318:
The complexity of scheduling with resource constraints is often seen in practice as well as in theory.

Token 8319:
A challenge problem posed in 1963—to ﬁnd the optimal schedule for aproblem involving just 10 machines and 10 jobs of 100 actions each—went unsolved for23 years (Lawler et al.

Token 8320:
, 1993).

Token 8321:
Many approaches have been tried, including branch-and- bound, simulated annealing, tabu search, constraint satisfaction, and other techniques from Chapters 3 and 4.

Token 8322:
One simple but popular heuristic is the minimum slack algorithm: on MINIMUM SLACK each iteration, schedule for the earliest possible start whichever unscheduled action has all its predecessors scheduled and has the least slack; then update the ESandLStimes for each affected action and repeat.

Token 8323:
The heuristic resembles the minimum-remaining-values (MRV)heuristic in constraint satisfaction.

Token 8324:
It often works well in practice, but for our assemblyproblem it yields a 130–minute solution, not the 115–minute solution of Figure 11.3.

Token 8325:
Up to this point, we have assumed that the set of actions and ordering constraints is ﬁxed.

Token 8326:
Under these assumptions, every scheduling problem can be solved by a nonoverlapping sequence that avoids all resource conﬂicts, provided that each action is feasible by itself.

Token 8327:
If a scheduling problem is proving very difﬁcult, however, it may not be a good idea to solveit this way—it may be better to reconsider the actions and constraints, in case that leads to amuch easier scheduling problem.

Token 8328:
Thus, it makes sense to integrate planning and scheduling by taking into account durations and overlaps during the construction of a partial-order plan.

Token 8329:
Several of the planning algorithms in Chapter 10 can be augmented to handle this information.

Token 8330:
For example, partial-order planners can detect resource constraint violations in much thesame way they detect conﬂicts with causal links.

Token 8331:
Heuristics can be devised to estimate thetotal completion time of a plan. This is currently an active area of research.

Token 8332:
406 Chapter 11.

Token 8333:
Planning and Acting in the Real World 11.2 H IERARCHICAL PLANNING The problem-solving and planning methods of the preceding chapters all operate with a ﬁxed set of atomic actions.

Token 8334:
Actions can be strung together into sequences or branching networks;state-of-the-art algorithms can generate solutions containing thousands of actions.

Token 8335:
For plans executed by the human brain, atomic actions are muscle activations.

Token 8336:
In very round numbers, we have about 10 3muscles to activate (639, by some counts, but many of them have multiple subunits); we can modulate their activation perhaps 10 times per second;and we are alive and awake for about 10 9seconds in all.

Token 8337:
Thus, a human life contains about 1013actions, give or take one or two orders of magnitude.

Token 8338:
Even if we restrict ourselves to planning over much shorter time horizons—for example, a two-week vacation in Hawaii—adetailed motor plan would contain around 10 10actions.

Token 8339:
This is a lot more than 1000. To bridge this gap, AI systems will probably have to do what humans appear to do: plan at higher levels of abstraction.

Token 8340:
A reasonable plan for the Hawaii vacation might be “Go to San Francisco airport; take Hawaiian Airlines ﬂight 11 to Honolulu; do vacation stuff for twoweeks; take Hawaiian Airlines ﬂight 12 back to San Francisco; go home.” Given such a plan,the action “Go to San Francisco airport” can be viewed as a planning task in itself, with asolution such as “Drive to the long-term parking lot; park; take the shuttle to the terminal.”Each of these actions, in turn, can be decomposed further, until we reach the level of actionsthat can be executed without deliberation to generate the required motor control sequences.

Token 8341:
In this example, we see that planning can occur both before and during the execution of the plan; for example, one would probably defer the problem of planning a route from aparking spot in long-term parking to the shuttle bus stop until a particular parking spot hasbeen found during execution.

Token 8342:
Thus, that particular action will remain at an abstract level prior to the execution phase. We defer discussion of this topic until Section 11.3.

Token 8343:
Here, we concentrate on the aspect of hierarchical decomposition , an idea that pervades almost all HIERARCHICAL DECOMPOSITION attempts to manage complexity.

Token 8344:
For example, complex software is created from a hierarchy of subroutines or object classes; armies operate as a hierarchy of units; governments and cor-porations have hierarchies of departments, subsidiaries, and branch ofﬁces.

Token 8345:
The key beneﬁtof hierarchical structure is that, at each level of the hierarchy, a computational task, militarymission, or administrative function is reduced to a small number of activities at the next lower level, so the computational cost of ﬁnding the correct way to arrange those activities for thecurrent problem is small.

Token 8346:
Nonhierarchical methods, on the other hand, reduce a task to alarge number of individual actions; for large-scale problems, this is completely impractical.

Token 8347:
11.2.1 High-level actions The basic formalism we adopt to understand hierarchical decomposition comes from the area ofhierarchical task networks or HTN planning.

Token 8348:
As in classical planning (Chapter 10), weHIERARCHICAL TASK NETWORK assume full observability and determinism and the availability of a set of actions, now called primitive actions , with standard precondition–effect schemas.

Token 8349:
The key additional concept is PRIMITIVE ACTION thehigh-level action or HLA—for example, the action “Go to San Francisco airport” in the HIGH-LEVEL ACTION

Token 8350:
Section 11.2.

Token 8351:
Hierarchical Planning 407 Reﬁnement (Go(Home ,SFO), STEPS:[Drive(Home ,SFOLongTermParking ), Shuttle (SFOLongTermParking ,SFO)] ) Reﬁnement (Go(Home ,SFO), STEPS:[Taxi(Home ,SFO)] ) Reﬁnement (Navigate ([a,b],[x, y]), PRECOND :a=x∧b=y STEPS:[]) Reﬁnement (Navigate ([a,b],[x, y]), PRECOND :Connected ([a,b],[a−1,b]) STEPS:[Left,Navigate ([a−1,b],[x, y])] ) Reﬁnement (Navigate ([a,b],[x, y]), PRECOND :Connected ([a,b],[a+1,b]) STEPS:[Right,Navigate ([a+1,b],[x, y])] ) ...

Token 8352:
Figure 11.4 Deﬁnitions of possible reﬁnements for two high-level actions: going to San Francisco airport and navigating in the vacuum world.

Token 8353:
In the latter case, note the recursive nature of the reﬁnements and the use of preconditions. example given earlier.

Token 8354:
Each HLA has one or more possible reﬁnements , into a sequence1REFINEMENT of actions, each of which may be an HLA or a primitive action (which has no reﬁnements by deﬁnition).

Token 8355:
For example, the action “Go to San Francisco airport,” represented formallyasGo(Home ,SFO), might have two possible reﬁnements, as shown in Figure 11.4.

Token 8356:
The same ﬁgure shows a recursive reﬁnement for navigation in the vacuum world: to get to a destination, take a step, and then go to the destination.

Token 8357:
These examples show that high-level actions and their reﬁnements embody knowledge about how to do things .

Token 8358:
For instance, the reﬁnements for Go(Home ,SFO)say that to get to the airport you can drive or take a taxi; buying milk, sitting down, and moving the knight toe4 are not to be considered.

Token 8359:
An HLA reﬁnement that contains only primitive actions is called an implementation IMPLEMENTATION of the HLA.

Token 8360:
For example, in the vacuum world, the sequences [Right,Right,Down ]and [Down ,Right,Right]both implement the HLA Navigate ([1,3],[3,2]).

Token 8361:
An implementation of a high-level plan (a sequence of HLAs) is the concatenation of implementations of eachHLA in the sequence.

Token 8362:
Given the precondition–effect deﬁnitions of each primitive action, it isstraightforward to determine whether any given implementation of a high-level plan achievesthe goal.

Token 8363:
We can say, then, that a high-level plan achieves the goal from a given state if at least one of its implementations achieves the goal from that state.

Token 8364:
The “at least one” in this deﬁnition is crucial—not allimplementations need to achieve the goal, because the agent gets 1HTN planners often allow reﬁnement into partially or dered plans, and they allow the reﬁnements of two different HLAs in a plan to share actions.

Token 8365:
We omit these important complications in the interest of understanding the basic concepts of hierarchical planning.

Token 8366:
408 Chapter 11. Planning and Acting in the Real World to decide which implementation it will execute.

Token 8367:
Thus, the set of possible implementations in HTN planning—each of which may have a different outcome—is not the same as the set ofpossible outcomes in nondeterministic planning.

Token 8368:
There, we required that a plan work for all outcomes because the agent doesn’t get to choose the outcome; nature does.

Token 8369:
The simplest case is an HLA that has exactly one implementation.

Token 8370:
In that case, we can compute the preconditions and effects of the HLA from those of the implementation (see Exercise 11.3) and then treat the HLA exactly as if it were a primitive action itself.

Token 8371:
Itcan be shown that the right collection of HLAs can result in the time complexity of blindsearch dropping from exponential in the solution depth to linear in the solution depth, al-though devising such a collection of HLAs may be a nontrivial task in itself.

Token 8372:
When HLAshave multiple possible implementations, there are two options: one is to search among theimplementations for one that works, as in Section 11.2.2; the other is to reason directly aboutthe HLAs—despite the multiplicity of implementations—as explained in Section 11.2.3.

Token 8373:
Thelatter method enables the derivation of provably correct abstract plans, without the need toconsider their implementations.

Token 8374:
11.2.2 Searching for primitive solutions HTN planning is often formulated with a single “top level” action called Act, where the aim is to ﬁnd an implementation of Actthat achieves the goal.

Token 8375:
This approach is entirely general.

Token 8376:
For example, classical planning problems can be deﬁned as follows: for each primitive action ai, provide one reﬁnement of Actwith steps [ai,Act].

Token 8377:
That creates a recursive deﬁnition of Act that lets us add actions.

Token 8378:
But we need some way to stop the recursion; we do that by providingone more reﬁnement for Act, one with an empty list of steps and with a precondition equal to the goal of the problem.

Token 8379:
This says that if the goal is already achieved, then the rightimplementation is to do nothing.

Token 8380:
The approach leads to a simple algorithm: repeatedly choose an HLA in the current plan and replace it with one of its reﬁnements, until the plan achieves the goal.

Token 8381:
One possible implementation based on breadth-ﬁrst tree search is shown in Figure 11.5.

Token 8382:
Plans are consid- ered in order of depth of nesting of the reﬁnements, rather than number of primitive steps.

Token 8383:
Itis straightforward to design a graph-search version of the algorithm as well as depth-ﬁrst anditerative deepening versions.

Token 8384:
In essence, this form of hierarchical search explores the space of sequences that conform to the knowledge contained in the HLA library about how things are to be done.

Token 8385:
A great dealof knowledge can be encoded, not just in the action sequences speciﬁed in each reﬁnement butalso in the preconditions for the reﬁnements.

Token 8386:
For some domains, HTN planners have beenable to generate huge plans with very little search.

Token 8387:
For example, O-P LAN (Bell and Tate, 1985), which combines HTN planning with scheduling, has been used to develop productionplans for Hitachi.

Token 8388:
A typical problem involves a product line of 350 different products, 35 assembly machines, and over 2000 different operations.

Token 8389:
The planner generates a 30-day schedule with three 8-hour shifts a day, involving tens of millions of steps.

Token 8390:
Another importantaspect of HTN plans is that they are, by deﬁnition, hierarchically structured; usually thismakes them easy for humans to understand.

Token 8391:
Section 11.2.

Token 8392:
Hierarchical Planning 409 function HIERARCHICAL -SEARCH (problem ,hierarchy )returns a solution, or failure frontier←a FIFO queue with [Act]as the only element loop do ifEMPTY ?

Token 8393:
(frontier )then return failure plan←POP(frontier ) /* chooses the shallowest plan in frontier */ hla←the ﬁrst HLA in plan ,o rnull if none preﬁx ,suﬃx←the action subsequences before and after hlainplan outcome←RESULT (problem .INITIAL -STATE ,preﬁx ) ifhlais null then /* soplan is primitive and outcome is its result */ ifoutcome satisﬁes problem .GOAL then return plan else for each sequence inREFINEMENTS (hla,outcome ,hierarchy )do frontier←INSERT (APPEND (preﬁx ,sequence ,suﬃx ),frontier ) Figure 11.5 A breadth-ﬁrst implementation of hierarchical forward planning search.

Token 8394:
The initial plan supplied to the algorithm is [Act].T h e R EFINEMENTS function returns a set of action sequences, one for each reﬁnement of the HLA whose preconditions are satisﬁed by the speciﬁed state, outcome .

Token 8395:
The computational beneﬁts of hierarchical search can be seen by examining an ide- alized case.

Token 8396:
Suppose that a planning problem has a solution with dprimitive actions.

Token 8397:
For a nonhierarchical, forward state-space planner with ballowable actions at each state, the cost is O(bd), as explained in Chapter 3.

Token 8398:
For an HTN planner, let us suppose a very reg- ular reﬁnement structure: each nonprimitive action has rpossible reﬁnements, each into kactions at the next lower level.

Token 8399:
We want to know how many different reﬁnement trees there are with this structure.

Token 8400:
Now, if there are dactions at the primitive level, then the number of levels below the root is logkd, so the number of internal reﬁnement nodes is 1+k+k2+···+klogkd−1=(d−1)/(k−1).

Token 8401:
Each internal node has rpossible reﬁne- ments, so r(d−1)/(k−1)possible regular decomposition trees could be constructed.

Token 8402:
Examining this formula, we see that keeping rsmall and klarge can result in huge savings: essentially we are taking the kth root of the nonhierarchical cost, if bandrare comparable.

Token 8403:
Small rand largekmeans a library of HLAs with a small number of reﬁnements each yielding a long action sequence (that nonetheless allows us to solve any problem).

Token 8404:
This is not always pos- sible: long action sequences that are usable across a wide range of problems are extremely precious.

Token 8405:
The key to HTN planning, then, is the construction of a plan library containing known methods for implementing complex, high-level actions.

Token 8406:
One method of constructing the li-brary is to learn the methods from problem-solving experience.

Token 8407:
After the excruciating ex- perience of constructing a plan from scratch, the agent can save the plan in the library as a method for implementing the high-level action deﬁned by the task.

Token 8408:
In this way, the agent can become more and more competent over time as new methods are built on top of old methods.One important aspect of this learning process is the ability to generalize the methods that are constructed, eliminating detail that is speciﬁc to the problem instance (e.g., the name of

Token 8409:
410 Chapter 11. Planning and Acting in the Real World the builder or the address of the plot of land) and keeping just the key elements of the plan.

Token 8410:
Methods for achieving this kind of generalization are described in Chapter 19.

Token 8411:
It seems to usinconceivable that humans could be as competent as they are without some such mechanism.

Token 8412:
11.2.3 Searching for abstract solutions The hierarchical search algorithm in the preceding section reﬁnes HLAs all the way to primi-tive action sequences to determine if a plan is workable.

Token 8413:
This contradicts common sense: oneshould be able to determine that the two-HLA high-level plan [Drive(Home ,SFOLongTermParking ),Shuttle (SFOLongTermParking ,SFO)] gets one to the airport without having to determine a precise route, choice of parking spot, and so on.

Token 8414:
The solution seems obvious: write precondition–effect descriptions of the HLAs,just as we write down what the primitive actions do.

Token 8415:
From the descriptions, it ought to beeasy to prove that the high-level plan achieves the goal.

Token 8416:
This is the holy grail, so to speak, ofhierarchical planning because if we derive a high-level plan that provably achieves the goal,working in a small search space of high-level actions, then we can commit to that plan andwork on the problem of reﬁning each step of the plan.

Token 8417:
This gives us the exponential reduction we seek.

Token 8418:
For this to work, it has to be the case that every high-level plan that “claims” to achieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal inthe sense deﬁned earlier: it must have at least one implementation that does achieve the goal.This property has been called the downward reﬁnement property for HLA descriptions.

Token 8419:
DOWNWARD REFINEMENT PROPERTYWriting HLA descriptions that satisfy the downward reﬁnement property is, in princi- ple, easy: as long as the descriptions are true, then any high-level plan that claims to achieve the goal must in fact do so—otherwise, the descriptions are making some false claim aboutwhat the HLAs do.

Token 8420:
We have already seen how to write true descriptions for HLAs that haveexactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple implementations.

Token 8421:
How can we describe the effects of an action that can be implemented in many different ways?

Token 8422:
One safe answer (at least for problems where all preconditions and goals are positive) is to include only the positive effects that are achieved by every implementation of the HLA and the negative effects of anyimplementation.

Token 8423:
Then the downward reﬁnement property would be satisﬁed. Unfortunately, this semantics for HLAs is much too conservative.

Token 8424:
Consider againthe HLA Go(Home ,SFO), which has two reﬁnements, and suppose, for the sake of argu- ment, a simple world in which one can always drive to the airport and park, but taking a taxirequires Cash as a precondition.

Token 8425:
In that case, Go(Home ,SFO)doesn’t always get you to the airport.

Token 8426:
In particular, it fails if Cash is false, and so we cannot assert At(Agent ,SFO)as an effect of the HLA.

Token 8427:
This makes no sense, however; if the agent didn’t have Cash , it would drive itself.

Token 8428:
Requiring that an effect hold for every implementation is equivalent to assuming that someone else —an adversary—will choose the implementation.

Token 8429:
It treats the HLA’s mul- tiple outcomes exactly as if the HLA were a nondeterministic action, as in Section 4.3.

Token 8430:
For our case, the agent itself will choose the implementation.

Token 8431:
The programming languages community has coined the term demonic nondetermin- ismfor the case where an adversary makes the choices, contrasting this with angelic nonde- DEMONIC NONDETERMINISM

Token 8432:
Section 11.2. Hierarchical Planning 411 (a) (b) Figure 11.6 Schematic examples of reachable sets. The set of goal states is shaded.

Token 8433:
Black and gray arrows indicate possible implementations of h1andh2, respectively.

Token 8434:
(a) The reach- able set of an HLA h1in a state s. (b) The reachable set for the sequence [h1,h2].

Token 8435:
Because this intersects the goal set, the sequence achieves the goal. terminism , where the agent itself makes the choices.

Token 8436:
We borrow this term to deﬁne angelicANGELIC NONDETERMINISM semantics for HLA descriptions.

Token 8437:
The basic concept required for understanding angelic se- ANGELIC SEMANTICS mantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h, REACHABLE SET written as R EACH(s,h), is the set of states reachable by any of the HLA’s implementations.

Token 8438:
The key idea is that the agent can choose which element of the reachable set it ends up in when it executes the HLA; thus, an HLA with multiple reﬁnements is more “powerful” thanthe same HLA with fewer reﬁnements.

Token 8439:
We can also deﬁne the reachable set of a sequences ofHLAs.

Token 8440:
For example, the reachable set of a sequence [h 1,h2]is the union of all the reachable sets obtained by applying h2in each state in the reachable set of h1: REACH(s,[h1,h2]) =/uniondisplay s/prime∈REACH (s, h 1)REACH(s/prime,h2).

Token 8441:
Given these deﬁnitions, a high-level plan—a sequence of HLAs—achieves the goal if its reachable set intersects the set of goal states.

Token 8442:
(Compare this to the much stronger condition for demonic semantics, where every member of the reachable set has to be a goal state.

Token 8443:
)Conversely, if the reachable set doesn’t intersect the goal, then the plan deﬁnitely doesn’twork. Figure 11.6 illustrates these ideas.

Token 8444:
The notion of reachable sets yields a straightforward algorithm: search among high- level plans, looking for one whose reachable set intersects the goal; once that happens, the algorithm can commit to that abstract plan, knowing that it works, and focus on reﬁning the plan further.

Token 8445:
We will come back to the algorithmic issues later; ﬁrst, we consider the question of how the effects of an HLA—the reachable set for each possible initial state—are represented.

Token 8446:
As with the classical action schemas of Chapter 10, we represent the changes

Token 8447:
412 Chapter 11. Planning and Acting in the Real World made to each ﬂuent. Think of a ﬂuent as a state variable.

Token 8448:
A primitive action can addordelete a variable or leave it unchanged .

Token 8449:
(With conditional effects (see Section 11.3.1) there is a fourth possibility: ﬂipping a variable to its opposite.)

Token 8450:
An HLA under angelic semantics can do more: it can control the value of a variable, setting it to true or false depending on which implementation is chosen.

Token 8451:
In fact, an HLA can have nine different effects on a variable: if the variable starts out true, it can always keep it true, always make it false, or have a choice; if the variable starts out false, it can alwayskeep it false, always make it true, or have a choice; and the three choices for each case canbe combined arbitrarily, making nine.

Token 8452:
Notationally, this is a bit challenging.

Token 8453:
We’ll use the /tildewide symbol to mean “possibly, if the agent so chooses.” Thus, an effect /tildewide+Ameans “possibly add A,” that is, either leave Aunchanged or make it true.

Token 8454:
Similarly, /tildewide−Ameans “possibly delete A”a n d/tildewide±Ameans “possibly add or delete A.” For example, the HLA Go(Home ,SFO), with the two reﬁnements shown in Figure 11.4, possibly deletes Cash (if the agent decides to take a taxi), so it should have the effect /tildewide−Cash .

Token 8455:
Thus, we see that the descriptions of HLAs arederivable , in principle, from the descriptions of their reﬁnements—in fact, this is required if we want true HLA descriptions, such that the downward reﬁnement property holds.

Token 8456:
Now,suppose we have the following schemas for the HLAs h 1andh2: Action (h1,PRECOND :¬A,EFFECT :A∧/tildewide−B), Action (h2,PRECOND :¬B,EFFECT :/tildewide+A∧/tildewide±C).

Token 8457:
That is, h1addsAand possible deletes B, while h2possibly adds Aand has full control over C. Now, if only Bis true in the initial state and the goal is A∧Cthen the sequence [h1,h2] achieves the goal: we choose an implementation of h1that makes Bfalse, then choose an implementation of h2that leaves Atrue and makes Ctrue.

Token 8458:
The preceding discussion assumes that the effects of an HLA—the reachable set for any given initial state—can be described exactly by describing the effect on each variable.

Token 8459:
Itwould be nice if this were always true, but in many cases we can only approximate the ef-fects because an HLA may have inﬁnitely many implementations and may produce arbitrarilywiggly reachable sets—rather like the wiggly-belief-state problem illustrated in Figure 7.21on page 271.

Token 8460:
For example, we said that Go(Home ,SFO)possibly deletes Cash ;i ta l s o possibly adds At(Car,SFOLongTermParking ); but it cannot do both—in fact, it must do exactly one.

Token 8461:
As with belief states, we may need to write approximate descriptions.

Token 8462:
We will use two kinds of approximation: an optimistic description R EACH+(s,h)of an HLA hmayOPTIMISTIC DESCRIPTION overstate the reachable set, while a pessimistic description REACH−(s,h)may understatePESSIMISTIC DESCRIPTION the reachable set.

Token 8463:
Thus, we have REACH−(s,h)⊆REACH(s,h)⊆REACH+(s,h).

Token 8464:
For example, an optimistic description of Go(Home ,SFO)says that it possible deletes Cash andpossibly adds At(Car,SFOLongTermParking ).

Token 8465:
Another good example arises in the 8-puzzle, half of whose states are unreachable from any given state (see Exercise 3.4 on page 113): the optimistic description of Actmight well include the whole state space, since the exact reachable set is quite wiggly.

Token 8466:
With approximate descriptions, the test for whether a plan achieves the goal needs to be modiﬁed slightly.

Token 8467:
If the optimistic reachable set for the plan doesn’t intersect the goal,

Token 8468:
Section 11.2. Hierarchical Planning 413 (a) (b) Figure 11.7 Goal achievement for high-level plans with approximate descriptions.

Token 8469:
The set of goal states is shaded. For each plan, the pe ssimistic (solid lines) and optimistic (dashed lines) reachable sets are shown.

Token 8470:
(a) The plan indi cated by the black arrow deﬁnitely achieves the goal, while the plan indicated by the gray arrow deﬁnitely doesn’t.

Token 8471:
(b) A plan that wouldneed to be reﬁned further to determine if it really does achieve the goal.

Token 8472:
then the plan doesn’t work; if the pessimistic reachable set intersects the goal, then the plan does work (Figure 11.7(a)).

Token 8473:
With exact descriptions, a plan either works or it doesn’t, butwith approximate descriptions, there is a middle ground: if the optimistic set intersects thegoal but the pessimistic set doesn’t, then we cannot tell if the plan works (Figure 11.7(b)).When this circumstance arises, the uncertainty can be resolved by reﬁning the plan.

Token 8474:
This isa very common situation in human reasoning.

Token 8475:
For example, in planning the aforementioned two-week Hawaii vacation, one might propose to spend two days on each of seven islands.

Token 8476:
Prudence would indicate that this ambitious plan needs to be reﬁned by adding details ofinter-island transportation.

Token 8477:
An algorithm for hierarchical planning with approximate angelic descriptions is shown in Figure 11.8.

Token 8478:
For simplicity, we have kept to the same overall scheme used previously in Figure 11.5, that is, a breadth-ﬁrst search in the space of reﬁnements.

Token 8479:
As just explained, the algorithm can detect plans that will and won’t work by checking the intersections of the opti- mistic and pessimistic reachable sets with the goal.

Token 8480:
(The details of how to compute the reach-able sets of a plan, given approximate descriptions of each step, are covered in Exercise 11.5.

Token 8481:
)When a workable abstract plan is found, the algorithm decomposes the original problem into subproblems, one for each step of the plan.

Token 8482:
The initial state and goal for each subproblem are obtained by regressing a guaranteed-reachable goal state through the action schemas for each step of the plan.

Token 8483:
(See Section 10.2.2 for a discussion of how regression works.)

Token 8484:
Fig-ure 11.6(b) illustrates the basic idea: the right-hand circled state is the guaranteed-reachablegoal state, and the left-hand circled state is the intermediate goal obtained by regressing the

Token 8485:
414 Chapter 11.

Token 8486:
Planning and Acting in the Real World function ANGELIC -SEARCH (problem ,hierarchy ,initialPlan )returns solution or fail frontier←a FIFO queue with initialPlan as the only element loop do ifEMPTY ?

Token 8487:
(frontier )then return fail plan←POP(frontier ) /* chooses the shallowest node in frontier */ ifREACH+(problem .INITIAL -STATE,plan)intersects problem .GOAL then ifplan is primitive then return plan /* R EACH+is exact for primitive plans */ guaranteed←REACH−(problem .INITIAL -STATE,plan)∩problem .GOAL ifguaranteed/negationslash={}and M AKING -PROGRESS (plan ,initialPlan )then ﬁnalState←any element of guaranteed return DECOMPOSE (hierarchy ,problem .INITIAL -STATE ,plan ,ﬁnalState ) hla←some HLA in plan preﬁx ,suﬃx←the action subsequences before and after hlainplan for each sequence inREFINEMENTS (hla,outcome ,hierarchy )do frontier←INSERT (APPEND (preﬁx ,sequence ,suﬃx ),frontier ) function DECOMPOSE (hierarchy ,s0,plan ,sf)returns a solution solution←an empty plan whileplan is not empty do action←REMOVE -LAST(plan ) si←a state in R EACH−(s0,plan)such that sf∈REACH−(si,action ) problem←a problem with I NITIAL -STATE =siand G OAL =sf solution←APPEND (ANGELIC -SEARCH (problem ,hierarchy ,action ),solution ) sf←si return solution Figure 11.8 A hierarchical planning algorithm that uses angelic semantics to identify and commit to high-level plans that work while avoiding high-level plans that don’t.

Token 8488:
The predi- cate M AKING -PROGRESS checks to make sure that we aren’t stuck in an inﬁnite regression of reﬁnements.

Token 8489:
At top level, call A NGELIC -SEARCH with[Act]as theinitialPlan . goal through the ﬁnal action.

Token 8490:
The ability to commit to or reject high-level plans can give A NGELIC -SEARCH as i g - niﬁcant computational advantage over H IERARCHICAL -SEARCH , which in turn may have a large advantage over plain old B READTH -FIRST-SEARCH .

Token 8491:
Consider, for example, clean- ing up a large vacuum world consisting of rectangular rooms connected by narrow corri-dors.

Token 8492:
It makes sense to have an HLA for Navigate (as shown in Figure 11.4) and one for CleanWholeRoom .

Token 8493:
(Cleaning the room could be implemented with the repeated application of another HLA to clean each row.)

Token 8494:
Since there are ﬁve actions in this domain, the costfor B READTH -FIRST-SEARCH grows as 5d,w h e r e dis the length of the shortest solution (roughly twice the total number of squares); the algorithm cannot manage even two 2×2 rooms.

Token 8495:
H IERARCHICAL -SEARCH is more efﬁcient, but still suffers from exponential growth because it tries all ways of cleaning that are consistent with the hierarchy.

Token 8496:
A NGELIC -SEARCH scales approximately linearly in the number of squares—it commits to a good high-level se-

Token 8497:
Section 11.3. Planning and Acting in Nondeterministic Domains 415 quence and prunes away the other options.

Token 8498:
Notice that cleaning a set of rooms by cleaning each room in turn is hardly rocket science: it is easy for humans precisely because of thehierarchical structure of the task.

Token 8499:
When we consider how difﬁcult humans ﬁnd it to solvesmall puzzles such as the 8-puzzle, it seems likely that the human capacity for solving com-plex problems derives to a great extent from their skill in abstracting and decomposing the problem to eliminate combinatorics.

Token 8500:
The angelic approach can be extended to ﬁnd least-cost solutions by generalizing the notion of reachable set.

Token 8501:
Instead of a state being reachable or not, it has a cost for the mostefﬁcient way to get there. (The cost is ∞for unreachable states.)

Token 8502:
The optimistic and pes- simistic descriptions bound these costs.

Token 8503:
In this way, angelic search can ﬁnd provably optimalabstract plans without considering their implementations.

Token 8504:
The same approach can be used toobtain effective hierarchical lookahead algorithms for online search, in the style of LRTA ∗ HIERARCHICAL LOOKAHEAD (page 152).

Token 8505:
In some ways, such algorithms mirror aspects of human deliberation in tasks such as planning a vacation to Hawaii—consideration of alternatives is done initially at an abstractlevel over long time scales; some parts of the plan are left quite abstract until execution time,such as how to spend two lazy days on Molokai, while others parts are planned in detail, suchas the ﬂights to be taken and lodging to be reserved—without these reﬁnements, there is no guarantee that the plan would be feasible.

Token 8506:
11.3 P LANNING AND ACTING IN NONDETERMINISTIC DOMAINS In this section we extend planning to handle partially observable, nondeterministic, and un- known environments.

Token 8507:
Chapter 4 extended search in similar ways, and the methods here arealso similar: sensorless planning (also known as conformant planning ) for environments with no observations; contingency planning for partially observable and nondeterministic environments; and online planning andreplanning for unknown environments.

Token 8508:
While the basic concepts are the same as in Chapter 4, there are also signiﬁcant dif- ferences.

Token 8509:
These arise because planners deal with factored representations rather than atomicrepresentations.

Token 8510:
This affects the way we represent the agent’s capability for action and obser-vation and the way we represent belief states —the sets of possible physical states the agent might be in—for unobservable and partially observable environments.

Token 8511:
We can also take ad- vantage of many of the domain-independent methods given in Chapter 10 for calculatingsearch heuristics.

Token 8512:
Consider this problem: given a chair and a table, the goal is to have them match—have the same color.

Token 8513:
In the initial state we have two cans of paint, but the colors of the paint andthe furniture are unknown.

Token 8514:
Only the table is initially in the agent’s ﬁeld of view: Init(Object (Table)∧Object (Chair )∧Can(C 1)∧Can(C2)∧InView (Table)) Goal(Color (Chair ,c)∧Color (Table,c)) There are two actions: removing the lid from a paint can and painting an object using the paint from an open can.

Token 8515:
The action schemas are straightforward, with one exception: we nowallow preconditions and effects to contain variables that are not part of the action’s variable

Token 8516:
416 Chapter 11. Planning and Acting in the Real World list.

Token 8517:
That is, Paint(x,can)does not mention the variable c, representing the color of the paint in the can.

Token 8518:
In the fully observable case, this is not allowed—we would have to namethe action Paint(x,can,c).

Token 8519:
But in the partially observable case, we might or might not know what color is in the can.

Token 8520:
(The variable cis universally quantiﬁed, just like all the other variables in an action schema.)

Token 8521:
Action (RemoveLid (can), P RECOND :Can(can) EFFECT :Open(can)) Action (Paint(x,can), PRECOND :Object (x)∧Can(can)∧Color (can,c)∧Open(can) EFFECT :Color (x,c)) To solve a partially observable problem, the agent will have to reason about the percepts it will obtain when it is executing the plan.

Token 8522:
The percept will be supplied by the agent’s sensors whenit is actually acting, but when it is planning it will need a model of its sensors.

Token 8523:
In Chapter 4,this model was given by a function, P ERCEPT (s).

Token 8524:
For planning, we augment PDDL with a new type of schema, the percept schema : PERCEPT SCHEMA Percept (Color (x,c), PRECOND :Object (x)∧InView (x) Percept (Color (can,c), PRECOND :Can(can)∧InView (can)∧Open(can) The ﬁrst schema says that whenever an object is in view, the agent will perceive the color of the object (that is, for the object x, the agent will learn the truth value of Color (x,c)for allc).

Token 8525:
The second schema says that if an open can is in view, then the agent perceives the color of the paint in the can.

Token 8526:
Because there are no exogenous events in this world, the color of an object will remain the same, even if it is not being perceived, until the agent performs an action to change the object’s color.

Token 8527:
Of course, the agent will need an action that causesobjects (one at a time) to come into view: Action (LookAt (x), P RECOND :InView (y)∧(x/negationslash=y) EFFECT :InView (x)∧¬InView (y)) For a fully observable environment, we would have a Percept axiom with no preconditions for each ﬂuent.

Token 8528:
A sensorless agent, on the other hand, has no Percept axioms at all. Note that even a sensorless agent can solve the painting problem.

Token 8529:
One solution is to open any canof paint and apply it to both chair and table, thus coercing them to be the same color (even though the agent doesn’t know what the color is).

Token 8530:
A contingent planning agent with sensors can generate a better plan.

Token 8531:
First, look at the table and chair to obtain their colors; if they are already the same then the plan is done.

Token 8532:
If not, look at the paint cans; if the paint in a can is the same color as one piece of furniture, then apply that paint to the other piece.

Token 8533:
Otherwise, paint both pieces with any color.

Token 8534:
Finally, an online planning agent might generate a contingent plan with fewer branches at ﬁrst—perhaps ignoring the possibility that no cans match any of the furniture—and deal

Token 8535:
Section 11.3. Planning and Acting in Nondeterministic Domains 417 with problems when they arise by replanning.

Token 8536:
It could also deal with incorrectness of its action schemas.

Token 8537:
Whereas a contingent planner simply assumes that the effects of an actionalways succeed—that painting the chair does the job—a replanning agent would check theresult and make an additional plan to ﬁx any unexpected failure, such as an unpainted area orthe original color showing through.

Token 8538:
In the real world, agents use a combination of approaches.

Token 8539:
Car manufacturers sell spare tires and air bags, which are physical embodiments of contingent plan branches designedto handle punctures or crashes.

Token 8540:
On the other hand, most car drivers never consider thesepossibilities; when a problem arises they respond as replanning agents.

Token 8541:
In general, agentsplan only for contingencies that have important consequences and a nonnegligible chanceof happening.

Token 8542:
Thus, a car driver contemplating a trip across the Sahara desert should makeexplicit contingency plans for breakdowns, whereas a trip to the supermarket requires lessadvance planning.

Token 8543:
We next look at each of the three approaches in more detail.

Token 8544:
11.3.1 Sensorless planning Section 4.4.1 (page 138) introduced the basic idea of searching in belief-state space to ﬁnda solution for sensorless problems.

Token 8545:
Conversion of a sensorless planning problem to a belief-state planning problem works much the same way as it did in Section 4.4.1; the main differ-ences are that the underlying physical transition model is represented by a collection of actionschemas and the belief state can be represented by a logical formula instead of an explicitlyenumerated set of states.

Token 8546:
For simplicity, we assume that the underlying planning problem isdeterministic.

Token 8547:
The initial belief state for the sensorless painting problem can ignore InView ﬂuents because the agent has no sensors.

Token 8548:
Furthermore, we take as given the unchanging facts Object (Table)∧Object (Chair )∧Can(C 1)∧Can(C2)because these hold in every be- lief state.

Token 8549:
The agent doesn’t know the colors of the cans or the objects, or whether the cansare open or closed, but it does know that objects and cans have colors: ∀x∃cColor (x,c).

Token 8550:
After Skolemizing, (see Section 9.5), we obtain the initial belief state: b 0=Color (x,C(x)).

Token 8551:
In classical planning, where the closed-world assumption is made, we would assume that any ﬂuent not mentioned in a state is false, but in sensorless (and partially observable) plan-ning we have to switch to an open-world assumption in which states contain both positive and negative ﬂuents, and if a ﬂuent does not appear, its value is unknown.

Token 8552:
Thus, the beliefstate corresponds exactly to the set of possible worlds that satisfy the formula.

Token 8553:
Given thisinitial belief state, the following action sequence is a solution: [RemoveLid (Can 1),Paint(Chair ,Can1),Paint(Table,Can1)].

Token 8554:
We now show how to progress the belief state through the action sequence to show that the ﬁnal belief state satisﬁes the goal.

Token 8555:
First, note that in a given belief state b, the agent can consider any action whose pre- conditions are satisﬁed by b.

Token 8556:
(The other actions cannot be used because the transition model doesn’t deﬁne the effects of actions whose preconditions might be unsatisﬁed.)

Token 8557:
According

Token 8558:
418 Chapter 11.

Token 8559:
Planning and Acting in the Real World to Equation (4.4) (page 139), the general formula for updating the belief state bgiven an applicable action ain a deterministic world is as follows: b/prime=RESULT (b,a)={s/prime:s/prime=RESULT P(s,a)ands∈b} where R ESULT Pdeﬁnes the physical transition model.

Token 8560:
For the time being, we assume that the initial belief state is always a conjunction of literals, that is, a 1-CNF formula.

Token 8561:
To construct the new belief state b/prime, we must consider what happens to each literal /lscriptin each physical state sinbwhen action ais applied.

Token 8562:
For literals whose truth value is already known in b, the truth value in b/primeis computed from the current value and the add list and delete list of the action.

Token 8563:
(For example, if /lscriptis in the delete list of the action, then ¬/lscriptis added to b/prime.)

Token 8564:
What about a literal whose truth value is unknown in b? There are three cases: 1.

Token 8565:
If the action adds /lscript,t h e n/lscriptwill be true in b/primeregardless of its initial value. 2.

Token 8566:
If the action deletes /lscript,t h e n/lscriptwill be false in b/primeregardless of its initial value. 3.

Token 8567:
If the action does not affect /lscript,t h e n/lscriptwill retain its initial value (which is unknown) and will not appear in b/prime.

Token 8568:
Hence, we see that the calculation of b/primeis almost identical to the observable case, which was speciﬁed by Equation (10.1) on page 368: b/prime=RESULT (b,a)=(b−DEL(a))∪ADD(a).

Token 8569:
We cannot quite use the set semantics because (1) we must make sure that b/primedoes not con- tain both /lscriptand¬/lscript, and (2) atoms may contain unbound variables.

Token 8570:
But it is still the case that R ESULT (b,a)is computed by starting with b, setting any atom that appears in D EL(a) to false, and setting any atom that appears in A DD(a)to true.

Token 8571:
For example, if we apply RemoveLid (Can1)to the initial belief state b0,w eg e t b1=Color (x,C(x))∧Open(Can1).

Token 8572:
When we apply the action Paint(Chair ,Can1), the precondition Color (Can1,c)is satisﬁed by the known literal Color (x,C(x))with binding{x/Can1,c/ C(Can1)}and the new belief state is b2=Color (x,C(x))∧Open(Can1)∧Color (Chair ,C(Can1)).

Token 8573:
Finally, we apply the action Paint(Table,Can1)to obtain b3=Color (x,C(x))∧Open(Can1)∧Color (Chair ,C(Can1)) ∧Color (Table,C(Can1)).

Token 8574:
The ﬁnal belief state satisﬁes the goal, Color (Table,c)∧Color (Chair ,c), with the variable cbound to C(Can1).

Token 8575:
The preceding analysis of the update rule has shown a very important fact: the family of belief states deﬁned as conjunctions of literals is closed under updates deﬁned by PDDL action schemas.

Token 8576:
That is, if the belief state starts as a conjunction of literals, then any update will yield a conjunction of literals.

Token 8577:
That means that in a world with nﬂuents, any belief state can be represented by a conjunction of size O(n).

Token 8578:
This is a very comforting result, considering that there are 2nstates in the world.

Token 8579:
It says we can compactly represent all the subsets of those 2nstates that we will ever need. Moreover, the process of checking for belief

Token 8580:
Section 11.3.

Token 8581:
Planning and Acting in Nondeterministic Domains 419 states that are subsets or supersets of previously visited belief states is also easy, at least in the propositional case.

Token 8582:
The ﬂy in the ointment of this pleasant picture is that it only works for action schemas that have the same effects for all states in which their preconditions are satisﬁed.

Token 8583:
It is this property that enables the preservation of the 1-CNF belief-state representation.

Token 8584:
As soon as the effect can depend on the state, dependencies are introduced between ﬂuents and the 1-CNF property is lost.

Token 8585:
Consider, for example, the simple vacuum world deﬁned in Section 3.2.1.Let the ﬂuents be AtL andAtR for the location of the robot and CleanL andCleanR for the state of the squares.

Token 8586:
According to the deﬁnition of the problem, the Suck action has no precondition—it can always be done.

Token 8587:
The difﬁculty is that its effect depends on the robot’s lo-cation: when the robot is AtL, the result is CleanL , but when it is AtR, the result is CleanR .

Token 8588:
For such actions, our action schemas will need something new: a conditional effect .

Token 8589:
These CONDITIONAL EFFECT have the syntax “ whencondition :eﬀect ,” where condition is a logical formula to be com- pared against the current state, and effect is a formula describing the resulting state.

Token 8590:
For the vacuum world, we have Action (Suck, EFFECT :whenAtL:CleanL∧whenAtR:CleanR ).

Token 8591:
When applied to the initial belief state True , the resulting belief state is (AtL∧CleanL )∨ (AtR∧CleanR ), which is no longer in 1-CNF.

Token 8592:
(This transition can be seen in Figure 4.14 on page 141.)

Token 8593:
In general, conditional effects can induce arbitrary dependencies among the ﬂuents in a belief state, leading to belief states of exponential size in the worst case.

Token 8594:
It is important to understand the difference between preconditions and conditional ef- fects.

Token 8595:
Allconditional effects whose conditions are satisﬁed have their effects applied to gener- ate the resulting state; if none are satisﬁed, then the resulting state is unchanged.

Token 8596:
On the otherhand, if a precondition is unsatisﬁed, then the action is inapplicable and the resulting state is undeﬁned.

Token 8597:
From the point of view of sensorless planning, it is better to have conditionaleffects than an inapplicable action.

Token 8598:
For example, we could split Suck into two actions with unconditional effects as follows: Action (SuckL , P RECOND :AtL;EFFECT :CleanL ) Action (SuckR , PRECOND :AtR;EFFECT :CleanR ).

Token 8599:
Now we have only unconditional schemas, so the belief states all remain in 1-CNF; unfortu- nately, we cannot determine the applicability of SuckL andSuckR in the initial belief state.

Token 8600:
It seems inevitable, then, that nontrivial problems will involve wiggly belief states, just like those encountered when we considered the problem of state estimation for the wumpusworld (see Figure 7.21 on page 271).

Token 8601:
The solution suggested then was to use a conservative approximation to the exact belief state; for example, the belief state can remain in 1-CNF if it contains all literals whose truth values can be determined and treats all other literals as unknown.

Token 8602:
While this approach is sound , in that it never generates an incorrect plan, it is incomplete because it may be unable to ﬁnd solutions to problems that necessarily involve interactions among literals.

Token 8603:
To give a trivial example, if the goal is for the robot to be on

Token 8604:
420 Chapter 11.

Token 8605:
Planning and Acting in the Real World a clean square, then [Suck]is a solution but a sensorless agent that insists on 1-CNF belief states will not ﬁnd it.

Token 8606:
Perhaps a better solution is to look for action sequences that keep the belief state as simple as possible.

Token 8607:
For example, in the sensorless vacuum world, the action sequence[Right,Suck,Left,Suck]generates the following sequence of belief states: b 0=True b1=AtR b2=AtR∧CleanR b3=AtL∧CleanR b4=AtL∧CleanR∧CleanL That is, the agent cansolve the problem while retaining a 1-CNF belief state, even though some sequences (e.g., those beginning with Suck) go outside 1-CNF.

Token 8608:
The general lesson is not lost on humans: we are always performing little actions (checking the time, patting ourpockets to make sure we have the car keys, reading street signs as we navigate through a city) to eliminate uncertainty and keep our belief state manageable.

Token 8609:
There is another, quite different approach to the problem of unmanageably wiggly be- lief states: don’t bother computing them at all.

Token 8610:
Suppose the initial belief state is b 0and we would like to know the belief state resulting from the action sequence [a1,...,a m].

Token 8611:
Instead of computing it explicitly, just represent it as “ b0then[a1,...,a m].” This is a lazy but un- ambiguous representation of the belief state, and it’s quite concise— O(n+m)where nis the size of the initial belief state (assumed to be in 1-CNF) and mis the maximum length of an action sequence.

Token 8612:
As a belief-state representation, it suffers from one drawback, how- ever: determining whether the goal is satisﬁed, or an action is applicable, may require a lotof computation.

Token 8613:
The computation can be implemented as an entailment test: if A mrepresents the collec- tion of successor-state axioms required to deﬁne occurrences of the actions a1,...,a m—as explained for SATP LAN in Section 10.4.1—and Gmasserts that the goal is true after msteps, then the plan achieves the goal if b0∧Am|=Gm,t h a ti s ,i f b0∧Am∧¬Gmis unsatisﬁable.

Token 8614:
Given a modern SAT solver, it may be possible to do this much more quickly than computingthe full belief state.

Token 8615:
For example, if none of the actions in the sequence has a particular goalﬂuent in its add list, the solver will detect this immediately.

Token 8616:
It also helps if partial resultsabout the belief state—for example, ﬂuents known to be true or false—are cached to simplifysubsequent computations.

Token 8617:
The ﬁnal piece of the sensorless planning puzzle is a heuristic function to guide the search.

Token 8618:
The meaning of the heuristic function is the same as for classical planning: an esti-mate (perhaps admissible) of the cost of achieving the goal from the given belief state.

Token 8619:
With belief states, we have one additional fact: solving any subset of a belief state is necessarily easier than solving the belief state: ifb 1⊆b2thenh∗(b1)≤h∗(b2).

Token 8620:
Hence, any admissible heuristic computed for a subset is admissible for the belief state itself.

Token 8621:
The most obvious candidates are the singleton subsets, that is, individual physical states. We

Token 8622:
Section 11.3.

Token 8623:
Planning and Acting in Nondeterministic Domains 421 can take any random collection of states s1,...,s Nthat are in the belief state b, apply any admissible heuristic hfrom Chapter 10, and return H(b)=m a x{h(s1),...,h (sN)} as the heuristic estimate for solving b.

Token 8624:
We could also use a planning graph directly on bitself: if it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layerof the graph.

Token 8625:
If bis not in 1-CNF, it may be possible to ﬁnd sets of literals that together entail b.

Token 8626:
For example, if bis in disjunctive normal form (DNF), each term of the DNF formula is a conjunction of literals that entails band can form the initial layer of a planning graph.

Token 8627:
As before, we can take the maximum of the heuristics obtained from each set of literals.

Token 8628:
We canalso use inadmissible heuristics such as the ignore-delete-lists heuristic (page 377), whichseems to work quite well in practice.

Token 8629:
11.3.2 Contingent planning We saw in Chapter 4 that contingent planning—the generation of plans with conditionalbranching based on percepts—is appropriate for environments with partial observability, non-determinism, or both.

Token 8630:
For the partially observable painting problem with the percept axiomsgiven earlier, one possible contingent solution is as follows: [LookAt (Table),LookAt (Chair ), ifColor (Table,c)∧Color (Chair ,c)thenNoOp else[RemoveLid (Can 1),LookAt (Can1),RemoveLid (Can2),LookAt (Can2), ifColor (Table,c)∧Color (can,c)thenPaint(Chair ,can) else ifColor (Chair ,c)∧Color (can,c)thenPaint(Table,can) else[Paint(Chair ,Can1),Paint(Table,Can1)]]] Variables in this plan should be considered existentially quantiﬁed; the second line says that if there exists some color cthat is the color of the table and the chair, then the agent need not do anything to achieve the goal.

Token 8631:
When executing this plan, a contingent-planningagent can maintain its belief state as a logical formula and evaluate each branch conditionby determining if the belief state entails the condition formula or its negation.

Token 8632:
(It is up tothe contingent-planning algorithm to make sure that the agent will never end up in a be-lief state where the condition formula’s truth value is unknown.)

Token 8633:
Note that with ﬁrst-order conditions, the formula may be satisﬁed in more than one way; for example, the condition Color (Table,c)∧Color (can,c)might be satisﬁed by {can/Can 1}and by{can/Can2}if both cans are the same color as the table.

Token 8634:
In that case, the agent can choose any satisfyingsubstitution to apply to the rest of the plan.

Token 8635:
As shown in Section 4.4.2, calculating the new belief state after an action and subse- quent percept is done in two stages.

Token 8636:
The ﬁrst stage calculates the belief state after the action,just as for the sensorless agent: ˆb=(b−D EL(a))∪ADD(a) where, as before, we have assumed a belief state represented as a conjunction of literals.

Token 8637:
The second stage is a little trickier. Suppose that percept literals p1,...,p kare received.

Token 8638:
One might think that we simply need to add these into the belief state; in fact, we can also infer

Token 8639:
422 Chapter 11. Planning and Acting in the Real World that the preconditions for sensing are satisﬁed.

Token 8640:
Now, if a percept phas exactly one percept axiom, Percept (p,PRECOND :c),w h e r e cis a conjunction of literals, then those literals can be thrown into the belief state along with p. On the other hand, if phas more than one percept axiom whose preconditions might hold according to the predicted belief state ˆb,t h e nw eh a v e to add in the disjunction of the preconditions.

Token 8641:
Obviously, this takes the belief state outside 1-CNF and brings up the same complications as conditional effects, with much the same classes of solutions.

Token 8642:
Given a mechanism for computing exact or approximate belief states, we can generate contingent plans with an extension of the AND –ORforward search over belief states used in Section 4.4.

Token 8643:
Actions with nondeterministic effects—which are deﬁned simply by using adisjunction in the E FFECT of the action schema—can be accommodated with minor changes to the belief-state update calculation and no change to the search algorithm.2For the heuristic function, many of the methods suggested for sensorless planning are also applicable in thepartially observable, nondeterministic case.

Token 8644:
11.3.3 Online replanning Imagine watching a spot-welding robot in a car plant.

Token 8645:
The robot’s fast, accurate motions arerepeated over and over again as each car passes down the line.

Token 8646:
Although technically im-pressive, the robot probably does not seem at all intelligent because the motion is a ﬁxed, preprogrammed sequence; the robot obviously doesn’t “know what it’s doing” in any mean- ingful sense.

Token 8647:
Now suppose that a poorly attached door falls off the car just as the robot is about to apply a spot-weld.

Token 8648:
The robot quickly replaces its welding actuator with a gripper, picks up the door, checks it for scratches, reattaches it to the car, sends an email to the ﬂoor supervisor, switches back to the welding actuator, and resumes its work.

Token 8649:
All of a sudden, the robot’s behavior seems purposive rather than rote; we assume it results not from a vast, precomputed contingent plan but from an online replanning process—which means that therobot does need to know what it’s trying to do.

Token 8650:
Replanning presupposes some form of execution monitoring to determine the need for EXECUTION MONITORING a new plan.

Token 8651:
One such need arises when a contingent planning agent gets tired of planning for every little contingency, such as whether the sky might fall on its head.3Some branches of a partially constructed contingent plan can simply say Replan ; if such a branch is reached during execution, the agent reverts to planning mode.

Token 8652:
As we mentioned earlier, the decision as to how much of the problem to solve in advance and how much to leave to replanningis one that involves tradeoffs among possible events with different costs and probabilities ofoccurring.

Token 8653:
Nobody wants to have their car break down in the middle of the Sahara desert andonly then think about having enough water.

Token 8654:
2If cyclic solutions are required for a nondeterministic problem, AND –ORsearch must be generalized to a loopy version such as LAO∗(Hansen and Zilberstein, 2001).

Token 8655:
3In 1954, a Mrs. Hodges of Alabama was hit by meteorite that crashed through her roof.

Token 8656:
In 1992, a piece of the Mbale meteorite hit a small boy on the head; fortunately, its descent was slowed by banana leaves (Jenniskens et al. , 1994).

Token 8657:
And in 2009, a German boy claimed to have been hit in the hand by a pea-sized meteorite.

Token 8658:
No serious injuries resulted from any of these incidents, suggesting t hat the need for preplanning against such contingencies is sometimes overstated.

Token 8659:
Section 11.3.

Token 8660:
Planning and Acting in Nondeterministic Domains 423 whole plan plan repairS P OE G continuation Figure 11.9 Before execution, the planner comes up with a plan, here called whole plan , to get from StoG.

Token 8661:
The agent executes steps of the plan until it expects to be in state E,b u t observes it is actually in O.

Token 8662:
The agent then replans for the minimal repair plus continuation to reach G. Replanning may also be needed if the agent’s model of the world is incorrect.

Token 8663:
The model for an action may have a missing precondition —for example, the agent may not know thatMISSING PRECONDITION removing the lid of a paint can often requires a screwdriver; the model may have a missing effect —for example, painting an object may get paint on the ﬂoor as well; or the model may MISSING EFFECT have a missing state variable —for example, the model given earlier has no notion of theMISSING STATE VARIABLE amount of paint in a can, of how its actions affect this amount, or of the need for the amount to be nonzero.

Token 8664:
The model may also lack provision for exogenous events such as someone EXOGENOUS EVENT knocking over the paint can.

Token 8665:
Exogenous events can also include changes in the goal, such as the addition of the requirement that the table and chair not be painted black.

Token 8666:
Without the ability to monitor and replan, an agent’s behavior is likely to be extremely fragile if it relieson absolute correctness of its model.

Token 8667:
The online agent has a choice of how carefully to monitor the environment.

Token 8668:
We distin- guish three levels: •Action monitoring : before executing an action, the agent veriﬁes that all the precondi- ACTION MONITORING tions still hold.

Token 8669:
•Plan monitoring : before executing an action, the agent veriﬁes that the remaining plan PLAN MONITORING will still succeed.

Token 8670:
•Goal monitoring : before executing an action, the agent checks to see if there is a better GOAL MONITORING set of goals it could be trying to achieve.

Token 8671:
In Figure 11.9 we see a schematic of action monitoring.

Token 8672:
The agent keeps track of both its original plan, wholeplan , and the part of the plan that has not been executed yet, which is denoted by plan .

Token 8673:
After executing the ﬁrst few steps of the plan, the agent expects to be in stateE. But the agent observes it is actually in state O.

Token 8674:
It then needs to repair the plan by ﬁnding some point Pon the original plan that it can get back to. (It may be that Pis the goal state,G.)

Token 8675:
The agent tries to minimize the total cost of the plan: the repair part (from OtoP) plus the continuation (from PtoG).

Token 8676:
424 Chapter 11. Planning and Acting in the Real World Now let’s return to the example problem of achieving a chair and table of matching color.

Token 8677:
Suppose the agent comes up with this plan: [LookAt (Table),LookAt (Chair ), ifColor (Table,c)∧Color (Chair ,c)thenNoOp else[RemoveLid (Can1),LookAt (Can1), ifColor (Table,c)∧Color (Can1,c)thenPaint(Chair ,Can1) elseREPLAN ]].

Token 8678:
Now the agent is ready to execute the plan. Suppose the agent observes that the table and can of paint are white and the chair is black.

Token 8679:
It then executes Paint(Chair ,Can1).A t t h i s point a classical planner would declare victory; the plan has been executed.

Token 8680:
But an onlineexecution monitoring agent needs to check the preconditions of the remaining empty plan—that the table and chair are the same color.

Token 8681:
Suppose the agent perceives that they do nothave the same color—in fact, the chair is now a mottled gray because the black paint isshowing through.

Token 8682:
The agent then needs to ﬁgure out a position in whole plan to aim for and a repair action sequence to get there.

Token 8683:
The agent notices that the current state is identicalto the precondition before the Paint(Chair ,Can 1)action, so the agent chooses the empty sequence for repair and makes its plan be the same [Paint]sequence that it just attempted.

Token 8684:
With this new plan in place, execution monitoring resumes, and the Paint action is retried.

Token 8685:
This behavior will loop until the chair is perceived to be completely painted.

Token 8686:
But notice that the loop is created by a process of plan–execute–replan, rather than by an explicit loop in aplan.

Token 8687:
Note also that the original plan need not cover every contingency.

Token 8688:
If the agent reachesthe step marked R EPLAN , it can then generate a new plan (perhaps involving Can2).

Token 8689:
Action monitoring is a simple method of execution monitoring, but it can sometimes lead to less than intelligent behavior.

Token 8690:
For example, suppose there is no black or white paint,and the agent constructs a plan to solve the painting problem by painting both the chair andtable red.

Token 8691:
Suppose that there is only enough red paint for the chair.

Token 8692:
With action monitoring,the agent would go ahead and paint the chair red, then notice that it is out of paint and cannotpaint the table, at which point it would replan a repair—perhaps painting both chair and tablegreen.

Token 8693:
A plan-monitoring agent can detect failure whenever the current state is such that the remaining plan no longer works.

Token 8694:
Thus, it would not waste time painting the chair red.

Token 8695:
Plan monitoring achieves this by checking the preconditions for success of the entire remainingplan—that is, the preconditions of each step in the plan, except those preconditions that areachieved by another step in the remaining plan.

Token 8696:
Plan monitoring cuts off execution of adoomed plan as soon as possible, rather than continuing until the failure actually occurs.

Token 8697:
4 Plan monitoring also allows for serendipity —accidental success.

Token 8698:
If someone comes along and paints the table red at the same time that the agent is painting the chair red, then the ﬁnalplan preconditions are satisﬁed (the goal has been achieved), and the agent can go home early.

Token 8699:
It is straightforward to modify a planning algorithm so that each action in the plan is annotated with the action’s preconditions, thus enabling action monitoring.

Token 8700:
It is slightly 4Plan monitoring means that ﬁnally, after 424 pages, we have an agent that is smarter than a dung beetle (see page 39).

Token 8701:
A plan-monitoring agent would notice that the dung ball was missing from its grasp and would replanto get another ball and plug its hole.

Token 8702:
Section 11.4. Multiagent Planning 425 more complex to enable plan monitoring.

Token 8703:
Partial-order and planning-graph planners have the advantage that they have already built up structures that contain the relations necessaryfor plan monitoring.

Token 8704:
Augmenting state-space planners with the necessary annotations can bedone by careful bookkeeping as the goal ﬂuents are regressed through the plan.

Token 8705:
Now that we have described a method for monitoring and replanning, we need to ask, “Does it work?” This is a surprisingly tricky question.

Token 8706:
If we mean, “Can we guarantee that the agent will always achieve the goal?” then the answer is no, because the agent couldinadvertently arrive at a dead end from which there is no repair.

Token 8707:
For example, the vacuumagent might have a faulty model of itself and not know that its batteries can run out. Oncethey do, it cannot repair any plans.

Token 8708:
If we rule out dead ends—assume that there exists a planto reach the goal from anystate in the environment—and assume that the environment is really nondeterministic, in the sense that such a plan always has some chance of success on any given execution attempt, then the agent will eventually reach the goal.

Token 8709:
Trouble occurs when an action is actually not nondeterministic, but rather depends on some precondition that the agent does not know about.

Token 8710:
For example, sometimes a paintcan may be empty, so painting from that can has no effect. No amount of retrying is going to change this.

Token 8711:
5One solution is to choose randomly from among the set of possible repair plans, rather than to try the same one each time.

Token 8712:
In this case, the repair plan of opening another canmight work. A better approach is to learn a better model.

Token 8713:
Every prediction failure is an opportunity for learning; an agent should be able to modify its model of the world to accordwith its percepts.

Token 8714:
From then on, the replanner will be able to come up with a repair that getsat the root problem, rather than relying on luck to choose a good repair.

Token 8715:
This kind of learningis described in Chapters 18 and 19.

Token 8716:
11.4 M ULTIAGENT PLANNING So far, we have assumed that only one agent is doing the sensing, planning, and acting.When there are multiple agents in the environment, each agent faces a multiagent planning problem in which it tries to achieve its own goals with the help or hindrance of others.

Token 8717:
MULTIAGENT PLANNINGPROBLEM Between the purely single-agent and truly multiagent cases is a wide spectrum of prob- lems that exhibit various degrees of decomposition of the monolithic agent.

Token 8718:
An agent withmultiple effectors that can operate concurrently—for example, a human who can type andspeak at the same time—needs to do multieffector planning to manage each effector while MULTIEFFECTOR PLANNING handling positive and negative interactions among the effectors.

Token 8719:
When the effectors are physically decoupled into detached units—as in a ﬂeet of delivery robots in a factory—multieffector planning becomes multibody planning .

Token 8720:
A multibody problem is still a “stan- MULTIBODY PLANNING dard” single-agent problem as long as the relevant sensor information collected by each body can be pooled—either centrally or within each body—to form a common estimate of the world state that then informs the execution of the overall plan; in this case, the multiple bod-ies act as a single body.

Token 8721:
When communication constraints make this impossible, we have 5Futile repetition of a plan repair is exactly the behavior exhibited by the sphex wasp (page 39).

Token 8722:
426 Chapter 11.

Token 8723:
Planning and Acting in the Real World what is sometimes called a decentralized planning problem; this is perhaps a misnomer, be-DECENTRALIZED PLANNING cause the planning phase is centralized but the execution phase is at least partially decoupled.

Token 8724:
In this case, the subplan constructed for each body may need to include explicit communica-tive actions with other bodies.

Token 8725:
For example, multiple reconnaissance robots covering a widearea may often be out of radio contact with each other and should share their ﬁndings during times when communication is feasible.

Token 8726:
When a single entity is doing the planning, there is really only one goal, which all the bodies necessarily share.

Token 8727:
When the bodies are distinct agents that do their own planning, theymay still share identical goals; for example, two human tennis players who form a doublesteam share the goal of winning the match.

Token 8728:
Even with shared goals, however, the multibodyand multiagent cases are quite different.

Token 8729:
In a multibody robotic doubles team, a single plandictates which body will go where on the court and which body will hit the ball.

Token 8730:
In a multi-agent doubles team, on the other hand, each agent decides what to do; without some methodforcoordination , both agents may decide to cover the same part of the court and each may COORDINATION leave the ball for the other to hit.

Token 8731:
The clearest case of a multiagent problem, of course, is when the agents have different goals.

Token 8732:
In tennis, the goals of two opposing teams are in direct conﬂict, leading to the zero- sum situation of Chapter 5.

Token 8733:
Spectators could be viewed as agents if their support or disdain is a signiﬁcant factor and can be inﬂuenced by the players’ conduct; otherwise, they can betreated as an aspect of nature—just like the weather—that is assumed to be indifferent to theplayers’ intentions.

Token 8734:
6 Finally, some systems are a mixture of centralized and multiagent planning.

Token 8735:
For ex- ample, a delivery company may do centralized, ofﬂine planning for the routes of its trucksand planes each day, but leave some aspects open for autonomous decisions by drivers andpilots who can respond individually to trafﬁc and weather situations.

Token 8736:
Also, the goals of thecompany and its employees are brought into alignment, to some extent, by the payment ofincentives (salaries and bonuses)—a sure sign that this is a true multiagent system.

Token 8737:
INCENTIVE The issues involved in multiagent planning can be divided roughly into two sets.

Token 8738:
The ﬁrst, covered in Section 11.4.1, involves issues of representing and planning for multiple simultaneous actions; these issues occur in all settings from multieffector to multiagent plan-ning.

Token 8739:
The second, covered in Section 11.4.2, involves issues of cooperation, coordination,and competition arising in true multiagent settings.

Token 8740:
11.4.1 Planning with multiple simultaneous actions For the time being, we will treat the multieffector, multibody, and multiagent settings in thesame way, labeling them generically as multiactor settings, using the generic term actor to MULTIACTOR ACTOR cover effectors, bodies, and agents.

Token 8741:
The goal of this section is to work out how to deﬁne transition models, correct plans, and efﬁcient planning algorithms for the multiactor setting.A correct plan is one that, if executed by the actors, achieves the goal.

Token 8742:
(In the true multiagentsetting, of course, the agents may not agree to execute any particular plan, but at least they 6We apologize to residents of the United Kingdom, where the mere act of contemplating a game of tennis guarantees rain.

Token 8743:
Section 11.4.

Token 8744:
Multiagent Planning 427 Actors (A, B) Init(At(A,LeftBaseline )∧At(B,RightNet )∧ Approaching (Ball,RightBaseline ))∧Partner (A, B)∧Partner (B,A) Goal(Returned (Ball)∧(At(a,RightNet )∨At(a,LeftNet )) Action (Hit(actor,Ball), PRECOND :Approaching (Ball,loc)∧At(actor,loc) EFFECT :Returned (Ball)) Action (Go(actor,to), PRECOND :At(actor,loc)∧to/negationslash=loc, EFFECT :At(actor,to)∧¬At(actor,loc)) Figure 11.10 The doubles tennis problem.

Token 8745:
Two actors AandBare playing together and can be in one of four locations: LeftBaseline ,RightBaseline ,LeftNet ,a n dRightNet .T h e ball can be returned only if a player is in the right place.

Token 8746:
Note that each action must includethe actor as an argument. will know what plans would work if they didagree to execute them.)

Token 8747:
For simplicity, we assume perfect synchronization : each action takes the same amount of time and actions at SYNCHRONIZATION each point in the joint plan are simultaneous.

Token 8748:
We begin with the transition model; for the deterministic case, this is the function RESULT (s,a).

Token 8749:
In the single-agent setting, there might be bdifferent choices for the action; bcan be quite large, especially for ﬁrst-order representations with many objects to act on, but action schemas provide a concise representation nonetheless.

Token 8750:
In the multiactor settingwithnactors, the single action ais replaced by a joint action/angbracketlefta 1,...,a n/angbracketright,w h e r e aiis the JOINT ACTION action taken by the ith actor.

Token 8751:
Immediately, we see two problems: ﬁrst, we have to describe the transition model for bndifferent joint actions; second, we have a joint planning problem with a branching factor of bn.

Token 8752:
Having put the actors together into a multiactor system with a huge branching factor, the principal focus of research on multiactor planning has been to decouple the actors to the extent possible, so that the complexity of the problem grows linearly with nrather than exponentially.

Token 8753:
If the actors have no interaction with one another—for example, nactors each playing a game of solitaire—then we can simply solve nseparate problems.

Token 8754:
If the actors are loosely coupled , can we attain something close to this exponential improvement?

Token 8755:
This is, of LOOSELY COUPLED course, a central question in many areas of AI.

Token 8756:
We have seen it explicitly in the context of CSPs, where “tree like” constraint graphs yielded efﬁcient solution methods (see page 225),as well as in the context of disjoint pattern databases (page 106) and additive heuristics forplanning (page 378).

Token 8757:
The standard approach to loosely coupled problems is to pretend the problems are com- pletely decoupled and then ﬁx up the interactions.

Token 8758:
For the transition model, this means writing action schemas as if the actors acted independently.

Token 8759:
Let’s see how this works for the doublestennis problem.

Token 8760:
Let’s suppose that at one point in the game, the team has the goal of returningthe ball that has been hit to them and ensuring that at least one of them is covering the net.

Token 8761:
428 Chapter 11. Planning and Acting in the Real World A ﬁrst pass at a multiactor deﬁnition might look like Figure 11.10.

Token 8762:
With this deﬁnition, it is easy to see that the following joint plan plan works: JOINT PLAN PLAN 1: A:[Go(A,RightBaseline ),Hit(A,Ball)] B:[NoOp (B),NoOp (B)].

Token 8763:
Problems arise, however, when a plan has both agents hitting the ball at the same time.

Token 8764:
In the real world, this won’t work, but the action schema for Hitsays that the ball will be returned successfully.

Token 8765:
Technically, the difﬁculty is that preconditions constrain the state in which an action can be executed successfully, but do not constrain other actions that might mess it up.We solve this by augmenting action schemas with one new feature: a concurrent action list CONCURRENT ACTION LIST stating which actions must or must not be executed concurrently.

Token 8766:
For example, the Hitaction could be described as follows: Action (Hit(a,Ball), CONCURRENT :b/negationslash=a⇒¬Hit(b,Ball) PRECOND :Approaching (Ball,loc)∧At(a,loc) EFFECT :Returned (Ball)).

Token 8767:
In other words, the Hitaction has its stated effect only if no other Hitaction by another agent occurs at the same time.

Token 8768:
(In the SATP LAN approach, this would be handled by a partial action exclusion axiom .)

Token 8769:
For some actions, the desired effect is achieved only when another action occurs concurrently.

Token 8770:
For example, two agents are needed to carry a cooler full of beverages to the tennis court: Action (Carry (a,cooler ,here,there), CONCURRENT :b/negationslash=a∧Carry (b,cooler ,here,there) PRECOND :At(a,here)∧At(cooler ,here)∧Cooler (cooler ) EFFECT :At(a,there)∧At(cooler ,there)∧¬At(a,here)∧¬At(cooler ,here)).

Token 8771:
With these kinds of action schemas, any of the planning algorithms described in Chapter 10 can be adapted with only minor modiﬁcations to generate multiactor plans.

Token 8772:
To the extent that the coupling among subplans is loose—meaning that concurrency constraints come into playonly rarely during plan search—one would expect the various heuristics derived for single-agent planning to also be effective in the multiactor context.

Token 8773:
We could extend this approachwith the reﬁnements of the last two chapters—HTNs, partial observability, conditionals, exe- cution monitoring, and replanning—but that is beyond the scope of this book.

Token 8774:
11.4.2 Planning with multiple agents: Cooperation and coordination Now let us consider the true multiagent setting in which each agent makes its own plan.

Token 8775:
To start with, let us assume that the goals and knowledge base are shared.

Token 8776:
One might thinkthat this reduces to the multibody case—each agent simply computes the joint solution andexecutes its own part of that solution.

Token 8777:
Alas, the “ the” in “the joint solution ” is misleading.

Token 8778:
For our doubles team, more than one joint solution exists: P LAN 2: A:[Go(A,LeftNet ),NoOp (A)] B:[Go(B,RightBaseline ),Hit(B,Ball)].

Token 8779:
Section 11.4. Multiagent Planning 429 If both agents can agree on either plan 1 or plan 2, the goal will be achieved.

Token 8780:
But if Achooses plan 2 and Bchooses plan 1, then nobody will return the ball.

Token 8781:
Conversely, if Achooses 1 and Bchooses 2, then they will both try to hit the ball.

Token 8782:
The agents may realize this, but how can they coordinate to make sure they agree on the plan?

Token 8783:
One option is to adopt a convention before engaging in joint activity. A convention is CONVENTION any constraint on the selection of joint plans.

Token 8784:
For example, the convention “stick to your side of the court” would rule out plan 1, causing the doubles partners to select plan 2.

Token 8785:
Drivers ona road face the problem of not colliding with each other; this is (partially) solved by adoptingthe convention “stay on the right side of the road” in most countries; the alternative, “stayon the left side,” works equally well as long as all agents in an environment agree.

Token 8786:
Similarconsiderations apply to the development of human language, where the important thing is notwhich language each individual should speak, but the fact that a community all speaks thesame language.

Token 8787:
When conventions are widespread, they are called social laws .

Token 8788:
SOCIAL LAWS In the absence of a convention, agents can use communication to achieve common knowledge of a feasible joint plan.

Token 8789:
For example, a tennis player could shout “Mine!” or“Yours!” to indicate a preferred joint plan.

Token 8790:
We cover mechanisms for communication in moredepth in Chapter 22, where we observe that communication does not necessarily involve a verbal exchange.

Token 8791:
For example, one player can communicate a preferred joint plan to the other simply by executing the ﬁrst part of it.

Token 8792:
If agent Aheads for the net, then agent Bis obliged to go back to the baseline to hit the ball, because plan 2 is the only joint plan that begins withA’s heading for the net.

Token 8793:
This approach to coordination, sometimes called plan recognition , PLAN RECOGNITION works when a single action (or short sequence of actions) is enough to determine a joint plan unambiguously.

Token 8794:
Note that communication can work as well with competitive agents as withcooperative ones. Conventions can also arise through evolutionary processes.

Token 8795:
For example, seed-eating harvester ants are social creatures that evolved from the less social wasps.

Token 8796:
Colonies of ants execute very elaborate joint plans without any centralized control—the queen’s job is to re- produce, not to do centralized planning—and with very limited computation, communica- tion, and memory capabilities in each ant (Gordon, 2000, 2007).

Token 8797:
The colony has many roles, including interior workers, patrollers, and foragers.

Token 8798:
Each ant chooses to perform a role ac- cording to the local conditions it observes.

Token 8799:
For example, foragers travel away from the nest,search for a seed, and when they ﬁnd one, bring it back immediately.

Token 8800:
Thus, the rate at whichforagers return to the nest is an approximation of the availability of food today.

Token 8801:
When therate is high, other ants abandon their current role and take on the role of scavenger.

Token 8802:
The antsappear to have a convention on the importance of roles—foraging is the most important—andants will easily switch into the more important roles, but not into the less important.

Token 8803:
There issome learning mechanism: a colony learns to make more successful and prudent actions overthe course of its decades-long life, even though individual ants live only about a year.

Token 8804:
One ﬁnal example of cooperative multiagent behavior appears in the ﬂocking behavior of birds.

Token 8805:
We can obtain a reasonable simulation of a ﬂock if each bird agent (sometimes called a boid ) observes the positions of its nearest neighbors and then chooses the heading BOID and acceleration that maximizes the weighted sum of these three components:

Token 8806:
430 Chapter 11. Planning and Acting in the Real World (a) (b) (c) Figure 11.11 (a) A simulated ﬂock of birds, using Reynold’s boids model.

Token 8807:
Image courtesy Giuseppe Randazzo, novastructura.net. (b) An actual ﬂock of starlings. Image by Eduardo (pastaboy sleeps on ﬂickr).

Token 8808:
(c) Two competitive teams of agents attempting to capture the towers in the N ERO game. Image courtesy Risto Miikkulainen. 1.

Token 8809:
Cohesion: a positive score for getting closer to the average position of the neighbors 2.

Token 8810:
Separation: a negative score for getting too close to any one neighbor 3.

Token 8811:
Alignment: a positive score for getting closer to the average heading of the neighbors If all the boids execute this policy, the ﬂock exhibits the emergent behavior of ﬂying as aEMERGENT BEHAVIOR pseudorigid body with roughly constant density that does not disperse over time, and that occasionally makes sudden swooping motions.

Token 8812:
You can see a still images in Figure 11.11(a) and compare it to an actual ﬂock in (b).

Token 8813:
As with ants, there is no need for each agent topossess a joint plan that models the actions of other agents.

Token 8814:
The most difﬁcult multiagent problems involve both cooperation with members of one’s own team and competition against members of opposing teams, all without centralized con-trol.

Token 8815:
We see this in games such as robotic soccer or the N ERO game shown in Figure 11.11(c), in which two teams of software agents compete to capture the control towers.

Token 8816:
As yet, meth-ods for efﬁcient planning in these kinds of environments—for example, taking advantage ofloose coupling—are in their infancy.

Token 8817:
11.5 S UMMARY This chapter has addressed some of the complications of planning and acting in the real world.The main points: •Many actions consume resources , such as money, gas, or raw materials.

Token 8818:
It is convenient to treat these resources as numeric measures in a pool rather than try to reason about,say, each individual coin and bill in the world.

Token 8819:
Actions can generate and consumeresources, and it is usually cheap and effective to check partial plans for satisfaction ofresource constraints before attempting further reﬁnements.

Token 8820:
•Time is one of the most important resources. It can be handled by specialized schedul- ing algorithms, or scheduling can be integrated with planning.

Token 8821:


Token 8822:
Bibliographical and Historical Notes 431 •Hierarchical task network (HTN) planning allows the agent to take advice from the domain designer in the form of high-level actions (HLAs) that can be implemented in various ways by lower-level action sequences.

Token 8823:
The effects of HLAs can be deﬁned withangelic semantics , allowing provably correct high-level plans to be derived without consideration of lower-level implementations.

Token 8824:
HTN methods can create the very large plans required by many real-world applications.

Token 8825:
•Standard planning algorithms assume complete and correct information and determin- istic, fully observable environments.

Token 8826:
Many domains violate this assumption.

Token 8827:
•Contingent plans allow the agent to sense the world during execution to decide what branch of the plan to follow.

Token 8828:
In some cases, sensorless orconformant planning can be used to construct a plan that works without the need for perception.

Token 8829:
Both conformantand contingent plans can be constructed by search in the space of belief states .

Token 8830:
Efﬁcient representation or computation of belief states is a key problem.

Token 8831:
•Anonline planning agent uses execution monitoring and splices in repairs as needed to recover from unexpected situations, which can be due to nondeterministic actions, exogenous events, or incorrect models of the environment.

Token 8832:
•Multiagent planning is necessary when there are other agents in the environment with which to cooperate or compete.

Token 8833:
Joint plans can be constructed, but must be augmentedwith some form of coordination if two agents are to agree on which joint plan to execute.

Token 8834:
•This chapter extends classic planning to cover nondeterministic environments (where outcomes of actions are uncertain), but it is not the last word on planning.

Token 8835:
Chapter 17describes techniques for stochastic environments (in which outcomes of actions haveprobabilities associated with them): Markov decision processes, partially observableMarkov decision processes, and game theory.

Token 8836:
In Chapter 21 we show that reinforcementlearning allows an agent to learn how to behave from past successes and failures.

Token 8837:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Planning with time constraints was ﬁrst dealt with by D EVISER (Vere, 1983).

Token 8838:
The repre- sentation of time in plans was addressed by Allen (1984) and by Dean et al. (1990) in the FORBIN system.

Token 8839:
N ONLIN + (Tate and Whiter, 1984) and S IPE(Wilkins, 1988, 1990) could reason about the allocation of limited resources to various plan steps.

Token 8840:
O-P LAN (Bell and Tate, 1985), an HTN planner, had a uniform, general representation for constraints on timeand resources.

Token 8841:
In addition to the Hitachi application mentioned in the text, O-P LAN has been applied to software procurement planning at Price Waterhouse and back-axle assemblyplanning at Jaguar Cars.

Token 8842:
The two planners S APA (Do and Kambhampati, 2001) and T4 (Haslum and Geffner, 2001) both used forward state-space search with sophisticated heuristics to handle actions with durations and resources.

Token 8843:
An alternative is to use very expressive action languages, butguide them by human-written domain-speciﬁc heuristics, as is done by ASPEN (Fukunagaet al.

Token 8844:
, 1997), HSTS (Jonsson et al. , 2000), and IxTeT (Ghallab and Laruelle, 1994).

Token 8845:
432 Chapter 11. Planning and Acting in the Real World A number of hybrid planning-and-scheduling systems have been deployed: I SIS(Fox et al.

Token 8846:
, 1982; Fox, 1990) has been used for job shop scheduling at Westinghouse, G ARI(De- scotte and Latombe, 1985) planned the machining and construction of mechanical parts, FORBIN was used for factory control, and N ONLIN + was used for naval logistics planning.

Token 8847:
We chose to present planning and scheduling as two separate problems; (Cushing et al.

Token 8848:
, 2007) show that this can lead to incompleteness on certain problems. There is a long history of scheduling in aerospace.

Token 8849:
T-S CHED (Drabble, 1990) was used to schedule mission-command sequences for the U OSAT -II satellite. O PTIMUM -AIV (Aarup et al.

Token 8850:
, 1994) and P LAN-ERS1 (Fuchs et al.

Token 8851:
, 1990), both based on O-P LAN, were used for spacecraft assembly and obser- vation planning, respectively, at the European Space Agency.

Token 8852:
S PIKE (Johnston and Adorf, 1992) was used for observation planning at NASA for the Hubble Space Telescope, whilethe Space Shuttle Ground Processing Scheduling System (Deale et al.

Token 8853:
, 1994) does job-shop scheduling of up to 16,000 worker-shifts. Remote Agent (Muscettola et al.

Token 8854:
, 1998) became the ﬁrst autonomous planner–scheduler to control a spacecraft when it ﬂew onboard the DeepSpace One probe in 1999.

Token 8855:
Space applications have driven the development of algorithms forresource allocations; see Laborie (2003) and Muscettola (2002).

Token 8856:
The literature on schedulingis presented in a classic survey article (Lawler et al.

Token 8857:
, 1993), a recent book (Pinedo, 2008), and an edited handbook (Blazewicz et al. , 2007).

Token 8858:
The facility in the S TRIPS program for learning macrops —“macro-operators” consist- MACROPS ing of a sequence of primitive steps—could be considered the ﬁrst mechanism for hierarchi- cal planning (Fikes et al.

Token 8859:
, 1972). Hierarchy was also used in the L AWALY system (Siklossy and Dreussi, 1973).

Token 8860:
The A BSTRIPS system (Sacerdoti, 1974) introduced the idea of an ab- straction hierarchy , whereby planning at higher levels was permitted to ignore lower-levelABSTRACTION HIERARCHY preconditions of actions in order to derive the general structure of a working plan.

Token 8861:
Austin Tate’s Ph.D. thesis (1975b) and work by Earl Sacerdoti (1977) developed the basic ideas of HTN planning in its modern form.

Token 8862:
Many practical planners, including O-P LAN and S IPE, are HTN planners. Yang (1990) discusses properties of actions that make HTN planning ef- ﬁcient.

Token 8863:
Erol, Hendler, and Nau (1994, 1996) present a complete hierarchical decomposition planner as well as a range of complexity results for pure HTN planners.

Token 8864:
Our presentation of HLAs and angelic semantics is due to Marthi et al. (2007, 2008). Kambhampati et al.

Token 8865:
(1998) have proposed an approach in which decompositions are just another form of plan reﬁnement,similar to the reﬁnements for non-hierarchical partial-order planning.

Token 8866:
Beginning with the work on macro-operators in S TRIPS , one of the goals of hierarchical planning has been the reuse of previous planning experience in the form of generalized plans.The technique of explanation-based learning , described in depth in Chapter 19, has been applied in several systems as a means of generalizing previously computed plans, including S OAR (Laird et al.

Token 8867:
, 1986) and P RODIGY (Carbonell et al. , 1989).

Token 8868:
An alternative approach is to store previously computed plans in their original form and then reuse them to solve new, similar problems by analogy to the original problem.

Token 8869:
This is the approach taken by the ﬁeld called case-based planning (Carbonell, 1983; Alterman, 1988; Hammond, 1989).

Token 8870:
Kamb-CASE-BASED PLANNING hampati (1994) argues that case-based planning should be analyzed as a form of reﬁnement planning and provides a formal foundation for case-based partial-order planning.

Token 8871:
Bibliographical and Historical Notes 433 Early planners lacked conditionals and loops, but some could use coercion to form conformant plans.

Token 8872:
Sacerdoti’s N OAH solved the “keys and boxes” problem, a planning chal- lenge problem in which the planner knows little about the initial state, using coercion.

Token 8873:
Ma-son (1993) argued that sensing often can and should be dispensed with in robotic planning,and described a sensorless plan that can move a tool into a speciﬁc position on a table by a sequence of tilting actions, regardless of the initial position.

Token 8874:
Goldman and Boddy (1996) introduced the term conformant planning , noting that sen- sorless plans are often effective even if the agent has sensors.

Token 8875:
The ﬁrst moderately efﬁcientconformant planner was Smith and Weld’s (1998) Conformant Graphplan or CGP.

Token 8876:
Ferrarisand Giunchiglia (2000) and Rintanen (1999) independently developed SATP LAN-based con- formant planners.

Token 8877:
Bonet and Geffner (2000) describe a conformant planner based on heuristicsearch in the space of belief states, drawing on ideas ﬁrst developed in the 1960s for partiallyobservable Markov decision processes, or POMDPs (see Chapter 17).

Token 8878:
Currently, there are three main approaches to conformant planning. The ﬁrst two use heuristic search in belief-state space: HSCP (Bertoli et al.

Token 8879:
, 2001a) uses binary decision diagrams (BDDs) to represent belief states, whereas Hoffmann and Brafman (2006) adoptthe lazy approach of computing precondition and goal tests on demand using a SAT solver.

Token 8880:
The third approach, championed primarily by Jussi Rintanen (2007), formulates the entire sensorless planning problem as a quantiﬁed Boolean formula (QBF) and solves it using ageneral-purpose QBF solver.

Token 8881:
Current conformant planners are ﬁve orders of magnitude fasterthan CGP.

Token 8882:
The winner of the 2006 conformant-planning track at the International PlanningCompetition was T 0(Palacios and Geffner, 2007), which uses heuristic search in belief-state space while keeping the belief-state representation simple by deﬁning derived literals that cover conditional effects.

Token 8883:
Bryce and Kambhampati (2007) discuss how a planning graph canbe generalized to generate good heuristics for conformant and contingent planning.

Token 8884:
There has been some confusion in the literature between the terms “conditional” and “contingent” planning.

Token 8885:
Following Majercik and Littman (2003), we use “conditional” tomean a plan (or action) that has different effects depending on the actual state of the world, and “contingent” to mean a plan in which the agent can choose different actions depending on the results of sensing.

Token 8886:
The problem of contingent planning received more attention afterthe publication of Drew McDermott’s (1978a) inﬂuential article, Planning and Acting .

Token 8887:
The contingent-planning approach described in the chapter is based on Hoffmann and Brafman (2005), and was inﬂuenced by the efﬁcient search algorithms for cyclic AND –OR graphs developed by Jimenez and Torras (2000) and Hansen and Zilberstein (2001).

Token 8888:
Bertoliet al. (2001b) describe MBP (Model-Based Planner), which uses binary decision diagrams to do conformant and contingent planning.

Token 8889:
In retrospect, it is now possible to see how the major classical planning algorithms led to extended versions for uncertain domains.

Token 8890:
Fast-forward heuristic search through state spaceled to forward search in belief space (Bonet and Geffner, 2000; Hoffmann and Brafman, 2005); SATP LAN led to stochastic SATP LAN (Majercik and Littman, 2003) and to planning with quantiﬁed Boolean logic (Rintanen, 2007); partial order planning led to UWL (Etzioniet al.

Token 8891:
, 1992) and CNLP (Peot and Smith, 1992); G RAPHPLAN led to Sensory Graphplan or SGP (Weld et al. , 1998).

Token 8892:
434 Chapter 11. Planning and Acting in the Real World The ﬁrst online planner with execution monitoring was P LANEX (Fikes et al.

Token 8893:
, 1972), which worked with the S TRIPS planner to control the robot Shakey.

Token 8894:
The N ASL planner (McDermott, 1978a) treated a planning problem simply as a speciﬁcation for carrying out acomplex action, so that execution and planning were completely uniﬁed.

Token 8895:
S IPE(System for Interactive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the ﬁrst planner to deal systematically with the problem of replanning.

Token 8896:
It has been used in demonstration projects in several domains, including planning operations on the ﬂight deck of an aircraftcarrier, job-shop scheduling for an Australian beer factory, and planning the construction ofmultistory buildings (Kartam and Levitt, 1990).

Token 8897:
In the mid-1980s, pessimism about the slow run times of planning systems led to the proposal of reﬂex agents called reactive planning systems (Brooks, 1986; Agre and Chap- REACTIVE PLANNING man, 1987).

Token 8898:
P ENGI (Agre and Chapman, 1987) could play a (fully observable) video game by using Boolean circuits combined with a “visual” representation of current goals and theagent’s internal state.

Token 8899:
“Universal plans” (Schoppers, 1987, 1989) were developed as a lookup-table method for reactive planning, but turned out to be a rediscovery of the idea of policies POLICY that had long been used in Markov decision processes (see Chapter 17).

Token 8900:
A universal plan (or a policy) contains a mapping from any state to the action that should be taken in that state.

Token 8901:
Koenig (2001) surveys online planning techniques, under the name Agent-Centered Search .

Token 8902:
Multiagent planning has leaped in popularity in recent years, although it does have a long history.

Token 8903:
Konolige (1982) formalizes multiagent planning in ﬁrst-order logic, whilePednault (1986) gives a S TRIPS -style description.

Token 8904:
The notion of joint intention, which is es- sential if agents are to execute a joint plan, comes from work on communicative acts (Cohenand Levesque, 1990; Cohen et al.

Token 8905:
, 1990). Boutilier and Brafman (2001) show how to adapt partial-order planning to a multiactor setting.

Token 8906:
Brafman and Domshlak (2008) devise a mul- tiactor planning algorithm whose complexity grows only linearly with the number of actors, provided that the degree of coupling (measured partly by the tree width of the graph of inter- actions among agents) is bounded.

Token 8907:
Petrik and Zilberstein (2009) show that an approach based on bilinear programming outperforms the cover-set approach we outlined in the chapter.

Token 8908:
We have barely skimmed the surface of work on negotiation in multiagent planning.

Token 8909:
Durfee and Lesser (1989) discuss how tasks can be shared out among agents by negotiation.Kraus et al.

Token 8910:
(1991) describe a system for playing Diplomacy, a board game requiring negoti- ation, coalition formation, and dishonesty.

Token 8911:
Stone (2000) shows how agents can cooperate asteammates in the competitive, dynamic, partially observable environment of robotic soccer.

Token 8912:
Ina later article, Stone (2003) analyzes two competitive multiagent environments—RoboCup,a robotic soccer competition, and TAC, the auction-based Trading Agents Competition—and ﬁnds that the computational intractability of our current theoretically well-founded ap-proaches has led to many multiagent systems being designed by ad hoc methods.

Token 8913:
In his highly inﬂuential Society of Mind theory, Marvin Minsky (1986, 2007) proposes that human minds are constructed from an ensemble of agents.

Token 8914:
Livnat and Pippenger (2006) prove that, for the problem of optimal path-ﬁnding, and given a limitation on the total amountof computing resources, the best architecture for an agent is an ensemble of subagents, eachof which tries to optimize its own objective, and all of which are in conﬂict with one another.

Token 8915:


Token 8916:
Exercises 435 The boid model on page 429 is due to Reynolds (1987), who won an Academy Award for its application to swarms of penguins in Batman Returns .T h eN ERO game and the meth- ods for learning strategies are described by Bryant and Miikkulainen (2007).

Token 8917:
Recent book on multiagent systems include those by Weiss (2000a), Young (2004), Vlassis (2008), and Shoham and Leyton-Brown (2009).

Token 8918:
There is an annual conference on autonomous agents and multiagent systems (AAMAS).

Token 8919:
EXERCISES 11.1 The goals we have considered so far all ask the planner to make the world satisfy the goal at just one time step.

Token 8920:
Not all goals can be expressed this way: you do not achieve thegoal of suspending a chandelier above the ground by throwing it in the air.

Token 8921:
More seriously,you wouldn’t want your spacecraft life-support system to supply oxygen one day but notthe next.

Token 8922:
A maintenance goal is achieved when the agent’s plan causes a condition to hold continuously from a given state onward.

Token 8923:
Describe how to extend the formalism of this chapter to support maintenance goals.

Token 8924:
11.2 You have a number of trucks with which to deliver a set of packages.

Token 8925:
Each package starts at some location on a grid map, and has a destination somewhere else.

Token 8926:
Each truck is di-rectly controlled by moving forward and turning. Construct a hierarchy of high-level actionsfor this problem.

Token 8927:
What knowledge about the solution does your hierarchy encode?

Token 8928:
11.3 Suppose that a high-level action has exactly one implementation as a sequence of primitive actions.

Token 8929:
Give an algorithm for computing its preconditions and effects, given the complete reﬁnement hierarchy and schemas for the primitive actions.

Token 8930:
11.4 Suppose that the optimistic reachable set of a high-level plan is a superset of the goal set; can anything be concluded about whether the plan achieves the goal?

Token 8931:
What if the pes- simistic reachable set doesn’t intersect the goal set? Explain.

Token 8932:
11.5 Write an algorithm that takes an initial state (speciﬁed by a set of propositional literals) and a sequence of HLAs (each deﬁned by preconditions and angelic speciﬁcations of opti- mistic and pessimistic reachable sets) and computes optimistic and pessimistic descriptions of the reachable set of the sequence.

Token 8933:
11.6 In Figure 11.2 we showed how to describe actions in a scheduling problem by using separate ﬁelds for D URATION ,USE,a n dC ONSUME .

Token 8934:
Now suppose we wanted to combine scheduling with nondeterministic planning, which requires nondeterministic and conditionaleffects.

Token 8935:
Consider each of the three ﬁelds and explain if they should remain separate ﬁelds, orif they should become effects of the action.

Token 8936:
Give an example for each of the three.

Token 8937:
11.7 Some of the operations in standard programming languages can be modeled as actions that change the state of the world.

Token 8938:
For example, the assignment operation changes the con-tents of a memory location, and the print operation changes the state of the output stream.

Token 8939:
Aprogram consisting of these operations can also be considered as a plan, whose goal is given

Token 8940:
436 Chapter 11. Planning and Acting in the Real World by the speciﬁcation of the program.

Token 8941:
Therefore, planning algorithms can be used to construct programs that achieve a given speciﬁcation. a.

Token 8942:
Write an action schema for the assignment operator (assigning the value of one variable to another).

Token 8943:
Remember that the original value will be overwritten! b.

Token 8944:
Show how object creation can be used by a planner to produce a plan for exchanging the values of two variables by using a temporary variable.

Token 8945:
11.8 Suppose the Flip action always changes the truth value of variable L.S h o w h o w to deﬁne its effects by using an action schema with conditional effects.

Token 8946:
Show that, despite the use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after aFlip.

Token 8947:
11.9 In the blocks world we were forced to introduce two action schemas, Move and MoveToTable , in order to maintain the Clear predicate properly.

Token 8948:
Show how conditional effects can be used to represent both of these cases with a single action.

Token 8949:
11.10 Conditional effects were illustrated for the Suck action in the vacuum world—which square becomes clean depends on which square the robot is in.

Token 8950:
Can you think of a new set of propositional variables to deﬁne states of the vacuum world, such that Suck has an uncondi- tional description?

Token 8951:
Write out the descriptions of Suck ,Left,a n dRight , using your proposi- tions, and demonstrate that they sufﬁce to describe all possible states of the world.

Token 8952:
11.11 Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken by the vacuum cleaner as accurately as you can.

Token 8953:
Explain it, with reference to the forms ofplanning discussed in this chapter.

Token 8954:
11.12 To the medication problem in the previous exercise, add a Test action that has the conditional effect CultureGrowth whenDisease is true and in any case has the perceptual effectKnown (CultureGrowth ).

Token 8955:
Diagram a conditional plan that solves the problem and minimizes the use of the Medicate action.

Token 8956:


Token 8957:
12KNOWLEDGE REPRESENTATION In which we show how to use ﬁrst-order logic to represent the most important aspects of the real world, such as action, space, time, thoughts, and shopping.

Token 8958:
The previous chapters described the technology for knowledge-based agents: the syntax, semantics, and proof theory of propositional and ﬁrst-order logic, and the implementation ofagents that use these logics.

Token 8959:
In this chapter we address the question of what content to put into such an agent’s knowledge base—how to represent facts about the world.

Token 8960:
Section 12.1 introduces the idea of a general ontology, which organizes everything in the world into a hierarchy of categories.

Token 8961:
Section 12.2 covers the basic categories of objects,substances, and measures; Section 12.3 covers events, and Section 12.4 discusses knowledge about beliefs.

Token 8962:
We then return to consider the technology for reasoning with this content: Section 12.5 discusses reasoning systems designed for efﬁcient inference with categories,and Section 12.6 discusses reasoning with default information.

Token 8963:
Section 12.7 brings all theknowledge together in the context of an Internet shopping environment.

Token 8964:
12.1 O NTOLOGICAL ENGINEERING In “toy” domains, the choice of representation is not that important; many choices will work.Complex domains such as shopping on the Internet or driving a car in trafﬁc require moregeneral and ﬂexible representations.

Token 8965:
This chapter shows how to create these representations,concentrating on general concepts—such as Events, Time, Physical Objects ,a n d Beliefs — that occur in many different domains.

Token 8966:
Representing these abstract concepts is sometimescalled ontological engineering .

Token 8967:
ONTOLOGICAL ENGINEERING The prospect of representing everything in the world is daunting.

Token 8968:
Of course, we won’t actually write a complete description of everything—that would be far too much for even a1000-page textbook—but we will leave placeholders where new knowledge for any domain can ﬁt in.

Token 8969:
For example, we will deﬁne what it means to be a physical object, and the details of different types of objects—robots, televisions, books, or whatever—can be ﬁlled in later.

Token 8970:
Thisis analogous to the way that designers of an object-oriented programming framework (such asthe Java Swing graphical framework) deﬁne general concepts like Window , expecting users to 437

Token 8971:
438 Chapter 12.

Token 8972:
Knowledge Representation Anything AbstractObjects Sets Numbers RepresentationalObjects Interval Places Processes PhysicalObjects HumansCategories Sentences Measurements Moments Things Stuff Times Weights Animals Agents Solid Liquid GasGeneralizedEvents Figure 12.1 The upper ontology of the world, showing the topics to be covered later in the chapter.

Token 8973:
Each link indicates that the lower concept is a specialization of the upper one.

Token 8974:
Specializations are not necessarily disjoint; a human is both an animal and an agent, for example.

Token 8975:
We will see in Section 12.3.3 why physical objects come under generalized events. use these to deﬁne more speciﬁc concepts like SpreadsheetWindow .

Token 8976:
The general framework of concepts is called an upper ontology because of the convention of drawing graphs with UPPER ONTOLOGY the general concepts at the top and the more speciﬁc concepts below them, as in Figure 12.1.

Token 8977:
Before considering the ontology further, we should state one important caveat.

Token 8978:
We have elected to use ﬁrst-order logic to discuss the content and organization of knowledge,although certain aspects of the real world are hard to capture in FOL.

Token 8979:
The principal difﬁcultyis that most generalizations have exceptions or hold only to a degree.

Token 8980:
For example, although“tomatoes are red” is a useful rule, some tomatoes are green, yellow, or orange.

Token 8981:
Similar exceptions can be found to almost all the rules in this chapter.

Token 8982:
The ability to handle exceptions and uncertainty is extremely important, but is orthogonal to the task of understanding thegeneral ontology.

Token 8983:
For this reason, we delay the discussion of exceptions until Section 12.5 ofthis chapter, and the more general topic of reasoning with uncertainty until Chapter 13.

Token 8984:
Of what use is an upper ontology? Consider the ontology for circuits in Section 8.4.2.

Token 8985:
It makes many simplifying assumptions: time is omitted completely; signals are ﬁxed and do not propagate; the structure of the circuit remains constant.

Token 8986:
A more general ontology would consider signals at particular times, and would include the wire lengths and propagation de-lays.

Token 8987:
This would allow us to simulate the timing properties of the circuit, and indeed suchsimulations are often carried out by circuit designers.

Token 8988:
We could also introduce more inter-esting classes of gates, for example, by descr ibing the technology (TTL, CMOS, and so on) as well as the input–output speciﬁcation.

Token 8989:
If we wanted to discuss reliability or diagnosis, we would include the possibility that the structure of the circuit or the properties of the gatesmight change spontaneously.

Token 8990:
To account for stray capacitances, we would need to representwhere the wires are on the board.

Token 8991:
Section 12.1. Ontological Engineering 439 If we look at the wumpus world, similar considerations apply.

Token 8992:
Although we do represent time, it has a simple structure: Nothing happens except when the agent acts, and all changesare instantaneous.

Token 8993:
A more general ontology, better suited for the real world, would allow forsimultaneous changes extended over time.

Token 8994:
We also used a Pitpredicate to say which squares have pits.

Token 8995:
We could have allowed for different kinds of pits by having several individuals belonging to the class of pits, each having different properties.

Token 8996:
Similarly, we might want to allow for other animals besides wumpuses.

Token 8997:
It might not be possible to pin down the exactspecies from the available percepts, so we would need to build up a biological taxonomy tohelp the agent predict the behavior of cave-dwellers from scanty clues.

Token 8998:
For any special-purpose ontology, it is possible to make changes like these to move toward greater generality.

Token 8999:
An obvious question then arises: do all these ontologies convergeon a general-purpose ontology?

Token 9000:
After centuries of philosophical and computational inves-tigation, the answer is “Maybe.” In this section, we present one general-purpose ontologythat synthesizes ideas from those centuries.

Token 9001:
Two major characteristics of general-purposeontologies distinguish them from collections of special-purpose ontologies: •A general-purpose ontology should be applicable in more or less any special-purpose domain (with the addition of domain-speciﬁc axioms).

Token 9002:
This means that no representa- tional issue can be ﬁnessed or brushed under the carpet.

Token 9003:
•In any sufﬁciently demanding domain, different areas of knowledge must be uniﬁed , because reasoning and problem solving could involve several areas simultaneously.

Token 9004:
Arobot circuit-repair system, for instance, needs to reason about circuits in terms of elec-trical connectivity and physical layout, and about time, both for circuit timing analysis and estimating labor costs.

Token 9005:
The sentences describing time therefore must be capable of being combined with those describing spatial layout and must work equally well fornanoseconds and minutes and for angstroms and meters.

Token 9006:
We should say up front that the enterprise of general ontological engineering has so far had only limited success.

Token 9007:
None of the top AI applications (as listed in Chapter 1) make use of a shared ontology—they all use special-purpose knowledge engineering.

Token 9008:
Social/political considerations can make it difﬁcult for competing parties to agree on an ontology.

Token 9009:
As TomGruber (2004) says, “Every ontology is a treaty—a social agreement—among people withsome common motive in sharing.” When competing concerns outweigh the motivation forsharing, there can be no common ontology.

Token 9010:
Those ontologies that do exist have been createdalong four routes: 1.

Token 9011:
By a team of trained ontologist/logicians, who architect the ontology and write axioms.

Token 9012:
The CYC system was mostly built this way (Lenat and Guha, 1990). 2.

Token 9013:
By importing categories, attributes, and values from an existing database or databases.

Token 9014:
DB PEDIA was built by importing structured facts from Wikipedia (Bizer et al. , 2007). 3.

Token 9015:
By parsing text documents and extracting information from them.

Token 9016:
T EXTRUNNER was built by reading a large corpus of Web pages (Banko and Etzioni, 2008). 4.

Token 9017:
By enticing unskilled amateurs to enter commonsense knowledge.

Token 9018:
The O PENMIND system was built by volunteers who proposed facts in English (Singh et al. , 2002; Chklovski and Gil, 2005).

Token 9019:
440 Chapter 12.

Token 9020:
Knowledge Representation 12.2 C ATEGORIES AND OBJECTS The organization of objects into categories is a vital part of knowledge representation.

Token 9021:
Al- CATEGORY though interaction with the world takes place at the level of individual objects, much reason- ing takes place at the level of categories.

Token 9022:
For example, a shopper would normally have the goal of buying a basketball, rather than a particular basketball such as BB9.

Token 9023:
Categories also serve to make predictions about objects once they are classiﬁed.

Token 9024:
One infers the presence ofcertain objects from perceptual input, infers category membership from the perceived proper-ties of the objects, and then uses category information to make predictions about the objects.For example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, redﬂesh, black seeds, and presence in the fruit aisle, one can infer that an object is a watermelon;from this, one infers that it would be useful for fruit salad.

Token 9025:
There are two choices for representing categories in ﬁrst-order logic: predicates and objects.

Token 9026:
That is, we can use the predicate Basketball (b), or we can reify 1the category as REIFICATION an object, Basketballs .

Token 9027:
We could then say Member (b,Basketballs ), which we will abbre- viate as b∈Basketballs , to say that bis a member of the category of basketballs.

Token 9028:
We say Subset (Basketballs ,Balls), abbreviated as Basketballs⊂Balls , to say that Basketballs is asubcategory ofBalls .

Token 9029:
We will use subcategory, subclass, and subset interchangeably.

Token 9030:
SUBCATEGORY Categories serve to organize and simplify the knowledge base through inheritance .I f INHERITANCE we say that all instances of the category Food are edible, and if we assert that Fruit is a subclass of Food andApples is a subclass of Fruit , then we can infer that every apple is edible.

Token 9031:
We say that the individual apples inherit the property of edibility, in this case from their membership in the Food category.

Token 9032:
Subclass relations organize categories into a taxonomy ,o rtaxonomic hierarchy .T a x - TAXONOMY onomies have been used explicitly for centuries in technical ﬁelds.

Token 9033:
The largest such taxonomy organizes about 10 million living and extinct species, many of them beetles,2into a single hi- erarchy; library science has developed a taxonomy of all ﬁelds of knowledge, encoded as the Dewey Decimal system; and tax authorities and other government departments have devel- oped extensive taxonomies of occupations and commercial products.

Token 9034:
Taxonomies are also animportant aspect of general commonsense knowledge.

Token 9035:
First-order logic makes it easy to state facts about categories, either by relating ob- jects to categories or by quantifying over their members.

Token 9036:
Here are some types of facts, withexamples of each: •An object is a member of a category.

Token 9037:
BB 9∈Basketballs •A category is a subclass of another category. Basketballs⊂Balls •All members of a category have some properties.

Token 9038:
(x∈Basketballs )⇒Spherical (x) 1Turning a proposition into an object is called reiﬁcation , from the Latin word res, or thing.

Token 9039:
John McCarthy proposed the term “thingiﬁcation,” but it never caught on. 2The famous biologist J.

Token 9040:
B. S. Haldane deduced “An inordinate fondness for beetles” on the part of the Creator.

Token 9041:
Section 12.2. Categories and Objects 441 •Members of a category can be recognized by some properties.

Token 9042:
Orange (x)∧Round (x)∧Diameter (x)=9.5/prime/prime∧x∈Balls⇒x∈Basketballs •A category as a whole has some properties.

Token 9043:
Dogs∈DomesticatedSpecies Notice that because Dogs is a category and is a member of DomesticatedSpecies , the latter must be a category of categories.

Token 9044:
Of course there are exceptions to many of the above rules (punctured basketballs are not spherical); we deal with these exceptions later.

Token 9045:
Although subclass and member relations are the most important ones for categories, we also want to be able to state relations between categories that are not subclasses of eachother.

Token 9046:
For example, if we just say that Males andFemales are subclasses of Animals ,t h e n we have not said that a male cannot be a female.

Token 9047:
We say that two or more categories aredisjoint if they have no members in common.

Token 9048:
And even if we know that males and females DISJOINT are disjoint, we will not know that an animal that is not a male must be a female, unless we say that males and females constitute an exhaustive decomposition of the animals.

Token 9049:
AEXHAUSTIVE DECOMPOSITION disjoint exhaustive decomposition is known as a partition .

Token 9050:
The following examples illustrate PARTITION these three concepts: Disjoint ({Animals ,Vegetables}) ExhaustiveDecomposition ({Americans ,Canadians ,Mexicans}, NorthAmericans ) Partition ({Males ,Females},Animals ).

Token 9051:
(Note that the ExhaustiveDecomposition ofNorthAmericans is not a Partition , because some people have dual citizenship.)

Token 9052:
The three predicates are deﬁned as follows: Disjoint (s)⇔(∀c1,c2c1∈s∧c2∈s∧c1/negationslash=c2⇒Intersection (c1,c2)={}) ExhaustiveDecomposition (s,c)⇔(∀ii∈c⇔∃c2c2∈s∧i∈c2) Partition (s,c)⇔Disjoint (s)∧ExhaustiveDecomposition (s,c).

Token 9053:
Categories can also be deﬁned by providing necessary and sufﬁcient conditions for membership.

Token 9054:
For example, a bachelor is an unmarried adult male: x∈Bachelors⇔Unmarried (x)∧x∈Adults∧x∈Males .

Token 9055:
As we discuss in the sidebar on natural kinds on page 443, strict logical deﬁnitions for cate- gories are neither always possible nor always necessary.

Token 9056:
12.2.1 Physical composition The idea that one object can be part of another is a familiar one.

Token 9057:
One’s nose is part of one’s head, Romania is part of Europe, and this chapter is part of this book.

Token 9058:
We use the generalPartOf relation to say that one thing is part of another.

Token 9059:
Objects can be grouped into PartOf hierarchies, reminiscent of the Subset hierarchy: PartOf (Bucharest ,Romania ) PartOf (Romania ,EasternEurope ) PartOf (EasternEurope ,Europe ) PartOf (Europe ,Earth ).

Token 9060:
442 Chapter 12. Knowledge Representation ThePartOf relation is transitive and reﬂexive; that is, PartOf (x,y)∧PartOf (y,z)⇒PartOf (x,z). PartOf (x,x).

Token 9061:
Therefore, we can conclude PartOf (Bucharest ,Earth ).

Token 9062:
Categories of composite objects are often characterized by structural relations among COMPOSITE OBJECT parts.

Token 9063:
For example, a biped has two legs attached to a body: Biped(a)⇒∃ l1,l2,bLeg(l1)∧Leg(l2)∧Body(b)∧ PartOf (l1,a)∧PartOf (l2,a)∧PartOf (b,a)∧ Attached (l1,b)∧Attached (l2,b)∧ l1/negationslash=l2∧[∀l3Leg(l3)∧PartOf (l3,a)⇒(l3=l1∨l3=l2)].

Token 9064:
The notation for “exactly two” is a little awkward; we are forced to say that there are two legs, that they are not the same, and that if anyone proposes a third leg, it must be the sameas one of the other two.

Token 9065:
In Section 12.5.2, we describe a formalism called description logicmakes it easier to represent constraints like “exactly two.” We can deﬁne a PartPartition relation analogous to the Partition relation for cate- gories.

Token 9066:
(See Exercise 12.8.) An object is composed of the parts in its PartPartition and can be viewed as deriving some properties from those parts.

Token 9067:
For example, the mass of a compos-ite object is the sum of the masses of the parts.

Token 9068:
Notice that this is not the case with categories,which have no mass, even though their elements might.

Token 9069:
It is also useful to deﬁne composite objects with deﬁnite parts but no particular struc- ture.

Token 9070:
For example, we might want to say “The apples in this bag weigh two pounds.” The temptation would be to ascribe this weight to the setof apples in the bag, but this would be a mistake because the set is an abstract mathematical concept that has elements but does not have weight.

Token 9071:
Instead, we need a new concept, which we will call a bunch .

Token 9072:
For example, if BUNCH the apples are Apple 1,Apple 2,a n dApple 3,t h e n BunchOf ({Apple 1,Apple 2,Apple 3}) denotes the composite object with the three apples as parts (not elements).

Token 9073:
We can then use the bunch as a normal, albeit unstructured, object.

Token 9074:
Notice that BunchOf ({x})=x.F u r t h e r m o r e , BunchOf (Apples )is the composite object consisting of all apples—not to be confused with Apples , the category or set of all apples.

Token 9075:
We can deﬁne BunchOf in terms of the PartOf relation. Obviously, each element of sis part of BunchOf (s): ∀xx∈s⇒PartOf (x,BunchOf (s)).

Token 9076:
Furthermore, BunchOf (s)is the smallest object satisfying this condition .

Token 9077:
In other words, BunchOf (s)must be part of any object that has all the elements of sas parts: ∀y[∀xx∈s⇒PartOf (x,y)]⇒PartOf (BunchOf (s),y).

Token 9078:
These axioms are an example of a general technique called logical minimization ,w h i c hLOGICAL MINIMIZATION means deﬁning an object as the smallest one satisfying certain conditions.

Token 9079:
Section 12.2.

Token 9080:
Categories and Objects 443 NATURAL KINDS Some categories have strict deﬁnitions: an object is a triangle if and only if it is a polygon with three sides.

Token 9081:
On the other hand, most categories in the real worldhave no clear-cut deﬁnition; these are called natural kind categories.

Token 9082:
For example, tomatoes tend to be a dull scarlet; roughly spherical; with an indentation at the topwhere the stem was; about two to four inches in diameter; with a thin but toughskin; and with ﬂesh, seeds, and juice inside.

Token 9083:
There is, however, variation: sometomatoes are yellow or orange, unripe tomatoes are green, some are smaller orlarger than average, and cherry tomatoes are uniformly small.

Token 9084:
Rather than havinga complete deﬁnition of tomatoes, we have a set of features that serves to identifyobjects that are clearly typical tomatoes, but might not be able to decide for other objects.

Token 9085:
(Could there be a tomato that is fuzzy like a peach?) This poses a problem for a logical agent.

Token 9086:
The agent cannot be sure that an object it has perceived is a tomato, and even if it were sure, it could not be cer-tain which of the properties of typical tomatoes this one has.

Token 9087:
This problem is aninevitable consequence of operating in partially observable environments.

Token 9088:
One useful approach is to separate what is true of all instances of a cate- gory from what is true only of typical instances.

Token 9089:
So in addition to the categoryTomatoes , we will also have the category Typical (Tomatoes ).

Token 9090:
Here, the Typical function maps a category to the subclass that contains only typical instances: Typical (c)⊆c.

Token 9091:
Most knowledge about natural kinds will actually be about their typical instances: x∈Typical (Tomatoes )⇒Red(x)∧Round (x).

Token 9092:
Thus, we can write down useful facts about categories without exact deﬁni- tions.

Token 9093:
The difﬁculty of providing exact deﬁnitions for most natural categories was explained in depth by Wittgenstein (1953).

Token 9094:
He used the example of games to show that members of a category shared “family resemblances” rather than necessary and sufﬁcient characteristics: what strict deﬁnition encompasses chess, tag, soli- taire, and dodgeball?

Token 9095:
The utility of the notion of strict deﬁnition was also challenged by Quine (1953).

Token 9096:
He pointed out that even the deﬁnition of “bachelor” as an un-married adult male is suspect; one might, for example, question a statement suchas “the Pope is a bachelor.” While not strictly false, this usage is certainly infe- licitous because it induces unintended inferences on the part of the listener.

Token 9097:
The tension could perhaps be resolved by distinguishing between logical deﬁnitionssuitable for internal knowledge representation and the more nuanced criteria forfelicitous linguistic usage.

Token 9098:
The latter may be achieved by “ﬁltering” the assertionsderived from the former.

Token 9099:
It is also possible that failures of linguistic usage serve as feedback for modifying internal deﬁnitions, so that ﬁltering becomes unnecessary.

Token 9100:
444 Chapter 12.

Token 9101:
Knowledge Representation 12.2.2 Measurements In both scientiﬁc and commonsense theories of the world, objects have height, mass, cost, and so on.

Token 9102:
The values that we assign for these properties are called measures .O r d i - MEASURE nary quantitative measures are quite easy to represent.

Token 9103:
We imagine that the universe in- cludes abstract “measure objects,” such as the length that is the length of this line seg- ment: .

Token 9104:
We can call this length 1.5 inches or 3.81 centimeters.

Token 9105:
Thus, the same length has different names in our language.We represent the length with a units function that takes a number as argument.

Token 9106:
(An alternative scheme is explored in Exer- UNITSFUNCTION cise 12.9.)

Token 9107:
If the line segment is called L1, we can write Length (L1)=Inches (1.5)=Centimeters (3.81).

Token 9108:
Conversion between units is done by equating multiples of one unit to another: Centimeters (2.54×d)=Inches (d).

Token 9109:
Similar axioms can be written for pounds and kilograms, seconds and days, and dollars and cents.

Token 9110:
Measures can be used to describe objects as follows: Diameter (Basketball 12)=Inches (9.5). ListPrice (Basketball 12)= $(19) .

Token 9111:
d∈Days⇒Duration (d)=Hours (24). Note that $(1) isnota dollar bill! One can have two dollar bills, but there is only one object named $(1).

Token 9112:
Note also that, while Inches (0)andCentimeters (0)refer to the same zero length, they are not identical to other zero measures, such as Seconds (0).

Token 9113:
Simple, quantitative measures are easy to represent. Other measures present more of a problem, because they have no agreed scale of values.

Token 9114:
Exercises have difﬁculty, desserts havedeliciousness, and poems have beauty, yet numbers cannot be assigned to these qualities.

Token 9115:
Onemight, in a moment of pure accountancy, dismiss such properties as useless for the purpose oflogical reasoning; or, still worse, attempt to impose a numerical scale on beauty.

Token 9116:
This wouldbe a grave mistake, because it is unnecessary.

Token 9117:
The most important aspect of measures is notthe particular numerical values, but the fact that measures can be ordered .

Token 9118:
Although measures are not numbers, we can still compare them, using an ordering symbol such as >.

Token 9119:
For example, we might well believe that Norvig’s exercises are tougher than Russell’s, and that one scores less on tougher exercises: e 1∈Exercises∧e2∈Exercises∧Wrote (Norvig ,e1)∧Wrote (Russell ,e2)⇒ Diﬃculty (e1)>Diﬃculty (e2).

Token 9120:
e1∈Exercises∧e2∈Exercises∧Diﬃculty (e1)>Diﬃculty (e2)⇒ ExpectedScore (e1)<ExpectedScore (e2).

Token 9121:
This is enough to allow one to decide which exercises to do, even though no numerical values for difﬁculty were ever used.

Token 9122:
(One does, however, have to discover who wrote which exer- cises.)

Token 9123:
These sorts of monotonic relationships among measures form the basis for the ﬁeld of qualitative physics , a subﬁeld of AI that investigates how to reason about physical systems without plunging into detailed equations and numerical simulations.

Token 9124:
Qualitative physics is discussed in the historical notes section.

Token 9125:
Section 12.2.

Token 9126:
Categories and Objects 445 12.2.3 Objects: Things and stuff The real world can be seen as consisting of primitive objects (e.g., atomic particles) and composite objects built from them.

Token 9127:
By reasoning at the level of large objects such as applesand cars, we can overcome the complexity involved in dealing with vast numbers of primitiveobjects individually.

Token 9128:
There is, however, a signiﬁcant portion of reality that seems to defy anyobvious individuation —division into distinct objects.

Token 9129:
We give this portion the generic name INDIVIDUATION stuff . For example, suppose I have some butter and an aardvark in front of me.

Token 9130:
I can say STUFF there is one aardvark, but there is no obvious number of “butter-objects,” because any part of a butter-object is also a butter-object, at least until we get to very small parts indeed.

Token 9131:
This isthe major distinction between stuff and things . If we cut an aardvark in half, we do not get two aardvarks (unfortunately).

Token 9132:
The English language distinguishes clearly between stuff andthings .

Token 9133:
We say “an aard- vark,” but, except in pretentious California restaurants, one cannot say “a butter.” Linguistsdistinguish between count nouns , such as aardvarks, holes, and theorems, and mass nouns , COUNT NOUNS MASS NOUN such as butter, water, and energy.

Token 9134:
Several competing ontologies claim to handle this distinc- tion. Here we describe just one; the others are covered in the historical notes section.

Token 9135:
To represent stuff properly, we begin with the obvious.

Token 9136:
We need to have as objects in our ontology at least the gross “lumps” of stuff we interact with.

Token 9137:
For example, we might recognize a lump of butter as the one left on the table the night before; we might pick it up,weigh it, sell it, or whatever.

Token 9138:
In these senses, it is an object just like the aardvark. Let uscall itButter 3. We also deﬁne the category Butter .

Token 9139:
Informally, its elements will be all those things of which one might say “It’s butter,” including Butter 3.

Token 9140:
With some caveats about very small parts that we w omit for now, any part of a butter-object is also a butter-object: b∈Butter∧PartOf (p,b)⇒p∈Butter .

Token 9141:
We can now say that butter melts at around 30 degrees centigrade: b∈Butter⇒MeltingPoint (b,Centigrade (30)).

Token 9142:
We could go on to say that butter is yellow, is less dense than water, is soft at room tempera- ture, has a high fat content, and so on.

Token 9143:
On the other hand, butter has no particular size, shape, or weight.

Token 9144:
We can deﬁne more specialized categories of butter such as UnsaltedButter , which is also a kind of stuff.

Token 9145:
Note that the category PoundOfButter , which includes as members all butter-objects weighing one pound, is not a kind of stuff.

Token 9146:
If we cut a pound of butter in half, we do not, alas, get two pounds of butter.

Token 9147:
What is actually going on is this: some properties are intrinsic : they belong to the very INTRINSIC substance of the object, rather than to the object as a whole.

Token 9148:
When you cut an instance of stuff in half, the two pieces retain the intrinsic properties—things like density, boiling point, ﬂavor, color, ownership, and so on.

Token 9149:
On the other hand, their extrinsic properties—weight, EXTRINSIC length, shape, and so on—are not retained under subdivision.

Token 9150:
A category of objects that includes in its deﬁnition only intrinsic properties is then a substance, or mass noun; a class that includes anyextrinsic properties in its deﬁnition is a count noun.

Token 9151:
The category Stuﬀ is the most general substance category, specifying no intrinsic properties.

Token 9152:
The category Thing is the most general discrete object category, specifying no extrinsic properties.

Token 9153:
446 Chapter 12. Knowledge Representation 12.3 E VENTS In Section 10.4.2, we showed how situation calculus represents actions and their effects.

Token 9154:
Situation calculus is limited in its applicability: it was designed to describe a world in whichactions are discrete, instantaneous, and happen one at a time.

Token 9155:
Consider a continuous action,such as ﬁlling a bathtub.

Token 9156:
Situation calculus can say that the tub is empty before the action andfull when the action is done, but it can’t talk about what happens during the action.

Token 9157:
It also can’t describe two actions happening at the same time—such as brushing one’s teeth whilewaiting for the tub to ﬁll.

Token 9158:
To handle such cases we introduce an alternative formalism knownasevent calculus , which is based on points of time rather than on situations.

Token 9159:
3EVENT CALCULUS Event calculus reiﬁes ﬂuents and events.

Token 9160:
The ﬂuent At(Shankar ,Berkeley )is an ob- ject that refers to the fact of Shankar being in Berkeley, but does not by itself say anythingabout whether it is true.

Token 9161:
To assert that a ﬂuent is actually true at some point in time we use the predicate T,a si n T(At(Shankar ,Berkeley ),t).

Token 9162:
Events are described as instances of event categories.

Token 9163:
4The event E1of Shankar ﬂying from San Francisco to Washington, D.C. is described as E1∈Flyings∧Flyer(E1,Shankar )∧Origin (E1,SF)∧Destination (E1,DC).

Token 9164:
If this is too verbose, we can deﬁne an alternative three-argument version of the category of ﬂying events and say E1∈Flyings (Shankar ,SF,DC).

Token 9165:
We then use Happens (E1,i)to say that the event E1took place over the time interval i,a n d we say the same thing in functional form with Extent (E1)=i.

Token 9166:
We represent time intervals by a (start, end) pair of times; that is, i=(t1,t2)is the time interval that starts at t1and ends att2.

Token 9167:
The complete set of predicates for one version of the event calculus is T(f,t) Fluent fis true at time t Happens (e,i) Event ehappens over the time interval i Initiates (e,f,t) Event ecauses ﬂuent fto start to hold at time t Terminates (e,f,t)Event ecauses ﬂuent fto cease to hold at time t Clipped (f,i) Fluent fceases to be true at some point during time interval i Restored (f,i) Fluent fbecomes true sometime during time interval i We assume a distinguished event, Start , that describes the initial state by saying which ﬂuents are initiated or terminated at the start time.

Token 9168:
We deﬁne Tby saying that a ﬂuent holds at a point in time if the ﬂuent was initiated by an event at some time in the past and was not made false (clipped) by an intervening event.

Token 9169:
A ﬂuent does not hold if it was terminated by an event and 3The terms “event” and “action” may be used intercha ngeably.

Token 9170:
Informally, “action” connotes an agent while “event” connotes the possibility of agentless actions.

Token 9171:
4Some versions of event calculus do not distinguish event categories from instances of the categories.

Token 9172:
Section 12.3. Events 447 not made true (restored) by another event.

Token 9173:
Formally, the axioms are: Happens (e,(t1,t2))∧Initiates (e,f,t 1)∧¬Clipped (f,(t1,t))∧t1<t⇒ T(f,t) Happens (e,(t1,t2))∧Terminates (e,f,t 1)∧¬Restored (f,(t1,t))∧t1<t⇒ ¬T(f,t) where Clipped andRestored are deﬁned by Clipped (f,(t1,t2))⇔ ∃e,t,t 3Happens (e,(t,t3))∧t1≤t<t 2∧Terminates (e,f,t) Restored (f,(t1,t2))⇔ ∃e,t,t 3Happens (e,(t,t3))∧t1≤t<t 2∧Initiates (e,f,t) It is convenient to extend Tto work over intervals as well as time points; a ﬂuent holds over an interval if it holds on every point within the interval: T(f,(t1,t2))⇔[∀t(t1≤t<t 2)⇒T(f,t)] Fluents and actions are deﬁned with domain-speciﬁc axioms that are similar to successor- state axioms.

Token 9174:
For example, we can say that the only way a wumpus-world agent gets an arrow is at the start, and the only way to use up an arrow is to shoot it: Initiates (e,HaveArrow (a),t)⇔e=Start Terminates (e,HaveArrow (a),t)⇔e∈Shootings (a) By reifying events we make it possible to add any amount of arbitrary information about them.

Token 9175:
For example, we can say that Shankar’s ﬂight was bumpy with Bumpy (E1).I n a n ontology where events are n-ary predicates, there would be no way to add extra information like this; moving to an n+1-ary predicate isn’t a scalable solution.

Token 9176:
We can extend event calculus to make it possible to represent simultaneous events (such as two people being necessary to ride a seesaw), exogenous events (such as the wind blowingand changing the location of an object), continuous events (such as the level of water in the bathtub continuously rising) and other complications.

Token 9177:
12.3.1 Processes The events we have seen so far are what we call discrete events —they have a deﬁnite struc- DISCRETE EVENTS ture.

Token 9178:
Shankar’s trip has a beginning, middle, and end.

Token 9179:
If interrupted halfway, the event would be something different—it would not be a trip from San Francisco to Washington, but instead a trip from San Francisco to somewhere over Kansas.

Token 9180:
On the other hand, the category of events denoted by Flyings has a different quality.

Token 9181:
If we take a small interval of Shankar’s ﬂight, say, the third 20-minute segment (while he waits anxiously for a bag of peanuts), that event is still a member of Flyings .

Token 9182:
In fact, this is true for any subinterval.

Token 9183:
Categories of events with this property are called process categories or liquid event PROCESS LIQUIDEVENT categories.

Token 9184:
Any process ethat happens over an interval also happens over any subinterval: (e∈Processes )∧Happens (e,(t1,t4))∧(t1<t2<t3<t4)⇒Happens (e,(t2,t3)).

Token 9185:
The distinction between liquid and nonliquid events is exactly analogous to the difference between substances, or stuff, and individual objects, or things .

Token 9186:
In fact, some have called liquid events temporal substances , whereas substances like butter are spatial substances .TEMPORAL SUBSTANCE SPATIAL SUBSTANCE

Token 9187:
448 Chapter 12.

Token 9188:
Knowledge Representation 12.3.2 Time intervals Event calculus opens us up to the possibility of talking about time, and time intervals.

Token 9189:
We will consider two kinds of time intervals: moments and extended intervals.

Token 9190:
The distinction isthat only moments have zero duration: Partition ({Moments ,ExtendedIntervals },Intervals ) i∈Moments⇔Duration (i)=Seconds (0).

Token 9191:
Next we invent a time scale and associate points on that scale with moments, giving us ab- solute times.

Token 9192:
The time scale is arbitrary; we measure it in seconds and say that the momentat midnight (GMT) on January 1, 1900, has time 0.

Token 9193:
The functions Begin andEnd pick out the earliest and latest moments in an interval, and the function Time delivers the point on the time scale for a moment.

Token 9194:
The function Duration gives the difference between the end time and the start time. Interval (i)⇒Duration (i)=(Time(End(i))−Time(Begin (i))).

Token 9195:
Time(Begin (AD1900))= Seconds (0). Time(Begin (AD2001))= Seconds (3187324800) . Time(End(AD2001))= Seconds (3218860800) .

Token 9196:
Duration (AD2001)= Seconds (31536000) .

Token 9197:
To make these numbers easier to read, we also introduce a function Date , which takes six arguments (hours, minutes, seconds, day, month, and year) and returns a time point: Time(Begin ( AD2001))= Date(0,0,0,1,Jan,2001) Date(0,20,21,24,1,1995)= Seconds (3000000000) .

Token 9198:
Two intervals Meet if the end time of the ﬁrst equals the start time of the second.

Token 9199:
The com- plete set of interval relations, as proposed by Allen (1983), is shown graphically in Figure 12.2and logically below: Meet(i,j)⇔End(i)=Begin (j) Before (i,j)⇔End(i)<Begin (j) After(j, i)⇔Before (i,j) During (i,j)⇔Begin (j)<Begin (i)<End(i)<End(j) Overlap (i,j)⇔Begin (i )<Begin (j)<End(i)<End(j) Begins (i,j)⇔Begin (i)=Begin (j) Finishes (i,j)⇔End(i)=End(j) Equals (i,j)⇔Begin (i)=Begin (j)∧End(i)=End(j) These all have their intuitive meaning, with the exception of Overlap : we tend to think of overlap as symmetric (if ioverlaps jthen joverlaps i), but in this deﬁnition, Overlap (i,j) only holds if ibegins before j.

Token 9200:
To say that the reign of Elizabeth II immediately followed that of George VI, and the reign of Elvis overlapped with the 1950s, we can write the following: Meets (ReignOf (GeorgeVI ),ReignOf (ElizabethII )).

Token 9201:
Overlap (Fifties ,ReignOf (Elvis)). Begin (Fifties )=Begin (AD1950) . End(Fifties )=End(AD1959) .

Token 9202:
Section 12.3. Events 449 Figure 12.2 Predicates on time intervals.

Token 9203:
time18011797 1789WashingtonAdamsJefferson Figure 12.3 A schematic view of the object President (USA)for the ﬁrst 15 years of its existence.

Token 9204:
12.3.3 Fluents and objects Physical objects can be viewed as generalized events, in the sense that a physical object is a chunk of space–time.

Token 9205:
For example, USA can be thought of as an event that began in, say, 1776 as a union of 13 states and is still in progress today as a union of 50.

Token 9206:
We candescribe the changing properties of USA using state ﬂuents, such as Population (USA).A property of the USA that changes every four or eight years, barring mishaps, is its president.

Token 9207:
One might propose that President (USA)is a logical term that denotes a different object at different times.

Token 9208:
Unfortunately, this is not possible, because a term denotes exactly oneobject in a given model structure.

Token 9209:
(The term President (USA,t)can denote different objects, depending on the value of t, but our ontology keeps time indices separate from ﬂuents.) The

Token 9210:
450 Chapter 12.

Token 9211:
Knowledge Representation only possibility is that President (USA)denotes a single object that consists of different people at different times.

Token 9212:
It is the object that is George Washington from 1789 to 1797, JohnAdams from 1797 to 1801, and so on, as in Figure 12.3.

Token 9213:
To say that George Washington waspresident throughout 1790, we can write T(Equals (President (USA),GeorgeWashington ),AD1790) .

Token 9214:
We use the function symbol Equals rather than the standard logical predicate =, because we cannot have a predicate as an argument to T, and because the interpretation is notthat GeorgeWashington andPresident (USA)are logically identical in 1790; logical identity is not something that can change over time.

Token 9215:
The identity is between the subevents of each objectthat are deﬁned by the period 1790.

Token 9216:
12.4 M ENTAL EVENTS AND MENTAL OBJECTS The agents we have constructed so far have beliefs and can deduce new beliefs.

Token 9217:
Yet noneof them has any knowledge about beliefs or about deduction.

Token 9218:
Knowledge about one’s own knowledge and reasoning processes is useful for controlling inference.

Token 9219:
For example, supposeAlice asks “what is the square root of 1764” and Bob replies “I don’t know.” If Alice insists“think harder,” Bob should realize that with some more thought, this question can in factbe answered.

Token 9220:
On the other hand, if the question were “Is your mother sitting down right now?” then Bob should realize that thinking harder is unlikely to help.

Token 9221:
Knowledge about the knowledge of other agents is also important; Bob should realize that his mother knowswhether she is sitting or not, and that asking her would be a way to ﬁnd out.

Token 9222:
What we need is a model of the mental objects that are in someone’s head (or some- thing’s knowledge base) and of the mental processes that manipulate those mental objects.

Token 9223:
The model does not have to be detailed.

Token 9224:
We do not have to be able to predict how many milliseconds it will take for a particular agent to make a deduction.

Token 9225:
We will be happy just to be able to conclude that mother knows whether or not she is sitting.

Token 9226:
We begin with the propositional attitudes that an agent can have toward mental ob- PROPOSITIONAL ATTITUDE jects: attitudes such as Believes ,Knows ,Wants ,Intends ,a n dInforms .

Token 9227:
The difﬁculty is that these attitudes do not behave like “normal” predicates.

Token 9228:
For example, suppose we try to assert that Lois knows that Superman can ﬂy: Knows (Lois,CanFly (Superman )).

Token 9229:
One minor issue with this is that we normally think of CanFly (Superman )as a sentence, but here it appears as a term.

Token 9230:
That issue can be patched up just be reifying CanFly (Superman ); making it a ﬂuent.

Token 9231:
A more serious problem is that, if it is true that Superman is Clark Kent,then we must conclude that Lois knows that Clark can ﬂy: (Superman =Clark)∧Knows (Lois,CanFly (Superman )) |=Knows (Lois,CanFly (Clark)).

Token 9232:
This is a consequence of the fact that equality reasoning is built into logic.

Token 9233:
Normally that is a good thing; if our agent knows that 2+2=4 and4<5, then we want our agent to know

Token 9234:
Section 12.4. Mental Events and Mental Objects 451 that2+2 <5.

Token 9235:
This property is called referential transparency —it doesn’t matter whatREFERENTIAL TRANSPARENCY term a logic uses to refer to an object, what matters is the object that the term names.

Token 9236:
But for propositional attitudes like believes andknows , we would like to have referential opacity—the terms used domatter, because not all agents know which terms are co-referential.

Token 9237:
Modal logic is designed to address this problem.

Token 9238:
Regular logic is concerned with a sin- MODAL LOGIC gle modality, the modality of truth, allowing us to express “ Pis true.” Modal logic includes special modal operators that take sentences (rather than terms) as arguments.

Token 9239:
For example,“Aknows P” is represented with the notation K AP,w h e r e Kis the modal operator for knowl- edge.

Token 9240:
It takes two arguments, an agent (written as the subscript) and a sentence.

Token 9241:
The syntaxof modal logic is the same as ﬁrst-order logic, except that sentences can also be formed withmodal operators.

Token 9242:
The semantics of modal logic is more complicated.

Token 9243:
In ﬁrst-order logic a model con- tains a set of objects and an interpretation that maps each name to the appropriate object,relation, or function.

Token 9244:
In modal logic we want to be able to consider both the possibility thatSuperman’s secret identity is Clark and that it isn’t.

Token 9245:
Therefore, we will need a more com-plicated model, one that consists of a collection of possible worlds rather than just one true POSSIBLE WORLD world.

Token 9246:
The worlds are connected in a graph by accessibility relations , one relation for eachACCESSIBILITY RELATIONS modal operator.

Token 9247:
We say that world w1is accessible from world w0with respect to the modal operator KAif everything in w1is consistent with what Aknows in w0, and we write this asAcc(KA,w0,w1).

Token 9248:
In diagrams such as Figure 12.4 we show accessibility as an arrow be- tween possible worlds.

Token 9249:
As an example, in the real world, Bucharest is the capital of Romania,but for an agent that did not know that, other possible worlds are accessible, including oneswhere the capital of Romania is Sibiu or Soﬁa.

Token 9250:
Presumably a world where 2+2=5 would not be accessible to any agent.

Token 9251:
In general, a knowledge atom K APis true in world wif and only if Pis true in every world accessible from w. The truth of more complex sentences is derived by recursive appli- cation of this rule and the normal rules of ﬁrst-order logic.

Token 9252:
That means that modal logic canbe used to reason about nested knowledge sentences: what one agent knows about another agent’s knowledge.

Token 9253:
For example, we can say that, even though Lois doesn’t know whether Superman’s secret identity is Clark Kent, she does know that Clark knows: K Lois[KClarkIdentity (Superman ,Clark)∨KClark¬Identity (Superman ,Clark)] Figure 12.4 shows some possible worlds for this domain, with accessibility relations for Lois and Superman.

Token 9254:
In the TOP-LEFT diagram, it is common knowledge that Superman knows his own iden- tity, and neither he nor Lois has seen the weather report.

Token 9255:
So in w0the worlds w0andw2are accessible to Superman; maybe rain is predicted, maybe not.

Token 9256:
For Lois all four worlds are ac- cessible from each other; she doesn’t know anything about the report or if Clark is Superman.

Token 9257:
But she does know that Superman knows whether he is Clark, because in every world that is accessible to Lois, either Superman knows I, or he knows¬I.

Token 9258:
Lois does not know which is the case, but either way she knows Superman knows.

Token 9259:
In the TOP-RIGHT diagram it is common knowledge that Lois has seen the weather report.

Token 9260:
So in w4she knows rain is predicted and in w6she knows rain is not predicted.

Token 9261:
452 Chapter 12.

Token 9262:
Knowledge Representation (a) (b) (c)w0: I,R w2: I,¬Rw3: ¬I,¬Rw1: ¬I,R w4: I,R w6: I,¬Rw7: ¬I,¬Rw5: ¬I,R w0: I,R w2: I,¬R w3: ¬I,¬Rw1: ¬I,Rw4: I,R w5: ¬I,R w6: I,¬R w7: ¬I,¬R Figure 12.4 Possible worlds with accessibility relations KSuperman (solid arrows) and KLois (dotted arrows).

Token 9263:
The proposition Rmeans “the weather report for tomorrow is rain” andImeans “Superman’s secret identity is Clark Kent.” All worlds are accessible to them- selves; the arrows from a world to itself are not shown.

Token 9264:
Superman does not know the report, but he knows that Lois knows, because in every world that is accessible to him, either she knows Ror she knows¬R.

Token 9265:
In the BOTTOM diagram we represent the scenario where it is common knowledge that Superman knows his identity, and Lois might or might not have seen the weather report.

Token 9266:
We represent this by combining the two top scenarios, and adding arrows to show that Supermandoes not know which scenario actually holds.

Token 9267:
Lois does know, so we don’t need to add anyarrows for her.

Token 9268:
In w 0Superman still knows Ibut not R, and now he does not know whether Lois knows R. From what Superman knows, he might be in w0orw2, in which case Lois does not know whether Ris true, or he could be in w4, in which case she knows R,o rw6,i n which case she knows ¬R.

Token 9269:
There are an inﬁnite number of possible worlds, so the trick is to introduce just the ones you need to represent what you are trying to model.

Token 9270:
A new possible world is needed to talk about different possible facts (e.g., rain is predicted or not), or to talk about different states of knowledge (e.g., does Lois know that rain is predicted).

Token 9271:
That means two possible worlds, such as w4andw0in Figure 12.4, might have the same base facts about the world, but differ in their accessibility relations, and therefore in facts about knowledge.

Token 9272:
Modal logic solves some tricky issues with the interplay of quantiﬁers and knowledge.

Token 9273:
The English sentence “Bond knows that someone is a spy” is ambiguous. The ﬁrst reading is

Token 9274:
Section 12.5.

Token 9275:
Reasoning Systems for Categories 453 that there is a particular someone who Bond knows is a spy; we can write this as ∃xKBondSpy(x), which in modal logic means that there is an xthat, in all accessible worlds, Bond knows to be a spy.

Token 9276:
The second reading is that Bond just knows that there is at least one spy: KBond∃xSpy(x).

Token 9277:
The modal logic interpretation is that in each accessible world there is an xthat is a spy, but it need not be the same xin each world.

Token 9278:
Now that we have a modal operator for knowledge, we can write axioms for it.

Token 9279:
First, we can say that agents are able to draw deductions; if an agent knows Pand knows that P implies Q, then the agent knows Q: (KaP∧Ka(P⇒Q))⇒KaQ.

Token 9280:
From this (and a few other rules about logical identities) we can establish that KA(P∨¬P) is a tautology; every agent knows every proposition Pis either true or false.

Token 9281:
On the other hand,(KAP)∨(KA¬P)is not a tautology; in general, there will be lots of propositions that an agent does not know to be true and does not know to be false.

Token 9282:
It is said (going back to Plato) that knowledge is justiﬁed true belief.

Token 9283:
That is, if it is true, if you believe it, and if you have an unassailably good reason, then you know it.

Token 9284:
That means that if you know something, it must be true, and we have the axiom: KaP⇒P.

Token 9285:
Furthermore, logical agents should be able to introspect on their own knowledge.

Token 9286:
If they know something, then they know that they know it: KaP⇒Ka(KaP).

Token 9287:
We can deﬁne similar axioms for belief (often denoted by B) and other modalities.

Token 9288:
However, one problem with the modal logic approach is that it assumes logical omniscience on theLOGICAL OMNISCIENCE part of agents.

Token 9289:
That is, if an agent knows a set of axioms, then it knows all consequences of those axioms.

Token 9290:
This is on shaky ground even for the somewhat abstract notion of knowledge, but it seems even worse for belief, because belief has more connotation of referring to things that are physically represented in the agent, not just potentially derivable.

Token 9291:
There have beenattempts to deﬁne a form of limited rationality for agents; to say that agents believe thoseassertions that can be derived with the application of no more than kreasoning steps, or no more than sseconds of computation.

Token 9292:
These attempts have been generally unsatisfactory.

Token 9293:
12.5 R EASONING SYSTEMS FOR CATEGORIES Categories are the primary building blocks of large-scale knowledge representation schemes.

Token 9294:
This section describes systems specially designed for organizing and reasoning with cate-gories.

Token 9295:
There are two closely related families of systems: semantic networks provide graph- ical aids for visualizing a knowledge base and efﬁcient algorithms for inferring properties

Token 9296:
454 Chapter 12.

Token 9297:
Knowledge Representation of an object on the basis of its category membership; and description logics provide a for- mal language for constructing and combining category deﬁnitions and efﬁcient algorithmsfor deciding subset and superset relationships between categories.

Token 9298:
12.5.1 Semantic networks In 1909, Charles S. Peirce proposed a graphical notation of nodes and edges called existential graphs that he called “the logic of the future.” Thus began a long-running debate betweenEXISTENTIAL GRAPHS advocates of “logic” and advocates of “semantic networks.” Unfortunately, the debate ob- scured the fact that semantics networks—at least those with well-deﬁned semantics— area form of logic.

Token 9299:
The notation that semantic networks provide for certain kinds of sentencesis often more convenient, but if we strip away the “human interface” issues, the underlyingconcepts—objects, relations, quantiﬁcation, and so on—are the same.

Token 9300:
There are many variants of semantic networks, but all are capable of representing in- dividual objects, categories of objects, and relations among objects.

Token 9301:
A typical graphical no-tation displays object or category names in ovals or boxes, and connects them with labeledlinks.

Token 9302:
For example, Figure 12.5 has a MemberOf link between Mary andFemalePersons , corresponding to the logical assertion Mary∈FemalePersons ; similarly, the SisterOf link between Mary andJohn corresponds to the assertion SisterOf (Mary,John).

Token 9303:
We can con- nect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows that one can get carried away.

Token 9304:
For example, we know that persons have female persons asmothers, so can we draw a HasMother link from Persons toFemalePersons ?

Token 9305:
The answer is no, because HasMother is a relation between a person and his or her mother, and categories do not have mothers.

Token 9306:
5 For this reason, we have used a special notation—the double-boxed link—in Figure 12.5.

Token 9307:
This link asserts that ∀xx∈Persons⇒[∀yHasMother (x,y)⇒y∈FemalePersons ].

Token 9308:
We might also want to assert that persons have two legs—that is, ∀xx∈Persons⇒Legs(x,2).

Token 9309:
As before, we need to be careful not to assert that a category has legs; the single-boxed link in Figure 12.5 is used to assert properties of every member of a category.

Token 9310:
The semantic network notation makes it convenient to perform inheritance reasoning of the kind introduced in Section 12.2.

Token 9311:
For example, by virtue of being a person, Mary inheritsthe property of having two legs.

Token 9312:
Thus, to ﬁnd out how many legs Mary has, the inheritancealgorithm follows the MemberOf link from Mary to the category she belongs to, and then follows SubsetOf links up the hierarchy until it ﬁnds a category for which there is a boxed Legs link—in this case, the Persons category.

Token 9313:
The simplicity and efﬁciency of this inference 5Several early systems failed to distinguish between pr operties of members of a category and properties of the category as a whole.

Token 9314:
This can lead directly to inconsistencies, as pointed out by Drew McDermott (1976) in his article “Artiﬁcial Intelligence Meets Natural Stupidity.” Another common problem was the use of IsAlinks for both subset and membership relations, in correspondenc e with English usage: “a cat is a mammal” and “Fiﬁ is a cat.” See Exercise 12.22 for more on these issues.

Token 9315:
Section 12.5.

Token 9316:
Reasoning Systems for Categories 455 Mammals John MaryPersons Male PersonsFemale Persons 12SubsetOf SubsetOf SubsetOf MemberOf MemberOf SisterOf LegsLegsHasMother Figure 12.5 A semantic network with four objects (John, Mary, 1, and 2) and four cate- gories.

Token 9317:
Relations are denoted by labeled links.

Token 9318:
MemberOfFlyEvents Fly17 Shankar NewYork NewDelhi YesterdayAgent Origin DestinationDuring Figure 12.6 A fragment of a semantic network showing the representation of the logical assertion Fly(Shankar ,NewYork ,NewDelhi ,Yesterday ).

Token 9319:
mechanism, compared with logical theorem proving, has been one of the main attractions of semantic networks.

Token 9320:
Inheritance becomes complicated when an object can belong to more than one category or when a category can be a subset of more than one other category; this is called multiple in- heritance .

Token 9321:
In such cases, the inheritance algorithm might ﬁnd two or more conﬂicting valuesMULTIPLE INHERITANCE answering the query.

Token 9322:
For this reason, multiple inheritance is banned in some object-oriented programming (OOP) languages, such as Java, that use inheritance in a class hierarchy.

Token 9323:
It is usually allowed in semantic networks, but we defer discussion of that until Section 12.6.

Token 9324:
The reader might have noticed an obvious drawback of semantic network notation, com- pared to ﬁrst-order logic: the fact that links between bubbles represent only binary relations.

Token 9325:
For example, the sentence Fly(Shankar ,NewYork ,NewDelhi ,Yesterday )cannot be as- serted directly in a semantic network.

Token 9326:
Nonetheless, we canobtain the effect of n-ary asser- tions by reifying the proposition itself as an event belonging to an appropriate event category.

Token 9327:
Figure 12.6 shows the semantic network structure for this particular event.

Token 9328:
Notice that the restriction to binary relations forces the creation of a rich ontology of reiﬁed concepts.

Token 9329:
Reiﬁcation of propositions makes it possible to represent every ground, function-free atomic sentence of ﬁrst-order logic in the semantic network notation.

Token 9330:
Certain kinds of univer-

Token 9331:
456 Chapter 12.

Token 9332:
Knowledge Representation sally quantiﬁed sentences can be asserted using inverse links and the singly boxed and doubly boxed arrows applied to categories, but that still leaves us a long way short of full ﬁrst-orderlogic.

Token 9333:
Negation, disjunction, nested function symbols, and existential quantiﬁcation are allmissing.

Token 9334:
Now it is possible to extend the notation to make it equivalent to ﬁrst-order logic—as in Peirce’s existential graphs—but doing so negates one of the main advantages of semantic networks, which is the simplicity and transparency of the inference processes.

Token 9335:
Designers can build a large network and still have a good idea about what queries will be efﬁcient, because(a) it is easy to visualize the steps that the inference procedure will go through and (b) in somecases the query language is so simple that difﬁcult queries cannot be posed.

Token 9336:
In cases wherethe expressive power proves to be too limiting, many semantic network systems provide forprocedural attachment to ﬁll in the gaps.

Token 9337:
Procedural attachment is a technique whereby a query about (or sometimes an assertion of) a certain relation results in a call to a specialprocedure designed for that relation rather than a general inference algorithm.

Token 9338:
One of the most important aspects of semantic networks is their ability to represent default values for categories.

Token 9339:
Examining Figure 12.5 carefully, one notices that John has one DEFAULT VALUE leg, despite the fact that he is a person and all persons have two legs.

Token 9340:
In a strictly logical KB, this would be a contradiction, but in a semantic network, the assertion that all persons have two legs has only default status; that is, a person is assumed to have two legs unless this is contradicted by more speciﬁc information.

Token 9341:
The default semantics is enforced naturally by theinheritance algorithm, because it follows links upwards from the object itself (John in thiscase) and stops as soon as it ﬁnds a value.

Token 9342:
We say that the default is overridden by the more OVERRIDING speciﬁc value.

Token 9343:
Notice that we could also override the default number of legs by creating a category of OneLeggedPersons , a subset of Persons of which John is a member.

Token 9344:
We can retain a strictly logical semantics for the network if we say that the Legs asser- tion for Persons includes an exception for John: ∀xx∈Persons∧x/negationslash=John⇒Legs(x,2).

Token 9345:
For a ﬁxed network, this is semantically adequate but will be much less concise than the network notation itself if there are lots of exceptions.

Token 9346:
For a network that will be updated withmore assertions, however, such an approach fails—we really want to say that any persons asyet unknown with one leg are exceptions too.

Token 9347:
Section 12.6 goes into more depth on this issueand on default reasoning in general.

Token 9348:
12.5.2 Description logics The syntax of ﬁrst-order logic is designed to make it easy to say things about objects.

Token 9349:
De- scription logics are notations that are designed to make it easier to describe deﬁnitions and DESCRIPTION LOGIC properties of categories.

Token 9350:
Description logic systems evolved from semantic networks in re- sponse to pressure to formalize what the networks mean while retaining the emphasis ontaxonomic structure as an organizing principle.

Token 9351:
The principal inference tasks for description logics are subsumption (checking if one SUBSUMPTION category is a subset of another by comparing their deﬁnitions) and classiﬁcation (checking CLASSIFICATION whether an object belongs to a category)..

Token 9352:
Some systems also include consistency of a cate- gory deﬁnition—whether the membership criteria are logically satisﬁable.

Token 9353:
Section 12.5.

Token 9354:
Reasoning Systems for Categories 457 Concept→Thing|ConceptName |And(Concept ,...) |All(RoleName ,Concept ) |AtLeast (Integer ,RoleName ) |AtMost (Integer ,RoleName ) |Fills(RoleName ,IndividualName ,...) |SameAs (Path,Path) |OneOf (IndividualName ,...) Path→[RoleName ,...] Figure 12.7 The syntax of descriptions in a subset of the C LASSIC language.

Token 9355:
The C LASSIC language (Borgida et al. , 1989) is a typical description logic.

Token 9356:
The syntax of C LASSIC descriptions is shown in Figure 12.7.6For example, to say that bachelors are unmarried adult males we would write Bachelor =And(Unmarried ,Adult,Male).

Token 9357:
The equivalent in ﬁrst-order logic would be Bachelor (x)⇔Unmarried (x)∧Adult(x)∧Male(x).

Token 9358:
Notice that the description logic has an an algebra of operations on predicates, which of course we can’t do in ﬁrst-order logic.

Token 9359:
Any description in C LASSIC can be translated into an equivalent ﬁrst-order sentence, but some descriptions are more straightforward in C LASSIC .

Token 9360:
For example, to describe the set of men with at least three sons who are all unemployedand married to doctors, and at most two daughters who are all professors in physics or mathdepartments, we would use And(Man,AtLeast (3,Son),AtMost (2,Daughter ), All(Son,And(Unemployed ,Married ,All(Spouse ,Doctor ))), All(Daughter ,And(Professor ,Fills(Department ,Physics ,Math)))).

Token 9361:
We leave it as an exercise to translate this into ﬁrst-order logic.

Token 9362:
Perhaps the most important aspect of description logics is their emphasis on tractability of inference.

Token 9363:
A problem instance is solved by describing it and then asking if it is subsumed by one of several possible solution categories.

Token 9364:
In standard ﬁrst-order logic systems, predicting the solution time is often impossible.

Token 9365:
It is frequently left to the user to engineer the represen- tation to detour around sets of sentences that seem to be causing the system to take severalweeks to solve a problem.

Token 9366:
The thrust in description logics, on the other hand, is to ensure that subsumption-testing can be solved in time polynomial in the size of the descriptions.

Token 9367:
7 6Notice that the language does notallow one to simply state that one concept, or category, is a subset of another.

Token 9368:
This is a deliberate policy: subsumption betw een categories must be derivable from some aspects of the descriptions of the categories.

Token 9369:
If not, then something is missing from the descriptions.

Token 9370:
7CLASSIC provides efﬁcient subsumption testing in practice, but the worst-case run time is exponential.

Token 9371:
458 Chapter 12.

Token 9372:
Knowledge Representation This sounds wonderful in principle, until one realizes that it can only have one of two consequences: either hard problems cannot be stated at all, or they require exponentiallylarge descriptions!

Token 9373:
However, the tractability results do shed light on what sorts of constructscause problems and thus help the user to understand how different representations behave.For example, description logics usually lack negation and disjunction .

Token 9374:
Each forces ﬁrst- order logical systems to go through a potentially exponential case analysis in order to ensure completeness.

Token 9375:
C LASSIC allows only a limited form of disjunction in the Fills andOneOf constructs, which permit disjunction over explicitly enumerated individuals but not over de-scriptions.

Token 9376:
With disjunctive descriptions, nested deﬁnitions can lead easily to an exponentialnumber of alternative routes by which one category can subsume another.

Token 9377:
12.6 R EASONING WITH DEFAULT INFORMATION In the preceding section, we saw a simple example of an assertion with default status: peoplehave two legs.

Token 9378:
This default can be overridden by more speciﬁc information, such as thatLong John Silver has one leg.

Token 9379:
We saw that the inheritance mechanism in semantic networksimplements the overriding of defaults in a simple and natural way.

Token 9380:
In this section, we studydefaults more generally, with a view toward understanding the semantics of defaults rather than just providing a procedural mechanism.

Token 9381:
12.6.1 Circumscription and default logic We have seen two examples of reasoning processes that violate the monotonicity property of logic that was proved in Chapter 7.8In this chapter we saw that a property inherited by all members of a category in a semantic network could be overridden by more speciﬁc informa- tion for a subcategory.

Token 9382:
In Section 9.4.5, we saw that under the closed-world assumption, if a proposition αis not mentioned in KBthenKB|=¬α,b u tKB∧α|=α.

Token 9383:
Simple introspection suggests that these failures of monotonicity are widespread in commonsense reasoning.

Token 9384:
It seems that humans often “jump to conclusions.” For example, when one sees a car parked on the street, one is normally willing to believe that it has fourwheels even though only three are visible.

Token 9385:
Now, probability theory can certainly provide aconclusion that the fourth wheel exists with high probability, yet, for most people, the possi-bility of the car’s not having four wheels does not arise unless some new evidence presents itself .

Token 9386:
Thus, it seems that the four-wheel conclusion is reached by default , in the absence of any reason to doubt it.

Token 9387:
If new evidence arrives—for example, if one sees the owner carryinga wheel and notices that the car is jacked up—then the conclusion can be retracted.

Token 9388:
This kindof reasoning is said to exhibit nonmonotonicity , because the set of beliefs does not grow NONMONOTONICITY monotonically over time as new evidence arrives.

Token 9389:
Nonmonotonic logics have been devisedNONMONOTONIC LOGIC with modiﬁed notions of truth and entailment in order to capture such behavior.

Token 9390:
We will look at two such logics that have been studied extensively: circumscription and default logic.

Token 9391:
8Recall that monotonicity requires all entailed sentences to remain entailed after new sentences are added to the KB. That is, if KB|=αthen KB∧β|=α.

Token 9392:
Section 12.6.

Token 9393:
Reasoning with Default Information 459 Circumscription can be seen as a more powerful and precise version of the closed- CIRCUMSCRIPTION world assumption.

Token 9394:
The idea is to specify particular predicates that are assumed to be “as false as possible”—that is, false for every object except those for which they are known to be true.For example, suppose we want to assert the default rule that birds ﬂy.

Token 9395:
We would introduce apredicate, say Abnormal 1(x), and write Bird(x)∧¬Abnormal 1(x)⇒Flies(x).

Token 9396:
If we say that Abnormal 1is to be circumscribed , a circumscriptive reasoner is entitled to assume¬Abnormal 1(x)unless Abnormal 1(x)is known to be true.

Token 9397:
This allows the con- clusion Flies(Tweety )to be drawn from the premise Bird(Tweety ), but the conclusion no longer holds if Abnormal 1(Tweety )is asserted.

Token 9398:
Circumscription can be viewed as an example of a model preference logic.

Token 9399:
In suchMODEL PREFERENCE logics, a sentence is entailed (with default status) if it is true in all preferred models of the KB, as opposed to the requirement of truth in allmodels in classical logic.

Token 9400:
For circumscription, one model is preferred to another if it has fewer abnormal objects.9Let us see how this idea works in the context of multiple inheritance in semantic networks.

Token 9401:
The standard example forwhich multiple inheritance is problematic is called the “Nixon diamond.” It arises from theobservation that Richard Nixon was both a Quaker (and hence by default a paciﬁst) and a Republican (and hence by default not a paciﬁst).

Token 9402:
We can write this as follows: Republican (Nixon )∧Quaker (Nixon ). Republican (x)∧¬Abnormal 2(x)⇒¬Paciﬁst (x). Quaker (x)∧¬Abnormal 3(x)⇒Paciﬁst (x).

Token 9403:
If we circumscribe Abnormal 2andAbnormal 3, there are two preferred models: one in whichAbnormal 2(Nixon )andPaciﬁst (Nixon )hold and one in which Abnormal 3(Nixon ) and¬Paciﬁst (Nixon )hold.

Token 9404:
Thus, the circumscriptive reasoner remains properly agnostic as to whether Nixon was a paciﬁst.

Token 9405:
If we wish, in addition, to assert that religious beliefs take precedence over political beliefs, we can use a formalism called prioritized circumscriptionPRIORITIZED CIRCUMSCRIPTION to give preference to models where Abnormal 3is minimized.

Token 9406:
Default logic is a formalism in which default rules can be written to generate contin- DEFAULT LOGIC DEFAULT RULES gent, nonmonotonic conclusions.

Token 9407:
A default rule looks like this: Bird(x):Flies(x)/Flies(x).

Token 9408:
This rule means that if Bird(x)is true, and if Flies(x)is consistent with the knowledge base, thenFlies(x)may be concluded by default.

Token 9409:
In general, a default rule has the form P:J1,...,J n/C where Pis called the prerequisite, Cis the conclusion, and Jiare the justiﬁcations—if any one of them can be proven false, then the conclusion cannot be drawn.

Token 9410:
Any variable that 9For the closed-world assumption, one model is preferred t o another if it has fewer true atoms—that is, preferred models are minimal models.

Token 9411:
There is a natural connection between the closed-world assumption and deﬁnite- clause KBs, because the ﬁxed point reached by forward chaining on deﬁnite-clause KBs is the unique minimalmodel.

Token 9412:
See page 258 for more on this point.

Token 9413:
460 Chapter 12.

Token 9414:
Knowledge Representation appears in JiorCmust also appear in P. The Nixon-diamond example can be represented in default logic with one fact and two default rules: Republican (Nixon )∧Quaker (Nixon ).

Token 9415:
Republican (x):¬Paciﬁst (x)/¬Paciﬁst (x). Quaker (x):Paciﬁst (x)/Paciﬁst (x).

Token 9416:
To interpret what the default rules mean, we deﬁne the notion of an extension of a default EXTENSION theory to be a maximal set of consequences of the theory.

Token 9417:
That is, an extension Sconsists of the original known facts and a set of conclusions from the default rules, such that noadditional conclusions can be drawn from Sand the justiﬁcations of every default conclusion inSare consistent with S. As in the case of the preferred models in circumscription, we have two possible extensions for the Nixon diamond: one wherein he is a paciﬁst and one whereinhe is not.

Token 9418:
Prioritized schemes exist in which some default rules can be given precedence over others, allowing some ambiguities to be resolved.

Token 9419:
Since 1980, when nonmonotonic logics were ﬁrst proposed, a great deal of progress has been made in understanding their mathematical properties.

Token 9420:
There are still unresolvedquestions, however. For example, if “Cars have four wheels” is false, what does it meanto have it in one’s knowledge base?

Token 9421:
What is a good set of default rules to have?

Token 9422:
If wecannot decide, for each rule separately, whether it belongs in our knowledge base, then wehave a serious problem of nonmodularity.

Token 9423:
Finally, how can beliefs that have default status beused to make decisions? This is probably the hardest issue for default reasoning.

Token 9424:
Decisionsoften involve tradeoffs, and one therefore needs to compare the strengths of belief in the outcomes of different actions, and the costs of making a wrong decision.

Token 9425:
In cases where the same kinds of decisions are being made repeatedly, it is possible to interpret default rules as “threshold probability” statements.

Token 9426:
For example, the default rule “My brakes are always OK” really means “The probability that my brakes are OK, given no other information, is sufﬁciently high that the optimal decision is for me to drive without checking them.” Whenthe decision context changes—for example, when one is driving a heavily laden truck down asteep mountain road—the default rule suddenly becomes inappropriate, even though there isno new evidence of faulty brakes.

Token 9427:
These considerations have led some researchers to considerhow to embed default reasoning within probability theory or utility theory.

Token 9428:
12.6.2 Truth maintenance systems We have seen that many of the inferences drawn by a knowledge representation system willhave only default status, rather than being absolutely certain.

Token 9429:
Inevitably, some of these in-ferred facts will turn out to be wrong and will have to be retracted in the face of new informa- tion.

Token 9430:
This process is called belief revision .

Token 9431:
10Suppose that a knowledge base KB contains BELIEF REVISION a sentence P—perhaps a default conclusion recorded by a forward-chaining algorithm, or perhaps just an incorrect assertion—and we want to execute T ELL(KB,¬P).

Token 9432:
To avoid cre- ating a contradiction, we must ﬁrst execute R ETRACT (KB,P). This sounds easy enough.

Token 9433:
10Belief revision is often contrasted with belief update , which occurs when a knowledge base is revised to reﬂect a change in the world rather than new information about a ﬁxed world.

Token 9434:
Belief update combines belief revisionwith reasoning about time and change; it is also related to the process of ﬁltering described in Chapter 15.

Token 9435:
Section 12.6. Reasoning with Default Information 461 Problems arise, however, if any additional sentences were inferred from Pand asserted in the KB.

Token 9436:
For example, the implication P⇒Qmight have been used to add Q.

Token 9437:
The obvious “solution”—retracting all sentences inferred from P—fails because such sentences may have other justiﬁcations besides P. For example, if RandR⇒Qare also in the KB, then Q does not have to be removed after all.

Token 9438:
Truth maintenance systems , or TMSs, are designedTRUTH MAINTENANCESYSTEM to handle exactly these kinds of complications.

Token 9439:
One simple approach to truth maintenance is to keep track of the order in which sen- tences are told to the knowledge base by numbering them from P1toPn.

Token 9440:
When the call RETRACT (KB,Pi) is made, the system reverts to the state just before Piwas added, thereby removing both Piand any inferences that were derived from Pi.

Token 9441:
The sentences Pi+1through Pncan then be added again.

Token 9442:
This is simple, and it guarantees that the knowledge base will be consistent, but retracting Pirequires retracting and reasserting n−isentences as well as undoing and redoing all the inferences drawn from those sentences.

Token 9443:
For systems to whichmany facts are being added—such as large commercial databases—this is impractical.

Token 9444:
A more efﬁcient approach is the justiﬁcation-based truth maintenance system, or JTMS .

Token 9445:
JTMS In a JTMS, each sentence in the knowledge base is annotated with a justiﬁcation consisting JUSTIFICATION of the set of sentences from which it was inferred.

Token 9446:
For example, if the knowledge base already contains P⇒Q,t h e nT ELL(P)will cause Qto be added with the justiﬁcation {P, P⇒Q}.

Token 9447:
In general, a sentence can have any number of justiﬁcations. Justiﬁca- tions make retraction efﬁcient.

Token 9448:
Given the call R ETRACT (P), the JTMS will delete exactly those sentences for which Pis a member of every justiﬁcation.

Token 9449:
So, if a sentence Qhad the single justiﬁcation {P, P⇒Q}, it would be removed; if it had the additional justi- ﬁcation{P, P∨R⇒Q}, it would still be removed; but if it also had the justiﬁcation {R, P∨R⇒Q}, then it would be spared.

Token 9450:
In this way, the time required for retraction of P depends only on the number of sentences derived from Prather than on the number of other sentences added since Pentered the knowledge base.

Token 9451:
The JTMS assumes that sentences that are considered once will probably be considered again, so rather than deleting a sentence from the knowledge base entirely when it loses all justiﬁcations, we merely mark the sentence as being outof the knowledge base.

Token 9452:
If a subsequent assertion restores one of the justiﬁcations, then we mark the sentence as beingback in.

Token 9453:
In this way, the JTMS retains all the inference chains that it uses and need not rederive sentences when a justiﬁcation becomes valid again.

Token 9454:
In addition to handling the retraction of incorrect information, TMSs can be used to speed up the analysis of multiple hypothetical situations.

Token 9455:
Suppose, for example, that theRomanian Olympic Committee is choosing sites for the swimming, athletics, and eques-trian events at the 2048 Games to be held in Romania.

Token 9456:
For example, let the ﬁrst hypothe-sis beSite(Swimming ,Pitesti ),Site(Athletics ,Bucharest ),a n dSite(Equestrian ,Arad).

Token 9457:
A great deal of reasoning must then be done to work out the logistical consequences andhence the desirability of this selection.

Token 9458:
If we want to consider Site(Athletics ,Sibiu)in- stead, the TMS avoids the need to start again from scratch.

Token 9459:
Instead, we simply retract Site(Athletics ,Bucharest )and assert Site(Athletics ,Sibiu)and the TMS takes care of the necessary revisions.

Token 9460:
Inference chains generated from the choice of Bucharest can be reusedwith Sibiu, provided that the conclusions are the same.

Token 9461:
462 Chapter 12.

Token 9462:
Knowledge Representation An assumption-based truth maintenance system, or ATMS , makes this type of context- ATMS switching between hypothetical worlds particularly efﬁcient.

Token 9463:
In a JTMS, the maintenance of justiﬁcations allows you to move quickly from one state to another by making a few retrac-tions and assertions, but at any time only one state is represented.

Token 9464:
An ATMS represents allthe states that have ever been considered at the same time.

Token 9465:
Whereas a JTMS simply labels each sentence as being inorout, an ATMS keeps track, for each sentence, of which assumptions would cause the sentence to be true.

Token 9466:
In other words, each sentence has a label that consists ofa set of assumption sets.

Token 9467:
The sentence holds just in those cases in which all the assumptionsin one of the assumption sets hold.

Token 9468:
Truth maintenance systems also provide a mechanism for generating explanations .

Token 9469:
EXPLANATION Technically, an explanation of a sentence Pis a set of sentences Esuch that Eentails P. If the sentences in Eare already known to be true, then Esimply provides a sufﬁcient ba- sis for proving that Pmust be the case.

Token 9470:
But explanations can also include assumptions — ASSUMPTION sentences that are not known to be true, but would sufﬁce to prove Pif they were true.

Token 9471:
For example, one might not have enough information to prove that one’s car won’t start, but areasonable explanation might include the assumption that the battery is dead.

Token 9472:
This, combinedwith knowledge of how cars operate, explains the observed nonbehavior.

Token 9473:
In most cases, we will prefer an explanation Ethat is minimal, meaning that there is no proper subset of Ethat is also an explanation.

Token 9474:
An ATMS can generate explanations for the “car won’t start” problemby making assumptions (such as “gas in car” or “battery dead”) in any order we like, even ifsome assumptions are contradictory.

Token 9475:
Then we look at the label for the sentence “car won’tstart” to read off the sets of assumptions that would justify the sentence.

Token 9476:
The exact algorithms used to implement truth maintenance systems are a little compli- cated, and we do not cover them here.

Token 9477:
The computational complexity of the truth maintenanceproblem is at least as great as that of propositional inference—that is, NP-hard.

Token 9478:
Therefore,you should not expect truth maintenance to be a panacea.

Token 9479:
When used carefully, however, aTMS can provide a substantial increase in the ability of a logical system to handle complexenvironments and hypotheses.

Token 9480:
12.7 T HEINTERNET SHOPPING WORLD In this ﬁnal section we put together all we have learned to encode knowledge for a shoppingresearch agent that helps a buyer ﬁnd product offers on the Internet.

Token 9481:
The shopping agent isgiven a product description by the buyer and has the task of producing a list of Web pagesthat offer such a product for sale, and ranking which offers are best.

Token 9482:
In some cases thebuyer’s product description will be precise, as in Canon Rebel XTi digital camera ,a n dt h e task is then to ﬁnd the store(s) with the best offer.

Token 9483:
In other cases the description will be only partially speciﬁed, as in digital camera for under $300, and the agent will have to compare different products.

Token 9484:
The shopping agent’s environment is the entire World Wide Web in its full complexity— not a toy simulated environment.

Token 9485:
The agent’s percepts are Web pages, but whereas a human

Token 9486:
Section 12.7.

Token 9487:
The Internet Shopping World 463 Example Online Store Select from our ﬁne line of products: •Computers •Cameras •Books •Videos •Music <h1>Example Online Store</h1> <i>Select</i> from our fine line of products: <ul><li> <a href="http://example.com/compu">Computers</a> <li> <a href="http://example.com/camer">Cameras</a> <li> <a href="http://example.com/books">Books</a><li> <a href="http://example.com/video">Videos</a> <li> <a href="http://example.com/music">Music</a> </ul> Figure 12.8 A Web page from a generic online store in the form perceived by the human user of a browser (top), and the corresponding HTML string as perceived by the browser or the shopping agent (bottom).

Token 9488:
In HTML, characters between <and>are markup directives that specify how the page is displayed.

Token 9489:
For example, the string <i>Select</i> means to switch to italic font, display the word Select , and then end the use of italic font.

Token 9490:
A page identiﬁer such as http://example.com/books is called a uniform resource locator (URL) .

Token 9491:
The markup <a href=" url">Books </a> means to create a hypertext link to url with the anchor text Books .

Token 9492:
Web user would see pages displayed as an array of pixels on a screen, the shopping agent will perceive a page as a character string consisting of ordinary words interspersed with for- matting commands in the HTML markup language.

Token 9493:
Figure 12.8 shows a Web page and acorresponding HTML character string.

Token 9494:
The perception problem for the shopping agent in-volves extracting useful information from percepts of this kind.

Token 9495:
Clearly, perception on Web pages is easier than, say, perception while driving a taxi in Cairo.

Token 9496:
Nonetheless, there are complications to the Internet perception task.

Token 9497:
The Web page inFigure 12.8 is simple compared to real shopping sites, which may include CSS, cookies, Java,Javascript, Flash, robot exclusion protocols, malformed HTML, sound ﬁles, movies, and textthat appears only as part of a JPEG image.

Token 9498:
An agent that can deal with allof the Internet is almost as complex as a robot that can move in the real world.

Token 9499:
We concentrate on a simple agent that ignores most of these complications.

Token 9500:
The agent’s ﬁrst task is to collect product offers that are relevant to a query.

Token 9501:
If the query is “laptops,” then a Web page with a review of the latest high-end laptop would be relevant, but if it doesn’t provide a way to buy, it isn’t an offer.

Token 9502:
For now, we can say a page is an offer if it contains the words “buy” or “price” or “add to cart” within an HTML link or form on the

Token 9503:
464 Chapter 12. Knowledge Representation page. For example, if the page contains a string of the form “ <a...add to cart ...</a” then it is an offer.

Token 9504:
This could be represented in ﬁrst-order logic, but it is more straightforwardto encode it into program code.

Token 9505:
We show how to do more sophisticated information extractionin Section 22.4.

Token 9506:
12.7.1 Following links The strategy is to start at the home page of an online store and consider all pages that can be reached by following relevant links.11The agent will have knowledge of a number of stores, for example: Amazon∈OnlineStores ∧Homepage (Amazon ,“amazon.com ”).

Token 9507:
Ebay∈OnlineStores ∧Homepage (Ebay,“ebay.com ”). ExampleStore ∈OnlineStores ∧Homepage (ExampleStore ,“example.com ”).

Token 9508:
These stores classify their goods into product categories, and provide links to the major cat- egories from their home page.

Token 9509:
Minor categories can be reached through a chain of relevantlinks, and eventually we will reach offers.

Token 9510:
In other words, a page is relevant to the query if itcan be reached by a chain of zero or more relevant category links from a store’s home page,and then from one more link to the product offer.

Token 9511:
We can deﬁne relevance: Relevant (page,query)⇔ ∃store,home store∈OnlineStores ∧Homepage (store,home) ∧∃url,url 2RelevantChain (home,url2,query)∧Link(url2,url) ∧page=Contents (url).

Token 9512:
Here the predicate Link(from,to)means that there is a hyperlink from the from URL to thetoURL.

Token 9513:
To deﬁne what counts as a RelevantChain , we need to follow not just any old hyperlinks, but only those links whose associated anchor text indicates that the link is relevantto the product query.

Token 9514:
For this, we use LinkText (from,to,text)to mean that there is a link between from andtowith textas the anchor text.

Token 9515:
A chain of links between two URLs, start and end, is relevant to a description dif the anchor text of each link is a relevant category name for d. The existence of the chain itself is determined by a recursive deﬁnition, with the empty chain ( start=end) as the base case: RelevantChain (start,end,query)⇔(start=end) ∨(∃u,text LinkText (start,u,text)∧RelevantCategoryName (query,text) ∧RelevantChain (u,end,query)).

Token 9516:
Now we must deﬁne what it means for textto be a RelevantCategoryName forquery . First, we need to relate strings to the categories they name.

Token 9517:
This is done using the predicateName (s,c), which says that string sis a name for category c—for example, we might assert thatName (“laptops ”,LaptopComputers ).

Token 9518:
Some more examples of the Name predicate appear in Figure 12.9(b). Next, we deﬁne relevance.

Token 9519:
Suppose that query is “laptops.” Then RelevantCategoryName (query,text)is true when one of the following holds: •The textandquery name the same category—e.g., “notebooks” and “laptops.” 11An alternative to the link-following strategy is to use an Internet search engine; the technology behind Internet search, information retrieval, will be covered in Section 22.3.

Token 9520:
Section 12.7.

Token 9521:
The Internet Shopping World 465 Books⊂Products MusicRecordings ⊂Products MusicCDs⊂MusicRecordings Electronics⊂Products DigitalCameras ⊂Electronics StereoEquipment ⊂Electronics Computers⊂Electronics DesktopComputers ⊂Computers LaptopComputers ⊂Computers ...Name (“books ”,Books ) Name (“music ”,MusicRecordings ) Name (“CDs ”,MusicCDs ) Name (“electronics ”,Electronics ) Name (“digital cameras ”,DigitalCameras ) Name (“stereos ”,StereoEquipment ) Name (“computers ”,Computers ) Name (“desktops ”,DesktopComputers ) Name (“laptops ”,LaptopComputers ) Name (“notebooks ”,LaptopComputers ) ...(a) (b) Figure 12.9 (a) Taxonomy of product categories.

Token 9522:
(b) Names for those categories.

Token 9523:
•The textnames a supercategory such as “computers.” •The textnames a subcategory such as “ultralight notebooks.” The logical deﬁnition of RelevantCategoryName is as follows: RelevantCategoryName (query,text)⇔ ∃c1,c2Name (query,c1)∧Name (text,c2)∧(c1⊆c2∨c2⊆c1).

Token 9524:
(12.1) Otherwise, the anchor text is irrelevant because it names a category outside this line, such as “clothes” or “lawn & garden.” To follow relevant links, then, it is essential to have a rich hierarchy of product cate- gories.

Token 9525:
The top part of this hierarchy might look like Figure 12.9(a).

Token 9526:
It will not be feasible to listallpossible shopping categories, because a buyer could always come up with some new desire and manufacturers will always come out with new products to satisfy them (electrickneecap warmers?).

Token 9527:
Nonetheless, an ontology of about a thousand categories will serve as avery useful tool for most buyers.

Token 9528:
In addition to the product hierarchy itself, we also need to have a rich vocabulary of names for categories.

Token 9529:
Life would be much easier if there were a one-to-one correspon-dence between categories and the character strings that name them.

Token 9530:
We have already seenthe problem of synonymy —two names for the same category, such as “laptop computers” and “laptops.” There is also the problem of ambiguity —one name for two or more different categories.

Token 9531:
For example, if we add the sentence Name (“CDs ”,CertiﬁcatesOfDeposit ) to the knowledge base in Figure 12.9(b), then “CDs” will name two different categories.

Token 9532:
Synonymy and ambiguity can cause a signiﬁcant increase in the number of paths that the agent has to follow, and can sometimes make it difﬁcult to determine whether a given page is indeed relevant.

Token 9533:
A much more serious problem is the very broad range of descriptions that a user can type and category names that a store can use.

Token 9534:
For example, the link might say “laptop” when the knowledge base has only “laptops” or the user might ask for “a computer

Token 9535:
466 Chapter 12.

Token 9536:
Knowledge Representation I can ﬁt on the tray table of an economy-class airline seat.” It is impossible to enumerate in advance all the ways a category can be named, so the agent will have to be able to do addi-tional reasoning in some cases to determine if the Name relation holds.

Token 9537:
In the worst case, this requires full natural language understanding, a topic that we will defer to Chapter 22.

Token 9538:
In prac-tice, a few simple rules—such as allowing “laptop” to match a category named “laptops”—go a long way.

Token 9539:
Exercise 12.10 asks you to develop a set of such rules after doing some research into online stores.

Token 9540:
Given the logical deﬁnitions from the preceding paragraphs and suitable knowledge bases of product categories and naming conventions, are we ready to apply an inferencealgorithm to obtain a set of relevant offers for our query?

Token 9541:
Not quite! The missing elementis theContents (url)function, which refers to the HTML page at a given URL.

Token 9542:
The agent doesn’t have the page contents of every URL in its knowledge base; nor does it have explicitrules for deducing what those contents might be.

Token 9543:
Instead, we can arrange for the right HTTPprocedure to be executed whenever a subgoal involves the Contents function.

Token 9544:
In this way, it appears to the inference engine as if the entire Web is inside the knowledge base.

Token 9545:
This is anexample of a general technique called procedural attachment , whereby particular predicates PROCEDURAL ATTACHMENT and functions can be handled by special-purpose methods.

Token 9546:
12.7.2 Comparing offers Let us assume that the reasoning processes of the preceding section have produced a set of offer pages for our “laptops” query.

Token 9547:
To compare those offers, the agent must extract the rele-vant information—price, speed, disk size, weight, and so on—from the offer pages.

Token 9548:
This canbe a difﬁcult task with real Web pages, for all the reasons mentioned previously.

Token 9549:
A commonway of dealing with this problem is to use programs called wrappers to extract information WRAPPER from a page.

Token 9550:
The technology of information extraction is discussed in Section 22.4.

Token 9551:
For now we assume that wrappers exist, and when given a page and a knowledge base, they add assertions to the knowledge base.

Token 9552:
Typically, a hierarchy of wrappers would be applied to a page: a very general one to extract dates and prices, a more speciﬁc one to extract attributesfor computer-related products, and if necessary a site-speciﬁc one that knows the format of aparticular store.

Token 9553:
Given a page on the example.com site with the text IBM ThinkBook 970.

Token 9554:
Our price: $399.00 followed by various technical speciﬁcations, we would like a wrapper to extract information such as the following: ∃c,oﬀer c∈LaptopComputers ∧oﬀer∈ProductOﬀers ∧ Manufacturer (c,IBM)∧Model (c,ThinkBook970 )∧ ScreenSize (c,Inches (14))∧ScreenType (c,ColorLCD )∧ MemorySize (c,Gigabytes (2))∧CPUSpeed (c,GHz(1.2))∧ OﬀeredProduct (oﬀer,c)∧Store(oﬀer,GenStore )∧ URL(oﬀer,“example .com/computers /34356 .html ”)∧ Price(oﬀer,$(399))∧Date(oﬀer,Today ).

Token 9555:
This example illustrates several issues that arise when we take seriously the task of knowledge engineering for commercial transactions.

Token 9556:
For example, notice that the price is an attribute of

Token 9557:
Section 12.8. Summary 467 theoffer, not the product itself.

Token 9558:
This is important because the offer at a given store may change from day to day even for the same individual laptop; for some categories—such ashouses and paintings—the same individual object may even be offered simultaneously bydifferent intermediaries at different prices.

Token 9559:
There are still more complications that we havenot handled, such as the possibility that the price depends on the method of payment and on the buyer’s qualiﬁcations for certain discounts.

Token 9560:
The ﬁnal task is to compare the offers that have been extracted. For example, consider these three offers: A:1.4 GHz CPU, 2GB RAM, 250 GB disk, $299 .

Token 9561:
B:1.2 GHz CPU, 4GB RAM, 350 GB disk, $500 . C:1.2 GHz CPU, 2GB RAM, 250 GB disk, $399 .

Token 9562:
Cisdominated byA;t h a ti s , Ais cheaper and faster, and they are otherwise the same.

Token 9563:
In general, Xdominates YifXhas a better value on at least one attribute, and is not worse on any attribute. But neither AnorBdominates the other.

Token 9564:
To decide which is better we need to know how the buyer weighs CPU speed and price against memory and disk space.

Token 9565:
Thegeneral topic of preferences among multiple attributes is addressed in Section 16.4; for now,our shopping agent will simply return a list of all undominated offers that meet the buyer’sdescription.

Token 9566:
In this example, both AandBare undominated.

Token 9567:
Notice that this outcome relies on the assumption that everyone prefers cheaper prices, faster processors, and more storage.

Token 9568:
Some attributes, such as screen size on a notebook, depend on the user’s particular preference (portability versus visibility); for these, the shopping agent will just have to ask the user.

Token 9569:
The shopping agent we have described here is a simple one; many reﬁnements are possible.

Token 9570:
Still, it has enough capability that with the right domain-speciﬁc knowledge it can actually be of use to a shopper.

Token 9571:
Because of its declarative construction, it extends easily to more complex applications.

Token 9572:
The main point of this section is to show that some knowledge representation—in particular, the product hierarchy—is necessary for such an agent, and that once we have some knowledge in this form, the rest follows naturally.

Token 9573:
12.8 S UMMARY By delving into the details of how one represents a variety of knowledge, we hope we havegiven the reader a sense of how real knowledge bases are constructed and a feeling for theinteresting philosophical issues that arise.

Token 9574:
The major points are as follows: •Large-scale knowledge representation requires a general-purpose ontology to organize and tie together the various speciﬁc domains of knowledge.

Token 9575:
•A general-purpose ontology needs to cover a wide variety of knowledge and should be capable, in principle, of handling any domain.

Token 9576:
•Building a large, general-purpose ontology is a signiﬁcant challenge that has yet to be fully realized, although current frameworks seem to be quite robust.

Token 9577:
•We presented an upper ontology based on categories and the event calculus.

Token 9578:
We covered categories, subcategories, parts, structured objects, measurements, substances,events, time and space, change, and beliefs.

Token 9579:
468 Chapter 12. Knowledge Representation •Natural kinds cannot be deﬁned completely in logic, but properties of natural kinds can be represented.

Token 9580:
•Actions, events, and time can be represented either in situation calculus or in more expressive representations such as event calculus.

Token 9581:
Such representations enable an agentto construct plans by logical inference.

Token 9582:
•We presented a detailed analysis of the Internet shopping domain, exercising the general ontology and showing how the domain knowledge can be used by a shopping agent.

Token 9583:
•Special-purpose representation systems, such as semantic networks anddescription logics , have been devised to help in organizing a hierarchy of categories.

Token 9584:
Inheritance is an important form of inference, allowing the properties of objects to be deduced fromtheir membership in categories.

Token 9585:
•The closed-world assumption , as implemented in logic programs, provides a simple way to avoid having to specify lots of negative information.

Token 9586:
It is best interpreted as adefault that can be overridden by additional information.

Token 9587:
•Nonmonotonic logics ,s u c ha s circumscription anddefault logic , are intended to cap- ture default reasoning in general.

Token 9588:
•Truth maintenance systems handle knowledge updates and revisions efﬁciently.

Token 9589:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Briggs (1985) claims that formal knowledge representation research began with classical In- dian theorizing about the grammar of Shastric Sanskrit, which dates back to the ﬁrst millen-nium B.C.

Token 9590:
In the West, the use of deﬁnitions of terms in ancient Greek mathematics can be regarded as the earliest instance: Aristotle’s Metaphysics (literally, what comes after the book on physics) is a near-synonym for Ontology .

Token 9591:
Indeed, the development of technical terminol- ogy in any ﬁeld can be regarded as a form of knowledge representation.

Token 9592:
Early discussions of representation in AI tended to focus on “ problem representation” rather than “ knowledge representation.” (See, for example, Amarel’s (1968) discussion of the Missionaries and Cannibals problem.)

Token 9593:
In the 1970s, AI emphasized the development of “ex-pert systems” (also called “knowledge-based systems”) that could, if given the appropriatedomain knowledge, match or exceed the performance of human experts on narrowly deﬁnedtasks.

Token 9594:
For example, the ﬁrst expert system, D ENDRAL (Feigenbaum et al. , 1971; Lindsay et al.

Token 9595:
, 1980), interpreted the output of a mass spectrometer (a type of instrument used to ana- lyze the structure of organic chemical compounds) as accurately as expert chemists.

Token 9596:
Althoughthe success of D ENDRAL was instrumental in convincing the AI research community of the importance of knowledge representation, the representational formalisms used in D ENDRAL are highly speciﬁc to the domain of chemistry.

Token 9597:
Over time, researchers became interested in standardized knowledge representation formalisms and ontologies that could streamline the process of creating new expert systems.

Token 9598:
In so doing, they ventured into territory previously explored by philosophers of science and of language.

Token 9599:
The discipline imposed in AI by the need for one’s theories to “work” has led to more rapid and deeper progress than was the case

Token 9600:


Token 9601:
Bibliographical and Historical Notes 469 when these problems were the exclusive domain of philosophy (although it has at times also led to the repeated reinvention of the wheel).

Token 9602:
The creation of comprehensive taxonomies or classiﬁcations dates back to ancient times. Aristotle (384–322 B.C.)

Token 9603:
strongly emphasized classiﬁcation and categorization schemes.

Token 9604:
His Organon , a collection of works on logic assembled by his students after his death, included a treatise called Categories in which he attempted to construct what we would now call an upper ontology.

Token 9605:
He also introduced the notions of genus andspecies for lower-level classiﬁcation.

Token 9606:
Our present system of biological classiﬁcation, including the use of “binomial nomenclature”(classiﬁcation via genus and species in the technical sense), was invented by the Swedishbiologist Carolus Linnaeus, or Carl von Linne (1707–1778).

Token 9607:
The problems associated withnatural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),Quine (1953), Lakoff (1987), and Schwartz (1977), among others.

Token 9608:
Interest in larger-scale ontologies is increasing, as documented by the Handbook on Ontologies (Staab, 2004).

Token 9609:
The O PENCYC project (Lenat and Guha, 1990; Matuszek et al.

Token 9610:
, 2006) has released a 150,000-concept ontology, with an upper ontology similar to the one inFigure 12.1 as well as speciﬁc concepts like “OLED Display” and “iPhone,” which is a typeof “cellular phone,” which in turn is a type of “consumer electronics,” “phone,” “wireless communication device,” and other concepts.

Token 9611:
The DB PEDIA project extracts structured data from Wikipedia; speciﬁcally from Infoboxes: the boxes of attribute/value pairs that accom-pany many Wikipedia articles (Wu and Weld, 2008; Bizer et al.

Token 9612:
, 2007). As of mid-2009, DB PEDIA contains 2.6 million concepts, with about 100 facts per concept.

Token 9613:
The IEEE work- ing group P1600.1 created the Suggested Upper Merged Ontology (SUMO) (Niles and Pease, 2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and links to over 20,000 domain-speciﬁc terms.

Token 9614:
Stoffel et al. (1997) describe algorithms for ef- ﬁciently managing a very large ontology.

Token 9615:
A survey of techniques for extracting knowledgefrom Web pages is given by Etzioni et al. (2008). On the Web, representation languages are emerging.

Token 9616:
RDF (Brickley and Guha, 2004) allows for assertions to be made in the form of relational triples, and provides some means for evolving the meaning of names over time.

Token 9617:
OWL (Smith et al. , 2004) is a description logic that supports inferences over these triples.

Token 9618:
So far, usage seems to be inversely proportional torepresentational complexity: the traditional HTML and CSS formats account for over 99% ofWeb content, followed by the simplest representation schemes, such as microformats (Khare,2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup toadd attributes to literal text.

Token 9619:
Usage of sophisticated RDF and OWL ontologies is not yetwidespread, and the full vision of the Semantic Web (Berners-Lee et al.

Token 9620:
, 2001) has not yet been realized.

Token 9621:
The conferences on Formal Ontology in Information Systems (FOIS) contain many interesting papers on both general and domain-speciﬁc ontologies.

Token 9622:
The taxonomy used in this chapter was developed by the authors and is based in part on their experience in the CYC project and in part on work by Hwang and Schubert (1993) and Davis (1990, 2005).

Token 9623:
An inspirational discussion of the general project of commonsense knowledge representation appears in Hayes’s (1978, 1985b) “Naive Physics Manifesto.” Successful deep ontologies within a speciﬁc ﬁeld include the Gene Ontology project (Consortium, 2008) and CML, the Chemical Markup Language (Murray-Rust et al.

Token 9624:
, 2003).

Token 9625:
470 Chapter 12.

Token 9626:
Knowledge Representation Doubts about the feasibility of a single ontology for allknowledge are expressed by Doctorow (2001), Gruber (2004), Halevy et al.

Token 9627:
(2009), and Smith (2004), who states, “the initial project of building one single ontology . . . has . . .

Token 9628:
largely been abandoned.” The event calculus was introduced by Kowalski and Sergot (1986) to handle continuous time, and there have been several variations (Sadri and Kowalski, 1995; Shanahan, 1997) and overviews (Shanahan, 1999; Mueller, 2006).

Token 9629:
van Lambalgen and Hamm (2005) show how the logic of events maps onto the language we use to talk about events.

Token 9630:
An alternative to theevent and situation calculi is the ﬂuent calculus (Thielscher, 1999).

Token 9631:
James Allen introducedtime intervals for the same reason (Allen, 1984), arguing that intervals were much more natu-ral than situations for reasoning about extended and concurrent events.

Token 9632:
Peter Ladkin (1986a,1986b) introduced “concave” time intervals (intervals with gaps; essentially, unions of ordi-nary “convex” time intervals) and applied the techniques of mathematical abstract algebra totime representation.

Token 9633:
Allen (1991) systematically investigates the wide variety of techniquesavailable for time representation; van Beek and Manchak (1996) analyze algorithms for tem-poral reasoning.

Token 9634:
There are signiﬁcant commonalities between the event-based ontology givenin this chapter and an analysis of events due to the philosopher Donald Davidson (1980).Thehistories in Pat Hayes’s (1985a) ontology of liquids and the chronicles in McDermott’s (1985) theory of plans were also important inﬂuences on the ﬁeld and this chapter.

Token 9635:
The question of the ontological status of substances has a long history.

Token 9636:
Plato proposed that substances were abstract entities entirely distinct from physical objects; he would say MadeOf (Butter 3,Butter )rather than Butter 3∈Butter .

Token 9637:
This leads to a substance hierar- chy in which, for example, UnsaltedButter is a more speciﬁc substance than Butter .

Token 9638:
The po- sition adopted in this chapter, in which substances are categories of objects, was championed by Richard Montague (1973).

Token 9639:
It has also been adopted in the CYC project. Copeland (1993)mounts a serious, but not invincible, attack.

Token 9640:
The alternative approach mentioned in the chap-ter, in which butter is one object consisting of all buttery objects in the universe, was proposedoriginally by the Polish logician Le´ sniewski (1916).

Token 9641:
His mereology (the name is derived from MEREOLOGY the Greek word for “part”) used the part–whole relation as a substitute for mathematical set theory, with the aim of eliminating abstract entities such as sets.

Token 9642:
A more readable exposition of these ideas is given by Leonard and Goodman (1940), and Goodman’s The Structure of Appearance (1977) applies the ideas to various problems in knowledge representation.

Token 9643:
While some aspects of the mereological approach are awkward—for example, the need for a sepa-rate inheritance mechanism based on part–whole relations—the approach gained the supportof Quine (1960).

Token 9644:
Harry Bunt (1985) has provided an extensive analysis of its use in knowl-edge representation.

Token 9645:
Casati and Varzi (1999) cover parts, wholes, and the spatial locations. Mental objects have been the subject of intensive study in philosophy and AI.

Token 9646:
There are three main approaches.

Token 9647:
The one taken in this chapter, based on modal logic and possibleworlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughesand Cresswell, 1996).

Token 9648:
The book Reasoning about Knowledge (Fagin et al. , 1995) provides a thorough introduction.

Token 9649:
The second approach is a ﬁrst-order theory in which mental objects are ﬂuents. Davis (2005) and Davis and Morgenstern (2005) describe this approach.

Token 9650:
It relieson the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985).

Token 9651:
Thethird approach is a syntactic theory , in which mental objects are represented by character SYNTACTIC THEORY

Token 9652:
Bibliographical and Historical Notes 471 strings.

Token 9653:
A string is just a complex term denoting a list of symbols, so CanFly (Clark)can be represented by the list of symbols [C,a,n,F,l,y, (,C,l,a,r ,k, )].

Token 9654:
The syntactic theory of mental objects was ﬁrst studied in depth by Kaplan and Montague (1960), who showedthat it led to paradoxes if not handled carefully.

Token 9655:
Ernie Davis (1990) provides an excellentcomparison of the syntactic and modal theories of knowledge.

Token 9656:
The Greek philosopher Porphyry (c. 234–305 A.D.), commenting on Aristotle’s Cat- egories , drew what might qualify as the ﬁrst semantic network.

Token 9657:
Charles S. Peirce (1909) developed existential graphs as the ﬁrst semantic network formalism using modern logic.Ross Quillian (1961), driven by an interest in human memory and language processing, ini-tiated work on semantic networks within AI.

Token 9658:
An inﬂuential paper by Marvin Minsky (1975)presented a version of semantic networks called frames ; a frame was a representation of an object or category, with attributes and relations to other objects or categories.

Token 9659:
The ques-tion of semantics arose quite acutely with respect to Quillian’s semantic networks (and thoseof others who followed his approach), with their ubiquitous and very vague “IS-A links”Woods’s (1975) famous article “What’s In a Link?” drew the attention of AI researchers to theneed for precise semantics in knowledge representation formalisms.

Token 9660:
Brachman (1979) elab-orated on this point and proposed solutions.

Token 9661:
Patrick Hayes’s (1979) “The Logic of Frames” cut even deeper, claiming that “Most of ‘frames’ is just a new syntax for parts of ﬁrst-order logic.” Drew McDermott’s (1978b) “Tarskian Semantics, or, No Notation without Denota-tion!” argued that the model-theoretic approach to semantics used in ﬁrst-order logic shouldbe applied to all knowledge representation formalisms.

Token 9662:
This remains a controversial idea;notably, McDermott himself has reversed his position in “A Critique of Pure Reason” (Mc-Dermott, 1987).

Token 9663:
Selman and Levesque (1993) discuss the complexity of inheritance withexceptions, showing that in most formulations it is NP-complete.

Token 9664:
The development of description logics is the most recent stage in a long line of re- search aimed at ﬁnding useful subsets of ﬁrst-order logic for which inference is computa-tionally tractable.

Token 9665:
Hector Levesque and Ron Brachman (1987) showed that certain logicalconstructs—notably, certain uses of disjunction and negation—were primarily responsible for the intractability of logical inference.

Token 9666:
Building on the KL-O NEsystem (Schmolze and Lipkis, 1983), several researchers developed systems that incorporate theoretical complex-ity analysis, most notably K RYPTON (Brachman et al.

Token 9667:
, 1983) and Classic (Borgida et al. , 1989).

Token 9668:
The result has been a marked increase in the speed of inference and a much betterunderstanding of the interaction between complexity and expressiveness in reasoning sys-tems.

Token 9669:
Calvanese et al. (1999) summarize the state of the art, and Baader et al. (2007) present a comprehensive handbook of description logic.

Token 9670:
Against this trend, Doyle and Patil (1991)have argued that restricting the expressiveness of a language either makes it impossible tosolve certain problems or encourages the user to circumvent the language restrictions throughnonlogical means.

Token 9671:
The three main formalisms for dealing with nonmonotonic inference—circumscription (McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott and Doyle, 1980)—were all introduced in one special issue of the AI Journal.

Token 9672:
Delgrande andSchaub (2003) discuss the merits of the variants, given 25 years of hindsight.

Token 9673:
Answer setprogramming can be seen as an extension of negation as failure or as a reﬁnement of circum-

Token 9674:
472 Chapter 12.

Token 9675:
Knowledge Representation scription; the underlying theory of stable model semantics was introduced by Gelfond and Lifschitz (1988), and the leading answer set programming systems are DLV (Eiter et al.

Token 9676:
, 1998) and S MODELS (Niemel¨ aet al. , 2000). The disk drive example comes from the S MODELS user manual (Syrj¨ anen, 2000).

Token 9677:
Lifschitz (2001) discusses the use of answer set programming for planning. Brewka et al.

Token 9678:
(1997) give a good overview of the various approaches to nonmono- tonic logic.

Token 9679:
Clark (1978) covers the negation-as-failure approach to logic programming and Clark completion.

Token 9680:
Van Emden and Kowalski (1976) show that every Prolog program withoutnegation has a unique minimal model.

Token 9681:
Recent years have seen renewed interest in applica-tions of nonmonotonic logics to large-scale knowledge representation systems.

Token 9682:
The B ENINQ systems for handling insurance-beneﬁt inquiries was perhaps the ﬁrst commercially success-ful application of a nonmonotonic inheritance system (Morgenstern, 1998).

Token 9683:
Lifschitz (2001)discusses the application of answer set programming to planning.

Token 9684:
A variety of nonmonotonicreasoning systems based on logic programming are documented in the proceedings of theconferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).

Token 9685:
The study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP (McAllester, 1980) systems, both of which were essentially JTMSs.

Token 9686:
Forbus and de Kleer(1993) explain in depth how TMSs can be used in AI applications.

Token 9687:
Nayak and Williams (1997) show how an efﬁcient incremental TMS called an ITMS makes it feasible to plan the operations of a NASA spacecraft in real time.

Token 9688:
This chapter could not cover every area of knowledge representation in depth.

Token 9689:
The three principal topics omitted are the following: Qualitative physics : Qualitative physics is a subﬁeld of knowledge representation concerned QUALITATIVE PHYSICS speciﬁcally with constructing a logical, nonnumeric theory of physical objects and processes.

Token 9690:
The term was coined by Johan de Kleer (1975), although the enterprise could be said to have started in Fahlman’s (1974) B UILD , a sophisticated planner for constructing complex towers of blocks.

Token 9691:
Fahlman discovered in the process of designing it that most of the effort (80%, by his estimate) went into modeling the physics of the blocks world to calculate the stability of various subassemblies of blocks, rather than into planning per se.

Token 9692:
He sketches a hypothetical naive-physics-like process to explain why young children can solve B UILD -like problems without access to the high-speed ﬂoating-point arithmetic used in B UILD ’s physical modeling.

Token 9693:
Hayes (1985a) uses “histories”—four-dimensional slices of space-time similar toDavidson’s events—to construct a fairly complex naive physics of liquids.

Token 9694:
Hayes was theﬁrst to prove that a bath with the plug in will eventually overﬂow if the tap keeps running andthat a person who falls into a lake will get wet all over.

Token 9695:
Davis (2008) gives an update to theontology of liquids that describes the pouring of liquids into containers.

Token 9696:
De Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde- pendently and almost simultaneously developed systems that can reason about a physicalsystem based on qualitative abstractions of the underlying equations.

Token 9697:
Qualitative physics soon developed to the point where it became possible to analyze an impressive variety of complex physical systems (Yip, 1991).

Token 9698:
Qualitative techniques have been used to constructnovel designs for clocks, windshield wipers, and six-legged walkers (Subramanian and Wang,1994).

Token 9699:
The collection Readings in Qualitative Reasoning about Physical Systems (Weld and

Token 9700:
Exercises 473 de Kleer, 1990) an encyclopedia article by Kuipers (2001), and a handbook article by Davis (2007) introduce to the ﬁeld.

Token 9701:
Spatial reasoning : The reasoning necessary to navigate in the wumpus world and shopping SPATIAL REASONING world is trivial in comparison to the rich spatial structure of the real world.

Token 9702:
The earliest serious attempt to capture commonsense reasoning about space appears in the work of Ernest Davis (1986, 1990).

Token 9703:
The region connection calculus of Cohn et al.

Token 9704:
(1997) supports a form of qualitative spatial reasoning and has led to new kinds of geographical information systems;see also (Davis, 2006).

Token 9705:
As with qualitative physics, an agent can go a long way, so to speak,without resorting to a full metric representation.

Token 9706:
When such a representation is necessary,techniques developed in robotics (Chapter 25) can be used.

Token 9707:
Psychological reasoning : Psychological reasoning involves the development of a working PSYCHOLOGICAL REASONING psychology for artiﬁcial agents to use in reasoning about themselves and other agents.

Token 9708:
This is often based on so-called folk psychology, the theory that humans in general are believedto use in reasoning about themselves and other humans.

Token 9709:
When AI researchers provide theirartiﬁcial agents with psychological theories for reasoning about other agents, the theories arefrequently based on the researchers’ description of the logical agents’ own design.

Token 9710:
Psycholog- ical reasoning is currently most useful within the context of natural language understanding, where divining the speaker’s intentions is of paramount importance.

Token 9711:
Minker (2001) collects papers by leading researchers in knowledge representation, sum- marizing 40 years of work in the ﬁeld.

Token 9712:
The proceedings of the international conferences onPrinciples of Knowledge Representation and Reasoning provide the most up-to-date sources f o rw o r ki nt h i sa r e a .

Token 9713:
Readings in Knowledge Representation (Brachman and Levesque, 1985) and Formal Theories of the Commonsense World (Hobbs and Moore, 1985) are ex- cellent anthologies on knowledge representation; the former focuses more on historicallyimportant papers in representation languages and formalisms, the latter on the accumulationof the knowledge itself.

Token 9714:
Davis (1990), Steﬁk (1995), and Sowa (1999) provide textbook in-troductions to knowledge representation, van Harmelen et al.

Token 9715:
(2007) contributes a handbook, and a special issue of AI Journal covers recent progress (Davis and Morgenstern, 2004).

Token 9716:
The biennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers applications of the theory of knowledge in AI, economics, and distributed systems.

Token 9717:
EXERCISES 12.1 Deﬁne an ontology in ﬁrst-order logic for tic-tac-toe.

Token 9718:
The ontology should contain situations, actions, squares, players, marks (X, O, or blank), and the notion of winning, losing,or drawing a game.

Token 9719:
Also deﬁne the notion of a forced win (or draw): a position from which a player can force a win (or draw) with the right sequence of actions.

Token 9720:
Write axioms for the domain. (Note: The axioms that enumerate the different squares and that characterize thewinning positions are rather long.

Token 9721:
You need not write these out in full, but indicate clearlywhat they look like.)

Token 9722:
474 Chapter 12. Knowledge Representation 12.2 Figure 12.1 shows the top levels of a hierarchy for everything.

Token 9723:
Extend it to include as many real categories as possible. A good way to do this is to cover all the things in youreveryday life.

Token 9724:
This includes objects and events. Start with waking up, and proceed in anorderly fashion noting everything that you see, touch, do, and think about.

Token 9725:
For example,a random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet, talking, Professor Fateman, chicken curry, tongue, $7, sun, the daily newspaper, and so on.

Token 9726:
You should produce both a single hierarchy chart (on a large sheet of paper) and a listing of objects and categories with the relations satisﬁed by members of each category.Every object should be in a category, and every category should be in the hierarchy.

Token 9727:
12.3 Develop a representational system for reasoning about windows in a window-based computer interface.

Token 9728:
In particular, your representation should be able to describe: •The state of a window: minimized, displayed, or nonexistent.

Token 9729:
•Which window (if any) is the active window. •The position of every window at a given time. •The order (front to back) of overlapping windows.

Token 9730:
•The actions of creating, destroying, resizing, and moving windows; changing the state of a window; and bringing a window to the front.

Token 9731:
Treat these actions as atomic; that is,do not deal with the issue of relating them to mouse actions.

Token 9732:
Give axioms describingthe effects of actions on ﬂuents. You may use either event or situation calculus.

Token 9733:
Assume an ontology containing situations, actions, integers (forxandycoordinates) and windows .

Token 9734:
Deﬁne a language over this ontology; that is, a list of constants, function symbols, and predicates with an English description of each.

Token 9735:
If you need to add more categories to theontology (e.g., pixels), you may do so, but be sure to specify these in your write-up.

Token 9736:
You may(and should) use symbols deﬁned in the text, but be sure to list these explicitly.

Token 9737:
12.4 State the following in the language you developed for the previous exercise: a.

Token 9738:
In situation S 0, window W1is behind W2but sticks out on the left and right. Do not state exact coordinates for these; describe the general situation.

Token 9739:
b. If a window is displayed, then its top edge is higher than its bottom edge. c. After you create a window w, it is displayed.

Token 9740:
d. A window can be minimized if it is displayed. 12.5 (Adapted from an example by Doug Lenat.)

Token 9741:
Your mission is to capture, in logical form, enough knowledge to answer a series of questions about the following simple scenario: Yesterday John went to the North Berkeley Safeway supermarket and bought two pounds of tomatoes and a pound of ground beef.

Token 9742:
Start by trying to represent the content of the sentence as a series of assertions.

Token 9743:
You should write sentences that have straightforward logical structure (e.g., statements that objects have certain properties, that objects are related in certain ways, that all objects satisfying one prop- erty satisfy another).

Token 9744:
The following might help you get started:

Token 9745:
Exercises 475 •Which classes, objects, and relations would you need? What are their parents, siblings and so on?

Token 9746:
(You will need events and temporal ordering, among other things.) •Where would they ﬁt in a more general hierarchy?

Token 9747:
•What are the constraints and interrelationships among them? •How detailed must you be about each of the various concepts?

Token 9748:
To answer the questions below, your knowledge base must include background knowledge.

Token 9749:
You’ll have to deal with what kind of things are at a supermarket, what is involved withpurchasing the things one selects, what the purchases will be used for, and so on.

Token 9750:
Try to makeyour representation as general as possible.

Token 9751:
To give a trivial example: don’t say “People buy food from Safeway,” because that won’t help you with those who shop at another supermarket.

Token 9752:
Also, don’t turn the questions into answers; for example, question (c) asks “Did John buy anymeat?”—not “Did John buy a pound of ground beef?” Sketch the chains of reasoning that would answer the questions.

Token 9753:
If possible, use a logical reasoning system to demonstrate the sufﬁciency of your knowledge base.

Token 9754:
Many of thethings you write might be only approximately correct in reality, but don’t worry too much;the idea is to extract the common sense that lets you answer these questions at all.

Token 9755:
A trulycomplete answer to this question is extremely difﬁcult, probably beyond the state of the art of current knowledge representation.

Token 9756:
But you should be able to put together a consistent set ofaxioms for the limited questions posed here. a. Is John a child or an adult? [Adult] b.

Token 9757:
Does John now have at least two tomatoes? [Yes] c. Did John buy any meat?

Token 9758:
[Yes] d. If Mary was buying tomatoes at the same time as John, did he see her? [Yes] e. Are the tomatoes made in the supermarket?

Token 9759:
[No] f. What is John going to do with the tomatoes? [Eat them] g. Does Safeway sell deodorant?

Token 9760:
[Yes] h. Did John bring some money or a credit card to the supermarket? [Yes] i. Does John have less money after going to the supermarket?

Token 9761:
[Yes] 12.6 Make the necessary additions or changes to your knowledge base from the previous exercise so that the questions that follow can be answered.

Token 9762:
Include in your report a discussionof your changes, explaining why they were needed, whether they were minor or major, andwhat kinds of questions would necessitate further changes.

Token 9763:
a. Are there other people in Safeway while John is there? [Yes—staff!] b. Is John a vegetarian? [No] c. Who owns the deodorant in Safeway?

Token 9764:
[Safeway Corporation] d. Did John have an ounce of ground beef? [Yes] e. Does the Shell station next door have any gas?

Token 9765:
[Yes] f. Do the tomatoes ﬁt in John’s car trunk? [Yes]

Token 9766:
476 Chapter 12.

Token 9767:
Knowledge Representation 12.7 Represent the following seven sentences using and extending the representations de- veloped in the chapter: a.

Token 9768:
Water is a liquid between 0 and 100 degrees. b. Water boils at 100 degrees. c. The water in John’s water bottle is frozen.

Token 9769:
d. Perrier is a kind of water. e. John has Perrier in his water bottle. f. All liquids have a freezing point.

Token 9770:
g. A liter of water weighs more than a liter of alcohol.

Token 9771:
12.8 Write deﬁnitions for the following: a.ExhaustivePartDecomposition b.PartPartition c.PartwiseDisjoint These should be analogous to the deﬁnitions for ExhaustiveDecomposition ,Partition ,a n d Disjoint .

Token 9772:
Is it the case that PartPartition (s,BunchOf (s))?

Token 9773:
If so, prove it; if not, give a counterexample and deﬁne sufﬁcient conditions under which it does hold.

Token 9774:
12.9 An alternative scheme for representing measures involves applying the units function to an abstract length object.

Token 9775:
In such a scheme, one would write Inches (Length (L1)) = 1.5. How does this scheme compare with the one in the chapter?

Token 9776:
Issues include conversion axioms, names for abstract quantities (such as “50 dollars”), and comparisons of abstract measures in different units (50 inches is more than 50 centimeters).

Token 9777:
12.10 Add sentences to extend the deﬁnition of the predicate Name (s,c)so that a string such as “laptop computer” matches the appropriate category names from a variety of stores.

Token 9778:
Try to make your deﬁnition general. Test it by looking at ten online stores, and at the category names they give for three different categories.

Token 9779:
For example, for the category of laptops, wefound the names “Notebooks,” “Laptops,” “Notebook Computers,” “Notebook,” “Laptopsand Notebooks,” and “Notebook PCs.” Some of these can be covered by explicit Name facts, while others could be covered by sentences for handling plurals, conjunctions, etc.

Token 9780:
12.11 Write event calculus axioms to describe the actions in the wumpus world.

Token 9781:
12.12 State the interval-algebra relation that holds between every pair of the following real- world events: LK: The life of President Kennedy.

Token 9782:
IK: The infancy of President Kennedy. PK: The presidency of President Kennedy. LJ: The life of President Johnson.

Token 9783:
PJ: The presidency of President Johnson. LO: The life of President Obama.

Token 9784:
Exercises 477 12.13 Investigate ways to extend the event calculus to handle simultaneous events.

Token 9785:
Is it possible to avoid a combinatorial explosion of axioms?

Token 9786:
12.14 Construct a representation for exchange rates between currencies that allows for daily ﬂuctuations.

Token 9787:
12.15 Deﬁne the predicate Fixed ,w h e r e Fixed(Location (x))means that the location of object xis ﬁxed over time.

Token 9788:
12.16 Describe the event of trading something for something else.

Token 9789:
Describe buying as a kind of trading in which one of the objects traded is a sum of money.

Token 9790:
12.17 The two preceding exercises assume a fairly primitive notion of ownership. For ex- ample, the buyer starts by owning the dollar bills.

Token 9791:
This picture begins to break down when, for example, one’s money is in the bank, because there is no longer any speciﬁc collectionof dollar bills that one owns.

Token 9792:
The picture is complicated still further by borrowing, leasing,renting, and bailment.

Token 9793:
Investigate the various commonsense and legal concepts of ownership,and propose a scheme by which they can be represented formally.

Token 9794:
12.18 (Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8 cards, 4 aces and 4 kings.

Token 9795:
The three players, Alice, Bob, and Carlos, are dealt two cards each.Without looking at them, they place the cards on their foreheads so that the other players cansee them.

Token 9796:
Then the players take turns either announcing that they know what cards are ontheir own forehead, thereby winning the game, or saying “I don’t know.” Everyone knowsthe players are truthful and are perfect at reasoning about beliefs.

Token 9797:
a. Game 1. Alice and Bob have both said “I don’t know.” Carlos sees that Alice has two aces (A-A) and Bob has two kings (K-K). What should Carlos say?

Token 9798:
( Hint: consider all three possible cases for Carlos: A-A, K-K, A-K.) b. Describe each step of Game 1 using the notation of modal logic. c. Game 2.

Token 9799:
Carlos, Alice, and Bob all said “I don’t know” on their ﬁrst turn. Alice holds K-K and Bob holds A-K. What should Carlos say on his second turn?

Token 9800:
d. Game 3. Alice, Carlos, and Bob all say “I don’t know” on their ﬁrst turn, as does Alice on her second turn.

Token 9801:
Alice and Bob both hold A-K. What should Carlos say? e. Prove that there will always be a winner to this game.

Token 9802:
12.19 The assumption of logical omniscience, discussed on page 453, is of course not true of any actual reasoners.

Token 9803:
Rather, it is an idealization of the reasoning process that may be more or less acceptable depending on the applications.

Token 9804:
Discuss the reasonableness of theassumption for each of the following applications of reasoning about knowledge: a.

Token 9805:
Partial knowledge adversary games, such as card games. Here one player wants to reason about what his opponent knows about the state of the game. b.

Token 9806:
Chess with a clock.

Token 9807:
Here the player may wish to reason about the limits of his oppo- nent’s or his own ability to ﬁnd the best move in the time available.

Token 9808:
For instance, if player A has much more time left than player B, then A will sometimes make a movethat greatly complicates the situation, in the hopes of gaining an advantage because hehas more time to work out the proper strategy.

Token 9809:
478 Chapter 12. Knowledge Representation c. A shopping agent in an environment in which there are costs of gathering information.

Token 9810:
d. Reasoning about public key cryptography, which rests on the intractability of certain computational problems.

Token 9811:
12.20 Translate the following description logic expression (from page 457) into ﬁrst-order logic, and comment on the result: And(Man,AtLeast (3,Son),AtMost (2,Daughter ), All(Son,And(Unemployed ,Married ,All(Spouse ,Doctor ))), All(Daughter ,And(Professor ,Fills(Department ,Physics ,Math)))).

Token 9812:
12.21 Recall that inheritance information in semantic networks can be captured logically by suitable implication sentences.

Token 9813:
This exercise investigates the efﬁciency of using suchsentences for inheritance. a.

Token 9814:
Consider the information in a used-car catalog such as Kelly’s Blue Book—for exam- ple, that 1973 Dodge vans are (or perhaps were once) worth $575.

Token 9815:
Suppose all thisinformation (for 11,000 models) is encoded as logical sentences, as suggested in thechapter.

Token 9816:
Write down three such sentences, including that for 1973 Dodge vans.

Token 9817:
Howwould you use the sentences to ﬁnd the value of a particular car, given a backward- chaining theorem prover such as Prolog? b.

Token 9818:
Compare the time efﬁciency of the backward-chaining method for solving this problem with the inheritance method used in semantic nets.

Token 9819:
c. Explain how forward chaining allows a logic-based system to solve the same problem efﬁciently, assuming that the KB contains only the 11,000 sentences about prices.

Token 9820:
d. Describe a situation in which neither forward nor backward chaining on the sentences will allow the price query for an individual car to be handled efﬁciently.

Token 9821:
e. Can you suggest a solution enabling this type of query to be solved efﬁciently in all cases in logic systems?

Token 9822:
( Hint: Remember that two cars of the same year and model have the same price.)

Token 9823:
12.22 One might suppose that the syntactic distinction between unboxed links and singly boxed links in semantic networks is unnecessary, because singly boxed links are always at-tached to categories; an inheritance algorithm could simply assume that an unboxed linkattached to a category is intended to apply to all members of that category.

Token 9824:
Show that thisargument is fallacious, giving examples of errors that would arise.

Token 9825:
12.23 One part of the shopping process that was not covered in this chapter is checking for compatibility between items.

Token 9826:
For example, if a digital camera is ordered, what accessory batteries, memory cards, and cases are compatible with the camera?

Token 9827:
Write a knowledge basethat can determine the compatibility of a set of items and suggest replacements or additionalitems if the shopper makes a choice that is not compatible.

Token 9828:
The knowledge base should workswith at least one line of products and extend easily to other lines.

Token 9829:
12.24 A complete solution to the problem of inexact matches to the buyer’s description in shopping is very difﬁcult and requires a full array of natural language processing and

Token 9830:
Exercises 479 information retrieval techniques. (See Chapters 22 and 23.)

Token 9831:
One small step is to allow the user to specify minimum and maximum values for various attributes.

Token 9832:
The buyer must use thefollowing grammar for product descriptions: Description →Category [Connector Modiﬁer ]∗ Connector →“with ”|“and”|“,” Modiﬁer →Attribute|Attribute Op Value Op →“=”|“>”|“<” Here,Category names a product category, Attribute is some feature such as “CPU” or “price,” and Value is the target value for the attribute.

Token 9833:
So the query “computer with at least a 2.5 GHz CPU for under $500” must be re-expressed as “computer with CPU >2.5 GHz and price<$500.” Implement a shopping agent that accepts descriptions in this language.

Token 9834:
12.25 Our description of Internet shopping omitted the all-important step of actually buying the product.

Token 9835:
Provide a formal logical description of buying, using event calculus.

Token 9836:
That is,deﬁne the sequence of events that occurs when a buyer submits a credit-card purchase andthen eventually gets billed and receives the product.

Token 9837:
13QUANTIFYING UNCERTAINTY In which we see how an agent can tame uncertainty with degrees of belief.

Token 9838:
13.1 A CTING UNDER UNCERTAINTY Agents may need to handle uncertainty , whether due to partial observability, nondetermin- UNCERTAINTY ism, or a combination of the two.

Token 9839:
An agent may never know for certain what state it’s in or where it will end up after a sequence of actions.

Token 9840:
We have seen problem-solving agents (Chapter 4) and logical agents (Chapters 7 and 11) designed to handle uncertainty by keeping track of a belief state —a representation of the set of all possible world states that it might be in—and generating a contingency plan that han-dles every possible eventuality that its sensors may report during execution.

Token 9841:
Despite its many virtues, however, this approach has signiﬁcant drawbacks when taken literally as a recipe for creating agent programs: •When interpreting partial sensor information, a logical agent must consider every log- ically possible explanation for the observations, no matter how unlikely.

Token 9842:
This leads to impossible large and complex belief-state representations.

Token 9843:
•A correct contingent plan that handles every eventuality can grow arbitrarily large and must consider arbitrarily unlikely contingencies.

Token 9844:
•Sometimes there is no plan that is guaranteed to achieve the goal—yet the agent must act.

Token 9845:
It must have some way to compare the merits of plans that are not guaranteed.

Token 9846:
Suppose, for example, that an automated taxi!automated has the goal of delivering a pas- senger to the airport on time.

Token 9847:
The agent forms a plan, A 90, that involves leaving home 90 minutes before the ﬂight departs and driving at a reasonable speed.

Token 9848:
Even though the airport is only about 5 miles away, a logical taxi agent will not be able to conclude with certainty that “Plan A90will get us to the airport in time.” Instead, it reaches the weaker conclusion “PlanA90will get us to the airport in time, as long as the car doesn’t break down or run out of gas, and I don’t get into an accident, and there are no accidents on the bridge, and the plane doesn’t leave early, and no meteorite hits the car, and ....” None of these conditions can be 480

Token 9849:
Section 13.1. Acting under Uncertainty 481 deduced for sure, so the plan’s success cannot be inferred.

Token 9850:
This is the qualiﬁcation problem (page 268), for which we so far have seen no real solution.

Token 9851:
Nonetheless, in some sense A90isin fact the right thing to do. What do we mean by this?

Token 9852:
As we discussed in Chapter 2, we mean that out of all the plans that could be executed,A 90is expected to maximize the agent’s performance measure (where the expectation is rel- ative to the agent’s knowledge about the environment).

Token 9853:
The performance measure includes getting to the airport in time for the ﬂight, avoiding a long, unproductive wait at the airport,and avoiding speeding tickets along the way.

Token 9854:
The agent’s knowledge cannot guarantee any ofthese outcomes for A 90, but it can provide some degree of belief that they will be achieved.

Token 9855:
Other plans, such as A180, might increase the agent’s belief that it will get to the airport on time, but also increase the likelihood of a long wait.

Token 9856:
The right thing to do—the rational decision —therefore depends on both the relative importance of various goals and the likeli- hood that, and degree to which, they will be achieved.

Token 9857:
The remainder of this section hones these ideas, in preparation for the development of the general theories of uncertain reasoningand rational decisions that we present in this and subsequent chapters.

Token 9858:
13.1.1 Summarizing uncertainty Let’s consider an example of uncertain reasoning: diagnosing a dental patient’s toothache.Diagnosis—whether for medicine, automobile repair, or whatever—almost always involvesuncertainty.

Token 9859:
Let us try to write rules for dental diagnosis using propositional logic, so that we can see how the logical approach breaks down.

Token 9860:
Consider the following simple rule: Toothache⇒Cavity . The problem is that this rule is wrong.

Token 9861:
Not all patients with toothaches have cavities; some of them have gum disease, an abscess, or one of several other problems: Toothache⇒Cavity∨GumProblem ∨Abscess ...

Token 9862:
Unfortunately, in order to make the rule true, we have to add an almost unlimited list of possible problems.

Token 9863:
We could try turning the rule into a causal rule: Cavity⇒Toothache . But this rule is not right either; not all cavities cause pain.

Token 9864:
The only way to ﬁx the rule is to make it logically exhaustive: to augment the left-hand side with all the qualiﬁcationsrequired for a cavity to cause a toothache.

Token 9865:
Trying to use logic to cope with a domain likemedical diagnosis thus fails for three main reasons: •Laziness : It is too much work to list the complete set of antecedents or consequents LAZINESS needed to ensure an exceptionless rule and too hard to use such rules.

Token 9866:
•Theoretical ignorance : Medical science has no complete theory for the domain.THEORETICAL IGNORANCE •Practical ignorance : Even if we know all the rules, we might be uncertain about aPRACTICAL IGNORANCE particular patient because not all the necessary tests have been or can be run.

Token 9867:
The connection between toothaches and cavities is just not a logical consequence in either direction.

Token 9868:
This is typical of the medical domain, as well as most other judgmental domains: law, business, design, automobile repair, gardening, dating, and so on.

Token 9869:
The agent’s knowledge

Token 9870:
482 Chapter 13. Quantifying Uncertainty can at best provide only a degree of belief in the relevant sentences.

Token 9871:
Our main tool for DEGREE OF BELIEF dealing with degrees of belief is probability theory .

Token 9872:
In the terminology of Section 8.1, thePROBABILITY THEORY ontological commitments of logic and probability theory are the same—that the world is composed of facts that do or do not hold in any particular case—but the epistemological commitments are different: a logical agent believes each sentence to be true or false or has no opinion, whereas a probabilistic agent may have a numerical degree of belief between 0 (for sentences that are certainly false) and 1 (certainly true).

Token 9873:
Probability provides a way of summarizing the uncertainty that comes from our lazi- ness and ignorance, thereby solving the qualiﬁcation problem.

Token 9874:
We might not know for sure what afﬂicts a particular patient, but we believe that there is, say, an 80% chance—that is,a probability of 0.8—that the patient who has a toothache has a cavity.

Token 9875:
That is, we expectthat out of all the situations that are indistinguishable from the current situation as far as ourknowledge goes, the patient will have a cavity in 80% of them.

Token 9876:
This belief could be derivedfrom statistical data—80% of the toothache patients seen so far have had cavities—or fromsome general dental knowledge, or from a combination of evidence sources.

Token 9877:
One confusing point is that at the time of our diagnosis, there is no uncertainty in the actual world: the patient either has a cavity or doesn’t.

Token 9878:
So what does it mean to say the probability of a cavity is 0.8? Shouldn’t it be either 0 or 1?

Token 9879:
The answer is that probability statements are made with respect to a knowledge state, not with respect to the real world.

Token 9880:
Wesay “The probability that the patient has a cavity, given that she has a toothache ,i s0 . 8 .

Token 9881:
”I fw e later learn that the patient has a history of gum disease, we can make a different statement: “The probability that the patient has a cavity, given that she has a toothache and a history of gum disease, is 0.4.” If we gather further conclusive evidence against a cavity, we can say “The probability that the patient has a cavity, given all we now know, is almost 0.” Note thatthese statements do not contradict each other; each is a separate assertion about a differentknowledge state.

Token 9882:
13.1.2 Uncertainty and rational decisions Consider again the A90plan for getting to the airport.

Token 9883:
Suppose it gives us a 97% chance of catching our ﬂight. Does this mean it is a rational choice?

Token 9884:
Not necessarily: there mightbe other plans, such as A 180, with higher probabilities.

Token 9885:
If it is vital not to miss the ﬂight, then it is worth risking the longer wait at the airport.

Token 9886:
What about A1440, a plan that involves leaving home 24 hours in advance?

Token 9887:
In most circumstances, this is not a good choice, becausealthough it almost guarantees getting there on time, it involves an intolerable wait—not tomention a possibly unpleasant diet of airport food.

Token 9888:
To make such choices, an agent must ﬁrst have preferences between the different pos- PREFERENCE sible outcomes of the various plans.

Token 9889:
An outcome is a completely speciﬁed state, including OUTCOME such factors as whether the agent arrives on time and the length of the wait at the airport.

Token 9890:
We useutility theory to represent and reason with preferences.

Token 9891:
(The term utility is used here in UTILITYTHEORY the sense of “the quality of being useful,” not in the sense of the electric company or water works.)

Token 9892:
Utility theory says that every state has a degree of usefulness, or utility, to an agentand that the agent will prefer states with higher utility.

Token 9893:
Section 13.2. Basic Probability Notation 483 The utility of a state is relative to an agent.

Token 9894:
For example, the utility of a state in which White has checkmated Black in a game of chess is obviously high for the agent playing White,but low for the agent playing Black.

Token 9895:
But we can’t go strictly by the scores of 1, 1/2, and 0 thatare dictated by the rules of tournament chess—some players (including the authors) might bethrilled with a draw against the world champion, whereas other players (including the former world champion) might not.

Token 9896:
There is no accounting for taste or preferences: you might think that an agent who prefers jalape˜ no bubble-gum ice cream to chocolate chocolate chip is odd or even misguided, but you could not say the agent is irrational.

Token 9897:
A utility function can accountfor any set of preferences—quirky or typical, noble or perverse.

Token 9898:
Note that utilities can accountfor altruism, simply by including the welfare of others as one of the factors.

Token 9899:
Preferences, as expressed by utilities, are combined with probabilities in the general theory of rational decisions called decision theory : DECISION THEORY Decision theory =probability theory +utility theory .

Token 9900:
The fundamental idea of decision theory is that an agent is rational if and only if it chooses the action that yields the highest expected utility, averaged over all the possible outcomes of the action.

Token 9901:
This is called the principle of maximum expected utility (MEU).

Token 9902:
Note thatMAXIMUM EXPECTED UTILITY “expected” might seem like a vague, hypothetical term, but as it is used here it has a precise meaning: it means the “average,” or “statistical mean” of the outcomes, weighted by theprobability of the outcome.

Token 9903:
We saw this principle in action in Chapter 5 when we touchedbrieﬂy on optimal decisions in backgammon; it is in fact a completely general principle.

Token 9904:
Figure 13.1 sketches the structure of an agent that uses decision theory to select actions.

Token 9905:
The agent is identical, at an abstract level, to the agents described in Chapters 4 and 7 thatmaintain a belief state reﬂecting the history of percepts to date.

Token 9906:
The primary difference isthat the decision-theoretic agent’s belief state represents not just the possibilities for world states but also their probabilities .

Token 9907:
Given the belief state, the agent can make probabilistic predictions of action outcomes and hence select the action with highest expected utility.

Token 9908:
Thischapter and the next concentrate on the task of representing and computing with probabilisticinformation in general.

Token 9909:
Chapter 15 deals with methods for the speciﬁc tasks of representingand updating the belief state over time and predicting the environment.

Token 9910:
Chapter 16 coversutility theory in more depth, and Chapter 17 develops algorithms for planning sequences ofactions in uncertain environments.

Token 9911:
13.2 B ASIC PROBABILITY NOTATION For our agent to represent and use probabilistic information, we need a formal language.

Token 9912:
The language of probability theory has traditionally been informal, written by human math- ematicians to other human mathematicians.

Token 9913:
Appendix A includes a standard introduction toelementary probability theory; here, we take an approach more suited to the needs of AI andmore consistent with the concepts of formal logic.

Token 9914:
484 Chapter 13.

Token 9915:
Quantifying Uncertainty function DT-A GENT (percept )returns anaction persistent :belief state , probabilistic beliefs about the current state of the world action , the agent’s action update belief state based on action andpercept calculate outcome probabilities for actions, given action descriptions and current belief state selectaction with highest expected utility given probabilities of outcomes and utility information return action Figure 13.1 A decision-theoretic agent that selects rational actions.

Token 9916:
13.2.1 What probabilities are about Like logical assertions, probabilistic assertions are about possible worlds.

Token 9917:
Whereas logical assertions say which possible worlds are strictly ruled out (all those in which the assertion isfalse), probabilistic assertions talk about how probable the various worlds are.

Token 9918:
In probabilitytheory, the set of all possible worlds is called the sample space .

Token 9919:
The possible worlds are SAMPLE SPACE mutually exclusive and exhaustive —two possible worlds cannot both be the case, and one possible world must be the case.

Token 9920:
For example, if we are about to roll two (distinguishable)dice, there are 36 possible worlds to consider: (1,1), (1,2), ..., (6,6).

Token 9921:
The Greek letter Ω (uppercase omega) is used to refer to the sample space, and ω(lowercase omega) refers to elements of the space, that is, particular possible worlds.

Token 9922:
A fully speciﬁed probability model associates a numerical probability P(ω)with each PROBABILITY MODEL possible world.1The basic axioms of probability theory say that every possible world has a probability between 0 and 1 and that the total probability of the set of possible worlds is 1: 0≤P(ω)≤1for every ωand/summationdisplay ω∈ΩP(ω)=1.

Token 9923:
(13.1) For example, if we assume that each die is fair and the rolls don’t interfere with each other, then each of the possible worlds (1,1), (1,2), ..., (6,6) has probability 1/36.

Token 9924:
On the other hand, if the dice conspire to produce the same number, then the worlds (1,1), (2,2), (3,3), etc., might have higher probabilities, leaving the others with lower probabilities.

Token 9925:
Probabilistic assertions and queries are not usually about particular possible worlds, but about sets of them.

Token 9926:
For example, we might be interested in the cases where the two dice addup to 11, the cases where doubles are rolled, and so on.

Token 9927:
In probability theory, these sets arecalled events —a term already used extensively in Chapter 12 for a different concept.

Token 9928:
In AI, EVENT the sets are always described by propositions in a formal language. (One such language is described in Section 13.2.2.)

Token 9929:
For each proposition, the corresponding set contains just thosepossible worlds in which the proposition holds.

Token 9930:
The probability associated with a proposition 1For now, we assume a discrete, countable set of worlds.

Token 9931:
The proper treatment of the continuous case brings in certain complications that are less relevant for most purposes in AI.

Token 9932:
Section 13.2.

Token 9933:
Basic Probability Notation 485 is deﬁned to be the sum of the probabilities of the worlds in which it holds: For any proposition φ, P(φ)=/summationdisplay ω∈φP(ω).

Token 9934:
(13.2) For example, when rolling fair dice, we have P(Total =11) = P((5,6)) + P((6,5)) = 1/36 + 1 /36 = 1 /18.

Token 9935:
Note that probability theory does not require complete knowledge of the probabilities of each possible world.

Token 9936:
For example, if we believe the dice conspire toproduce the same number, we might assert thatP(doubles )=1/4without knowing whether the dice prefer double 6 to double 2.

Token 9937:
Just as with logical assertions, this assertion constrains the underlying probability model without fully determining it.

Token 9938:
Probabilities such as P(Total = 11) andP(doubles )are called unconditional orprior UNCONDITIONAL PROBABILITY probabilities (and sometimes just “priors” for short); they refer to degrees of belief in propo- PRIOR PROBABILITY sitions in the absence of any other information .

Token 9939:
Most of the time, however, we have some information, usually called evidence , that has already been revealed.

Token 9940:
For example, the ﬁrst EVIDENCE die may already be showing a 5 and we are waiting with bated breath for the other one to stop spinning.

Token 9941:
In that case, we are interested not in the unconditional probability of rollingdoubles, but the conditional orposterior probability (or just “posterior” for short) of rolling CONDITIONAL PROBABILITY POSTERIOR PROBABILITY doubles given that the ﬁrst die is a 5 .

Token 9942:
This probability is written P(doubles|Die1=5 ),w h e r e the “|” is pronounced “given.” Similarly, if I am going to the dentist for a regular checkup, the probability P(cavity )=0.2might be of interest; but if I go to the dentist because I have a toothache, it’s P(cavity|toothache )=0.6that matters.

Token 9943:
Note that the precedence of “ |”i s such that any expression of the form P(...|...)always means P((...)|(...)).

Token 9944:
It is important to understand that P(cavity )=0.2is still valid aftertoothache is ob- served; it just isn’t especially useful.

Token 9945:
When making decisions, an agent needs to conditiononallthe evidence it has observed.

Token 9946:
It is also important to understand the difference be- tween conditioning and logical implication.

Token 9947:
The assertion that P(cavity|toothache )=0.6 does not mean “Whenever toothache is true, conclude that cavity is true with probabil- ity 0.6” rather it means “Whenever toothache is true and we have no further information , conclude that cavity is true with probability 0.6.” The extra condition is important; for ex- ample, if we had the further information that the dentist found no cavities, we deﬁnitelywould not want to conclude that cavity is true with probability 0.6; instead we need to use P(cavity|toothache∧¬cavity )=0 .

Token 9948:
Mathematically speaking, conditional probabilities are deﬁned in terms of uncondi- tional probabilities as follows: for any propositions aandb,w eh a v e P(a| b)=P(a∧b) P(b), (13.3) which holds whenever P(b)>0.

Token 9949:
For example, P(doubles|Die1=5 )=P(doubles∧Die1=5 ) P(Die1=5 ).

Token 9950:
The deﬁnition makes sense if you remember that observing brules out all those possible worlds where bis false, leaving a set whose total probability is just P(b).

Token 9951:
Within that set, the a-worlds satisfy a∧band constitute a fraction P(a∧b)/P(b).

Token 9952:
486 Chapter 13.

Token 9953:
Quantifying Uncertainty The deﬁnition of conditional probability, Equation (13.3), can be written in a different form called the product rule : PRODUCT RULE P(a∧b)=P(a|b)P(b), The product rule is perhaps easier to remember: it comes from the fact that, for aandbto be true, we need bto be true, and we also need ato be true given b.

Token 9954:
13.2.2 The language of propositions in probability assertions In this chapter and the next, propositions describing sets of possible worlds are written in a notation that combines elements of propositional logic and constraint satisfaction notation.

Token 9955:
Inthe terminology of Section 2.4.7, it is a factored representation , in which a possible world is represented by a set of variable/value pairs.

Token 9956:
Variables in probability theory are called random variables and their names begin with RANDOM VARIABLE an uppercase letter.

Token 9957:
Thus, in the dice example, Total andDie1are random variables. Every random variable has a domain —the set of possible values it can take on.

Token 9958:
The domain of DOMAIN Total for two dice is the set {2,...,12}and the domain of Die1is{1,...,6}.

Token 9959:
A Boolean random variable has the domain {true,false}(notice that values are always lowercase); for example, the proposition that doubles are rolled can be written as Doubles =true.

Token 9960:
By con- vention, propositions of the form A=true are abbreviated simply as a, while A=false is abbreviated as ¬a.

Token 9961:
(The uses of doubles ,cavity ,a n dtoothache in the preceding section are abbreviations of this kind.)

Token 9962:
As in CSPs, domains can be sets of arbitrary tokens; we mightchoose the domain of Age to be{juvenile ,teen,adult}and the domain of Weather might be{sunny ,rain,cloudy ,snow}.

Token 9963:
When no ambiguity is possible, it is common to use a value by itself to stand for the proposition that a particular variable has that value; thus, sunny can stand for Weather =sunny .

Token 9964:
The preceding examples all have ﬁnite domains.

Token 9965:
Variables can have inﬁnite domains, too—either discrete (like the integers) or continuous (like the reals).

Token 9966:
For any variable with an ordered domain, inequalities are also allowed, such as NumberOfAtomsInUniverse ≥10 70.

Token 9967:
Finally, we can combine these sorts of elementary propositions (including the abbre- viated forms for Boolean variables) by using the connectives of propositional logic.

Token 9968:
Forexample, we can express “The probability that the patient has a cavity, given that she is ateenager with no toothache, is 0.1” as follows: P(cavity|¬toothache∧teen)=0.1.

Token 9969:
Sometimes we will want to talk about the probabilities of allthe possible values of a random variable.

Token 9970:
We could write: P(Weather =sunny )=0.6 P(Weather =rain)=0.1 P(Weather =cloudy )=0.29 P(Weather =snow)=0.01, but as an abbreviation we will allow P(Weather )=/angbracketleft0.6,0.1,0.29,0.01/angbracketright,

Token 9971:
Section 13.2.

Token 9972:
Basic Probability Notation 487 where the bold Pindicates that the result is a vector of numbers, and where we assume a pre- deﬁned ordering /angbracketleftsunny ,rain,cloudy ,snow/angbracketrighton the domain of Weather .

Token 9973:
We say that the Pstatement deﬁnes a probability distribution for the random variable Weather .T h e Pnota-PROBABILITY DISTRIBUTION tion is also used for conditional distributions: P(X|Y)gives the values of P(X=xi|Y=yj) for each possible i,jpair.

Token 9974:
For continuous variables, it is not possible to write out the entire distribution as a vector, because there are inﬁnitely many values.

Token 9975:
Instead, we can deﬁne the probability that a randomvariable takes on some value xas a parameterized function of x.

Token 9976:
For example, the sentence P(NoonTemp =x)=Uniform [18C,26C](x) expresses the belief that the temperature at noon is distributed uniformly between 18 and 26 degrees Celsius.

Token 9977:
We call this a probability density function .PROBABILITY DENSITY FUNCTION Probability density functions (sometimes called pdfs) differ in meaning from discrete distributions.

Token 9978:
Saying that the probability density is uniform from 18Cto26Cmeans that there is a 100% chance that the temperature will fall somewhere in that 8C-wide region and a 50% chance that it will fall in any 4C-wide region, and so on.

Token 9979:
We write the probability density for a continuous random variable Xat value xasP(X=x)or just P(x); the intuitive deﬁnition of P(x)is the probability that Xfalls within an arbitrarily small region beginning atx, divided by the width of the region: P(x) = lim dx→0P(x≤X≤x+dx)/dx .

Token 9980:
ForNoonTemp we have P(NoonTemp =x)=Uniform[18C,26C](x)=/braceleftbigg1 8Cif18C≤x≤26C 0otherwise, where Cstands for centigrade (not for a constant).

Token 9981:
In P(NoonTemp =20.18C)=1 8C, note that1 8Cis not a probability, it is a probability density.

Token 9982:
The probability that NoonTemp is exactly 20.18Cis zero, because 20.18Cis a region of width 0.

Token 9983:
Some authors use different symbols for discrete distributions and density functions; we use Pin both cases, since confu- sion seldom arises and the equations are usually identical.

Token 9984:
Note that probabilities are unitless numbers, whereas density functions are measured with a unit, in this case reciprocal degrees.

Token 9985:
In addition to distributions on single variables, we need notation for distributions on multiple variables. Commas are used for this.

Token 9986:
For example, P(Weather ,Cavity )denotes the probabilities of all combinations of the values of Weather andCavity .T h i s i s a 4×2 table of probabilities called the joint probability distribution ofWeather andCavity .W eJOINT PROBABILITY DISTRIBUTION can also mix variables with and without values; P(sunny ,Cavity )would be a two-element vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.ThePnotation makes certain expressions much more concise than they might otherwise be.

Token 9987:
For example, the product rules for all possible values of Weather andCavity can be written as a single equation: P(Weather ,Cavity )=P(Weather|Cavity )P(Cavity ),

Token 9988:
488 Chapter 13.

Token 9989:
Quantifying Uncertainty instead of as these 4×2=8 equations (using abbreviations WandC): P(W=sunny∧C=true)=P(W=sunny|C=true)P(C=true) P(W=rain∧C=true)=P(W=rain|C=true)P(C=true) P(W=cloudy∧C=true)=P(W=cloudy|C=true)P(C=true) P(W=snow∧C=true)=P(W=snow|C=true)P(C=true) P(W=sunny∧C=false)=P(W=sunny|C=false)P(C=false) P(W=rain∧C=false)=P(W=rain|C=false)P(C=false) P(W=cloudy∧C=false)=P(W=cloudy|C=false)P(C=false) P(W=snow∧C=false)=P(W=snow|C=false)P(C=false).

Token 9990:
As a degenerate case, P(sunny ,cavity )has no variables and thus is a one-element vec- tor that is the probability of a sunny day with a cavity, which could also be written asP(sunny ,cavity )orP(sunny∧cavity ).

Token 9991:
We will sometimes use Pnotation to derive results about individual Pvalues, and when we say “ P(sunny )=0.6” it is really an abbreviation for “P(sunny )is the one-element vector /angbracketleft0.6/angbracketright, which means that P(sunny )=0.6.” Now we have deﬁned a syntax for propositions and probability assertions and we have given part of the semantics: Equation (13.2) deﬁnes the probability of a proposition as the sum of the probabilities of worlds in which it holds.

Token 9992:
To complete the semantics, we need to say what the worlds are and how to determine whether a proposition holds in a world.

Token 9993:
We borrowthis part directly from the semantics of propositional logic, as follows.

Token 9994:
A possible world is deﬁned to be an assignment of values to all of the random variables under consideration.

Token 9995:
It is easy to see that this deﬁnition satisﬁes the basic requirement that possible worlds be mutuallyexclusive and exhaustive (Exercise 13.5).

Token 9996:
For example, if the random variables are Cavity , Toothache ,a n dWeather , then there are 2×2×4=16 possible worlds.

Token 9997:
Furthermore, the truth of any given proposition, no matter how complex, can be determined easily in suchworlds using the same recursive deﬁnition of truth as for formulas in propositional logic.

Token 9998:
From the preceding deﬁnition of possible worlds, it follows that a probability model is completely determined by the joint distribution for all of the random variables—the so-called full joint probability distribution .

Token 9999:
For example, if the variables are Cavity ,Toothache , FULL JOINT PROBABILITYDISTRIBUTION andWeather , then the full joint distribution is given by P(Cavity ,Toothache ,Weather ).

Token 10000:
This joint distribution can be represented as a 2×2×4table with 16 entries.

Token 10001:
Because every proposition’s probability is a sum over possible worlds, a full joint distribution sufﬁces, in principle, for calculating the probability of any proposition.

Token 10002:
13.2.3 Probability axioms and their reasonableness The basic axioms of probability (Equations (13.1) and (13.2)) imply certain relationshipsamong the degrees of belief that can be accorded to logically related propositions.

Token 10003:
For exam-ple, we can derive the familiar relationship between the probability of a proposition and theprobability of its negation: P(¬a)=/summationtext ω∈¬aP(ω) by Equation (13.2) =/summationtext ω∈¬aP(ω)+/summationtext ω∈aP(ω)−/summationtext ω∈aP(ω) =/summationtext ω∈ΩP(ω)−/summationtext ω∈aP(ω) grouping the ﬁrst two terms =1−P(a) by (13.1) and (13.2).

Token 10004:
Section 13.2.

Token 10005:
Basic Probability Notation 489 We can also derive the well-known formula for the probability of a disjunction, sometimes called the inclusion–exclusion principle :INCLUSION– EXCLUSION PRINCIPLE P(a∨b)=P(a)+P(b)−P(a∧b).

Token 10006:
(13.4) This rule is easily remembered by noting that the cases where aholds, together with the cases where bholds, certainly cover all the cases where a∨bholds; but summing the two sets of cases counts their intersection twice, so we need to subtract P(a∧b).

Token 10007:
The proof is left as an exercise (Exercise 13.6).

Token 10008:
Equations (13.1) and (13.4) are often called Kolmogorov’s axioms in honor of the Rus-KOLMOGOROV’S AXIOMS sian mathematician Andrei Kolmogorov, who showed how to build up the rest of probability theory from this simple foundation and how to handle the difﬁculties caused by continuousvariables.

Token 10009:
2While Equation (13.2) has a deﬁnitional ﬂavor, Equation (13.4) reveals that the axioms really do constrain the degrees of belief an agent can have concerning logically re-lated propositions.

Token 10010:
This is analogous to the fact that a logical agent cannot simultaneously believe A,B,a n d¬(A∧B), because there is no possible world in which all three are true.

Token 10011:
With probabilities, however, statements refer not to the world directly, but to the agent’s ownstate of knowledge.

Token 10012:
Why, then, can an agent not hold the following set of beliefs (even thoughthey violate Kolmogorov’s axioms)? P(a)=0.4 P(a∧b)=0.0 P(b)=0.3 P(a∨b)=0.8.

Token 10013:
(13.5) This kind of question has been the subject of decades of intense debate between those who advocate the use of probabilities as the only legitimate form for degrees of belief and thosewho advocate alternative approaches.

Token 10014:
One argument for the axioms of probability, ﬁrst stated in 1931 by Bruno de Finetti (and translated into English in de Finetti (1993)), is as follows: If an agent has some degree ofbelief in a proposition a, then the agent should be able to state odds at which it is indifferent to a bet for or against a.

Token 10015:
3Think of it as a game between two agents: Agent 1 states, “my degree of belief in event ais 0.4.” Agent 2 is then free to choose whether to wager for or against aat stakes that are consistent with the stated degree of belief.

Token 10016:
That is, Agent 2 could choose to accept Agent 1’s bet that awill occur, offering $6 against Agent 1’s $4.

Token 10017:
Or Agent 2 could accept Agent 1’s bet that ¬awill occur, offering $4 against Agent 1’s $6.

Token 10018:
Then we observe the outcome of a, and whoever is right collects the money.

Token 10019:
If an agent’s degrees of belief do not accurately reﬂect the world, then you would expect that it would tend to losemoney over the long run to an opposing agent whose beliefs more accurately reﬂect the stateof the world.

Token 10020:
But de Finetti proved something much stronger: If Agent 1 expresses a set of degrees of belief that violate the axioms of probability theory then there is a combination of bets by Agent 2 that guarantees that Agent 1 will lose money every time.

Token 10021:
For example, suppose that Agent 1 has the set of degrees of belief from Equation (13.5).

Token 10022:
Figure 13.2 shows that if Agent 2The difﬁculties include the Vitali set , a well-deﬁned subset of the interval [0,1]with no well-deﬁned size.

Token 10023:
3One might argue that the agent’s preferences for different bank balances are such that the possibility of losing $1 is not counterbalanced by an equal possibility of winning $1.

Token 10024:
One possible response is to make the bet amountssmall enough to avoid this problem. Savage’s analysis (1954) circumvents the issue altogether.

Token 10025:
490 Chapter 13.

Token 10026:
Quantifying Uncertainty 2 chooses to bet $4 on a,$ 3o n b, and $2 on¬(a∨b), then Agent 1 always loses money, regardless of the outcomes for aandb.

Token 10027:
De Finetti’s theorem implies that no rational agent can have beliefs that violate the axioms of probability.

Token 10028:
Agent 1 Agent 2 Outcomes and payoffs to Agent 1 Proposition Belief Bet Stakes a,b a,¬b¬a,b¬a,¬b a 0.4 a 4t o6 –6 –6 4 4 b 0.3 b 3t o7 –7 3 –7 3 a∨b 0.8 ¬(a∨b) 2t o8 22 2 – 8 –11 –1 –1 –1 Figure 13.2 Because Agent 1 has inconsistent beliefs, Agent 2 is able to devise a set of bets that guarantees a loss for Agent 1, no matter what the outcome of aandb.

Token 10029:
One common objection to de Finetti’s theorem is that this betting game is rather con- trived. For example, what if one refuses to bet?

Token 10030:
Does that end the argument?

Token 10031:
The answer isthat the betting game is an abstract model for the decision-making situation in which everyagent is unavoidably involved at every moment.

Token 10032:
Every action (including inaction) is a kind of bet, and every outcome can be seen as a payoff of the bet.

Token 10033:
Refusing to bet is like refusingto allow time to pass.

Token 10034:
Other strong philosophical arguments have been put forward for the use of probabilities, most notably those of Cox (1946), Carnap (1950), and Jaynes (2003).

Token 10035:
They each construct a set of axioms for reasoning with degrees of beliefs: no contradictions, correspondence withordinary logic (for example, if belief in Agoes up, then belief in ¬Amust go down), and so on.

Token 10036:
The only controversial axiom is that degrees of belief must be numbers, or at least act like numbers in that they must be transitive (if belief in Ais greater than belief in B,w h i c hi s greater than belief in C, then belief in Amust be greater than C) and comparable (the belief inAmust be one of equal to, greater than, or less than belief in B).

Token 10037:
It can then be proved that probability is the only approach that satisﬁes these axioms.

Token 10038:
The world being the way it is, however, practical demonstrations sometimes speak louder than proofs.

Token 10039:
The success of reasoning systems based on probability theory has beenmuch more effective in making converts.

Token 10040:
We now look at how the axioms can be deployed to make inferences.

Token 10041:
13.3 I NFERENCE USING FULL JOINT DISTRIBUTIONS In this section we describe a simple method for probabilistic inference —that is, the compu-PROBABILISTIC INFERENCE tation of posterior probabilities for query propositions given observed evidence.

Token 10042:
We use the full joint distribution as the “knowledge base” from which answers to all questions may be de-rived.

Token 10043:
Along the way we also introduce several useful techniques for manipulating equationsinvolving probabilities.

Token 10044:
Section 13.3. Inference Using Full Joint Distributions 491 WHERE DOPROBABILITIES COME FROM ?

Token 10045:
There has been endless debate over the source and status of probability numbers.

Token 10046:
The frequentist position is that the numbers can come only from experiments :i f we test 100 people and ﬁnd that 10 of them have a cavity, then we can say thatthe probability of a cavity is approximately 0.1.

Token 10047:
In this view, the assertion “theprobability of a cavity is 0.1” means that 0.1 is the fraction that would be observedin the limit of inﬁnitely many samples.

Token 10048:
From any ﬁnite sample, we can estimatethe true fraction and also calculate how accurate our estimate is likely to be.

Token 10049:
The objectivist view is that probabilities are real aspects of the universe— propensities of objects to behave in certain ways—rather than being just descrip-tions of an observer’s degree of belief.

Token 10050:
For example, the fact that a fair coin comes up heads with probability 0.5 is a propensity of the coin itself.

Token 10051:
In this view, fre- quentist measurements are attempts to observe these propensities.

Token 10052:
Most physicistsagree that quantum phenomena are objectively probabilistic, but uncertainty at themacroscopic scale—e.g., in coin tossing—usually arises from ignorance of initialconditions and does not seem consistent with the propensity view.

Token 10053:
The subjectivist view describes probabilities as a way of characterizing an agent’s beliefs, rather than as having any external physical signiﬁcance.

Token 10054:
The sub-jective Bayesian view allows any self-consistent ascription of prior probabilities to propositions, but then insists on proper Bayesian updating as evidence arrives.

Token 10055:
In the end, even a strict frequentist position involves subjective analysis be- cause of the reference class problem: in trying to determine the outcome probabil- ity of a particular experiment, the frequentist has to place it in a reference class of “similar” experiments with known outcome frequencies.

Token 10056:
I. J.

Token 10057:
Good (1983, p. 27)wrote, “every event in life is unique, and every real-life probability that we esti-mate in practice is that of an event that has never occurred before.” For example,given a particular patient, a frequentist who wants to estimate the probability of acavity will consider a reference class of other patients who are similar in importantways—age, symptoms, diet—and see what proportion of them had a cavity.

Token 10058:
If thedentist considers everything that is known about the patient—weight to the nearestgram, hair color, mother’s maiden name—then the reference class becomes empty.This has been a vexing problem in the philosophy of science.

Token 10059:
The principle of indifference attributed to Laplace (1816) states that propo- sitions that are syntactically “symmetric” with respect to the evidence should be accorded equal probability.

Token 10060:
Various reﬁnements have been proposed, culminatingin the attempt by Carnap and others to develop a rigorous inductive logic , capa- ble of computing the correct probability for any proposition from any collection ofobservations.

Token 10061:
Currently, it is believed that no unique inductive logic exists; rather,any such logic rests on a subjective prior probability distribution whose effect isdiminished as more observations are collected.

Token 10062:
492 Chapter 13.

Token 10063:
Quantifying Uncertainty toothache ¬toothache catch ¬catch catch ¬catch cavity 0.108 0.012 0.072 0.008 ¬cavity 0.016 0.064 0.144 0.576 Figure 13.3 A full joint distribution for the Toothache ,Cavity ,Catch world.

Token 10064:
We begin with a simple example: a domain consisting of just the three Boolean variables Toothache ,Cavity ,a n dCatch (the dentist’s nasty steel probe catches in my tooth).

Token 10065:
The full joint distribution is a 2×2×2table as shown in Figure 13.3.

Token 10066:
Notice that the probabilities in the joint distribution sum to 1, as required by the axioms of probability.

Token 10067:
Notice also that Equation (13.2) gives us a direct way to calculate the probabil-ity of any proposition, simple or complex: simply identify those possible worlds in which theproposition is true and add up their probabilities.

Token 10068:
For example, there are six possible worldsin which cavity∨toothache holds: P(cavity∨toothache )=0.108 + 0 .012 + 0 .072 + 0 .008 + 0 .016 + 0 .064 = 0 .28.

Token 10069:
One particularly common task is to extract the distribution over some subset of variables or a single variable.

Token 10070:
For example, adding the entries in the ﬁrst row gives the unconditional ormarginal probability 4ofcavity :MARGINAL PROBABILITY P(cavity )=0.108 + 0 .012 + 0 .072 + 0 .008 = 0 .2.

Token 10071:
This process is called marginalization ,o rsumming out —because we sum up the probabil- MARGINALIZATION ities for each possible value of the other variables, thereby taking them out of the equation.

Token 10072:
We can write the following general marginalization rule for any sets of variables YandZ: P(Y)=/summationdisplay z∈ZP(Y,z), (13.6) where/summationtext z∈Zmeans to sum over all the possible combinations of values of the set of variables Z.

Token 10073:
We sometimes abbreviate this as/summationtext z, leaving Zimplicit.

Token 10074:
We just used the rule as P(Cavity )=/summationdisplay z∈{Catch ,Toothache }P(Cavity ,z).

Token 10075:
(13.7) A variant of this rule involves conditional probabilities instead of joint probabilities, using the product rule: P(Y)=/summationdisplay zP(Y|z)P(z).

Token 10076:
(13.8) This rule is called conditioning .

Token 10077:
Marginalization and conditioning turn out to be useful rules CONDITIONING for all kinds of derivations involving probability expressions.

Token 10078:
In most cases, we are interested in computing conditional probabilities of some vari- ables, given evidence about others.

Token 10079:
Conditional probabilities can be found by ﬁrst using 4So called because of a common practice among actuaries of writing the sums of observed frequencies in the margins of insurance tables.

Token 10080:
Section 13.3.

Token 10081:
Inference Using Full Joint Distributions 493 Equation (13.3) to obtain an expression in terms of unconditional probabilities and then eval- uating the expression from the full joint distribution.

Token 10082:
For example, we can compute theprobability of a cavity, given evidence of a toothache, as follows: P(cavity|toothache )=P(cavity∧toothache ) P(toothache ) =0.108 + 0 .012 0.108 + 0 .012 + 0 .016 + 0 .064=0.6.

Token 10083:
Just to check, we can also compute the probability that there is no cavity, given a toothache: P(¬cavity|toothache )=P(¬cavity∧toothache ) P(toothache ) =0.016 + 0 .064 0.108 + 0 .012 + 0 .016 + 0 .064=0.4.

Token 10084:
The two values sum to 1.0, as they should.

Token 10085:
Notice that in these two calculations the term 1/P(toothache )remains constant, no matter which value of Cavity we calculate.

Token 10086:
In fact, it can be viewed as a normalization constant for the distribution P(Cavity|toothache ), NORMALIZATION ensuring that it adds up to 1.

Token 10087:
Throughout the chapters dealing with probability, we use αto denote such constants.

Token 10088:
With this notation, we can write the two preceding equations in one: P(Cavity|toothache )=αP(Cavity ,toothache ) =α[P(Cavity ,toothache ,catch)+P(Cavity ,toothache ,¬catch)] =α[/angbracketleft0.108,0.016/angbracketright+/angbracketleft0.012,0.064/angbracketright]=α/angbracketleft0.12,0.08/angbracketright=/angbracketleft0.6,0.4/angbracketright.

Token 10089:
In other words, we can calculate P(Cavity|toothache )even if we don’t know the value of P(toothache )!

Token 10090:
We temporarily forget about the factor 1/P(toothache )and add up the values forcavity and¬cavity , getting 0.12 and 0.08.

Token 10091:
Those are the correct relative proportions, but they don’t sum to 1, so we normalize them by dividing each one by 0.12 + 0 .08, getting the true probabilities of 0.6 and 0.4.

Token 10092:
Normalization turns out to be a useful shortcut in many probability calculations, both to make the computation easier and to allow us to proceed when some probability assessment (such as P(toothache )) is not available.

Token 10093:
From the example, we can extract a general inference procedure.

Token 10094:
We begin with the case in which the query involves a single variable, X(Cavity in the example).

Token 10095:
Let Ebe the list of evidence variables (just Toothache in the example), let ebe the list of observed values for them, and let Ybe the remaining unobserved variables (just Catch in the example).

Token 10096:
The query is P(X|e)and can be evaluated as P(X|e)=αP(X,e)=α/summationdisplay yP(X,e,y), (13.9) where the summation is over all possible ys (i.e., all possible combinations of values of the unobserved variables Y).

Token 10097:
Notice that together the variables X,E,a n d Yconstitute the com- plete set of variables for the domain, so P(X,e,y)is simply a subset of probabilities from the full joint distribution.

Token 10098:
Given the full joint distribution to work with, Equation (13.9) can answer probabilistic queries for discrete variables.

Token 10099:
It does not scale well, however: for a domain described by n Boolean variables, it requires an input table of size O(2n)and takes O(2n)time to process the

Token 10100:
494 Chapter 13. Quantifying Uncertainty table. In a realistic problem we could easily have n>100,m a k i n g O(2n)impractical.

Token 10101:
The full joint distribution in tabular form is just not a practical tool for building reasoning systems.Instead, it should be viewed as the theoretical foundation on which more effective approachesmay be built, just as truth tables formed a theoretical foundation for more practical algorithmslike DPLL.

Token 10102:
The remainder of this chapter introduces some of the basic ideas required in preparation for the development of realistic systems in Chapter 14.

Token 10103:
13.4 I NDEPENDENCE Let us expand the full joint distribution in Figure 13.3 by adding a fourth variable, Weather .

Token 10104:
The full joint distribution then becomes P(Toothache ,Catch ,Cavity ,Weather ), which has 2×2×2×4=3 2 entries.

Token 10105:
It contains four “editions” of the table shown in Figure 13.3, one for each kind of weather.

Token 10106:
What relationship do these editions have to each other and tothe original three-variable table?

Token 10107:
For example, how are P(toothache ,catch,cavity ,cloudy ) andP(toothache ,catch,cavity )related?

Token 10108:
We can use the product rule: P(toothache ,catch,cavity ,cloudy ) =P(cloudy|toothache ,catch,cavity )P(toothache ,catch,cavity ).

Token 10109:
Now, unless one is in the deity business, one should not imagine that one’s dental problems inﬂuence the weather.

Token 10110:
And for indoor dentistry, at least, it seems safe to say that the weather does not inﬂuence the dental variables.

Token 10111:
Therefore, the following assertion seems reasonable: P(cloudy|toothache ,catch,cavity )=P(cloudy ).

Token 10112:
(13.10) From this, we can deduce P(toothache ,catch,cavity ,cloudy )=P(cloudy )P(toothache ,catch,cavity ).

Token 10113:
A similar equation exists for every entry inP(Toothache ,Catch ,Cavity ,Weather ).

Token 10114:
In fact, we can write the general equation P(Toothache ,Catch ,Cavity ,Weather )=P(Toothache ,Catch ,Cavity )P(Weather ).

Token 10115:
Thus, the 32-element table for four variables can be constructed from one 8-element table and one 4-element table.

Token 10116:
This decomposition is illustrated schematically in Figure 13.4(a).

Token 10117:
The property we used in Equation (13.10) is called independence (also marginal in- INDEPENDENCE dependence andabsolute independence ).

Token 10118:
In particular, the weather is independent of one’s dental problems.

Token 10119:
Independence between propositions aandbcan be written as P(a|b)=P(a)orP(b|a)=P(b)orP(a∧b)=P(a)P(b).

Token 10120:
(13.11) All these forms are equivalent (Exercise 13.12).

Token 10121:
Independence between variables XandY can be written as follows (again, these are all equivalent): P(X|Y)=P(X)or P(Y|X)=P(Y)or P(X,Y)=P(X)P(Y).

Token 10122:
Independence assertions are usually based on knowledge of the domain.

Token 10123:
As the toothache– weather example illustrates, they can dramatically reduce the amount of information nec-essary to specify the full joint distribution.

Token 10124:
If the complete set of variables can be divided

Token 10125:
Section 13.5.

Token 10126:
Bayes’ Rule and Its Use 495 WeatherToothache CatchCavity decomposes into Weather Toothache CatchCavitydecomposes intoCoin1Coinn Coin1 Coinn (a) (b) Figure 13.4 Two examples of factoring a large joint distribution into smaller distributions, using absolute independence.

Token 10127:
(a) Weather an d dental problems are independent. (b) Coin ﬂips are independent.

Token 10128:
into independent subsets, then the full joint distribution can be factored into separate joint distributions on those subsets.

Token 10129:
For example, the full joint distribution on the outcome of n independent coin ﬂips, P(C1,...,C n),h a s2nentries, but it can be represented as the prod- uct of nsingle-variable distributions P(Ci).

Token 10130:
In a more practical vein, the independence of dentistry and meteorology is a good thing, because otherwise the practice of dentistry mightrequire intimate knowledge of meteorology, and vice versa.

Token 10131:
When they are available, then, independence assertions can help in reducing the size of the domain representation and the complexity of the inference problem.

Token 10132:
Unfortunately, cleanseparation of entire sets of variables by independence is quite rare.

Token 10133:
Whenever a connection,however indirect, exists between two variables, independence will fail to hold.

Token 10134:
Moreover, even independent subsets can be quite large—for example, dentistry might involve dozens of diseases and hundreds of symptoms, all of which are interrelated.

Token 10135:
To handle such problems,we need more subtle methods than the straightforward concept of independence.

Token 10136:
13.5 B AYES ’RULE AND ITSUSE On page 486, we deﬁned the product rule .

Token 10137:
It can actually be written in two forms: P(a∧b)=P(a|b)P(b) and P(a∧b)=P(b|a)P(a).

Token 10138:
Equating the two right-hand sides and dividing by P(a),w eg e t P(b|a)=P(a|b)P(b) P(a).

Token 10139:
(13.12) This equation is known as Bayes’ rule (also Bayes’ law or Bayes’ theorem).

Token 10140:
This simple BAYES’ RULE equation underlies most modern AI systems for probabilistic inference.

Token 10141:
496 Chapter 13.

Token 10142:
Quantifying Uncertainty The more general case of Bayes’ rule for multivalued variables can be written in the P notation as follows: P(Y|X)=P(X|Y)P(Y) P(X), As before, this is to be taken as representing a set of equations, each dealing with speciﬁc val- ues of the variables.

Token 10143:
We will also have occasion to use a more general version conditionalizedon some background evidence e: P(Y|X,e)=P(X|Y,e)P(Y|e) P(X|e).

Token 10144:
(13.13) 13.5.1 Applying Bayes’ rule: The simple case On the surface, Bayes’ rule does not seem very useful.

Token 10145:
It allows us to compute the single termP(b|a)in terms of three terms: P(a|b),P(b),a n dP(a).

Token 10146:
That seems like two steps backwards, but Bayes’ rule is useful in practice because there are many cases where we dohave good probability estimates for these three numbers and need to compute the fourth.Often, we perceive as evidence the effect of some unknown cause and we would like to determine that cause.

Token 10147:
In that case, Bayes’ rule becomes P(cause|effect)=P(effect|cause)P(cause) P(effect).

Token 10148:
The conditional probability P(effect|cause)quantiﬁes the relationship in the causal direc- CAUSAL tion, whereas P(cause|effect)describes the diagnostic direction.

Token 10149:
In a task such as medical DIAGNOSTIC diagnosis, we often have conditional probabilities on causal relationships (that is, the doctor knows P(symptoms|disease )) and want to derive a diagnosis, P(disease|symptoms ).F o r example, a doctor knows that the disease meningitis causes the patient to have a stiff neck,say, 70% of the time.

Token 10150:
The doctor also knows some unconditional facts: the prior probabil-ity that a patient has meningitis is 1/50,000, and the prior probability that any patient has a stiff neck is 1%.

Token 10151:
Letting sbe the proposition that the patient has a stiff neck and mbe the proposition that the patient has meningitis, we have P(s|m)=0 .7 P(m)=1 /50000 P(s)=0 .01 P(m|s)=P(s|m)P(m) P(s)=0.7×1/50000 0.01=0.0014.

Token 10152:
(13.14) That is, we expect less than 1 in 700 patients with a stiff neck to have meningitis.

Token 10153:
Notice that even though a stiff neck is quite strongly indicated by meningitis (with probability 0.7), the probability of meningitis in the patient remains small.

Token 10154:
This is because the prior probability of stiff necks is much higher than that of meningitis.

Token 10155:
Section 13.3 illustrated a process by which one can avoid assessing the prior probability of the evidence (here, P(s)) by instead computing a posterior probability for each value of

Token 10156:
Section 13.5. Bayes’ Rule and Its Use 497 the query variable (here, mand¬m) and then normalizing the results.

Token 10157:
The same process can be applied when using Bayes’ rule. We have P(M|s)=α/angbracketleftP(s|m)P(m),P(s|¬m)P(¬m)/angbracketright.

Token 10158:
Thus, to use this approach we need to estimate P(s|¬m)instead of P(s). There is no free lunch—sometimes this is easier, sometimes it is harder.

Token 10159:
The general form of Bayes’ rule withnormalization is P(Y|X)=αP(X|Y)P(Y), (13.15) where αis the normalization constant needed to make the entries in P(Y|X)sum to 1.

Token 10160:
One obvious question to ask about Bayes’ rule is why one might have available the conditional probability in one direction, but not the other.

Token 10161:
In the meningitis domain, perhapsthe doctor knows that a stiff neck implies meningitis in 1 out of 5000 cases; that is, the doctorhas quantitative information in the diagnostic direction from symptoms to causes.

Token 10162:
Such a doctor has no need to use Bayes’ rule. Unfortunately, diagnostic knowledge is often more fragile than causal knowledge.

Token 10163:
If there is a sudden epidemic of meningitis, the unconditional probability of meningitis, P(m), will go up.

Token 10164:
The doctor who derived the diagnostic proba- bilityP(m|s)directly from statistical observation of patients before the epidemic will have no idea how to update the value, but the doctor who computes P(m|s)from the other three values will see that P(m|s)should go up proportionately with P(m).

Token 10165:
Most important, the causal information P(s|m)isunaffected by the epidemic, because it simply reﬂects the way meningitis works.

Token 10166:
The use of this kind of direct causal or model-based knowledge providesthe crucial robustness needed to make probabilistic systems feasible in the real world.

Token 10167:
13.5.2 Using Bayes’ rule: Combining evidence We have seen that Bayes’ rule can be useful for answering probabilistic queries conditionedon one piece of evidence—for example, the stiff neck.

Token 10168:
In particular, we have argued thatprobabilistic information is often available in the form P(effect|cause).

Token 10169:
What happens when we have two or more pieces of evidence?

Token 10170:
For example, what can a dentist conclude if hernasty steel probe catches in the aching tooth of a patient?

Token 10171:
If we know the full joint distribution(Figure 13.3), we can read off the answer: P(Cavity|toothache∧catch)=α/angbracketleft0.108,0.016/angbracketright≈/angbracketleft0.871,0.129/angbracketright.

Token 10172:
We know, however, that such an approach does not scale up to larger numbers of variables.

Token 10173:
We can try using Bayes’ rule to reformulate the problem: P(Cavity|toothache∧catch) =αP(toothache∧catch|Cavity )P(Cavity ).

Token 10174:
(13.16) For this reformulation to work, we need to know the conditional probabilities of the conjunc- tiontoothache∧catch for each value of Cavity .

Token 10175:
That might be feasible for just two evidence variables, but again it does not scale up.

Token 10176:
If there are npossible evidence variables (X rays, diet, oral hygiene, etc.

Token 10177:
), then there are 2 npossible combinations of observed values for which we would need to know conditional probabilities.

Token 10178:
We might as well go back to using thefull joint distribution. This is what ﬁrst led researchers away from probability theory toward

Token 10179:
498 Chapter 13.

Token 10180:
Quantifying Uncertainty approximate methods for evidence combination that, while giving incorrect answers, require fewer numbers to give any answer at all.

Token 10181:
Rather than taking this route, we need to ﬁnd some additional assertions about the domain that will enable us to simplify the expressions.

Token 10182:
The notion of independence in Sec- tion 13.4 provides a clue, but needs reﬁning.

Token 10183:
It would be nice if Toothache andCatch were independent, but they are not: if the probe catches in the tooth, then it is likely that the tooth has a cavity and that the cavity causes a toothache.

Token 10184:
These variables areindependent, how- ever, given the presence or the absence of a cavity .

Token 10185:
Each is directly caused by the cavity, but neither has a direct effect on the other: toothache depends on the state of the nerves in thetooth, whereas the probe’s accuracy depends on the dentist’s skill, to which the toothache isirrelevant.

Token 10186:
5Mathematically, this property is written as P(toothache∧catch|Cavity )=P(toothache|Cavity )P(catch|Cavity ).

Token 10187:
(13.17) This equation expresses the conditional independence oftoothache andcatch givenCavity .CONDITIONAL INDEPENDENCE We can plug it into Equation (13.16) to obtain the probability of a cavity: P(Cavity|toothache∧catch) =αP(toothache|Cavity )P(catch|Cavity )P(Cavity ).

Token 10188:
(13.18) Now the information requirements are the same as for inference, using each piece of evi- dence separately: the prior probability P(Cavity )for the query variable and the conditional probability of each effect, given its cause.

Token 10189:
The general deﬁnition of conditional independence of two variables XandY,g i v e na third variable Z,i s P(X,Y|Z)=P(X|Z)P(Y|Z).

Token 10190:
In the dentist domain, for example, it seems reasonable to assert conditional independence of the variables Toothache andCatch ,g i v e n Cavity : P(Toothache ,Catch|Cavity )=P(Toothache|Cavity )P(Catch|Cavity ).

Token 10191:
(13.19) Notice that this assertion is somewhat stronger than Equation (13.17), which asserts indepen- dence only for speciﬁc values of Toothache andCatch .

Token 10192:
As with absolute independence in Equation (13.11), the equivalent forms P(X|Y,Z)=P(X|Z)and P(Y|X,Z)=P(Y|Z) can also be used (see Exercise 13.17).

Token 10193:
Section 13.4 showed that absolute independence as- sertions allow a decomposition of the full joint distribution into much smaller pieces.

Token 10194:
It turnsout that the same is true for conditional independence assertions.

Token 10195:
For example, given theassertion in Equation (13.19), we can derive a decomposition as follows: P(Toothache ,Catch ,Cavity ) =P(Toothache ,Catch|Cavity )P(Cavity )(product rule) =P(Toothache|Cavity )P(Catch|Cavity )P(Cavity )(using 13.19).

Token 10196:
(The reader can easily check that this equation does in fact hold in Figure 13.3.)

Token 10197:
In this way, the original large table is decomposed into three smaller tables.

Token 10198:
The original table has seven 5We assume that the patient and dentist are distinct individuals.

Token 10199:
Section 13.6. The Wumpus World Revisited 499 independent numbers ( 23=8entries in the table, but they must sum to 1, so 7 are indepen- dent).

Token 10200:
The smaller tables contain ﬁve independent numbers (for a conditional probabilitydistributions such as P(T|Cthere are two rows of two numbers, and each row sums to 1, so that’s two independent numbers; for a prior distribution like P(C)there is only one indepen- dent number).

Token 10201:
Going from seven to ﬁve might not seem like a major triumph, but the point is that, for nsymptoms that are all conditionally independent given Cavity , the size of the representation grows as O(n)instead of O(2 n).

Token 10202:
That means that conditional independence assertions can allow probabilistic systems to scale up; moreover, they are much more com- monly available than absolute independence assertions.

Token 10203:
Conceptually, Cavity separates SEPARATION Toothache andCatch because it is a direct cause of both of them.

Token 10204:
The decomposition of large probabilistic domains into weakly connected subsets through conditional independenceis one of the most important developments in the recent history of AI.

Token 10205:
The dentistry example illustrates a commonly occurring pattern in which a single cause directly inﬂuences a number of effects, all of which are conditionally independent, given thecause.

Token 10206:
The full joint distribution can be written as P(Cause ,Effect 1,..., Effectn)=P(Cause )/productdisplay iP(Effecti|Cause ).

Token 10207:
Such a probability distribution is called a naive Bayes model—“naive” because it is often NAIVE BAYES used (as a simplifying assumption) in cases where the “effect” variables are notactually conditionally independent given the cause variable.

Token 10208:
(The naive Bayes model is sometimescalled a Bayesian classiﬁer , a somewhat careless usage that has prompted true Bayesians to call it the idiot Bayes model.)

Token 10209:
In practice, naive Bayes systems can work surprisingly well, even when the conditional independence assumption is not true.

Token 10210:
Chapter 20 describes methods for learning naive Bayes distributions from observations.

Token 10211:
13.6 T HEWUMPUS WORLD REVISITED We can combine of the ideas in this chapter to solve probabilistic reasoning problems in thewumpus world.

Token 10212:
(See Chapter 7 for a complete description of the wumpus world.)

Token 10213:
Uncertaintyarises in the wumpus world because the agent’s sensors give only partial information aboutthe world.

Token 10214:
For example, Figure 13.5 shows a situation in which each of the three reachablesquares—[1,3], [2,2], and [3,1]—might contain a pit.

Token 10215:
Pure logical inference can concludenothing about which square is most likely to be safe, so a logical agent might have to chooserandomly.

Token 10216:
We will see that a probabilistic agent can do much better than the logical agent.

Token 10217:
Our aim is to calculate the probability that each of the three squares contains a pit. (For this example we ignore the wumpus and the gold.)

Token 10218:
The relevant properties of the wumpus world are that (1) a pit causes breezes in all neighboring squares, and (2) each square other than [1,1] contains a pit with probability 0.2.

Token 10219:
The ﬁrst step is to identify the set of randomvariables we need: •As in the propositional logic case, we want one Boolean variable P ijfor each square, w h i c hi st r u ei f fs q u a r e[ i,j] actually contains a pit.

Token 10220:
500 Chapter 13.

Token 10221:
Quantifying Uncertainty OK 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 OK OK 3,4 4,4 B B 1,1 2,1 3,1 4,1 1,2 2,2 3,2 4,2 1,3 2,3 3,3 4,3 1,4 2,4 3,4 4,4 KNOWNFRONTIERQUERYOTHER (a) (b) Figure 13.5 (a) After ﬁnding a breeze in both [1,2] and [2,1], the agent is stuck—there is no safe place to explore.

Token 10222:
(b) Division of the squares into Known ,Frontier ,a n dOther ,f o r a query about [1,3].

Token 10223:
•We also have Boolean variables Bijthat are true iff square [ i,j] is breezy; we include these variables only for the observed squares—in this case, [1,1], [1,2], and [2,1].

Token 10224:
The next step is to specify the full joint distribution, P(P1,1,...,P 4,4,B1,1,B1,2,B2,1).A p - plying the product rule, we have P(P1,1,...,P 4,4,B1,1,B1,2,B2,1)= P(B1,1,B1,2,B2,1|P1,1,...,P 4,4)P(P1,1,...,P 4,4).

Token 10225:
This decomposition makes it easy to see what the joint probability values should be.

Token 10226:
The ﬁrst term is the conditional probability distribution of a breeze conﬁguration, given a pitconﬁguration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise.

Token 10227:
Thesecond term is the prior probability of a pit conﬁguration.

Token 10228:
Each square contains a pit withprobability 0.2, independently of the other squares; hence, P(P 1,1,...,P 4,4)=4,4/productdisplay i,j=1,1P(Pi,j).

Token 10229:
(13.20) For a particular conﬁguration with exactly npits,P(P1,1,...,P 4,4)=0.2n×0.816−n.

Token 10230:
In the situation in Figure 13.5(a), the evidence consists of the observed breeze (or its absence) in each square that is visited, combined with the fact that each such square contains no pit.

Token 10231:
We abbreviate these facts as b=¬b1,1∧b1,2∧b2,1andknown =¬p1,1∧¬p1,2∧¬p2,1.

Token 10232:
We are interested in answering queries such as P(P1,3|known ,b): how likely is it that [1,3] contains a pit, given the observations so far?

Token 10233:
To answer this query, we can follow the standard approach of Equation (13.9), namely, summing over entries from the full joint distribution.

Token 10234:
Let Unknown be the set of Pi,jvari-

Token 10235:
Section 13.6. The Wumpus World Revisited 501 ables for squares other than the Known squares and the query square [1,3].

Token 10236:
Then, by Equa- tion (13.9), we have P(P1,3|known ,b)=α/summationdisplay unknownP(P1,3,unknown ,known ,b).

Token 10237:
The full joint probabilities have already been speciﬁed, so we are done—that is, unless we care about computation.

Token 10238:
There are 12 unknown squares; hence the summation contains2 12= 4096 terms. In general, the summation grows exponentially with the number of squares.

Token 10239:
Surely, one might ask, aren’t the other squares irrelevant? How could [4,4] affect whether [1,3] has a pit? Indeed, this intuition is correct.

Token 10240:
Let Frontier be the pit variables (other than the query variable) that are adjacent to visited squares, in this case just [2,2] and[3,1].

Token 10241:
Also, let Other be the pit variables for the other unknown squares; in this case, there are 10 other squares, as shown in Figure 13.5(b).

Token 10242:
The key insight is that the observed breezes areconditionally independent of the other variables, given the known, frontier, and query vari- ables.

Token 10243:
To use the insight, we manipulate the query formula into a form in which the breezesare conditioned on all the other variables, and then we apply conditional independence: P(P 1,3|known ,b) =α/summationdisplay unknownP(P1,3,known ,b,unknown ) (by Equation (13.9)) =α/summationdisplay unknownP(b|P1,3,known ,unknown )P(P1,3,known ,unknown ) (by the product rule) =α/summationdisplay frontier/summationdisplay otherP(b|known ,P1,3,frontier ,other)P(P1,3,known ,frontier ,other) =α/summationdisplay frontier/summationdisplay otherP(b|known ,P1,3,frontier )P(P1,3,known ,frontier ,other), where the ﬁnal step uses conditional independence: bis independent of other givenknown , P1,3,a n dfrontier .

Token 10244:
Now, the ﬁrst term in this expression does not depend on the Other variables, so we can move the summation inward: P(P1,3|known ,b) =α/summationdisplay frontierP(b|known ,P1,3,frontier )/summationdisplay otherP(P1,3,known ,frontier ,other).

Token 10245:
By independence, as in Equation (13.20), the prior term can be factored, and then the terms can be reordered: P(P1,3|known ,b) =α/summationdisplay frontierP(b|known ,P1,3,frontier )/summationdisplay otherP(P1,3)P(known )P(frontier )P(other) =αP(known )P(P1,3)/summationdisplay frontierP(b|known ,P1,3,frontier )P(frontier )/summationdisplay otherP(other) =α/primeP(P1,3)/summationdisplay frontierP(b|known ,P1,3,frontier )P(frontier ),

Token 10246:
502 Chapter 13.

Token 10247:
Quantifying Uncertainty OK 1,1 2,1 3,1 1,2 OK OKB BOK 1,1 2,1 1,2 2,2 OK OKB BOK 1,1 2,1 3,1 1,2 OK OKB B 0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16 0.8 x 0.2 = 0.16OK 1,1 2,1 1,2 1,3 OK OKB BOK 1,1 2,1 3,1 1,2 1,3 OK OKB B 0.2 x 0.2 = 0.04 0.2 x 0.8 = 0.16 (a) (b) 2,2 1,3 3,1 1,3 2,2 1,3 3,1 2,2 2,2 Figure 13.6 Consistent models for the frontier variables P2,2andP3,1, showing P(frontier )for each model: (a) three models with P1,3=true showing two or three pits, and (b) two models with P1,3=false showing one or two pits.

Token 10248:
where the last step folds P(known )into the normalizing constant and uses the fact that/summationtext otherP(other)equals 1.

Token 10249:
Now, there are just four terms in the summation over the frontier variables P2,2and P3,1.

Token 10250:
The use of independence and conditional independence has completely eliminated the other squares from consideration.

Token 10251:
Notice that the expression P(b|known ,P1,3,frontier )is 1 when the frontier is consis- tent with the breeze observations, and 0 otherwise.

Token 10252:
Thus, for each value of P1,3, we sum over thelogical models for the frontier variables that are consistent with the known facts.

Token 10253:
(Com- pare with the enumeration over models in Figure 7.5 on page 241.)

Token 10254:
The models and theirassociated prior probabilities— P(frontier )—are shown in Figure 13.6.

Token 10255:
We have P(P 1,3|known ,b)=α/prime/angbracketleft0.2(0.04 + 0 .16 + 0 .16),0.8(0.04 + 0 .16)/angbracketright≈/angbracketleft0.31,0.69/angbracketright.

Token 10256:
That is, [1,3] (and [3,1] by symmetry) contains a pit with roughly 31% probability.

Token 10257:
A similar calculation, which the reader might wish to perform, shows that [2,2] contains a pit with roughly 86% probability.

Token 10258:
The wumpus agent should deﬁnitely avoid [2,2]! Note that our logical agent from Chapter 7 did not know that [2,2] was worse than the other squares.

Token 10259:
Logic can tell us that it is unknown whether there is a pit in [2, 2], but we need probability to tell ushow likely it is.

Token 10260:
What this section has shown is that even seemingly complicated problems can be for- mulated precisely in probability theory and solved with simple algorithms.

Token 10261:
To get efﬁcient solutions, independence and conditional independence relationships can be used to simplifythe summations required.

Token 10262:
These relationships often correspond to our natural understandingof how the problem should be decomposed.

Token 10263:
In the next chapter, we develop formal represen-tations for such relationships as well as algorithms that operate on those representations toperform probabilistic inference efﬁciently.

Token 10264:
Section 13.7.

Token 10265:
Summary 503 13.7 S UMMARY This chapter has suggested probability theory as a suitable foundation for uncertain reasoning and provided a gentle introduction to its use.

Token 10266:
•Uncertainty arises because of both laziness and ignorance. It is inescapable in complex, nondeterministic, or partially observable environments.

Token 10267:
•Probabilities express the agent’s inability to reach a deﬁnite decision regarding the truth of a sentence.

Token 10268:
Probabilities summarize the agent’s beliefs relative to the evidence.

Token 10269:
•Decision theory combines the agent’s beliefs and desires, deﬁning the best action as the one that maximizes expected utility.

Token 10270:
•Basic probability statements include prior probabilities andconditional probabilities over simple and complex propositions.

Token 10271:
•The axioms of probability constrain the possible assignments of probabilities to propo- sitions.

Token 10272:
An agent that violates the axioms must behave irrationally in some cases.

Token 10273:
•The full joint probability distribution speciﬁes the probability of each complete as- signment of values to random variables.

Token 10274:
It is usually too large to create or use in itsexplicit form, but when it is available it can be used to answer queries simply by adding up entries for the possible worlds corresponding to the query propositions.

Token 10275:
•Absolute independence between subsets of random variables allows the full joint dis- tribution to be factored into smaller joint distributions, greatly reducing its complexity.

Token 10276:
Absolute independence seldom occurs in practice.

Token 10277:
•Bayes’ rule allows unknown probabilities to be computed from known conditional probabilities, usually in the causal direction.

Token 10278:
Applying Bayes’ rule with many pieces ofevidence runs into the same scaling problems as does the full joint distribution.

Token 10279:
•Conditional independence brought about by direct causal relationships in the domain might allow the full joint distribution to be factored into smaller, conditional distri-butions.

Token 10280:
The naive Bayes model assumes the conditional independence of all effect variables, given a single cause variable, and grows linearly with the number of effects.

Token 10281:
•A wumpus-world agent can calculate probabilities for unobserved aspects of the world, thereby improving on the decisions of a purely logical agent.

Token 10282:
Conditional independencemakes these calculations tractable.

Token 10283:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Probability theory was invented as a way of analyzing games of chance.

Token 10284:
In about 850 A.D. the Indian mathematician Mahaviracarya described how to arrange a set of bets that can’t lose (what we now call a Dutch book).

Token 10285:
In Europe, the ﬁrst signiﬁcant systematic analyses wereproduced by Girolamo Cardano around 1565, although publication was posthumous (1663).By that time, probability had been established as a mathematical discipline due to a series of

Token 10286:
504 Chapter 13. Quantifying Uncertainty results established in a famous correspondence between Blaise Pascal and Pierre de Fermat in 1654.

Token 10287:
As with probability itself, the results were initially motivated by gambling problems(see Exercise 13.9).

Token 10288:
The ﬁrst published textbook on probability was De Ratiociniis in Ludo Aleae (Huygens, 1657).

Token 10289:
The “laziness and ignorance” view of uncertainty was described by John Arbuthnot in the preface of his translation of Huygens (Arbuthnot, 1692): “It is impossible for a Die, with such determin’d force and direction, not to fall on such determin’d side, only I don’t know the force and direction which makes it fall on such determin’d side,and therefore I call it Chance, which is nothing but the want of art...” Laplace (1816) gave an exceptionally accurate and modern overview of probability; he was the ﬁrst to use the example “take two urns, A and B, the ﬁrst containing four white andt w ob l a c kb a l l s ,...” T h eR e v .T h o m a sB a y e s( 1702–1761) introduced the rule for reasoning about conditional probabilities that was named after him (Bayes, 1763).

Token 10290:
Bayes only con-sidered the case of uniform priors; it was Laplace who independently developed the generalcase.

Token 10291:
Kolmogorov (1950, ﬁrst published in German in 1933) presented probability theory ina rigorously axiomatic framework for the ﬁrst time.

Token 10292:
R´ enyi (1970) later gave an axiomatic presentation that took conditional probability, rather than absolute probability, as primitive.

Token 10293:
Pascal used probability in ways that required both the objective interpretation, as a prop- erty of the world based on symmetry or relative frequency, and the subjective interpretation, based on degree of belief—the former in his analyses of probabilities in games of chance, thelatter in the famous “Pascal’s wager” argument about the possible existence of God.

Token 10294:
How-ever, Pascal did not clearly realize the distinction between these two interpretations.

Token 10295:
Thedistinction was ﬁrst drawn clearly by James Bernoulli (1654–1705).

Token 10296:
Leibniz introduced the “classical” notion of probability as a proportion of enumerated, equally probable cases, which was also used by Bernoulli, although it was brought to promi-nence by Laplace (1749–1827).

Token 10297:
This notion is ambiguous between the frequency interpreta-tion and the subjective interpretation.

Token 10298:
The cases can be thought to be equally probable eitherbecause of a natural, physical symmetry between them, or simply because we do not haveany knowledge that would lead us to consider one more probable than another.

Token 10299:
The use of this latter, subjective consideration to justify assigning equal probabilities is known as the principle of indifference .

Token 10300:
The principle is often attributed to Laplace, but he never isolated PRINCIPLE OF INDIFFERENCE the principle explicitly.

Token 10301:
George Boole and John Venn both referred to it as the principle of insufﬁcient reason ; the modern name is due to Keynes (1921).PRINCIPLE OF INSUFFICIENTREASON The debate between objectivists and subjectivists became sharper in the 20th century.

Token 10302:
Kolmogorov (1963), R. A. Fisher (1922), and Richard von Mises (1928) were advocates ofthe relative frequency interpretation.

Token 10303:
Karl Popper’s (1959, ﬁrst published in German in 1934)“propensity” interpretation traces relative frequencies to an underlying physical symmetry.Frank Ramsey (1931), Bruno de Finetti (1937), R. T. Cox (1946), Leonard Savage (1954),Richard Jeffrey (1983), and E. T. Jaynes (2003) interpreted probabilities as the degrees ofbelief of speciﬁc individuals.

Token 10304:
Their analyses of degree of belief were closely tied to utili- ties and to behavior—speciﬁcally, to the willingness to place bets.

Token 10305:
Rudolf Carnap, following Leibniz and Laplace, offered a different kind of subjective interpretation of probability—not as any actual individual’s degree of belief, but as the degree of belief that an idealizedindividual should have in a particular proposition a, given a particular body of evidence e.

Token 10306:


Token 10307:
Bibliographical and Historical Notes 505 Carnap attempted to go further than Leibniz or Laplace by making this notion of degree of conﬁrmation mathematically precise, as a logical relation between aande.

Token 10308:
The study of this CONFIRMATION relation was intended to constitute a mathematical discipline called inductive logic ,a n a l o - INDUCTIVELOGIC gous to ordinary deductive logic (Carnap, 1948, 1950).

Token 10309:
Carnap was not able to extend his inductive logic much beyond the propositional case, and Putnam (1963) showed by adversar- ial arguments that some fundamental difﬁculties would prevent a strict extension to languages capable of expressing arithmetic.

Token 10310:
Cox’s theorem (1946) shows that any system for uncertain reasoning that meets his set of assumptions is equivalent to probability theory.

Token 10311:
This gave renewed conﬁdence to thosewho already favored probability, but others were not convinced, pointing to the assumptions(primarily that belief must be represented by a single number, and thus the belief in ¬pmust be a function of the belief in p).

Token 10312:
Halpern (1999) describes the assumptions and shows some gaps in Cox’s original formulation.

Token 10313:
Horn (2003) shows how to patch up the difﬁculties.Jaynes (2003) has a similar argument that is easier to read.

Token 10314:
The question of reference classes is closely tied to the attempt to ﬁnd an inductive logic.

Token 10315:
The approach of choosing the “most speciﬁc” reference class of sufﬁcient size was formallyproposed by Reichenbach (1949).

Token 10316:
Various attempts have been made, notably by Henry Ky- burg (1977, 1983), to formulate more sophisticated policies in order to avoid some obvious fallacies that arise with Reichenbach’s rule, but such approaches remain somewhat ad hoc .

Token 10317:
More recent work by Bacchus, Grove, Halpern, and Koller (1992) extends Carnap’s methodsto ﬁrst-order theories, thereby avoiding many of the difﬁculties associated with the straight-forward reference-class method.

Token 10318:
Kyburg and Teng (2006) contrast probabilistic inferencewith nonmonotonic logic.

Token 10319:
Bayesian probabilistic reasoning has been used in AI since the 1960s, especially in medical diagnosis.

Token 10320:
It was used not only to make a diagnosis from available evidence, but also to select further questions and tests by using the theory of information value (Section 16.6) when available evidence was inconclusive (Gorry, 1968; Gorry et al.

Token 10321:
, 1973). One system outperformed human experts in the diagnosis of acute abdominal illnesses (de Dombal et al. , 1974). Lucas et al.

Token 10322:
(2004) gives an overview. These early Bayesian systems suffered from a number of problems, however.

Token 10323:
Because they lacked any theoretical model of the conditionsthey were diagnosing, they were vulnerable to unrepresentative data occurring in situationsfor which only a small sample was available (de Dombal et al.

Token 10324:
, 1981).

Token 10325:
Even more fundamen- tally, because they lacked a concise formalism (such as the one to be described in Chapter 14)for representing and using conditional independence information, they depended on the ac-quisition, storage, and processing of enormous tables of probabilistic data.

Token 10326:
Because of thesedifﬁculties, probabilistic methods for coping with uncertainty fell out of favor in AI from the1970s to the mid-1980s.

Token 10327:
Developments since the late 1980s are described in the next chapter.

Token 10328:
The naive Bayes model for joint distributions has been studied extensively in the pat- tern recognition literature since the 1950s (Duda and Hart, 1973).

Token 10329:
It has also been used, often unwittingly, in information retrieval, beginning with the work of Maron (1961).

Token 10330:
The proba-bilistic foundations of this technique, described further in Exercise 13.22, were elucidated byRobertson and Sparck Jones (1976).

Token 10331:
Domingos and Pazzani (1997) provide an explanation

Token 10332:
506 Chapter 13.

Token 10333:
Quantifying Uncertainty for the surprising success of naive Bayesian reasoning even in domains where the indepen- dence assumptions are clearly violated.

Token 10334:
There are many good introductory textbooks on probability theory, including those by Bertsekas and Tsitsiklis (2008) and Grinstead and Snell (1997).

Token 10335:
DeGroot and Schervish(2001) offer a combined introduction to probability and statistics from a Bayesian stand- point.

Token 10336:
Richard Hamming’s (1991) textbook gives a mathematically sophisticated introduc- tion to probability theory from the standpoint of a propensity interpretation based on physicalsymmetry.

Token 10337:
Hacking (1975) and Hald (1990) cover the early history of the concept of proba-bility.

Token 10338:
Bernstein (1996) gives an entertaining popular account of the story of risk. EXERCISES 13.1 Show from ﬁrst principles that P(a|b∧a)=1 .

Token 10339:
13.2 Using the axioms of probability, prove that any probability distribution on a discrete random variable must sum to 1.

Token 10340:
13.3 For each of the following statements, either prove it is true or give a counterexample.

Token 10341:
a.I fP(a|b,c)=P(b|a,c),t h e nP(a|c)=P(b|c) b.I fP(a|b,c)=P(a),t h e nP(b|c)=P(b) c.I fP(a|b)=P(a),t h e nP(a|b,c)=P(a|c) 13.4 Would it be rational for an agent to hold the three beliefs P(A)=0.4,P(B)=0.3,a n d P(A∨B)=0.5?

Token 10342:
If so, what range of probabilities would be rational for the agent to hold for A∧B?

Token 10343:
Make up a table like the one in Figure 13.2, and show how it supports your argument about rationality.

Token 10344:
Then draw another version of the table where P(A∨B)=0.7.

Token 10345:
Explain why it is rational to have this probability, even though the table shows one case that is a lossand three that just break even.

Token 10346:
( Hint: what is Agent 1 committed to about the probability of each of the four cases, especially the case that is a loss?)

Token 10347:
13.5 This question deals with the properties of possible worlds, deﬁned on page 488 as assignments to all random variables.

Token 10348:
We will work with propositions that correspond toexactly one possible world because they pin down the assignments of all the variables.

Token 10349:
Inprobability theory, such propositions are called atomic events .

Token 10350:
For example, with Boolean ATOMIC EVENT variables X1,X2,X3, the proposition x1∧¬x2∧¬x3ﬁxes the assignment of the variables; in the language of propositional logic, we would say it has exactly one model.

Token 10351:
a.

Token 10352:
Prove, for the case of nBoolean variables, that any two distinct atomic events are mutually exclusive; that is, their conjunction is equivalent to false .

Token 10353:
b. Prove that the disjunction of all possible atomic events is logically equivalent to true.

Token 10354:
c. Prove that any proposition is logically equivalent to the disjunction of the atomic events that entail its truth.

Token 10355:
Exercises 507 13.6 Prove Equation (13.4) from Equations (13.1) and (13.2).

Token 10356:
13.7 Consider the set of all possible ﬁve-card poker hands dealt fairly from a standard deck of ﬁfty-two cards. a.

Token 10357:
How many atomic events are there in the joint probability distribution (i.e., how many ﬁve-card hands are there)? b.

Token 10358:
What is the probability of each atomic event? c. What is the probability of being dealt a royal straight ﬂush? Four of a kind?

Token 10359:
13.8 Given the full joint distribution shown in Figure 13.3, calculate the following: a.P(toothache ). b.P(Cavity ). c.P(Toothache|cavity ).

Token 10360:
d.P(Cavity|toothache∨catch).

Token 10361:
13.9 In his letter of August 24, 1654, Pascal was trying to show how a pot of money should be allocated when a gambling game must end prematurely.

Token 10362:
Imagine a game where each turn consists of the roll of a die, player Egets a point when the die is even, and player Ogets a point when the die is odd.

Token 10363:
The ﬁrst player to get 7 points wins the pot. Suppose the game isinterrupted with Eleading 4–2. How should the money be fairly split in this case?

Token 10364:
What is the general formula? (Fermat and Pascal made several errors before solving the problem, butyou should be able to get it right the ﬁrst time.)

Token 10365:
13.10 Deciding to put probability theory to good use, we encounter a slot machine with three independent wheels, each producing one of the four symbols BAR,BELL ,LEMON ,o r CHERRY with equal probability.

Token 10366:
The slot machine has the following payout scheme for a bet of 1 coin (where “?” denotes that we don’t care what comes up for that wheel): BAR/BAR/BAR pays 20 coins BELL /BELL /BELL pays 15 coins LEMON /LEMON /LEMON pays 5 coins CHERRY /CHERRY /CHERRY pays 3 coins CHERRY /CHERRY /?

Token 10367:
pays 2 coins CHERRY /?/? pays 1 coin a. Compute the expected “payback” percentage of the machine.

Token 10368:
In other words, for each coin played, what is the expected coin return?

Token 10369:
b. Compute the probability that playing the slot machine once will result in a win.

Token 10370:
c. Estimate the mean and median number of plays you can expect to make until you go broke, if you start with 10 coins.

Token 10371:
You can run a simulation to estimate this, rather thantrying to compute an exact answer.

Token 10372:
13.11 We wish to transmit an n-bit message to a receiving agent.

Token 10373:
The bits in the message are independently corrupted (ﬂipped) during transmission with /epsilon1probability each.

Token 10374:
With an extra parity bit sent along with the original information, a message can be corrected by the receiver

Token 10375:
508 Chapter 13. Quantifying Uncertainty if at most one bit in the entire message (including the parity bit) has been corrupted.

Token 10376:
Suppose we want to ensure that the correct message is received with probability at least 1−δ.W h a ti s the maximum feasible value of n?

Token 10377:
Calculate this value for the case /epsilon1=0.001,δ=0.01. 13.12 Show that the three forms of independence in Equation (13.11) are equivalent.

Token 10378:
13.13 Consider two medical tests, A and B, for a virus.

Token 10379:
Test A is 95% effective at recog- nizing the virus when it is present, but has a 10% false positive rate (indicating that the virusis present, when it is not).

Token 10380:
Test B is 90% effective at recognizing the virus, but has a 5% falsepositive rate. The two tests use independent methods of identifying the virus.

Token 10381:
The virus iscarried by 1% of all people.

Token 10382:
Say that a person is tested for the virus using only one of the tests,and that test comes back positive for carrying the virus.

Token 10383:
Which test returning positive is moreindicative of someone really carrying the virus? Justify your answer mathematically.

Token 10384:
13.14 Suppose you are given a coin that lands heads with probability xandtails with probability 1−x.

Token 10385:
Are the outcomes of successive ﬂips of the coin independent of each other given that you know the value of x?

Token 10386:
Are the outcomes of successive ﬂips of the coin independent of each other if you do notknow the value of x? Justify your answer.

Token 10387:
13.15 After your yearly checkup, the doctor has bad news and good news.

Token 10388:
The bad news is that you tested positive for a serious disease and that the test is 99% accurate (i.e., theprobability of testing positive when you do have the disease is 0.99, as is the probability oftesting negative when you don’t have the disease).

Token 10389:
The good news is that this is a rare disease,striking only 1 in 10,000 people of your age.

Token 10390:
Why is it good news that the disease is rare?What are the chances that you actually have the disease?

Token 10391:
13.16 It is quite often useful to consider the effect of some speciﬁc propositions in the context of some general background evidence that remains ﬁxed, rather than in the completeabsence of information.

Token 10392:
The following questions ask you to prove more general versions ofthe product rule and Bayes’ rule, with respect to some background evidence e: a.

Token 10393:
Prove the conditionalized version of the general product rule: P(X,Y|e)=P(X|Y,e)P(Y|e). b.

Token 10394:
Prove the conditionalized version of Bayes’ rule in Equation (13.13).

Token 10395:
13.17 Show that the statement of conditional independence P(X,Y|Z)=P(X|Z)P(Y|Z) is equivalent to each of the statements P(X |Y,Z)=P(X|Z)and P(B|X,Z)=P(Y|Z).

Token 10396:
13.18 Suppose you are given a bag containing nunbiased coins.

Token 10397:
You are told that n−1of these coins are normal, with heads on one side and tails on the other, whereas one coin is a fake, with heads on both sides.

Token 10398:
a. Suppose you reach into the bag, pick out a coin at random, ﬂip it, and get a head.

Token 10399:
What is the (conditional) probability that the coin you chose is the fake coin?

Token 10400:
Exercises 509 b. Suppose you continue ﬂipping the coin for a total of ktimes after picking it and see k heads.

Token 10401:
Now what is the conditional probability that you picked the fake coin?

Token 10402:
c. Suppose you wanted to decide whether the chosen coin was fake by ﬂipping it ktimes.

Token 10403:
The decision procedure returns fake if allkﬂips come up heads; otherwise it returns normal .

Token 10404:
What is the (unconditional) probability that this procedure makes an error?

Token 10405:
13.19 In this exercise, you will complete the normalization calculation for the meningitis example.

Token 10406:
First, make up a suitable value for P(s|¬m), and use it to calculate unnormalized values for P(m|s)andP(¬m|s)(i.e., ignoring the P(s)term in the Bayes’ rule expression, Equation (13.14)).

Token 10407:
Now normalize these values so that they add to 1. 13.20 LetX,Y,Zbe Boolean random variables.

Token 10408:
Label the eight entries in the joint dis- tribution P(X,Y,Z )asathrough h. Express the statement that XandYare conditionally independent given Z, as a set of equations relating athrough h.H o w m a n y nonredundant equations are there?

Token 10409:
13.21 (Adapted from Pearl (1988).) Suppose you are a witness to a nighttime hit-and-run accident involving a taxi in Athens.

Token 10410:
All taxis in Athens are blue or green. You swear, under oath, that the taxi was blue.

Token 10411:
Extensive testing shows that, under the dim lighting conditions,discrimination between blue and green is 75% reliable. a.

Token 10412:
Is it possible to calculate the most likely color for the taxi?

Token 10413:
( Hint: distinguish carefully between the proposition that the taxi isblue and the proposition that it appears blue.) b.

Token 10414:
What if you know that 9 out of 10 Athenian taxis are green?

Token 10415:
13.22 Text categorization is the task of assigning a given document to one of a ﬁxed set of categories on the basis of the text it contains.

Token 10416:
Naive Bayes models are often used for this task.

Token 10417:
In these models, the query variable is the document category, and the “effect” variablesare the presence or absence of each word in the language; the assumption is that words occurindependently in documents, with frequencies determined by the document category.

Token 10418:
a. Explain precisely how such a model can be constructed, given as “training data” a set of documents that have been assigned to categories. b.

Token 10419:
Explain precisely how to categorize a new document. c. Is the conditional independence assumption reasonable? Discuss.

Token 10420:
13.23 In our analysis of the wumpus world, we used the fact that each square contains a pit with probability 0.2, independently of the contents of the other squares.

Token 10421:
Suppose instead that exactly N/5pits are scattered at random among the Nsquares other than [1,1]. Are the variables Pi,jandPk,lstill independent?

Token 10422:
What is the joint distribution P(P1,1,...,P 4,4) now? Redo the calculation for the probabilities of pits in [1,3] and [2,2].

Token 10423:
13.24 Redo the probability calculation for pits in [1,3] and [2,2], assuming that each square contains a pit with probability 0.01, independent of the other squares.

Token 10424:
What can you sayabout the relative performance of a logical versus a probabilistic agent in this case?

Token 10425:
13.25 Implement a hybrid probabilistic agent for the wumpus world, based on the hybrid agent in Figure 7.20 and the probabilistic inference procedure outlined in this chapter.

Token 10426:
14PROBABILISTIC REASONING In which we explain how to build network models to reason under uncertainty according to the laws of probability theory.

Token 10427:
Chapter 13 introduced the basic elements of probability theory and noted the importance of independence and conditional independence relationships in simplifying probabilistic repre- sentations of the world.

Token 10428:
This chapter introduces a systematic way to represent such relation-ships explicitly in the form of Bayesian networks .

Token 10429:
We deﬁne the syntax and semantics of these networks and show how they can be used to capture uncertain knowledge in a natu-ral and efﬁcient way.

Token 10430:
We then show how probabilistic inference, although computationallyintractable in the worst case, can be done efﬁciently in many practical situations.

Token 10431:
We alsodescribe a variety of approximate inference algorithms that are often applicable when exactinference is infeasible.

Token 10432:
We explore ways in which probability theory can be applied to worldswith objects and relations—that is, to ﬁrst-order , as opposed to propositional , representations.

Token 10433:
Finally, we survey alternative approaches to uncertain reasoning.

Token 10434:
14.1 R EPRESENTING KNOWLEDGE IN AN UNCERTAIN DOMAIN In Chapter 13, we saw that the full joint probability distribution can answer any question aboutthe domain, but can become intractably large as the number of variables grows.

Token 10435:
Furthermore,specifying probabilities for possible worlds one by one is unnatural and tedious.

Token 10436:
We also saw that independence and conditional independence relationships among vari- ables can greatly reduce the number of probabilities that need to be speciﬁed in order to deﬁne the full joint distribution.

Token 10437:
This section introduces a data structure called a Bayesian network 1BAYESIAN NETWORK to represent the dependencies among variables.

Token 10438:
Bayesian networks can represent essentially anyfull joint probability distribution and in many cases can do so very concisely.

Token 10439:
1This is the most common name, but there are many synonyms, including belief network ,probabilistic net- work ,causal network ,a n d knowledge map .

Token 10440:
In statistics, the term graphical model refers to a somewhat broader class that includes Bayesian networks.

Token 10441:
An extension of Bayesian networks called a decision network or inﬂuence diagram is covered in Chapter 16. 510

Token 10442:
Section 14.1.

Token 10443:
Representing Knowledge in an Uncertain Domain 511 A Bayesian network is a directed graph in which each node is annotated with quantita- tive probability information.

Token 10444:
The full speciﬁcation is as follows: 1. Each node corresponds to a random variable, which may be discrete or continuous. 2.

Token 10445:
A set of directed links or arrows connects pairs of nodes. If there is an arrow from node Xto node Y,Xis said to be a parent ofY.

Token 10446:
The graph has no directed cycles (and hence is a directed acyclic graph, or DAG. 3.

Token 10447:
Each node Xihas a conditional probability distribution P(Xi|Parents (Xi))that quan- tiﬁes the effect of the parents on the node.

Token 10448:
The topology of the network—the set of nodes and links—speciﬁes the conditional indepen- dence relationships that hold in the domain, in a way that will be made precise shortly.

Token 10449:
Theintuitive meaning of an arrow is typically that Xhas a direct inﬂuence onY, which suggests that causes should be parents of effects.

Token 10450:
It is usually easy for a domain expert to decide whatdirect inﬂuences exist in the domain—much easier, in fact, than actually specifying the prob-abilities themselves.

Token 10451:
Once the topology of the Bayesian network is laid out, we need onlyspecify a conditional probability distribution for each variable, given its parents.

Token 10452:
We willsee that the combination of the topology and the conditional distributions sufﬁces to specify(implicitly) the full joint distribution for all the variables.

Token 10453:
Recall the simple world described in Chapter 13, consisting of the variables Toothache , Cavity ,Catch ,a n dWeather .

Token 10454:
We argued that Weather is independent of the other vari- ables; furthermore, we argued that Toothache andCatch are conditionally independent, givenCavity .

Token 10455:
These relationships are represented by the Bayesian network structure shown in Figure 14.1.

Token 10456:
Formally, the conditional independence of Toothache andCatch ,g i v e n Cavity , is indicated by the absence of a link between Toothache andCatch .

Token 10457:
Intuitively, the network represents the fact that Cavity is a direct cause of Toothache andCatch , whereas no direct causal relationship exists between Toothache andCatch .

Token 10458:
Now consider the following example, which is just a little more complex. You have a new burglar alarm installed at home.

Token 10459:
It is fairly reliable at detecting a burglary, but alsoresponds on occasion to minor earthquakes.

Token 10460:
(This example is due to Judea Pearl, a residentof Los Angeles—hence the acute interest in earthquakes.)

Token 10461:
You also have two neighbors, Johnand Mary, who have promised to call you at work when they hear the alarm.

Token 10462:
John nearlyalways calls when he hears the alarm, but sometimes confuses the telephone ringing with WeatherCavity Toothache Catch Figure 14.1 A simple Bayesian network in which Weather is independent of the other three variables and Toothache andCatch are conditionally independent, given Cavity .

Token 10463:
512 Chapter 14.

Token 10464:
Probabilistic Reasoning .001P(B) AlarmEarthquake MaryCalls JohnCallsBurglary AP(J) t f.90 .05B t t f fE t f t fP(A) .95 .29 .001.94.002P(E) AP(M) t f.70 .01 Figure 14.2 A typical Bayesian network, showing both the topology an d the conditional probability tables (CPTs).

Token 10465:
In the CPTs, the letters B,E,A,J,a n dMstand for Burglary , Earthquake ,Alarm ,JohnCalls ,a n dMaryCalls , respectively.

Token 10466:
the alarm and calls then, too. Mary, on the other hand, likes rather loud music and often misses the alarm altogether.

Token 10467:
Given the evidence of who has or has not called, we would liketo estimate the probability of a burglary.

Token 10468:
A Bayesian network for this domain appears in Figure 14.2.

Token 10469:
The network structure shows that burglary and earthquakes directly affect the probability of the alarm’s going off,but whether John and Mary call depends only on the alarm.

Token 10470:
The network thus representsour assumptions that they do not perceive burglaries directly, they do not notice minor earth-quakes, and they do not confer before calling.

Token 10471:
The conditional distributions in Figure 14.2 are shown as a conditional probability table , or CPT.

Token 10472:
(This form of table can be used for discrete variables; other representations, CONDITIONAL PROBABILITY TABLE including those suitable for continuous variables, are described in Section 14.2.)

Token 10473:
Each row in a CPT contains the conditional probability of each node value for a conditioning case .

Token 10474:
CONDITIONINGCASE A conditioning case is just a possible combination of values for the parent nodes—a minia- ture possible world, if you like.

Token 10475:
Each row must sum to 1, because the entries represent an exhaustive set of cases for the variable.

Token 10476:
For Boolean variables, once you know that the prob- ability of a true value is p, the probability of false must be 1 – p, so we often omit the second number, as in Figure 14.2.

Token 10477:
In general, a table for a Boolean variable with kBoolean parents contains 2kindependently speciﬁable probabilities.

Token 10478:
A node with no parents has only one row, representing the prior probabilities of each possible value of the variable.

Token 10479:
Notice that the network does not have nodes corresponding to Mary’s currently listening to loud music or to the telephone ringing and confusing John.

Token 10480:
These factors are summarized in the uncertainty associated with the links from Alarm toJohnCalls andMaryCalls .T h i s shows both laziness and ignorance in operation: it would be a lot of work to ﬁnd out why thosefactors would be more or less likely in any particular case, and we have no reasonable way toobtain the relevant information anyway.

Token 10481:
The probabilities actually summarize a potentially

Token 10482:
Section 14.2.

Token 10483:
The Semantics of Bayesian Networks 513 inﬁnite set of circumstances in which the alarm might fail to go off (high humidity, power failure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.)

Token 10484:
or John or Marymight fail to call and report it (out to lunch, on vacation, temporarily deaf, passing helicopter,etc.).

Token 10485:
In this way, a small agent can cope with a very large world, at least approximately.

Token 10486:
Thedegree of approximation can be improved if we introduce additional relevant information.

Token 10487:
14.2 T HESEMANTICS OF BAYESIAN NETWORKS The previous section described what a network is, but not what it means.

Token 10488:
There are twoways in which one can understand the semantics of Bayesian networks.

Token 10489:
The ﬁrst is to seethe network as a representation of the joint probability distribution.

Token 10490:
The second is to viewit as an encoding of a collection of conditional independence statements.

Token 10491:
The two views areequivalent, but the ﬁrst turns out to be helpful in understanding how to construct networks, whereas the second is helpful in designing inference procedures.

Token 10492:
14.2.1 Representing the full joint distribution Viewed as a piece of “syntax,” a Bayesian network is a directed acyclic graph with somenumeric parameters attached to each node.

Token 10493:
One way to deﬁne what the network means—itssemantics—is to deﬁne the way in which it represents a speciﬁc joint distribution over all thevariables.

Token 10494:
To do this, we ﬁrst need to retract (temporarily) what we said earlier about the pa-rameters associated with each node.

Token 10495:
We said that those parameters correspond to conditionalprobabilities P(X i|Parents (Xi)); this is a true statement, but until we assign semantics to the network as a whole, we should think of them just as numbers θ(Xi|Parents (Xi)).

Token 10496:
A generic entry in the joint distribution is the probability of a conjunction of particular assignments to each variable, such as P(X1=x1∧...∧Xn=xn).

Token 10497:
We use the notation P(x1,...,x n)as an abbreviation for this.

Token 10498:
The value of this entry is given by the formula P(x1,...,x n)=n/productdisplay i=1θ(xi|parents (Xi)), (14.1) where parents (Xi)denotes the values of Parents (Xi)that appear in x1,...,x n. Thus, each entry in the joint distribution is represented by the product of the appropriate elementsof the conditional probability tables (CPTs) in the Bayesian network.

Token 10499:
From this deﬁnition, it is easy to prove that the parameters θ(X i|Parents (Xi))are exactly the conditional probabilities P(Xi|Parents (Xi))implied by the joint distribution (see Exercise 14.2).

Token 10500:
Hence, we can rewrite Equation (14.1) as P(x1,...,x n)=n/productdisplay i=1P(xi|parents (Xi)).

Token 10501:
(14.2) In other words, the tables we have been calling conditional probability tables really arecon- ditional probability tables according to the semantics deﬁned in Equation (14.1).

Token 10502:
To illustrate this, we can calculate the probability that the alarm has sounded, but neither a burglary nor an earthquake has occurred, and both John and Mary call.

Token 10503:
We multiply entries

Token 10504:
514 Chapter 14.

Token 10505:
Probabilistic Reasoning from the joint distribution (using single-letter names for the variables): P(j,m,a,¬b,¬e)=P(j|a)P(m|a)P(a|¬b∧¬e)P(¬b)P(¬e) =0.90×0.70×0.001×0.999×0.998 = 0 .000628 .

Token 10506:
Section 13.3 explained that the full joint distribution can be used to answer any query about the domain.

Token 10507:
If a Bayesian network is a representation of the joint distribution, then it too canbe used to answer any query, by summing all the relevant joint entries.

Token 10508:
Section 14.4 explainshow to do this, but also describes methods that are much more efﬁcient.

Token 10509:
A method for constructing Bayesian networks Equation (14.2) deﬁnes what a given Bayesian network means.

Token 10510:
The next step is to explain how to construct a Bayesian network in such a way that the resulting joint distribution is a good representation of a given domain.

Token 10511:
We will now show that Equation (14.2) implies certainconditional independence relationships that can be used to guide the knowledge engineer inconstructing the topology of the network.

Token 10512:
First, we rewrite the entries in the joint distributionin terms of conditional probability, using the product rule (see page 486): P(x 1,...,x n)=P(xn|xn−1,...,x 1)P(xn−1,...,x 1).

Token 10513:
Then we repeat the process, reducing each conjunctive probability to a conditional probability and a smaller conjunction.

Token 10514:
We end up with one big product: P(x1,...,x n)=P(xn|xn−1,...,x 1)P(xn−1|xn−2,...,x 1)···P(x2|x1)P(x1) =n/productdisplay i=1P(xi|xi−1,...,x 1).

Token 10515:
This identity is called the chain rule . It holds for any set of random variables.

Token 10516:
Comparing it CHAIN RULE with Equation (14.2), we see that the speciﬁcation of the joint distribution is equivalent to the general assertion that, for every variable Xiin the network, P(Xi|Xi−1,...,X 1)=P(Xi|Parents (Xi)), (14.3) provided that Parents (Xi)⊆{Xi−1,...,X 1}.

Token 10517:
This last condition is satisﬁed by numbering the nodes in a way that is consistent with the partial order implicit in the graph structure.

Token 10518:
What Equation (14.3) says is that the Bayesian network is a correct representation of the domain only if each node is conditionally independent of its other predecessors in the node ordering, given its parents.

Token 10519:
We can satisfy this condition with this methodology: 1.Nodes: First determine the set of variables that are required to model the domain.

Token 10520:
Now order them,{X1,...,X n}.

Token 10521:
Any order will work, but the resulting network will be more compact if the variables are ordered such that causes precede effects.

Token 10522:
2.Links: Fori=1t o ndo: •Choose, from X1,...,X i−1, a minimal set of parents for Xi, such that Equa- tion (14.3) is satisﬁed.

Token 10523:
•For each parent insert a link from the parent to Xi. •CPTs: Write down the conditional probability table, P(Xi|Parents (Xi)).

Token 10524:
Section 14.2.

Token 10525:
The Semantics of Bayesian Networks 515 Intuitively, the parents of node Xishould contain all those nodes in X1, ...,X i−1that directly inﬂuence Xi.

Token 10526:
For example, suppose we have completed the network in Figure 14.2 except for the choice of parents for MaryCalls .MaryCalls is certainly inﬂuenced by whether there is a Burglary or anEarthquake , but not directly inﬂuenced.

Token 10527:
Intuitively, our knowledge of the domain tells us that these events inﬂuence Mary’s calling behavior only through their effect on the alarm.

Token 10528:
Also, given the state of the alarm, whether John calls has no inﬂuence on Mary’s calling.

Token 10529:
Formally speaking, we believe that the following conditional independencestatement holds: P(MaryCalls|JohnCalls ,Alarm ,Earthquake ,Burglary )=P(MaryCalls|Alarm ).

Token 10530:
Thus,Alarm will be the only parent node for MaryCalls .

Token 10531:
Because each node is connected only to earlier nodes, this construction method guaran- tees that the network is acyclic.

Token 10532:
Another important property of Bayesian networks is that theycontain no redundant probability values.

Token 10533:
If there is no redundancy, then there is no chancefor inconsistency: it is impossible for the knowledge engineer or domain expert to create a Bayesian network that violates the axioms of probability.

Token 10534:
Compactness and node ordering As well as being a complete and nonredundant representation of the domain, a Bayesian net- work can often be far more compact than the full joint distribution.

Token 10535:
This property is what makes it feasible to handle domains with many variables.

Token 10536:
The compactness of Bayesian net-works is an example of a general property of locally structured (also called sparse )s y s t e m s .

Token 10537:
LOCALLY STRUCTURED SPARSE In a locally structured system, each subcomponent interacts directly with only a bounded number of other components, regardless of the total number of components.

Token 10538:
Local structureis usually associated with linear rather than exponential growth in complexity.

Token 10539:
In the case ofBayesian networks, it is reasonable to suppose that in most domains each random variableis directly inﬂuenced by at most kothers, for some constant k. If we assume nBoolean variables for simplicity, then the amount of information needed to specify each conditional probability table will be at most 2 knumbers, and the complete network can be speciﬁed by n2knumbers.

Token 10540:
In contrast, the joint distribution contains 2nnumbers. To make this concrete, suppose we have n=3 0 nodes, each with ﬁve parents ( k=5).

Token 10541:
Then the Bayesian network requires 960 numbers, but the full joint distribution requires over a billion.

Token 10542:
There are domains in which each variable can be inﬂuenced directly by all the others, so that the network is fully connected.

Token 10543:
Then specifying the conditional probability tables re-quires the same amount of information as specifying the joint distribution.

Token 10544:
In some domains,there will be slight dependencies that should strictly be included by adding a new link.

Token 10545:
Butif these dependencies are tenuous, then it may not be worth the additional complexity in thenetwork for the small gain in accuracy.

Token 10546:
For example, one might object to our burglary net-work on the grounds that if there is an earthquake, then John and Mary would not call even if they heard the alarm, because they assume that the earthquake is the cause.

Token 10547:
Whether to add the link from Earthquake toJohnCalls andMaryCalls (and thus enlarge the tables) depends on comparing the importance of getting more accurate probabilities with the cost ofspecifying the extra information.

Token 10548:
516 Chapter 14.

Token 10549:
Probabilistic Reasoning JohnCallsMaryCalls Alarm Burglary EarthquakeMaryCalls AlarmEarthquake BurglaryJohnCalls (a) (b) Figure 14.3 Network structure depends on orde r of introduction.

Token 10550:
In each network, we have introduced nodes in top-to-bottom order.

Token 10551:
Even in a locally structured domain, we will get a compact Bayesian network only if we choose the node ordering well.

Token 10552:
What happens if we happen to choose the wrong or-der? Consider the burglary example again.

Token 10553:
Suppose we decide to add the nodes in the orderMaryCalls ,JohnCalls ,Alarm ,Burglary ,Earthquake .

Token 10554:
We then get the somewhat more complicated network shown in Figure 14.3(a). The process goes as follows: •Adding MaryCalls : No parents.

Token 10555:
•Adding JohnCalls : If Mary calls, that probably means the alarm has gone off, which of course would make it more likely that John calls.

Token 10556:
Therefore, JohnCalls needs MaryCalls as a parent.

Token 10557:
•Adding Alarm : Clearly, if both call, it is more likely that the alarm has gone off than if just one or neither calls, so we need both MaryCalls andJohnCalls as parents.

Token 10558:
•Adding Burglary : If we know the alarm state, then the call from John or Mary might give us information about our phone ringing or Mary’s music, but not about burglary: P(Burglary|Alarm ,JohnCalls ,MaryCalls )=P(Burglary|Alarm ).

Token 10559:
Hence we need just Alarm as parent. •Adding Earthquake : If the alarm is on, it is more likely that there has been an earth- quake.

Token 10560:
(The alarm is an earthquake detector of sorts.)

Token 10561:
But if we know that there has been a burglary, then that explains the alarm, and the probability of an earthquake would be only slightly above normal.

Token 10562:
Hence, we need both Alarm andBurglary as parents.

Token 10563:
The resulting network has two more links than the original network in Figure 14.2 and re- quires three more probabilities to be speciﬁed.

Token 10564:
What’s worse, some of the links represent tenuous relationships that require difﬁcult and unnatural probability judgments, such as as-

Token 10565:
Section 14.2. The Semantics of Bayesian Networks 517 sessing the probability of Earthquake ,g i v e n Burglary andAlarm .

Token 10566:
This phenomenon is quite general and is related to the distinction between causal anddiagnostic models intro- duced in Section 13.5.1 (see also Exercise 8.13).

Token 10567:
If we try to build a diagnostic model withlinks from symptoms to causes (as from MaryCalls toAlarm orAlarm toBurglary ), we end up having to specify additional dependencies between otherwise independent causes (and often between separately occurring symptoms as well).

Token 10568:
If we stick to a causal model, we end up having to specify fewer numbers, and the numbers will often be easier to come up with.

Token 10569:
In the domain of medicine, for example, it has been shown by Tversky and Kahneman (1982)that expert physicians prefer to give probability judgments for causal rules rather than fordiagnostic ones.

Token 10570:
Figure 14.3(b) shows a very bad node ordering: MaryCalls ,JohnCalls ,Earthquake , Burglary ,Alarm .

Token 10571:
This network requires 31 distinct probabilities to be speciﬁed—exactly the same number as the full joint distribution.

Token 10572:
It is important to realize, however, that any of thethree networks can represent exactly the same joint distribution .

Token 10573:
The last two versions simply fail to represent all the conditional independence relationships and hence end up specifying alot of unnecessary numbers instead.

Token 10574:
14.2.2 Conditional independence relations in Bayesian networks We have provided a “numerical” semantics for Bayesian networks in terms of the represen-tation of the full joint distribution, as in Equation (14.2).

Token 10575:
Using this semantics to derive a method for constructing Bayesian networks, we were led to the consequence that a node is conditionally independent of its other predecessors, given its parents.

Token 10576:
It turns out that wecan also go in the other direction.

Token 10577:
We can start from a “topological” semantics that speciﬁesthe conditional independence relationships encoded by the graph structure, and from this wecan derive the “numerical” semantics.

Token 10578:
The topological semantics 2speciﬁes that each vari- able is conditionally independent of its non- descendants , given its parents.

Token 10579:
For example, in DESCENDANT Figure 14.2, JohnCalls is independent of Burglary ,Earthquake ,a n dMaryCalls given the value of Alarm .

Token 10580:
The deﬁnition is illustrated in Figure 14.4(a).

Token 10581:
From these conditional inde- pendence assertions and the interpretation of the network parameters θ(Xi|Parents (Xi)) as speciﬁcations of conditional probabilities P(Xi|Parents (Xi)), the full joint distribution given in Equation (14.2) can be reconstructed.

Token 10582:
In this sense, the “numerical” semantics and the “topological” semantics are equivalent.

Token 10583:
Another important independence property is implied by the topological semantics: a node is conditionally independent of all other nodes in the network, given its parents, children,and children’s parents—that is, given its Markov blanket .

Token 10584:
(Exercise 14.7 asks you to prove MARKOV BLANKET this.)

Token 10585:
For example, Burglary is independent of JohnCalls andMaryCalls ,g i v e n Alarm and Earthquake . This property is illustrated in Figure 14.4(b).

Token 10586:
2There is also a general topological criterion called d-separation for deciding whether a set of nodes Xis conditionally independent of another set Y,g i v e nat h i r ds e t Z.

Token 10587:
The criterion is rather complicated and is not needed for deriving the algorithms in this chapter, so we omit it.

Token 10588:
Details may be found in Pearl (1988) or Darwiche(2009). Shachter (1998) gives a more intuitive method of ascertaining d-separation.

Token 10589:
518 Chapter 14. Probabilistic Reasoning . . .. . . U1 XUm YnZnj Y1Z1j . . .. . .

Token 10590:
U1 Um YnZnj Y1Z1jX (a) (b) Figure 14.4 (a) A node Xis conditionally independent of its non-descendants (e.g., the Zijs) given its parents (the Uis shown in the gray area).

Token 10591:
(b) A node Xis conditionally independent of all other nodes in the network given its Markov blanket (the gray area).

Token 10592:
14.3 E FFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS Even if the maximum number of parents kis smallish, ﬁlling in the CPT for a node requires up toO(2k)numbers and perhaps a great deal of experience with all the possible conditioning cases.

Token 10593:
In fact, this is a worst-case scenario in which the relationship between the parents andthe child is completely arbitrary.

Token 10594:
Usually, such relationships are describable by a canonical distribution that ﬁts some standard pattern.

Token 10595:
In such cases, the complete table can be speciﬁed CANONICAL DISTRIBUTION by naming the pattern and perhaps supplying a few parameters—much easier than supplying an exponential number of parameters.

Token 10596:
The simplest example is provided by deterministic nodes .

Token 10597:
A deterministic node hasDETERMINISTIC NODES its value speciﬁed exactly by the values of its parents, with no uncertainty.

Token 10598:
The relationship can be a logical one: for example, the relationship between the parent nodes Canadian ,US, Mexican and the child node NorthAmerican is simply that the child is the disjunction of the parents.

Token 10599:
The relationship can also be numerical: for example, if the parent nodes arethe prices of a particular model of car at several dealers and the child node is the price thata bargain hunter ends up paying, then the child node is the minimum of the parent values;or if the parent nodes are a lake’s inﬂows (rivers, runoff, precipitation) and outﬂows (rivers,evaporation, seepage) and the child is the change in the water level of the lake, then the valueof the child is the sum of the inﬂow parents minus the sum of the outﬂow parents.

Token 10600:
Uncertain relationships can often be characterized by so-called noisy logical relation- ships.

Token 10601:
The standard example is the noisy-OR relation, which is a generalization of the log- NOISY-OR ical OR.

Token 10602:
In propositional logic, we might say that Fever is true if and only if Cold ,Flu,o r Malaria is true.

Token 10603:
The noisy-OR model allows for uncertainty about the ability of each par- ent to cause the child to be true—the causal relationship between parent and child may be

Token 10604:
Section 14.3. Efﬁcient Representation of Conditional Distributions 519 inhibited , and so a patient could have a cold, but not exhibit a fever.

Token 10605:
The model makes two assumptions. First, it assumes that all the possible causes are listed.

Token 10606:
(If some are missing,we can always add a so-called leak node that covers “miscellaneous causes.”) Second, it LEAK NODE assumes that inhibition of each parent is independent of inhibition of any other parents: for example, whatever inhibits Malaria from causing a fever is independent of whatever inhibits Flufrom causing a fever.

Token 10607:
Given these assumptions, Fever isfalse if and only if all its true parents are inhibited, and the probability of this is the product of the inhibition probabilitiesqfor each parent.

Token 10608:
Let us suppose these individual inhibition probabilities are as follows: q cold=P(¬fever|cold,¬ﬂu,¬malaria )=0.6, qﬂu=P(¬fever|¬cold,ﬂu,¬malaria )=0.2, qmalaria =P(¬fever|¬cold,¬ﬂu,malaria )=0.1.

Token 10609:
Then, from this information and the noisy-OR assumptions, the entire CPT can be built.

Token 10610:
The general rule is that P(xi|parents (Xi)) = 1−/productdisplay {j:Xj=true}qj, where the product is taken over the parents that are set to true for that row of the CPT.

Token 10611:
The following table illustrates this calculation: Cold Flu Malaria P(Fever) P(¬Fever) FFF 0.0 1.0 FFT 0.9 0.1 FTF 0.8 0.2 FTT 0.98 0.02 = 0 .2×0.1 TFF 0.4 0.6 TFT 0.94 0.06 = 0 .6×0.1 TTF 0.88 0.12 = 0 .6×0.2 TTT 0.988 0.012 = 0 .6×0.2×0.1 In general, noisy logical relationships in which a variable depends on kparents can be de- scribed using O(k)parameters instead of O(2k)for the full conditional probability table.

Token 10612:
This makes assessment and learning much easier. For example, the CPCS network (Prad- hanet al.

Token 10613:
, 1994) uses noisy-OR and noisy-MAX distributions to model relationships among diseases and symptoms in internal medicine.

Token 10614:
With 448 nodes and 906 links, it requires only8,254 values instead of 133,931,430 for a network with full CPTs.

Token 10615:
Bayesian nets with continuous variables Many real-world problems involve continuous quantities, such as height, mass, temperature, and money; in fact, much of statistics deals with random variables whose domains are contin- uous.

Token 10616:
By deﬁnition, continuous variables have an inﬁnite number of possible values, so it isimpossible to specify conditional probabilities explicitly for each value.

Token 10617:
One possible way tohandle continuous variables is to avoid them by using discretization —that is, dividing up the DISCRETIZATION

Token 10618:
520 Chapter 14.

Token 10619:
Probabilistic Reasoning Harvest Subsidy BuysCost Figure 14.5 A simple network with discrete variables ( Subsidy andBuys ) and continuous variables ( Harvest andCost ).

Token 10620:
possible values into a ﬁxed set of intervals. For example, temperatures could be divided into (<0oC), (0oC−100oC), and ( >100oC).

Token 10621:
Discretization is sometimes an adequate solution, but often results in a considerable loss of accuracy and very large CPTs.

Token 10622:
The most com- mon solution is to deﬁne standard families of probability density functions (see Appendix A) that are speciﬁed by a ﬁnite number of parameters .

Token 10623:
For example, a Gaussian (or normal) PARAMETER distribution N(μ,σ2)(x)has the mean μand the variance σ2as parameters.

Token 10624:
Yet another solution—sometimes called a nonparametric representation—is to deﬁne the conditional NONPARAMETRIC distribution implicitly with a collection of instances, each containing speciﬁc values of the parent and child variables.

Token 10625:
We explore this approach further in Chapter 18. A network with both discrete and continuous variables is called a hybrid Bayesian network .

Token 10626:
To specify a hybrid network, we have to specify two new kinds of distributions:HYBRID BAYESIAN NETWORK the conditional distribution for a continuous variable given discrete or continuous parents; and the conditional distribution for a discrete variable given continuous parents.

Token 10627:
Consider thesimple example in Figure 14.5, in which a customer buys some fruit depending on its cost, which depends in turn on the size of the harvest and whether the government’s subsidy scheme is operating.

Token 10628:
The variable Cost is continuous and has continuous and discrete parents; the variable Buys is discrete and has a continuous parent.

Token 10629:
For the Cost variable, we need to specify P(Cost|Harvest ,Subsidy ).

Token 10630:
The discrete parent is handled by enumeration—that is, by specifying both P(Cost|Harvest ,subsidy ) andP(Cost|Harvest ,¬subsidy ).

Token 10631:
To handle Harvest , we specify how the distribution over the cost cdepends on the continuous value hofHarvest .

Token 10632:
In other words, we specify the parameters of the cost distribution as a function of h. The most common choice is the linear Gaussian distribution, in which the child has a Gaussian distribution whose mean μvaries LINEAR GAUSSIAN linearly with the value of the parent and whose standard deviation σis ﬁxed.

Token 10633:
We need two distributions, one for subsidy and one for¬subsidy , with different parameters: P(c|h,subsidy )=N(ath+bt,σ2 t)(c)=1 σt√ 2πe−1 2“c−(ath+bt) σt”2 P(c|h,¬subsidy )=N(afh+bf,σ2 f)(c)=1 σf√ 2πe−1 2„ c−(afh+bf) σf«2 .

Token 10634:
For this example, then, the conditional distribution for Cost is speciﬁed by naming the linear Gaussian distribution and providing the parameters at,bt,σt,af,bf,a n dσf.

Token 10635:
Figures 14.6(a)

Token 10636:
Section 14.3.

Token 10637:
Efﬁcient Representation of Conditional Distributions 521 0246810Cost c024681012 Harvest h00.10.20.30.4P(c | h,subsidy ) 0246810Cost c024681012 Harvest h00.10.20.30.4P(c | h,¬subsidy ) 0246810Cost c024681012 Harvest h00.10.20.30.4P(c | h) (a) (b) (c) Figure 14.6 The graphs in (a) and (b) show the probability distribution over Cost as a function of Harvest size, with Subsidy true and false, respectively.

Token 10638:
Graph (c) shows the distribution P(Cost|Harvest ), obtained by summing over the two subsidy cases. and (b) show these two relationships.

Token 10639:
Notice that in each case the slope is negative, because cost decreases as supply increases.

Token 10640:
(Of course, the assumption of linearity implies that thecost becomes negative at some point; the linear model is reasonable only if the harvest size islimited to a narrow range.)

Token 10641:
Figure 14.6(c) shows the distribution P(c|h), averaging over the two possible values of Subsidy and assuming that each has prior probability 0.5.

Token 10642:
This shows that even with very simple models, quite interesting distributions can be represented.

Token 10643:
The linear Gaussian conditional distribution has some special properties.

Token 10644:
A network containing only continuous variables with linear Gaussian distributions has a joint distribu-tion that is a multivariate Gaussian distribution (see Appendix A) over all the variables (Exer-cise 14.9).

Token 10645:
Furthermore, the posterior distribution given any evidence also has this property.

Token 10646:
3 When discrete variables are added as parents (not as children) of continuous variables, the network deﬁnes a conditional Gaussian , or CG, distribution: given any assignment to theCONDITIONAL GAUSSIAN discrete variables, the distribution over the continuous variables is a multivariate Gaussian.

Token 10647:
Now we turn to the distributions for discrete variables with continuous parents. Con- sider, for example, the Buys node in Figure 14.5.

Token 10648:
It seems reasonable to assume that the customer will buy if the cost is low and will not buy if it is high and that the probability of buying varies smoothly in some intermediate region.

Token 10649:
In other words, the conditional distribu- tion is like a “soft” threshold function.

Token 10650:
One way to make soft thresholds is to use the integral of the standard normal distribution: Φ(x)=/integraldisplayx −∞N(0,1)(x)dx .

Token 10651:
Then the probability of Buys givenCost might be P(buys|Cost=c)=Φ ( (−c+μ)/σ), which means that the cost threshold occurs around μ, the width of the threshold region is pro- portional to σ, and the probability of buying decreases as cost increases.

Token 10652:
This probit distri- 3It follows that inference in linear Gaussian networks takes only O(n3)time in the worst case, regardless of the network topology.

Token 10653:
In Section 14.4, we see that inferen ce for networks of discrete variables is NP-hard.

Token 10654:
522 Chapter 14.

Token 10655:
Probabilistic Reasoning 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12P(c) Cost c 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12P(buys | c) Cost cLogit Probit (a) (b) Figure 14.7 (a) A normal (Gaussian) distribution for the cost threshold, centered on μ=6.0with standard deviation σ=1.0.

Token 10656:
(b) Logit and probit distributions for the probability ofbuys givencost, for the parameters μ=6.0andσ=1.0.

Token 10657:
bution (pronounced “pro-bit” and short for “probability unit”) is illustrated in Figure 14.7(a).PROBIT DISTRIBUTION The form can be justiﬁed by proposing that the underlying decision process has a hard thresh- old, but that the precise location of the threshold is subject to random Gaussian noise.

Token 10658:
An alternative to the probit model is the logit distribution (pronounced “low-jit”).

Token 10659:
It LOGIT DISTRIBUTION uses the logistic function 1/(1 +e−x)to produce a soft threshold: LOGISTIC FUNCTION P(buys|Cost=c)=1 1+exp(−2−c+μ σ).

Token 10660:
This is illustrated in Figure 14.7(b).

Token 10661:
The two distributions look similar, but the logit actually has much longer “tails.” The probit is often a better ﬁt to real situations, but the logit is some-times easier to deal with mathematically.

Token 10662:
It is used widely in neural networks (Chapter 20).Both probit and logit can be generalized to handle multiple continuous parents by taking alinear combination of the parent values.

Token 10663:
14.4 E XACT INFERENCE IN BAYESIAN NETWORKS The basic task for any probabilistic inference system is to compute the posterior probabilitydistribution for a set of query variables , given some observed event —that is, some assign- EVENT ment of values to a set of evidence variables .

Token 10664:
To simplify the presentation, we will consider only one query variable at a time; the algorithms can easily be extended to queries with mul-tiple variables.

Token 10665:
We will use the notation from Chapter 13: Xdenotes the query variable; E denotes the set of evidence variables E 1,...,E m,a n d eis a particular observed event; Ywill denotes the nonevidence, nonquery variables Y1,...,Y l(called the hidden variables ).

Token 10666:
Thus, HIDDEN VARIABLE the complete set of variables is X={X}∪E∪Y. A typical query asks for the posterior probability distribution P(X|e).

Token 10667:
Section 14.4.

Token 10668:
Exact Inference in Bayesian Networks 523 In the burglary network, we might observe the event in which JohnCalls =true and MaryCalls =true.

Token 10669:
We could then ask for, say, the probability that a burglary has occurred: P(Burglary|JohnCalls =true,MaryCalls =true)=/angbracketleft0.284,0.716/angbracketright.

Token 10670:
In this section we discuss exact algorithms for computing posterior probabilities and will consider the complexity of this task.

Token 10671:
It turns out that the general case is intractable, so Sec-tion 14.5 covers methods for approximate inference.

Token 10672:
14.4.1 Inference by enumeration Chapter 13 explained that any conditional probability can be computed by summing termsfrom the full joint distribution.

Token 10673:
More speciﬁcally, a query P(X|e)can be answered using Equation (13.9), which we repeat here for convenience: P(X|e)=αP(X,e)=α/summationdisplay yP(X,e,y).

Token 10674:
Now, a Bayesian network gives a complete representation of the full joint distribution.

Token 10675:
More speciﬁcally, Equation (14.2) on page 513 shows that the terms P(x,e,y)in the joint distri- bution can be written as products of conditional probabilities from the network.

Token 10676:
Therefore, a query can be answered using a Bayesian network by computing sums of products of condi- tional probabilities from the network.

Token 10677:
Consider the query P(Burglary|JohnCalls =true,MaryCalls =true). The hidden variables for this query are Earthquake andAlarm .

Token 10678:
From Equation (13.9), using initial letters for the variables to shorten the expressions, we have4 P(B|j, m)=αP(B,j,m )=α/summationdisplay e/summationdisplay aP(B,j,m,e,a, ).

Token 10679:
The semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms of CPT entries.

Token 10680:
For simplicity, we do this just for Burglary =true: P(b|j, m)=α/summationdisplay e/summationdisplay aP(b)P(e)P(a|b,e)P(j|a)P(m|a).

Token 10681:
To compute this expression, we have to add four terms, each computed by multiplying ﬁve numbers.

Token 10682:
In the worst case, where we have to sum out almost all the variables, the complexityof the algorithm for a network with nBoolean variables is O(n2 n).

Token 10683:
An improvement can be obtained from the following simple observations: the P(b) term is a constant and can be moved outside the summations over aande,a n dt h e P(e)term can be moved outside the summation over a.

Token 10684:
Hence, we have P(b|j, m)=αP(b)/summationdisplay eP(e)/summationdisplay aP(a|b,e)P(j|a)P(m|a).

Token 10685:
(14.4) This expression can be evaluated by looping through the variables in order, multiplying CPT entries as we go.

Token 10686:
For each summation, we also need to loop over the variable’s possible 4An expression such asP eP(a,e)means to sum P(A=a,E=e)for all possible values of e.W h e n Eis Boolean, there is an ambiguity in that P(e)is used to mean both P(E=true)andP(E=e), but it should be clear from context which is intended; in particular, in the context of a sum the latter is intended.

Token 10687:
524 Chapter 14. Probabilistic Reasoning values. The structure of this computation is shown in Figure 14.8.

Token 10688:
Using the numbers from Figure 14.2, we obtain P(b|j, m)=α×0.00059224 .

Token 10689:
The corresponding computation for ¬byields α×0.0014919 ; hence, P(B|j, m)=α/angbracketleft0.00059224 ,0.0014919/angbracketright≈/angbracketleft0.284,0.716/angbracketright.

Token 10690:
That is, the chance of a burglary, given calls from both neighbors, is about 28%.

Token 10691:
The evaluation process for the expression in Equation (14.4) is shown as an expression tree in Figure 14.8.

Token 10692:
The E NUMERATION -ASKalgorithm in Figure 14.9 evaluates such trees using depth-ﬁrst recursion.

Token 10693:
The algorithm is very similar in structure to the backtracking al-gorithm for solving CSPs (Figure 6.5) and the DPLL algorithm for satisﬁability (Figure 7.17).

Token 10694:
The space complexity of E NUMERATION -ASKis only linear in the number of variables: the algorithm sums over the full joint distribution without ever constructing it explicitly.

Token 10695:
Un-fortunately, its time complexity for a network with nBoolean variables is always O(2 n)— better than the O(n2n)for the simple approach described earlier, but still rather grim.

Token 10696:
Note that the tree in Figure 14.8 makes explicit the repeated subexpressions evalu- ated by the algorithm.

Token 10697:
The products P(j|a)P(m|a)andP(j|¬a)P(m|¬a)are computed twice, once for each value of e. The next section describes a general method that avoids such wasted computations.

Token 10698:
14.4.2 The variable elimination algorithm The enumeration algorithm can be improved substantially by eliminating repeated calcula-tions of the kind illustrated in Figure 14.8.

Token 10699:
The idea is simple: do the calculation once andsave the results for later use. This is a form of dynamic programming.

Token 10700:
There are several ver- sions of this approach; we present the variable elimination algorithm, which is the simplest.

Token 10701:
VARIABLE ELIMINATION Variable elimination works by evaluating expressions such as Equation (14.4) in right-to-left order (that is, bottom up in Figure 14.8).

Token 10702:
Intermediate results are stored, and summations over each variable are done only for those portions of the expression that depend on the variable.

Token 10703:
Let us illustrate this process for the burglary network.

Token 10704:
We evaluate the expression P(B|j, m)=αP(B)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright f1(B)/summationdisplay eP(e)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright f2(E)/summationdisplay aP(a|B,e)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright f3(A,B,E )P(j|a)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright f4(A)P(m|a)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright f5(A).

Token 10705:
Notice that we have annotated each part of the expression with the name of the corresponding factor ; each factor is a matrix indexed by the values of its argument variables.

Token 10706:
For example, FACTOR the factors f4(A)andf5(A)corresponding to P(j|a)andP(m|a)depend just on Abecause JandMare ﬁxed by the query.

Token 10707:
They are therefore two-element vectors: f4(A)=/parenleftbiggP(j|a) P(j|¬a)/parenrightbigg =/parenleftbigg0.90 0.05/parenrightbigg f5(A)=/parenleftbiggP(m|a) P(m|¬a)/parenrightbigg =/parenleftbigg0.70 0.01/parenrightbigg .

Token 10708:
f3(A,B,E )will be a 2×2×2matrix, which is hard to show on the printed page.

Token 10709:
(The “ﬁrst” element is given by P(a|b,e)=0.95and the “last” by P(¬a|¬b,¬e)=0.999.)

Token 10710:
In terms of factors, the query expression is written as P(B|j, m)=αf1(B)×/summationdisplay ef2(E)×/summationdisplay af3(A,B,E )×f4(A)×f5(A)

Token 10711:
Section 14.4.

Token 10712:
Exact Inference in Bayesian Networks 525 P(j|a) .90 P(m|a) .70 .01P(m|¬a).05P( j|¬a)P( j|a) .90 P(m|a) .70 .01P(m|¬a).05P( j|¬a)P(b) .001 P(e) .002P(¬e) .998 P(a|b,e) .95 .06P(¬a|b,¬e) .05P(¬a|b,e) .94P(a|b,¬e) Figure 14.8 The structure of the expression shown in Equation (14.4).

Token 10713:
The evaluation proceeds top down, multiplying values along each path and summing at the “+” nodes. Notice the repetition of the paths for jandm.

Token 10714:
function ENUMERATION -ASK(X,e,bn)returns a distribution over X inputs :X, the query variable e, observed values for variables E bn, a Bayes net with variables {X}∪ E∪Y/*Y= hidden variables */ Q(X)←a distribution over X, initially empty for each valuexiofXdo Q(xi)←ENUMERATE -ALL(bn.VARS,exi) where exiiseextended with X=xi return NORMALIZE (Q(X)) function ENUMERATE -ALL(vars ,e)returns a real number ifEMPTY ?

Token 10715:
(vars )then return 1.0 Y←FIRST (vars ) ifYhas value yine then return P(y|parents (Y))×ENUMERATE -ALL(REST(vars ),e) else return/summationtext yP(y|parents (Y))×ENUMERATE -ALL(REST(vars ),ey) where eyiseextended with Y=y Figure 14.9 The enumeration algorithm for answering queries on Bayesian networks.

Token 10716:
526 Chapter 14.

Token 10717:
Probabilistic Reasoning where the “×” operator is not ordinary matrix multiplication but instead the pointwise prod- uctoperation, to be described shortly.POINTWISE PRODUCT The process of evaluation is a process of summing out variables (right to left) from pointwise products of factors to produce new factors, eventually yielding a factor that is thesolution, i.e., the posterior distribution over the query variable.

Token 10718:
The steps are as follows: •First, we sum out Afrom the product of f 3,f4,a n d f5.

Token 10719:
This gives us a new 2×2factor f6(B,E)whose indices range over just BandE: f6(B,E)=/summationdisplay af3(A,B,E )×f4(A)×f5(A) =(f3(a,B,E )×f4(a)×f5(a)) + ( f3(¬a,B,E )×f4(¬a)×f5(¬a)).

Token 10720:
Now we are left with the expression P(B|j, m)=αf1(B)×/summationdisplay ef2(E)×f6(B,E).

Token 10721:
•Next, we sum out Efrom the product of f2andf6: f7(B)=/summationdisplay ef2(E)×f6(B,E) =f2(e)×f6(B,e)+f2(¬e)×f6(B,¬e).

Token 10722:
This leaves the expression P(B|j, m)=αf1(B)×f7(B) which can be evaluated by taking the pointwise product and normalizing the result.

Token 10723:
Examining this sequence, we see that two basic computational operations are required: point- wise product of a pair of factors, and summing out a variable from a product of factors.

Token 10724:
Thenext section describes each of these operations.

Token 10725:
Operations on factors The pointwise product of two factors f 1andf2yields a new factor fwhose variables are theunion of the variables in f1andf2and whose elements are given by the product of the corresponding elements in the two factors.

Token 10726:
Suppose the two factors have variables Y1,...,Y k in common. Then we have f(X1...X j,Y1...Y k,Z1...Z l)=f1(X1...X j,Y1...Y k)f2(Y1...Y k,Z,...Z l).

Token 10727:
If all the variables are binary, then f1andf2have2j+kand2k+lentries, respectively, and the pointwise product has 2j+k+lentries.

Token 10728:
For example, given two factors f1(A,B)and f2(B,C), the pointwise product f1×f2=f3(A,B,C )has21+1+1=8entries, as illustrated in Figure 14.10.

Token 10729:
Notice that the factor resulting from a pointwise product can contain more variables than any of the factors being multiplied and that the size of a factor is exponential in the number of variables.

Token 10730:
This is where both space and time complexity arise in the variable elimination algorithm.

Token 10731:
Section 14.4.

Token 10732:
Exact Inference in Bayesian Networks 527 A B f1(A,B) B C f2(B,C) A B C f3(A,B,C ) T T .3 T T .2 T T T .3×.2=.06 T F .7 T F .8 T T F .3×.8=.24 F T .9 F T .6 T F T .7×.6=.42 F F .1 F F .4 T F F .7×.4=.28 F T T .9×.2=.18 F T F .9×.8=.72 F F T .1×.6=.06 F F F .1×.4=.04 Figure 14.10 Illustrating pointwise multiplication: f1(A, B)×f2(B,C)=f3(A, B, C ).

Token 10733:
Summing out a variable from a product of factors is done by adding up the submatrices formed by ﬁxing the variable to each of its values in turn.

Token 10734:
For example, to sum out Afrom f3(A,B,C ), we write f(B,C)=/summationdisplay af3(A,B,C )=f3(a,B,C )+f3(¬a,B,C ) =/parenleftbigg.06.24 .42.28/parenrightbigg +/parenleftbigg.18.72 .06.04/parenrightbigg =/parenleftbigg.24.96 .48.32/parenrightbigg .

Token 10735:
The only trick is to notice that any factor that does notdepend on the variable to be summed out can be moved outside the summation.

Token 10736:
For example, if we were to sum out Eﬁrst in the burglary network, the relevant part of the expression would be/summationdisplay ef2(E)×f3(A,B,E )×f4(A)×f5(A)=f4(A)×f5(A)×/summationdisplay ef2(E)×f3(A,B,E ).

Token 10737:
Now the pointwise product inside the summation is computed, and the variable is summed out of the resulting matrix.

Token 10738:
Notice that matrices are notmultiplied until we need to sum out a variable from the accumulated product.

Token 10739:
At that point, we multiply just those matrices that include the variableto be summed out.

Token 10740:
Given functions for pointwise product and summing out, the variableelimination algorithm itself can be written quite simply, as shown in Figure 14.11.

Token 10741:
Variable ordering and variable relevance The algorithm in Figure 14.11 includes an unspeciﬁed O RDER function to choose an ordering for the variables.

Token 10742:
Every choice of ordering yields a valid algorithm, but different orderingscause different intermediate factors to be generated during the calculation.

Token 10743:
For example, inthe calculation shown previously, we eliminated Abefore E; if we do it the other way, the calculation becomes P(B|j, m)=αf 1(B)×/summationdisplay af4(A)×f5(A)×/summationdisplay ef2(E)×f3(A,B,E ), during which a new factor f6(A,B)will be generated.

Token 10744:
In general, the time and space requirements of variable elimination are dominated by the size of the largest factor constructed during the operation of the algorithm.

Token 10745:
This in turn

Token 10746:
528 Chapter 14.

Token 10747:
Probabilistic Reasoning function ELIMINATION -ASK(X,e,bn)returns a distribution over X inputs :X, the query variable e, observed values for variables E bn, a Bayesian network specifying joint distribution P(X1,...,X n) factors←[] for each varinORDER (bn.VARS)do factors←[MAKE-FACTOR (var,e)|factors ] ifvaris a hidden variable thenfactors←SUM-OUT(var,factors ) return NORMALIZE (POINTWISE -PRODUCT (factors )) Figure 14.11 The variable elimination algorithm for inference in Bayesian networks.

Token 10748:
is determined by the order of elimination of variables and by the structure of the network.

Token 10749:
It turns out to be intractable to determine the optimal ordering, but several good heuristicsare available.

Token 10750:
One fairly effective method is a greedy one: eliminate whichever variableminimizes the size of the next factor to be constructed.

Token 10751:
Let us consider one more query: P(JohnCalls|Burglary =true).

Token 10752:
As usual, the ﬁrst step is to write out the nested summation: P(J|b)=αP(b)/summationdisplay eP(e)/summationdisplay aP(a|b,e)P(J|a)/summationdisplay mP(m|a).

Token 10753:
Evaluating this expression from right to left, we notice something interesting:/summationtext mP(m|a) is equal to 1 by deﬁnition!

Token 10754:
Hence, there was no need to include it in the ﬁrst place; the vari-ableMisirrelevant to this query.

Token 10755:
Another way of saying this is that the result of the query P(JohnCalls|Burglary =true)is unchanged if we remove MaryCalls from the network altogether.

Token 10756:
In general, we can remove any leaf node that is not a query variable or an evidencevariable.

Token 10757:
After its removal, there may be some more leaf nodes, and these too may be irrele-vant.

Token 10758:
Continuing this process, we eventually ﬁnd that every variable that is not an ancestor of a query variable or evidence variable is irrelevant to the query.

Token 10759:
A variable elimination algorithm can therefore remove all these variables before evaluating the query.

Token 10760:
14.4.3 The complexity of exact inference The complexity of exact inference in Bayesian networks depends strongly on the structure ofthe network.

Token 10761:
The burglary network of Figure 14.2 belongs to the family of networks in whichthere is at most one undirected path between any two nodes in the network.

Token 10762:
These are calledsingly connected networks or polytrees , and they have a particularly nice property: The time SINGLY CONNECTED POLYTREE and space complexity of exact inference in polytrees is linear in the size of the network.

Token 10763:
Here, the size is deﬁned as the number of CPT entries; if the number of parents of each node is bounded by a constant, then the complexity will also be linear in the number of nodes.

Token 10764:
Formultiply connected networks, such as that of Figure 14.12(a), variable eliminationMULTIPLY CONNECTED can have exponential time and space complexity in the worst case, even when the number of parents per node is bounded.

Token 10765:
This is not surprising when one considers that because it

Token 10766:
Section 14.4.

Token 10767:
Exact Inference in Bayesian Networks 529 P(C)=.5 CP (R) t f.80.20CP (S) t f.10.50 SR tt tf ftff.90 .90.00.99Cloudy Rain Sprinkler Wet Grass P(W)P(C)=.5 t f.08 .02 .72 .18P(S+R=x ) S+R P (W) tt tf ft ff.90 .90 .00.99Cloudy Spr+Rain Wet Grass.10 .40 .10 .40Ct tt ff tf f (a) (b) Figure 14.12 (a) A multiply connected network with conditional probability tables.

Token 10768:
(b) A clustered equivalent of the multiply connected network.

Token 10769:
includes inference in propositional logic as a special case, inference in Bayesian networks is NP-hard.

Token 10770:
In fact, it can be shown (Exercise 14.16) that the problem is as hard as that of com- puting the number of satisfying assignments for a propositional logic formula.

Token 10771:
This means that it is #P-hard (“number-P hard”)—that is, strictly harder than NP-complete problems.

Token 10772:
There is a close connection between the complexity of Bayesian network inference and the complexity of constraint satisfaction problems (CSPs).

Token 10773:
As we discussed in Chapter 6,the difﬁculty of solving a discrete CSP is related to how “treelike” its constraint graph is.Measures such as tree width , which bound the complexity of solving a CSP, can also be applied directly to Bayesian networks.

Token 10774:
Moreover, the variable elimination algorithm can be generalized to solve CSPs as well as Bayesian networks.

Token 10775:
14.4.4 Clustering algorithms The variable elimination algorithm is simple and efﬁcient for answering individual queries.

Token 10776:
Ifwe want to compute posterior probabilities for all the variables in a network, however, it canbe less efﬁcient.

Token 10777:
For example, in a polytree network, one would need to issue O(n)queries costing O(n)each, for a total of O(n 2)time.

Token 10778:
Using clustering algorithms (also known as CLUSTERING join tree algorithms), the time can be reduced to O(n).

Token 10779:
For this reason, these algorithms are JOIN TREE widely used in commercial Bayesian network tools.

Token 10780:
The basic idea of clustering is to join individual nodes of the network to form clus- ter nodes in such a way that the resulting network is a polytree.

Token 10781:
For example, the multiplyconnected network shown in Figure 14.12(a) can be converted into a polytree by combin-ing the Sprinkler andRain node into a cluster node called Sprinkler +Rain ,a ss h o w ni n Figure 14.12(b).

Token 10782:
The two Boolean nodes are replaced by a “meganode” that takes on four possible values: tt,tf,ft,a n dff.

Token 10783:
The meganode has only one parent, the Boolean variable Cloudy , so there are two conditioning cases.

Token 10784:
Although this example doesn’t show it, the process of clustering often produces meganodes that share some variables.

Token 10785:
530 Chapter 14.

Token 10786:
Probabilistic Reasoning Once the network is in polytree form, a special-purpose inference algorithm is required, because ordinary inference methods cannot handle meganodes that share variables with eachother.

Token 10787:
Essentially, the algorithm is a form of constraint propagation (see Chapter 6) where theconstraints ensure that neighboring meganodes agree on the posterior probability of any vari-ables that they have in common.

Token 10788:
With careful bookkeeping, this algorithm is able to compute posterior probabilities for all the nonevidence nodes in the network in time linear in the size of the clustered network.

Token 10789:
However, the NP-hardness of the problem has not disappeared: if anetwork requires exponential time and space with variable elimination, then the CPTs in theclustered network will necessarily be exponentially large.

Token 10790:
14.5 A PPROXIMATE INFERENCE IN BAYESIAN NETWORKS Given the intractability of exact inference in large, multiply connected networks, it is essen-tial to consider approximate inference methods.

Token 10791:
This section describes randomized samplingalgorithms, also called Monte Carlo algorithms, that provide approximate answers whose MONTE CARLO accuracy depends on the number of samples generated.

Token 10792:
Monte Carlo algorithms, of which simulated annealing (page 126) is an example, are used in many branches of science to es-timate quantities that are difﬁcult to calculate exactly.

Token 10793:
In this section, we are interested in sampling applied to the computation of posterior probabilities.

Token 10794:
We describe two families of algorithms: direct sampling and Markov chain sampling.

Token 10795:
Two other approaches—variationalmethods and loopy propagation—are mentioned in the notes at the end of the chapter.

Token 10796:
14.5.1 Direct sampling methods The primitive element in any sampling algorithm is the generation of samples from a knownprobability distribution.

Token 10797:
For example, an unbiased coin can be thought of as a random variableCoin with values/angbracketleftheads,tails/angbracketrightand a prior distribution P(Coin)=/angbracketleft0.5,0.5/angbracketright.

Token 10798:
Sampling from this distribution is exactly like ﬂipping the coin: with probability 0.5 it will return heads , and with probability 0.5 it will return tails .

Token 10799:
Given a source of random numbers uniformly distributed in the range [0,1], it is a simple matter to sample any distribution on a single variable, whether discrete or continuous.

Token 10800:
(See Exercise 14.17.)

Token 10801:
The simplest kind of random sampling process for Bayesian networks generates events from a network that has no evidence associated with it.

Token 10802:
The idea is to sample each variable in turn, in topological order.

Token 10803:
The probability distribution from which the value is sampled is conditioned on the values already assigned to the variable’s parents.

Token 10804:
This algorithm is shownin Figure 14.13.

Token 10805:
We can illustrate its operation on the network in Figure 14.12(a), assumingan ordering [Cloudy ,Sprinkler ,Rain,WetGrass ]: 1.

Token 10806:
Sample from P(Cloudy )=/angbracketleft0.5,0.5/angbracketright,v a l u ei s true. 2.

Token 10807:
Sample from P(Sprinkler|Cloudy =true)=/angbracketleft0.1,0.9/angbracketright,v a l u ei s false . 3.

Token 10808:
Sample from P(Rain|Cloudy =true)=/angbracketleft0.8,0 .2/angbracketright,v a l u ei s true. 4.

Token 10809:
Sample from P(WetGrass|Sprinkler =false,Rain=true)=/angbracketleft0.9,0.1/angbracketright,v a l u ei s true.

Token 10810:
In this case, P RIOR -SAMPLE returns the event [true,false,true,true].

Token 10811:
Section 14.5.

Token 10812:
Approximate Inference in Bayesian Networks 531 function PRIOR -SAMPLE (bn)returns an event sampled from the prior speciﬁed by bn inputs :bn, a Bayesian network specifying joint distribution P(X1,...,X n) x←an event with nelements foreach variable XiinX1,...,X ndo x[i]←a random sample from P(Xi|parents (Xi)) return x Figure 14.13 A sampling algorithm that generates ev ents from a Bayesian network.

Token 10813:
Each variable is sampled according to the conditional distribution given the values already sampledfor the variable’s parents.

Token 10814:
It is easy to see that P RIOR -SAMPLE generates samples from the prior joint distribution speciﬁed by the network.

Token 10815:
First, let SPS(x1,...,x n)be the probability that a speciﬁc event is generated by the P RIOR -SAMPLE algorithm.

Token 10816:
Just looking at the sampling process ,w eh a v e SPS(x1...x n)=n/productdisplay i=1P(xi|parents (Xi)) because each sampling step depends only on the parent values.

Token 10817:
This expression should look familiar, because it is also the probability of the event according to the Bayesian net’s repre-sentation of the joint distribution, as stated in Equation (14.2).

Token 10818:
That is, we have S PS(x1...x n)=P(x1...x n). This simple fact makes it easy to answer questions by using samples.

Token 10819:
In any sampling algorithm, the answers are computed by counting the actual samples generated.

Token 10820:
Suppose there are Ntotal samples, and let NPS(x1,...,x n)be the number of times the speciﬁc event x1,...,x noccurs in the set of samples.

Token 10821:
We expect this number, as a fraction of the total, to converge in the limit to its expected value according to the samplingprobability: lim N→∞NPS(x1,...,x n) N=SPS(x1,...,x n)=P(x1,...,x n).

Token 10822:
(14.5) For example, consider the event produced earlier: [true,false,true,true].

Token 10823:
The sampling probability for this event is SPS(true,false,true,true)=0.5×0.9×0.8×0.9=0.324.

Token 10824:
Hence, in the limit of large N, we expect 32.4% of the samples to be of this event.

Token 10825:
Whenever we use an approximate equality (“ ≈”) in what follows, we mean it in exactly this sense—that the estimated probability becomes exact in the large-sample limit.

Token 10826:
Such anestimate is called consistent .

Token 10827:
For example, one can produce a consistent estimate of the CONSISTENT probability of any partially speciﬁed event x1,...,x m,w h e r e m≤n, as follows: P(x1,...,x m)≈NPS(x1,...,x m)/N .

Token 10828:
(14.6) That is, the probability of the event can be estimated as the fraction of all complete events generated by the sampling process that match the partially speciﬁed event.

Token 10829:
For example, if

Token 10830:
532 Chapter 14.

Token 10831:
Probabilistic Reasoning we generate 1000 samples from the sprinkler network, and 511 of them have Rain=true, then the estimated probability of rain, written as ˆP(Rain=true), is 0.511.

Token 10832:
Rejection sampling in Bayesian networks Rejection sampling is a general method for producing samples from a hard-to-sample distri-REJECTION SAMPLING bution given an easy-to-sample distribution.

Token 10833:
In its simplest form, it can be used to compute conditional probabilities—that is, to determine P(X|e).T h eR EJECTION -SAMPLING algo- rithm is shown in Figure 14.14.

Token 10834:
First, it generates samples from the prior distribution speciﬁedby the network. Then, it rejects all those that do not match the evidence.

Token 10835:
Finally, the estimate ˆP(X=x|e)is obtained by counting how often X=xoccurs in the remaining samples.

Token 10836:
LetˆP(X|e)be the estimated distribution that the algorithm returns. From the deﬁnition of the algorithm, we have ˆP(X|e)=αN PS(X,e)=NPS(X,e) NPS(e).

Token 10837:
From Equation (14.6), this becomes ˆP(X|e)≈P(X,e) P(e)=P(X|e). That is, rejection sampling produces a consistent estimate of the true probability.

Token 10838:
Continuing with our example from Figure 14.12(a), let us assume that we wish to esti- mate P(Rain|Sprinkler =true), using 100 samples.

Token 10839:
Of the 100 that we generate, suppose that 73 have Sprinkler =false and are rejected, while 27 have Sprinkler =true; of the 27, 8h a v e Rain=true a n d1 9h a v e Rain=false .

Token 10840:
Hence, P(Rain|Sprinkler =true)≈NORMALIZE (/angbracketleft8,19/angbracketright)=/angbracketleft0.296,0.704/angbracketright.

Token 10841:
The true answer is /angbracketleft0.3,0.7/angbracketright. As more samples are collected, the estimate will converge to the true answer.

Token 10842:
The standard deviation of the error in each probability will be proportionalto1/√ n,w h e r e nis the number of samples used in the estimate.

Token 10843:
The biggest problem with rejection sampling is that it rejects so many samples!

Token 10844:
The fraction of samples consistent with the evidence edrops exponentially as the number of evi- dence variables grows, so the procedure is simply unusable for complex problems.

Token 10845:
Notice that rejection sampling is very similar to the estimation of conditional probabili- ties directly from the real world.

Token 10846:
For example, to estimate P(Rain|RedSkyAtNight =true), one can simply count how often it rains after a red sky is observed the previous evening—ignoring those evenings when the sky is not red.

Token 10847:
(Here, the world itself plays the role of the sample-generation algorithm.)

Token 10848:
Obviously, this could take a long time if the sky is very seldom red, and that is the weakness of rejection sampling.

Token 10849:
Likelihood weighting Likelihood weighting avoids the inefﬁciency of rejection sampling by generating only events LIKELIHOOD WEIGHTING that are consistent with the evidence e. It is a particular instance of the general statistical technique of importance sampling , tailored for inference in Bayesian networks.

Token 10850:
We begin byIMPORTANCE SAMPLING

Token 10851:
Section 14.5.

Token 10852:
Approximate Inference in Bayesian Networks 533 function REJECTION -SAMPLING (X,e,bn,N)returns an estimate of P(X|e) inputs :X, the query variable e, observed values for variables E bn, a Bayesian network N, the total number of samples to be generated local variables :N, a vector of counts for each value of X, initially zero forj=1t o Ndo x←PRIOR -SAMPLE (bn) if xis consistent with et h e n N[x]←N[x]+1 where xis the value of Xinx return NORMALIZE (N) Figure 14.14 The rejection-sampling algorithm for answering queries given evidence in a Bayesian network.

Token 10853:
describing how the algorithm works; then we show that it works correctly—that is, generates consistent probability estimates.

Token 10854:
LIKELIHOOD -WEIGHTING (see Figure 14.15) ﬁxes the values for the evidence vari- ables Eand samples only the nonevidence variables.

Token 10855:
This guarantees that each event gener- ated is consistent with the evidence. Not all events are equal, however.

Token 10856:
Before tallying the counts in the distribution for the query variable, each event is weighted by the likelihood that the event accords to the evidence, as measured by the product of the conditional probabilitiesfor each evidence variable, given its parents.

Token 10857:
Intuitively, events in which the actual evidenceappears unlikely should be given less weight.

Token 10858:
Let us apply the algorithm to the network shown in Figure 14.12(a), with the query P(Rain|Cloudy =true,WetGrass =true)and the ordering Cloudy ,Sprinkler ,Rain ,Wet- Grass .

Token 10859:
(Any topological ordering will do.) The process goes as follows: First, the weight w is set to 1.0.

Token 10860:
Then an event is generated: 1.Cloudy is an evidence variable with value true. Therefore, we set w←w×P(Cloudy =true)=0.5.

Token 10861:
2.Sprinkler is not an evidence variable, so sample from P(Sprinkler|Cloudy =true)= /angbracketleft0.1,0.9/angbracketright; suppose this returns false .

Token 10862:
3. Similarly, sample from P(Rain|Cloudy =true)=/angbracketleft0.8,0.2/angbracketright; suppose this returns true.

Token 10863:
4.WetGrass is an evidence variable with value true. Therefore, we set w←w×P(WetGrass =true|Sprinkler =false,Rain=true)=0.45.

Token 10864:
Here W EIGHTED -SAMPLE returns the event [true,false,true,true]with weight 0.45, and this is tallied under Rain=true.

Token 10865:
To understand why likelihood weighting works, we start by examining the sampling probability SWSfor W EIGHTED -SAMPLE .

Token 10866:
Remember that the evidence variables Eare ﬁxed

Token 10867:
534 Chapter 14.

Token 10868:
Probabilistic Reasoning function LIKELIHOOD -WEIGHTING (X,e,bn,N)returns an estimate of P(X|e) inputs :X, the query variable e, observed values for variables E bn, a Bayesian network specifying joint distribution P(X1,...,X n) N, the total number of samples to be generated local variables :W, a vector of weighted counts for each value of X, initially zero forj=1t o Ndo x,w←WEIGHTED -SAMPLE (bn,e) W[x]←W[x]+wwhere xis the value of Xinx return NORMALIZE (W) function WEIGHTED -SAMPLE (bn,e)returns an event and a weight w←1;x←an event with nelements initialized from e foreach variable XiinX1,...,X ndo ifXiis an evidence variable with value xiine thenw←w×P(Xi=xi|parents (Xi)) else x[i]←a random sample from P(Xi|parents (Xi)) return x ,w Figure 14.15 The likelihood-weighting algorithm for inference in Bayesian networks.

Token 10869:
In WEIGHTED -SAMPLE , each nonevidence variable is sam pled according to the conditional distribution given the values already sampled for the variable’s parents, while a weight isaccumulated based on the likeli hood for each evidence variable.

Token 10870:
with values e. We call the nonevidence variables Z(including the query variable X).

Token 10871:
The algorithm samples each variable in Zgiven its parent values: SWS(z,e)=l/productdisplay i=1P(zi|parents (Zi)).

Token 10872:
(14.7) Notice that Parents (Zi)can include both nonevidence variables and evidence variables.

Token 10873:
Un- like the prior distribution P(z), the distribution SWSpays some attention to the evidence: the sampled values for each Ziwill be inﬂuenced by evidence among Zi’s ancestors.

Token 10874:
For exam- ple, when sampling Sprinkler the algorithm pays attention to the evidence Cloudy =true in its parent variable.

Token 10875:
On the other hand, SWSpays less attention to the evidence than does the true posterior distribution P(z|e), because the sampled values for each Ziignore evidence among Zi’s non-ancestors.5For example, when sampling Sprinkler andRain the algorithm ignores the evidence in the child variable WetGrass =true; this means it will generate many samples with Sprinkler =false andRain=false despite the fact that the evidence actually rules out this case.

Token 10876:
5Ideally, we would like to use a sampling distribution equal to the true posterior P(z|e), to take all the evidence into account.

Token 10877:
This cannot be done efﬁciently, however.

Token 10878:
If it could, then we could approximate the desired probability to arbitrary accuracy with a polynomial number of samples.

Token 10879:
It can be shown that no such polynomial-time approximation scheme can exist.

Token 10880:
Section 14.5.

Token 10881:
Approximate Inference in Bayesian Networks 535 The likelihood weight wmakes up for the difference between the actual and desired sampling distributions.

Token 10882:
The weight for a given sample x, composed from zande,i st h e product of the likelihoods for each evidence variable given its parents (some or all of whichmay be among the Z is): w(z,e)=m/productdisplay i=1P(ei|parents (Ei)).

Token 10883:
(14.8) Multiplying Equations (14.7) and (14.8), we see that the weighted probability of a sample has the particularly convenient form SWS(z,e)w(z,e)=l/productdisplay i=1P(zi|parents (Zi))m/productdisplay i=1P(ei|parents (Ei)) =P(z,e) (14.9) because the two products cover all the variables in the network, allowing us to use Equa- tion (14.2) for the joint probability.

Token 10884:
Now it is easy to show that likelihood weighting estimates are consistent.

Token 10885:
For any particular value xofX, the estimated posterior probability can be calculated as follows: ˆP(x|e)=α/summationdisplay yNWS(x,y,e)w(x,y,e) from L IKELIHOOD -WEIGHTING ≈α/prime/summationdisplay ySWS(x,y,e)w(x,y,e) for large N =α/prime/summationdisplay yP(x,y,e) by Equation (14.9) =α/primeP(x,e)=P(x|e).

Token 10886:
Hence, likelihood weighting returns consistent estimates.

Token 10887:
Because likelihood weighting uses all the samples generated, it can be much more ef- ﬁcient than rejection sampling.

Token 10888:
It will, however, suffer a degradation in performance as thenumber of evidence variables increases.

Token 10889:
This is because most samples will have very lowweights and hence the weighted estimate will be dominated by the tiny fraction of samplesthat accord more than an inﬁnitesimal likelihood to the evidence.

Token 10890:
The problem is exacerbatedif the evidence variables occur late in the variable ordering, because then the nonevidencevariables will have no evidence in their parents and ancestors to guide the generation of sam-ples.

Token 10891:
This means the samples will be simulations that bear little resemblance to the realitysuggested by the evidence.

Token 10892:
14.5.2 Inference by Markov chain simulation Markov chain Monte Carlo (MCMC) algorithms work quite differently from rejection sam-MARKOV CHAIN MONTE CARLO pling and likelihood weighting.

Token 10893:
Instead of generating each sample from scratch, MCMC al- gorithms generate each sample by making a random change to the preceding sample.

Token 10894:
It istherefore helpful to think of an MCMC algorithm as being in a particular current state speci- fying a value for every variable and generating a next state by making random changes to the

Token 10895:
536 Chapter 14. Probabilistic Reasoning current state.

Token 10896:
(If this reminds you of simulated annealing from Chapter 4 or W ALKSAT from Chapter 7, that is because both are members of the MCMC family.)

Token 10897:
Here we describe a par-ticular form of MCMC called Gibbs sampling , which is especially well suited for Bayesian GIBBS SAMPLING networks.

Token 10898:
(Other forms, some of them signiﬁcantly more powerful, are discussed in the notes at the end of the chapter.)

Token 10899:
We will ﬁrst describe what the algorithm does, then we will explain why it works.

Token 10900:
Gibbs sampling in Bayesian networks The Gibbs sampling algorithm for Bayesian networks starts with an arbitrary state (with the evidence variables ﬁxed at their observed values) and generates a next state by randomly sampling a value for one of the nonevidence variables Xi.

Token 10901:
The sampling for Xiis done conditioned on the current values of the variables in the Markov blanket of Xi.

Token 10902:
(Recall from page 517 that the Markov blanket of a variable consists of its parents, children, and children’sparents.)

Token 10903:
The algorithm therefore wanders randomly around the state space—the space ofpossible complete assignments—ﬂipping one variable at a time, but keeping the evidencevariables ﬁxed.

Token 10904:
Consider the query P(Rain|Sprinkler =true,WetGrass =true)applied to the net- work in Figure 14.12(a).

Token 10905:
The evidence variables Sprinkler andWetGrass a r eﬁ x e dt ot h e i r observed values and the nonevidence variables Cloudy andRain are initialized randomly— let us say to true andfalse respectively.

Token 10906:
Thus, the initial state is [true,true,false,true]. Now the nonevidence variables are sampled repeatedly in an arbitrary order.

Token 10907:
For example: 1.Cloudy is sampled, given the current values of its Markov blanket variables: in this case, we sample from P(Cloudy|Sprinkler =true,Rain=false).

Token 10908:
(Shortly, we will show how to calculate this distribution.)

Token 10909:
Suppose the result is Cloudy =false .T h e n the new current state is [false,true,false,true].

Token 10910:
2.Rain is sampled, given the current values of its Markov blanket variables: in this case, we sample from P(Rain|Cloudy =false,Sprinkler =true,WetGrass =true).

Token 10911:
Sup- pose this yields Rain=true. The new current state is [false,true,true,true].

Token 10912:
Each state visited during this process is a sample that contributes to the estimate for the query variable Rain .

Token 10913:
If the process visits 20 states where Rain is true and 60 states where Rain is false, then the answer to the query is N ORMALIZE (/angbracketleft20,60/angbracketright)=/angbracketleft0.25,0.75/angbracketright.

Token 10914:
The complete algorithm is shown in Figure 14.16.

Token 10915:
Why Gibbs sampling works We will now show that Gibbs sampling returns consistent estimates for posterior probabil- ities.

Token 10916:
The material in this section is quite technical, but the basic claim is straightforward:the sampling process settles into a “dynamic equilibrium” in which the long-run fraction of time spent in each state is exactly proportional to its posterior probability.

Token 10917:
This remarkable property follows from the speciﬁc transition probability with which the process moves fromTRANSITION PROBABILITY one state to another, as deﬁned by the conditional distribution given the Markov blanket of the variable being sampled.

Token 10918:
Section 14.5.

Token 10919:
Approximate Inference in Bayesian Networks 537 function GIBBS -ASK(X,e,bn,N)returns an estimate of P(X|e) local variables :N, a vector of counts for each value of X, initially zero Z, the nonevidence variables in bn x, the current state of the network, initially copied from e initialize xwith random values for the variables in Z forj=1t o Ndo for each ZiinZd o set the value of Ziinxby sampling from P(Zi|mb(Zi)) N[x]←N[x]+1 where xis the value of Xinx return NORMALIZE (N) Figure 14.16 The Gibbs sampling algorithm for approximate inference in Bayesian net- works; this version cycles through the variables, but choosing variables at random also works.

Token 10920:
Letq(x→x/prime)be the probability that the process makes a transition from state xto state x/prime.

Token 10921:
This transition probability deﬁnes what is called a Markov chain on the state space.

Token 10922:
MARKOV CHAIN (Markov chains also ﬁgure prominently in Chapters 15 and 17.)

Token 10923:
Now suppose that we run the Markov chain for tsteps, and let πt(x)be the probability that the system is in state xat timet.

Token 10924:
Similarly, let πt+1(x/prime)be the probability of being in state x/primeat time t+1.G i v e n πt(x), we can calculate πt+1(x/prime)by summing, for all states the system could be in at time t, the probability of being in that state times the probability of making the transition to x/prime: πt+1(x/prime)=/summationdisplay xπt(x)q(x→x/prime).

Token 10925:
We say that the chain has reached its stationary distribution ifπt=πt+1.

Token 10926:
Let us call thisSTATIONARY DISTRIBUTION stationary distribution π; its deﬁning equation is therefore π(x/prime)=/summationdisplay xπ(x)q(x→x/prime) for all x/prime.

Token 10927:
(14.10) Provided the transition probability distribution qisergodic —that is, every state is reachable ERGODIC from every other and there are no strictly periodic cycles—there is exactly one distribution π satisfying this equation for any given q.

Token 10928:
Equation (14.10) can be read as saying that the expected “outﬂow” from each state (i.e., its current “population”) is equal to the expected “inﬂow” from all the states.

Token 10929:
One obviousway to satisfy this relationship is if the expected ﬂow between any pair of states is the samein both directions; that is, π(x)q(x→x /prime)=π(x/prime)q(x/prime→x) for all x,x/prime.

Token 10930:
(14.11) When these equations hold, we say that q(x→x/prime)is in detailed balance withπ(x).

Token 10931:
DETAILED BALANCE We can show that detailed balance implies stationarity simply by summing over xin Equation (14.11).

Token 10932:
We have /summationdisplay xπ(x)q(x→x/prime)=/summationdisplay xπ(x/prime)q(x/prime→x)=π(x/prime)/summationdisplay xq(x/prime→x)=π(x/prime)

Token 10933:
538 Chapter 14. Probabilistic Reasoning where the last step follows because a transition from x/primeis guaranteed to occur.

Token 10934:
The transition probability q(x→x/prime)deﬁned by the sampling step in G IBBS -ASKis actually a special case of the more general deﬁnition of Gibbs sampling, according to whicheach variable is sampled conditionally on the current values of allthe other variables.

Token 10935:
We start by showing that this general deﬁnition of Gibbs sampling satisﬁes the detailed balance equation with a stationary distribution equal to P(x|e), (the true posterior distribution on the nonevidence variables).

Token 10936:
Then, we simply observe that, for Bayesian networks, samplingconditionally on all variables is equivalent to sampling conditionally on the variable’s Markovblanket (see page 517).

Token 10937:
To analyze the general Gibbs sampler, which samples each X iin turn with a transition probability qithat conditions on all the other variables, we deﬁne Xito be these other vari- ables (except the evidence variables); their values in the current state are xi.

Token 10938:
If we sample a new value x/prime iforXiconditionally on all the other variables, including the evidence, we have qi(x→x/prime)=qi((xi, xi)→(x/prime i, xi)) =P(x/prime i| xi,e).

Token 10939:
Now we show that the transition probability for each step of the Gibbs sampler is in detailed balance with the true posterior: π(x)qi(x→x/prime)=P(x|e)P(x/prime i| xi,e)=P(xi, xi|e)P(x/prime i| xi,e) =P(xi| xi,e)P( xi|e)P(x/prime i| xi,e) (using the chain rule on the ﬁrst term) =P(xi| xi,e)P(x/prime i, xi|e) (using the chain rule backward) =π(x/prime)qi(x/prime→x).

Token 10940:
We can think of the loop “ for each ZiinZd o ” in Figure 14.16 as deﬁning one large transition probability qthat is the sequential composition q1◦q2◦···◦ qnof the transition probabilities for the individual variables.

Token 10941:
It is easy to show (Exercise 14.19) that if each of qiandqjhas πas its stationary distribution, then the sequential composition qi◦qjdoes too; hence the transition probability qfor the whole loop has P(x|e)as its stationary distribution.

Token 10942:
Finally, unless the CPTs contain probabilities of 0 or 1—which can cause the state space to becomedisconnected—it is easy to see that qis ergodic.

Token 10943:
Hence, the samples generated by Gibbs sampling will eventually be drawn from the true posterior distribution.

Token 10944:
The ﬁnal step is to show how to perform the general Gibbs sampling step—sampling X ifrom P(Xi| xi,e)—in a Bayesian network.

Token 10945:
Recall from page 517 that a variable is inde- pendent of all other variables given its Markov blanket; hence, P(x/prime i| xi,e)=P(x/prime i|mb(Xi)), where mb(Xi)denotes the values of the variables in Xi’s Markov blanket, MB(Xi).A s shown in Exercise 14.7, the probability of a variable given its Markov blanket is proportional to the probability of the variable given its parents times the probability of each child given itsrespective parents: P(x /prime i|mb(Xi)) =αP(x/prime i|parents (Xi))×/productdisplay Yj∈Children (Xi)P(yj|parents (Yj)).

Token 10946:
(14.12) Hence, to ﬂip each variable Xiconditioned on its Markov blanket, the number of multiplica- tions required is equal to the number of Xi’s children.

Token 10947:
Section 14.6.

Token 10948:
Relational and First-Order Probability Models 539 Recommendation (C1, B1)Honesty (C1)Kindness (C1)Quality (B1) Recommendation (C1, B1)Honesty (C1)Kindness (C1)Quality (B1) Recommendation (C2, B1)Honesty (C2)Kindness (C2)Quality (B2) Recommendation (C1, B2)Recommendation (C2, B2) (a) (b) Figure 14.17 (a) Bayes net for a single customer C1recommending a single book B1.

Token 10949:
Honest (C1)is Boolean, while the other variables have integer values from 1 to 5. (b) Bayes net with two customers and two books.

Token 10950:
14.6 R ELATIONAL AND FIRST-ORDER PROBABILITY MODELS In Chapter 8, we explained the representational advantages possessed by ﬁrst-order logic in comparison to propositional logic.

Token 10951:
First-order logic commits to the existence of objects andrelations among them and can express facts about some orallof the objects in a domain.

Token 10952:
This often results in representations that are vastly more concise than the equivalent propositionaldescriptions.

Token 10953:
Now, Bayesian networks are essentially propositional: the set of random vari-ables is ﬁxed and ﬁnite, and each has a ﬁxed domain of possible values.

Token 10954:
This fact limits theapplicability of Bayesian networks.

Token 10955:
If we can ﬁnd a way to combine probability theory with the expressive power of ﬁrst-order representations, we expect to be able to increase dramati- cally the range of problems that can be handled.

Token 10956:
For example, suppose that an online book retailer would like to provide overall evalu- ations of products based on recommendations received from its customers.

Token 10957:
The evaluation will take the form of a posterior distribution over the quality of the book, given the avail-able evidence.

Token 10958:
The simplest solution to base the evaluation on the average recommendation,perhaps with a variance determined by the number of recommendations, but this fails to takeinto account the fact that some customers are kinder than others and some are less honest thanothers.

Token 10959:
Kind customers tend to give high recommendations even to fairly mediocre books,while dishonest customers give very high or very low recommendations for reasons otherthan quality—for example, they might work for a publisher.

Token 10960:
6 For a single customer C1, recommending a single book B1, the Bayes net might look like the one shown in Figure 14.17(a).

Token 10961:
(Just as in Section 9.1, expressions with parenthesessuch as Honest (C 1)are just fancy symbols—in this case, fancy names for random variables.)

Token 10962:
6A game theorist would advise a dishonest customer to avoid detection by occasionally recommending a good book from a competitor. See Chapter 17.

Token 10963:
540 Chapter 14. Probabilistic Reasoning With two customers and two books, the Bayes net looks like the one in Figure 14.17(b).

Token 10964:
For larger numbers of books and customers, it becomes completely impractical to specify thenetwork by hand.

Token 10965:
Fortunately, the network has a lot of repeated structure.

Token 10966:
Each Recommendation (c,b) variable has as its parents the variables Honest (c),Kindness (c),a n dQuality (b).

Token 10967:
Moreover, the CPTs for all the Recommendation (c,b)variables are identical, as are those for all the Honest (c)variables, and so on.

Token 10968:
The situation seems tailor-made for a ﬁrst-order language.

Token 10969:
We would like to say something like Recommendation (c,b)∼RecCPT (Honest (c),Kindness (c),Quality (b)) with the intended meaning that a customer’s recommendation for a book depends on the customer’s honesty and kindness and the book’s quality according to some ﬁxed CPT.

Token 10970:
Thissection develops a language that lets us say exactly this, and a lot more besides.

Token 10971:
14.6.1 Possible worlds Recall from Chapter 13 that a probability model deﬁnes a set Ωof possible worlds with a probability P(ω)for each world ω.

Token 10972:
For Bayesian networks, the possible worlds are as- signments of values to variables; for the Boolean case in particular, the possible worlds areidentical to those of propositional logic.

Token 10973:
For a ﬁrst-order probability model, then, it seemswe need the possible worlds to be those of ﬁrst-order logic—that is, a set of objects withrelations among them and an interpretation that maps constant symbols to objects, predicate symbols to relations, and function symbols to functions on those objects.

Token 10974:
(See Section 8.2.)

Token 10975:
The model also needs to deﬁne a probability for each such possible world, just as a Bayesiannetwork deﬁnes a probability for each assignment of values to variables.

Token 10976:
Let us suppose, for a moment, that we have ﬁgured out how to do this.

Token 10977:
Then, as usual (see page 485), we can obtain the probability of any ﬁrst-order logical sentence φas a sum over the possible worlds where it is true: P(φ)=/summationdisplay ω:φis true in ωP(ω).

Token 10978:
(14.13) Conditional probabilities P(φ|e)can be obtained similarly, so we can, in principle, ask any question we want of our model—e.g., “Which books are most likely to be recommendedhighly by dishonest customers?”—and get an answer.

Token 10979:
So far, so good. There is, however, a problem: the set of ﬁrst-order models is inﬁnite.

Token 10980:
We saw this explicitly in Figure 8.4 on page 293, which we show again in Figure 14.18 (top).

Token 10981:
This meansthat (1) the summation in Equation (14.13) could be infeasible, and (2) specifying a complete,consistent distribution over an inﬁnite set of worlds could be very difﬁcult.

Token 10982:
Section 14.6.2 explores one approach to dealing with this problem.

Token 10983:
The idea is to borrow not from the standard semantics of ﬁrst-order logic but from the database seman- ticsdeﬁned in Section 8.2.8 (page 299).

Token 10984:
The database semantics makes the unique names assumption —here, we adopt it for the constant symbols.

Token 10985:
It also assumes domain closure — there are no more objects than those that are named.

Token 10986:
We can then guarantee a ﬁnite set ofpossible worlds by making the set of objects in each world be exactly the set of constant

Token 10987:
Section 14.6. Relational and First-Order Probability Models 541 RJ RJ RJ RJ RJ RJ . . . . . . . . . . .

Token 10988:
.RJ R JRJ R JRJ R JRJ R JRJ R J Figure 14.18 Top: Some members of the set of all possible worlds for a language with two constant symbols, RandJ, and one binary relation symbol, under the standard semantics for ﬁrst-order logic.

Token 10989:
Bottom: the possible worlds under database semantics.

Token 10990:
The interpretationof the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.

Token 10991:
symbols that are used; as shown in Figure 14.18 (bottom), there is no uncertainty about the mapping from symbols to objects or about the objects that exist.

Token 10992:
We will call models deﬁnedin this way relational probability models , or RPMs.

Token 10993:
7The most signiﬁcant difference be-RELATIONAL PROBABILITY MODEL tween the semantics of RPMs and the database semantics introduced in Section 8.2.8 is that RPMs do not make the closed-world assumption—obviously, assuming that every unknownfact is false doesn’t make sense in a probabilistic reasoning system!

Token 10994:
When the underlying assumptions of database semantics fail to hold, RPMs won’t work well.

Token 10995:
For example, a book retailer might use an ISBN (International Standard Book Number)as a constant symbol to name each book, even though a given “logical” book (e.g., “Gone With the Wind”) may have several ISBNs.

Token 10996:
It would make sense to aggregate recommenda- tions across multiple ISBNs, but the retailer may not know for sure which ISBNs are reallythe same book.

Token 10997:
(Note that we are not reifying the individual copies of the book, which might be necessary for used-book sales, car sales, and so on.)

Token 10998:
Worse still, each customer is iden- tiﬁed by a login ID, but a dishonest customer may have thousands of IDs!

Token 10999:
In the computer security ﬁeld, these multiple IDs are called sibyls and their use to confound a reputation sys- SIBYL tem is called a sibyl attack .

Token 11000:
Thus, even a simple application in a relatively well-deﬁned, SIBYL ATTACK online domain involves both existence uncertainty (what are the real books and customersEXISTENCE UNCERTAINTY underlying the observed data) and identity uncertainty (which symbol really refer to theIDENTITY UNCERTAINTY same object).

Token 11001:
We need to bite the bullet and deﬁne probability models based on the standard semantics of ﬁrst-order logic, for which the possible worlds vary in the objects they contain and in the mappings from symbols to objects.

Token 11002:
Section 14.6.3 shows how to do this.

Token 11003:
7The name relational probability model was given by Pfeffer (2000) to a slightly different representation, but the underlying ideas are the same.

Token 11004:
542 Chapter 14.

Token 11005:
Probabilistic Reasoning 14.6.2 Relational probability models Like ﬁrst-order logic, RPMs have constant, function, and predicate symbols.

Token 11006:
(It turns out to be easier to view predicates as functions that return true orfalse .)

Token 11007:
We will also assume a type signature for each function, that is, a speciﬁcation of the type of each argument and the TYPE SIGNATURE function’s value.

Token 11008:
If the type of each object is known, many spurious possible worlds are elim- inated by this mechanism.

Token 11009:
For the book-recommendation domain, the types are Customer andBook , and the type signatures for the functions and predicates are as follows: Honest :Customer→{true,false}Kindness :Customer→{1,2,3,4,5} Quality :Book→{1,2,3,4,5} Recommendation :Customer×Book→{1,2,3,4,5} The constant symbols will be whatever customer and book names appear in the retailer’s data set.

Token 11010:
In the example given earlier (Figure 14.17(b)), these were C1,C2andB1,B2.

Token 11011:
Given the constants and their types, together with the functions and their type signa- tures, the random variables of the RPM are obtained by instantiating each function with eachpossible combination of objects: Honest (C 1),Quality (B2),Recommendation (C1,B2), and so on.

Token 11012:
These are exactly the variables appearing in Figure 14.17(b).

Token 11013:
Because each typehas only ﬁnitely many instances, the number of basic random variables is also ﬁnite.

Token 11014:
To complete the RPM, we have to write the dependencies that govern these random variables.

Token 11015:
There is one dependency statement for each function, where each argument of the function is a logical variable (i.e., a variable that ranges over objects, as in ﬁrst-order logic): Honest (c)∼/angbracketleft0.99,0.01/angbracketright Kindness (c)∼/angbracketleft0.1,0.1,0.2,0.3,0.3/angbracketright Quality (b)∼/angbracketleft0.05,0.2,0.4,0.2,0.15/angbracketright Recommendation (c,b)∼RecCPT (Honest (c),Kindness (c),Quality (b)) where RecCPT is a separately deﬁned conditional distribution with 2×5×5=50 rows, each with 5 entries.

Token 11016:
The semantics of the RPM can be obtained by instantiating these de- pendencies for all known constants, giving a Bayesian network (as in Figure 14.17(b)) that deﬁnes a joint distribution over the RPM’s random variables.8 We can reﬁne the model by introducing a context-speciﬁc independence to reﬂect theCONTEXT-SPECIFIC INDEPENDENCE fact that dishonest customers ignore quality when giving a recommendation; moreover, kind- ness plays no role in their decisions.

Token 11017:
A context-speciﬁc independence allows a variable to beindependent of some of its parents given certain values of others; thus, Recommendation (c,b) is independent of Kindness (c)andQuality (b)whenHonest (c)=false : Recommendation (c,b)∼ ifHonest (c)then HonestRecCPT (Kindness (c),Quality (b)) else/angbracketleft0.4,0.1,0.0,0.1,0.4/angbracketright.

Token 11018:
8Some technical conditions must be observed to guarantee that the RPM deﬁnes a proper distribution.

Token 11019:
First, the dependencies must be acyclic , otherwise the resulting Bayesian network will have cycles and will not deﬁne a proper distribution.

Token 11020:
Second, the dependencies must be well-founded , that is, there can be no inﬁnite ancestor chains, such as might arise from recursive dependencies.

Token 11021:
Under some circumstances (see Exercise 14.6), a ﬁxed-point calculation yields a well-deﬁned probability model for a recursive RPM.

Token 11022:
Section 14.6.

Token 11023:
Relational and First-Order Probability Models 543 Recommendation (C1, B1)Honesty (C1)Kindness (C1) Quality (B1) Recommendation (C2, B1)Quality (B2)Fan(C1, A1)Fan(C1, A2)Author (B2) Figure 14.19 Fragment of the equivalent Bayes net when Author (B2)is unknown.

Token 11024:
This kind of dependency may look like an ordinary if–then–else statement on a programming language, but there is a key difference: the inference engine doesn’t necessarily know the value of the conditional test !

Token 11025:
We can elaborate this model in endless ways to make it more realistic.

Token 11026:
For example, suppose that an honest customer who is a fan of a book’s author always gives the book a 5, regardless of quality: Recommendation (c,b)∼ ifHonest (c)then ifFan(c,Author (b))thenExactly (5) elseHonestRecCPT (Kindness (c),Quality (b)) else/angbracketleft0.4,0.1,0.0,0.1,0.4/angbracketright Again, the conditional test Fan(c,Author (b))is unknown, but if a customer gives only 5s to a particular author’s books and is not otherwise especially kind, then the posterior probability that the customer is a fan of that author will be high.

Token 11027:
Furthermore, the posterior distribution will tend to discount the customer’s 5s in evaluating the quality of that author’s books.

Token 11028:
In the preceding example, we implicitly assumed that the value of Author (b)is known for every b, but this may not be the case.

Token 11029:
How can the system reason about whether, say, C1 is a fan of Author (B2)whenAuthor (B2)is unknown?

Token 11030:
The answer is that the system may have to reason about all possible authors .

Token 11031:
Suppose (to keep things simple) that there are just two authors, A1andA2.T h e n Author (B2)is a random variable with two possible values, A1andA2, and it is a parent of Recommendation (C1,B2).T h ev a r i a b l e s Fan(C1,A1)and Fan(C1,A2)are parents too.

Token 11032:
The conditional distribution for Recommendation (C1,B2)is then essentially a multiplexer in which the Author (B2)parent acts as a selector to choose MULTIPLEXER which of Fan(C1,A1)andFan(C1,A2)actually gets to inﬂuence the recommendation.

Token 11033:
A fragment of the equivalent Bayes net is shown in Figure 14.19.

Token 11034:
Uncertainty in the valueofAuthor (B 2), which affects the dependency structure of the network, is an instance of relational uncertainty .RELATIONAL UNCERTAINTY In case you are wondering how the system can possibly work out who the author of B2is: consider the possibility that three other customers are fans of A1(and have no other favorite authors in common) and all three have given B2a 5, even though most other cus- tomers ﬁnd it quite dismal.

Token 11035:
In that case, it is extremely likely that A1is the author of B2.

Token 11036:
544 Chapter 14.

Token 11037:
Probabilistic Reasoning The emergence of sophisticated reasoning like this from an RPM model of just a few lines is an intriguing example of how probabilistic inﬂuences spread through the web of intercon-nections among objects in the model.

Token 11038:
As more dependencies and more objects are added, thepicture conveyed by the posterior distribution often becomes clearer and clearer.

Token 11039:
The next question is how to do inference in RPMs.

Token 11040:
One approach is to collect the evidence and query and the constant symbols therein, construct the equivalent Bayes net, and apply any of the inference methods discussed in this chapter.

Token 11041:
This technique is calledunrolling . The obvious drawback is that the resulting Bayes net may be very large.

Token 11042:
Further- UNROLLING more, if there are many candidate objects for an unknown relation or function—for example, the unknown author of B2—then some variables in the network may have many parents.

Token 11043:
Fortunately, much can be done to improve on generic inference algorithms.

Token 11044:
First, the presence of repeated substructure in the unrolled Bayes net means that many of the factorsconstructed during variable elimination (and similar kinds of tables constructed by cluster-ing algorithms) will be identical; effective caching schemes have yielded speedups of threeorders of magnitude for large networks.

Token 11045:
Second, inference methods developed to take advan-tage of context-speciﬁc independence in Bayes nets ﬁnd many applications in RPMs.

Token 11046:
Third,MCMC inference algorithms have some interesting properties when applied to RPMs with relational uncertainty.

Token 11047:
MCMC works by sampling complete possible worlds, so in each state the relational structure is completely known.

Token 11048:
In the example given earlier, each MCMC statewould specify the value of Author (B 2), and so the other potential authors are no longer par- ents of the recommendation nodes for B2.

Token 11049:
For MCMC, then, relational uncertainty causes no increase in network complexity; instead, the MCMC process includes transitions that changethe relational structure, and hence the dependency structure, of the unrolled network.

Token 11050:
All of the methods just described assume that the RPM has to be partially or completely unrolled into a Bayesian network.

Token 11051:
This is exactly analogous to the method of proposition- alization for ﬁrst-order logical inference. (See page 322.)

Token 11052:
Resolution theorem-provers and logic programming systems avoid propositionalizing by instantiating the logical variablesonly as needed to make the inference go through; that is, they liftthe inference process above the level of ground propositional sentences and make each lifted step do the work of many ground steps.

Token 11053:
The same idea applied in probabilistic inference.

Token 11054:
For example, in the variableelimination algorithm, a lifted factor can represent an entire set of ground factors that assignprobabilities to random variables in the RPM, where those random variables differ only in theconstant symbols used to construct them.

Token 11055:
The details of this method are beyond the scope ofthis book, but references are given at the end of the chapter.

Token 11056:
14.6.3 Open-universe probability models We argued earlier that database semantics was appropriate for situations in which we knowexactly the set of relevant objects that exist and can identify them unambiguously.

Token 11057:
(In partic- ular, all observations about an object are correctly associated with the constant symbol that names it.)

Token 11058:
In many real-world settings, however, these assumptions are simply untenable.

Token 11059:
Wegave the examples of multiple ISBNs and sibyl attacks in the book-recommendation domain(to which we will return in a moment), but the phenomenon is far more pervasive:

Token 11060:
Section 14.6.

Token 11061:
Relational and First-Order Probability Models 545 •A vision system doesn’t know what exists, if anything, around the next corner, and may not know if the object it sees now is the same one it saw a few minutes ago.

Token 11062:
•A text-understanding system does not know in advance the entities that will be featured in a text, and must reason about whether phrases such as “Mary,” “Dr.

Token 11063:
Smith,” “she,”“his cardiologist,” “his mother,” and so on refer to the same object.

Token 11064:
•An intelligence analyst hunting for spies never knows how many spies there really are and can only guess whether various pseudonyms, phone numbers, and sightings belongto the same individual.

Token 11065:
In fact, a major part of human cognition seems to require learning what objects exist and being able to connect observations—which almost never come with unique IDs attached—tohypothesized objects in the world.

Token 11066:
For these reasons, we need to be able to write so-called open-universe probability OPEN UNIVERSE models or OUPMs based on the standard semantics of ﬁrst-order logic, as illustrated at the top of Figure 14.18.

Token 11067:
A language for OUPMs provides a way of writing such models easilywhile guaranteeing a unique, consistent probability distribution over the inﬁnite space ofpossible worlds.

Token 11068:
The basic idea is to understand how ordinary Bayesian networks and RPMs manage to deﬁne a unique probability model and to transfer that insight to the ﬁrst-order setting.

Token 11069:
In essence, a Bayes net generates each possible world, event by event, in the topological order deﬁned by the network structure, where each event is an assignment of a value to a variable.

Token 11070:
An RPM extends this to entire sets of events, deﬁned by the possible instantiations of thelogical variables in a given predicate or function.

Token 11071:
OUPMs go further by allowing generativesteps that add objects to the possible world under construction, where the number and type of objects may depend on the objects that are already in that world.

Token 11072:
That is, the event beinggenerated is not the assignment of a value to a variable, but the very existence of objects.

Token 11073:
One way to do this in OUPMs is to add statements that deﬁne conditional distributions over the numbers of objects of various kinds.

Token 11074:
For example, in the book-recommendationdomain, we might want to distinguish between customers (real people) and their login IDs .

Token 11075:
Suppose we expect somewhere between 100 and 10,000 distinct customers (whom we cannotobserve directly).

Token 11076:
We can express this as a prior log-normal distribution 9as follows: #Customer∼LogNormal [6.9,2.32]().

Token 11077:
We expect honest customers to have just one ID, whereas dishonest customers might have anywhere between 10 and 1000 IDs: #LoginID (Owner =c)∼ ifHonest (c)thenExactly (1) elseLogNormal [6.9,2.32]().

Token 11078:
This statement deﬁnes the number of login IDs for a given owner, who is a customer.

Token 11079:
The Owner function is called an origin function because it says where each generated object ORIGIN FUNCTION came from.

Token 11080:
In the formal semantics of B LOG (as distinct from ﬁrst-order logic), the domain elements in each possible world are actually generation histories (e.g., “the fourth login ID ofthe seventh customer”) rather than simple tokens.

Token 11081:
9A distribution LogNormal [μ, σ2](x)is equivalent to a distribution N[μ, σ2](x)overloge(x).

Token 11082:
546 Chapter 14.

Token 11083:
Probabilistic Reasoning Subject to technical conditions of acyclicity and well-foundedness similar to those for RPMs, open-universe models of this kind deﬁne a unique distribution over possible worlds.Furthermore, there exist inference algorithms such that, for every such well-deﬁned modeland every ﬁrst-order query, the answer returned approaches the true posterior arbitrarilyclosely in the limit.

Token 11084:
There are some tricky issues involved in designing these algorithms.

Token 11085:
For example, an MCMC algorithm cannot sample directly in the space of possible worlds when the size of those worlds is unbounded; instead, it samples ﬁnite, partial worlds, rely-ing on the fact that only ﬁnitely many objects can be relevant to the query in distinct ways.Moreover, transitions must allow for merging two objects into one or splitting one into two.

Token 11086:
(Details are given in the references at the end of the chapter.)

Token 11087:
Despite these complications,the basic principle established in Equation (14.13) still holds: the probability of any sentenceis well deﬁned and can be calculated.

Token 11088:
Research in this area is still at an early stage, but already it is becoming clear that ﬁrst- order probabilistic reasoning yields a tremendous increase in the effectiveness of AI systemsat handling uncertain information.

Token 11089:
Potential applications include those mentioned above—computer vision, text understanding, and intelligence analysis—as well as many other kindsof sensor interpretation.

Token 11090:
14.7 O THER APPROACHES TO UNCERTAIN REASONING Other sciences (e.g., physics, genetics, and economics) have long favored probability as amodel for uncertainty.

Token 11091:
In 1819, Pierre Laplace said, “Probability theory is nothing but com-mon sense reduced to calculation.” In 1850, James Maxwell said, “The true logic for thisworld is the calculus of Probabilities, which takes account of the magnitude of the probabil-ity which is, or ought to be, in a reasonable man’s mind.” Given this long tradition, it is perhaps surprising that AI has considered many alterna- tives to probability.

Token 11092:
The earliest expert systems of the 1970s ignored uncertainty and usedstrict logical reasoning, but it soon became clear that this was impractical for most real-worlddomains.

Token 11093:
The next generation of expert systems (especially in medical domains) used prob-abilistic techniques.

Token 11094:
Initial results were promising, but they did not scale up because of the exponential number of probabilities required in the full joint distribution.

Token 11095:
(Efﬁcient Bayesian network algorithms were unknown then.)

Token 11096:
As a result, probabilistic approaches fell out offavor from roughly 1975 to 1988, and a variety of alternatives to probability were tried for avariety of reasons: •One common view is that probability theory is essentially numerical, whereas human judgmental reasoning is more “qualitative.” Certainly, we are not consciously aware of doing numerical calculations of degrees of belief.

Token 11097:
(Neither are we aware of doing uniﬁcation, yet we seem to be capable of some kind of logical reasoning.)

Token 11098:
It might be that we have some kind of numerical degrees of belief encoded directly in strengths of connections and activations in our neurons.

Token 11099:
In that case, the difﬁculty of consciousaccess to those strengths is not surprising. One should also note that qualitative reason-

Token 11100:
Section 14.7.

Token 11101:
Other Approaches to Uncertain Reasoning 547 ing mechanisms can be built directly on top of probability theory, so the “no numbers” argument against probability has little force.

Token 11102:
Nonetheless, some qualitative schemeshave a good deal of appeal in their own right.

Token 11103:
One of the best studied is default rea- soning , which treats conclusions not as “believed to a certain degree,” but as “believed until a better reason is found to believe something else.” Default reasoning is covered in Chapter 12.

Token 11104:
•Rule-based approaches to uncertainty have also been tried.

Token 11105:
Such approaches hope to build on the success of logical rule-based systems, but add a sort of “fudge factor” toeach rule to accommodate uncertainty.

Token 11106:
These methods were developed in the mid-1970sand formed the basis for a large number of expert systems in medicine and other areas.

Token 11107:
•One area that we have not addressed so far is the question of ignorance , as opposed to uncertainty. Consider the ﬂipping of a coin.

Token 11108:
If we know that the coin is fair, thena probability of 0.5 for heads is reasonable.

Token 11109:
If we know that the coin is biased, butwe do not know which way, then 0.5 for heads is again reasonable.

Token 11110:
Obviously, thetwo cases are different, yet the outcome probability seems not to distinguish them.

Token 11111:
TheDempster–Shafer theory uses interval-valued degrees of belief to represent an agent’s knowledge of the probability of a proposition.

Token 11112:
•Probability makes the same ontological commitment as logic: that propositions are true or false in the world, even if the agent is uncertain as to which is the case.

Token 11113:
Researchersinfuzzy logic have proposed an ontology that allows vagueness : that a proposition can be “sort of” true.

Token 11114:
Vagueness and uncertainty are in fact orthogonal issues. The next three subsections treat some of these approaches in slightly more depth.

Token 11115:
We will not provide detailed technical material, but we cite references for further study.

Token 11116:
14.7.1 Rule-based methods for uncertain reasoning Rule-based systems emerged from early work on practical and intuitive systems for logicalinference.

Token 11117:
Logical systems in general, and logical rule-based systems in particular, have threedesirable properties: •Locality : In logical systems, whenever we have a rule of the form A⇒B, we can LOCALITY conclude B, given evidence A,without worrying about any other rules.

Token 11118:
In probabilistic systems, we need to consider allthe evidence.

Token 11119:
•Detachment : Once a logical proof is found for a proposition B, the proposition can be DETACHMENT used regardless of how it was derived.

Token 11120:
That is, it can be detached from its justiﬁcation.

Token 11121:
In dealing with probabilities, on the other hand, the source of the evidence for a beliefis important for subsequent reasoning.

Token 11122:
•Truth-functionality : In logic, the truth of complex sentences can be computed from TRUTH- FUNCTIONALITY the truth of the components.

Token 11123:
Probability combination does not work this way, except under strong global independence assumptions.

Token 11124:
There have been several attempts to devise uncertain reasoning schemes that retain these advantages.

Token 11125:
The idea is to attach degrees of belief to propositions and rules and to devisepurely local schemes for combining and propagating those degrees of belief.

Token 11126:
The schemes

Token 11127:
548 Chapter 14.

Token 11128:
Probabilistic Reasoning are also truth-functional; for example, the degree of belief in A∨Bis a function of the belief inAand the belief in B.

Token 11129:
The bad news for rule-based systems is that the properties of locality, detachment, and truth-functionality are simply not appropriate for uncertain reasoning.

Token 11130:
Let us look at truth- functionality ﬁrst.

Token 11131:
Let H1be the event that a fair coin ﬂip comes up heads, let T1be the event that the coin comes up tails on that same ﬂip, and let H2be the event that the coin comes up heads on a second ﬂip.

Token 11132:
Clearly, all three events have the same probability, 0.5, and so atruth-functional system must assign the same belief to the disjunction of any two of them.But we can see that the probability of the disjunction depends on the events themselves andnot just on their probabilities: P(A) P(B) P(A∨B) P(H1)=0.5 P(H1∨H1)=0.50 P(H1)=0.5 P(T1)=0.5 P(H1∨T1)=1.00 P(H2)=0.5 P(H1∨H2)=0.75 It gets worse when we chain evidence together.

Token 11133:
Truth-functional systems have rules of the formA/mapsto→Bthat allow us to compute the belief in Bas a function of the belief in the rule and the belief in A.

Token 11134:
Both forward- and backward-chaining systems can be devised.

Token 11135:
The belief in the rule is assumed to be constant and is usually speciﬁed by the knowledge engineer—forexample, as A/mapsto→ 0.9B.

Token 11136:
Consider the wet-grass situation from Figure 14.12(a) (page 529).

Token 11137:
If we wanted to be able to do both causal and diagnostic reasoning, we would need the two rules Rain/mapsto→WetGrass and WetGrass/mapsto→Rain.

Token 11138:
These two rules form a feedback loop: evidence for Rain increases the belief in WetGrass , which in turn increases the belief in Rain even more.

Token 11139:
Clearly, uncertain reasoning systems need to keep track of the paths along which evidence is propagated.

Token 11140:
Intercausal reasoning (or explaining away) is also tricky.

Token 11141:
Consider what happens when we have the two rules Sprinkler/mapsto→WetGrass and WetGrass/mapsto→Rain. Suppose we see that the sprinkler is on.

Token 11142:
Chaining forward through our rules, this increases the belief that the grass will be wet, which in turn increases the belief that it is raining.

Token 11143:
But thisis ridiculous: the fact that the sprinkler is on explains away the wet grass and should reduce the belief in rain.

Token 11144:
A truth-functional system acts as if it also believes Sprinkler/mapsto→Rain .

Token 11145:
Given these difﬁculties, how can truth-functional systems be made useful in practice?

Token 11146:
The answer lies in restricting the task and in carefully engineering the rule base so that un-desirable interactions do not occur.

Token 11147:
The most famous example of a truth-functional systemfor uncertain reasoning is the certainty factors model, which was developed for the M YCIN CERTAINTY FACTOR medical diagnosis program and was widely used in expert systems of the late 1970s and 1980s.

Token 11148:
Almost all uses of certainty factors involved rule sets that were either purely diagnos-tic (as in M YCIN ) or purely causal.

Token 11149:
Furthermore, evidence was entered only at the “roots” of the rule set, and most rule sets were singly connected. Heckerman (1986) has shown that,

Token 11150:
Section 14.7.

Token 11151:
Other Approaches to Uncertain Reasoning 549 under these circumstances, a minor variation on certainty-factor inference was exactly equiv- alent to Bayesian inference on polytrees.

Token 11152:
In other circumstances, certainty factors could yielddisastrously incorrect degrees of belief through overcounting of evidence.

Token 11153:
As rule sets be-came larger, undesirable interactions between rules became more common, and practitionersfound that the certainty factors of many other rules had to be “tweaked” when new rules were added.

Token 11154:
For these reasons, Bayesian networks have largely supplanted rule-based methods for uncertain reasoning.

Token 11155:
14.7.2 Representing ignorance: Dempster–Shafer theory The Dempster–Shafer theory is designed to deal with the distinction between uncertaintyDEMPSTER–SHAFER THEORY and ignorance .

Token 11156:
Rather than computing the probability of a proposition, it computes the probability that the evidence supports the proposition.

Token 11157:
This measure of belief is called abelief function , written Bel(X). BELIEF FUNCTION We return to coin ﬂipping for an example of belief functions.

Token 11158:
Suppose you pick a coin from a magician’s pocket.

Token 11159:
Given that the coin might or might not be fair, what beliefshould you ascribe to the event that it comes up heads?

Token 11160:
Dempster–Shafer theory says that because you have no evidence either way, you have to say that the belief Bel(Heads )=0 and also that Bel(¬Heads )=0 .

Token 11161:
This makes Dempster–Shafer reasoning systems skeptical in a way that has some intuitive appeal.

Token 11162:
Now suppose you have an expert at your disposalwho testiﬁes with 90% certainty that the coin is fair (i.e., he is 90% sure that P(Heads )= 0.5).

Token 11163:
Then Dempster–Shafer theory gives Bel(Heads )=0 .9×0.5=0 .45and likewise Bel(¬Heads )=0.45.

Token 11164:
There is still a 10 percentage point “gap” that is not accounted for by the evidence.

Token 11165:
The mathematical underpinnings of Dempster–Shafer theory have a similar ﬂavor to those of probability theory; the main difference is that, instead of assigning probabilities to possible worlds, the theory assigns masses tosetsof possible world, that is, to events.

Token 11166:
MASS The masses still must add to 1 over all possible events.

Token 11167:
Bel(A)is deﬁned to be the sum of masses for all events that are subsets of (i.e., that entail) A, including Aitself.

Token 11168:
With this deﬁnition, Bel(A)andBel(¬A)sum to at most 1, and the gap—the interval between Bel(A) and1−Bel(¬A)—is often interpreted as bounding the probability of A.

Token 11169:
As with default reasoning, there is a problem in connecting beliefs to actions.

Token 11170:
Whenever there is a gap in the beliefs, then a decision problem can be deﬁned such that a Dempster–Shafer system is unable to make a decision.

Token 11171:
In fact, the notion of utility in the Dempster–Shafer model is not yet well understood because the meanings of masses and beliefs them-selves have yet to be understood.

Token 11172:
Pearl (1988) has argued that Bel(A)should be interpreted not as a degree of belief in Abut as the probability assigned to all the possible worlds (now interpreted as logical theories) in which Aisprovable .

Token 11173:
While there are cases in which this quantity might be of interest, it is not the same as the probability that Ais true.

Token 11174:
A Bayesian analysis of the coin-ﬂipping example would suggest that no new formalism is necessary to handle such cases.

Token 11175:
The model would have two variables: the Bias of the coin (a number between 0 and 1, where 0 is a coin that always shows tails and 1 a coin that alwaysshows heads) and the outcome of the next Flip.

Token 11176:
The prior probability distribution for Bias

Token 11177:
550 Chapter 14.

Token 11178:
Probabilistic Reasoning would reﬂect our beliefs based on the source of the coin (the magician’s pocket): some small probability that it is fair and some probability that it is heavily biased toward heads or tails.The conditional distribution P(Flip|Bias)simply deﬁnes how the bias operates.

Token 11179:
If P(Bias) is symmetric about 0.5, then our prior probability for the ﬂip is P(Flip=heads)=/integraldisplay 1 0P(Bias=x)P(Flip=heads|Bias=x)dx=0.5.

Token 11180:
This is the same prediction as if we believe strongly that the coin is fair, but that does not mean that probability theory treats the two situations identically.

Token 11181:
The difference arises after the ﬂips in computing the posterior distribution for Bias .

Token 11182:
If the coin came from a bank, then seeing it come up heads three times running would have almost no effect on our strong priorbelief in its fairness; but if the coin comes from the magician’s pocket, the same evidencewill lead to a stronger posterior belief that the coin is biased toward heads.

Token 11183:
Thus, a Bayesianapproach expresses our “ignorance” in terms of how our beliefs would change in the face offuture information gathering.

Token 11184:
14.7.3 Representing vagueness: Fuzzy sets and fuzzy logic Fuzzy set theory is a means of specifying how well an object satisﬁes a vague description.

Token 11185:
FUZZY SET THEORY For example, consider the proposition “Nate is tall.” Is this true if Nate is 5/prime10/prime/prime?M o s t people would hesitate to answer “true” or “false,” preferring to say, “sort of.” Note that thisis not a question of uncertainty about the external world—we are sure of Nate’s height.

Token 11186:
Theissue is that the linguistic term “tall” does not refer to a sharp demarcation of objects into twoclasses—there are degrees of tallness.

Token 11187:
For this reason, fuzzy set theory is not a method for uncertain reasoning at all.

Token 11188:
Rather, fuzzy set theory treats Tall as a fuzzy predicate and says that the truth value of Tall(Nate)is a number between 0 and 1, rather than being just true orfalse .

Token 11189:
The name “fuzzy set” derives from the interpretation of the predicate as implicitly deﬁning a set of its members—a set that does not have sharp boundaries.

Token 11190:
Fuzzy logic is a method for reasoning with logical expressions describing membership FUZZY LOGIC in fuzzy sets.

Token 11191:
For example, the complex sentence Tall(Nate)∧Heavy (Nate)has a fuzzy truth value that is a function of the truth values of its components.

Token 11192:
The standard rules forevaluating the fuzzy truth, T, of a complex sentence are T(A∧B)=m i n ( T(A),T(B)) T(A∨B)=m a x ( T(A),T(B)) T(¬A)=1−T(A).

Token 11193:
Fuzzy logic is therefore a truth-functional system—a fact that causes serious difﬁculties.

Token 11194:
For example, suppose that T(Tall(Nate))= 0.6andT(Heavy (Nate))= 0.4.T h e nw eh a v e T(Tall(Nate)∧Heavy (Nate))= 0.4, which seems reasonable, but we also get the result T(Tall(Nate)∧¬Tall(Nate))= 0.4, which does not.

Token 11195:
Clearly, the problem arises from the inability of a truth-functional approach to take into account the correlations or anticorrelations among the component propositions.

Token 11196:
Fuzzy control is a methodology for constructing control systems in which the mapping FUZZY CONTROL between real-valued input and output parameters is represented by fuzzy rules.

Token 11197:
Fuzzy con- trol has been very successful in commercial products such as automatic transmissions, video

Token 11198:
Section 14.8. Summary 551 cameras, and electric shavers.

Token 11199:
Critics (see, e.g., Elkan, 1993) argue that these applications are successful because they have small rule bases, no chaining of inferences, and tunableparameters that can be adjusted to improve the system’s performance.

Token 11200:
The fact that they areimplemented with fuzzy operators might be incidental to their success; the key is simply toprovide a concise and intuitive way to specify a smoothly interpolated, real-valued function.

Token 11201:
There have been attempts to provide an explanation of fuzzy logic in terms of probabil- ity theory.

Token 11202:
One idea is to view assertions such as “Nate is Tall” as discrete observations madeconcerning a continuous hidden variable, Nate’s actual Height .

Token 11203:
The probability model speci- ﬁesP(Observer says Nate is tall |Height ), perhaps using a probit distribution as described on page 522.

Token 11204:
A posterior distribution over Nate’s height can then be calculated in the usualway, for example, if the model is part of a hybrid Bayesian network.

Token 11205:
Such an approach is nottruth-functional, of course.

Token 11206:
For example, the conditional distribution P(Observer says Nate is tall and heavy |Height ,Weight ) allows for interactions between height and weight in the causing of the observation.

Token 11207:
Thus, someone who is eight feet tall and weighs 190 pounds is very unlikely to be called “tall andheavy,” even though “eight feet” counts as “tall” and “190 pounds” counts as “heavy.” Fuzzy predicates can also be given a probabilistic interpretation in terms of random sets—that is, random variables whose possible values are sets of objects.

Token 11208:
For example, Tall RANDOM SET is a random set whose possible values are sets of people.

Token 11209:
The probability P(Tall=S1), where S1is some particular set of people, is the probability that exactly that set would be identiﬁed as “tall” by an observer.

Token 11210:
Then the probability that “Nate is tall” is the sum of theprobabilities of all the sets of which Nate is a member.

Token 11211:
Both the hybrid Bayesian network approach and the random sets approach appear to capture aspects of fuzziness without introducing degrees of truth.

Token 11212:
Nonetheless, there remainmany open issues concerning the proper representation of linguistic observations and contin-uous quantities—issues that have been neglected by most outside the fuzzy community.

Token 11213:
14.8 S UMMARY This chapter has described Bayesian networks , a well-developed representation for uncertain knowledge.

Token 11214:
Bayesian networks play a role roughly analogous to that of propositional logicfor deﬁnite knowledge.

Token 11215:
•A Bayesian network is a directed acyclic graph whose nodes correspond to random variables; each node has a conditional distribution for the node, given its parents.

Token 11216:
•Bayesian networks provide a concise way to represent conditional independence rela- tionships in the domain.

Token 11217:
•A Bayesian network speciﬁes a full joint distribution; each joint entry is deﬁned as the product of the corresponding entries in the local conditional distributions.

Token 11218:
A Bayesiannetwork is often exponentially smaller than an explicitly enumerated joint distribution.

Token 11219:
•Many conditional distributions can be represented compactly by canonical families of

Token 11220:
552 Chapter 14. Probabilistic Reasoning distributions.

Token 11221:
Hybrid Bayesian networks , which include both discrete and continuous variables, use a variety of canonical distributions.

Token 11222:
•Inference in Bayesian networks means computing the probability distribution of a set of query variables, given a set of evidence variables.

Token 11223:
Exact inference algorithms, such asvariable elimination , evaluate sums of products of conditional probabilities as efﬁ- ciently as possible.

Token 11224:
•Inpolytrees (singly connected networks), exact inference takes time linear in the size of the network.

Token 11225:
In the general case, the problem is intractable.

Token 11226:
•Stochastic approximation techniques such as likelihood weighting andMarkov chain Monte Carlo can give reasonable estimates of the true posterior probabilities in a net- work and can cope with much larger networks than can exact algorithms.

Token 11227:
•Probability theory can be combined with representational ideas from ﬁrst-order logic to produce very powerful systems for reasoning under uncertainty.

Token 11228:
Relational probabil- ity models (RPMs) include representational restrictions that guarantee a well-deﬁned probability distribution that can be expressed as an equivalent Bayesian network.

Token 11229:
Open- universe probability models handle existence andidentity uncertainty , deﬁning prob- abilty distributions over the inﬁnite space of ﬁrst-order possible worlds.

Token 11230:
•Various alternative systems for reasoning under uncertainty have been suggested.

Token 11231:
Gen- erally speaking, truth-functional systems are not well suited for such reasoning.

Token 11232:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The use of networks to represent probabilistic information began early in the 20th century, with the work of Sewall Wright on the probabilistic analysis of genetic inheritance and an-imal growth factors (Wright, 1921, 1934).

Token 11233:
I. J.

Token 11234:
Good (1961), in collaboration with AlanTuring, developed probabilistic representations and Bayesian inference methods that couldbe regarded as a forerunner of modern Bayesian networks—although the paper is not oftencited in this context.

Token 11235:
10The same paper is the original source for the noisy-OR model.

Token 11236:
The inﬂuence diagram representation for decision problems, which incorporated a DAG representation for random variables, was used in decision analysis in the late 1970s(see Chapter 16), but only enumeration was used for evaluation.

Token 11237:
Judea Pearl developed themessage-passing method for carrying out inference in tree networks (Pearl, 1982a) and poly-tree networks (Kim and Pearl, 1983) and explained the importance of causal rather than di-agnostic probability models, in contrast to the certainty-factor systems then in vogue.

Token 11238:
The ﬁrst expert system using Bayesian networks was C ONVINCE (Kim, 1983).

Token 11239:
Early applications in medicine included the M UNIN system for diagnosing neuromuscular disorders (Andersen et al.

Token 11240:
, 1989) and the P ATHFINDER system for pathology (Heckerman, 1991). The CPCS system (Pradhan et al.

Token 11241:
, 1994) is a Bayesian network for internal medicine consisting 10I. J. Good was chief statistician for Turing’s code-breaking team in World War II.

Token 11242:
In 2001: A Space Odyssey (Clarke, 1968a), Good and Minsky are credited with making the breakthrough that led to the development of theHAL 9000 computer.

Token 11243:
Bibliographical and Historical Notes 553 of 448 nodes, 906 links and 8,254 conditional probability values.

Token 11244:
(The front cover shows a portion of the network.)

Token 11245:
Applications in engineering include the Electric Power Research Institute’s work on monitoring power generators (Morjaria et al.

Token 11246:
, 1995), NASA’s work on displaying time- critical information at Mission Control in Houston (Horvitz and Barry, 1995), and the general ﬁeld of network tomography , which aims to infer unobserved local properties of nodes and links in the Internet from observations of end-to-end message performance (Castro et al.

Token 11247:
, 2004).

Token 11248:
Perhaps the most widely used Bayesian network systems have been the diagnosis-and-repair modules (e.g., the Printer Wizard) in Microsoft Windows (Breese and Heckerman,1996) and the Ofﬁce Assistant in Microsoft Ofﬁce (Horvitz et al.

Token 11249:
, 1998).

Token 11250:
Another impor- tant application area is biology: Bayesian networks have been used for identifying humangenes by reference to mouse genes (Zhang et al.

Token 11251:
, 2003), inferring cellular networks Friedman (2004), and many other tasks in bioinformatics.

Token 11252:
We could go on, but instead we’ll refer youto Pourret et al. (2008), a 400-page guide to applications of Bayesian networks.

Token 11253:
Ross Shachter (1986), working in the inﬂuence diagram community, developed the ﬁrst complete algorithm for general Bayesian networks.

Token 11254:
His method was based on goal-directedreduction of the network using posterior-preserving transformations.

Token 11255:
Pearl (1986) developed a clustering algorithm for exact inference in general Bayesian networks, utilizing a conversion to a directed polytree of clusters in which message passing was used to achieve consistencyover variables shared between clusters.

Token 11256:
A similar approach, developed by the statisticiansDavid Spiegelhalter and Steffen Lauritzen (Lauritzen and Spiegelhalter, 1988), is based onconversion to an undirected form of graphical model called a Markov network .

Token 11257:
This ap- MARKOV NETWORK proach is implemented in the H UGIN system, an efﬁcient and widely used tool for uncertain reasoning (Andersen et al. , 1989).

Token 11258:
Boutilier et al. (1996) show how to exploit context-speciﬁc independence in clustering algorithms.

Token 11259:
The basic idea of variable elimination—that repeated computations within the overall sum-of-products expression can be avoided by caching—appeared in the symbolic probabilis-tic inference (SPI) algorithm (Shachter et al.

Token 11260:
, 1990). The elimination algorithm we describe is closest to that developed by Zhang and Poole (1994).

Token 11261:
Criteria for pruning irrelevant vari- ables were developed by Geiger et al. (1990) and by Lauritzen et al.

Token 11262:
(1990); the criterion we give is a simple special case of these.

Token 11263:
Dechter (1999) shows how the variable elimination ideais essentially identical to nonserial dynamic programming (Bertele and Brioschi, 1972), an NONSERIAL DYNAMIC PROGRAMMING algorithmic approach that can be applied to solve a range of inference problems in Bayesian networks—for example, ﬁnding the most likely explanation for a set of observations.

Token 11264:
This connects Bayesian network algorithms to related methods for solving CSPs and gives a directmeasure of the complexity of exact inference in terms of the tree width of the network.

Token 11265:
Wexlerand Meek (2009) describe a method of preventing exponential growth in the size of factorscomputed in variable elimination; their algorithm breaks down large factors into products ofsmaller factors and simultaneously computes an error bound for the resulting approximation.

Token 11266:
The inclusion of continuous random variables in Bayesian networks was considered by Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks contain-ing only continuous variables with linear Gaussian distributions.

Token 11267:
The inclusion of discretevariables has been investigated by Lauritzen and Wermuth (1989) and implemented in the

Token 11268:
554 Chapter 14. Probabilistic Reasoning cHUGIN system (Olesen, 1993).

Token 11269:
Further analysis of linear Gaussian models, with connec- tions to many other models used in statistics, appears in Roweis and Ghahramani (1999) Theprobit distribution is usually attributed to Gaddum (1933) and Bliss (1934), although it hadbeen discovered several times in the 19th century.

Token 11270:
Bliss’s work was expanded considerablyby Finney (1947).

Token 11271:
The probit has been used widely for modeling discrete choice phenomena and can be extended to handle more than two choices (Daganzo, 1979).

Token 11272:
The logit model was introduced by Berkson (1944); initially much derided, it eventually became more popularthan the probit model.

Token 11273:
Bishop (1995) gives a simple justiﬁcation for its use.

Token 11274:
Cooper (1990) showed that the general problem of inference in unconstrained Bayesian networks is NP-hard, and Paul Dagum and Mike Luby (1993) showed the correspondingapproximation problem to be NP-hard.

Token 11275:
Space complexity is also a serious problem in bothclustering and variable elimination methods.

Token 11276:
The method of cutset conditioning ,w h i c hw a s developed for CSPs in Chapter 6, avoids the construction of exponentially large tables.

Token 11277:
In aBayesian network, a cutset is a set of nodes that, when instantiated, reduces the remainingnodes to a polytree that can be solved in linear time and space.

Token 11278:
The query is answered bysumming over all the instantiations of the cutset, so the overall space requirement is still lin-ear (Pearl, 1988).

Token 11279:
Darwiche (2001) describes a recursive conditioning algorithm that allows a complete range of space/time tradeoffs.

Token 11280:
The development of fast approximation algorithms for Bayesian network inference is a very active area, with contributions from statistics, computer science, and physics.

Token 11281:
The rejection sampling method is a general technique that is long known to statisticians; it was ﬁrst applied to Bayesian networks by Max Henrion (1988), who called it logic sampling .

Token 11282:
Likelihood weighting, which was developed by Fung and Chang (1989) and Shachter and Peot (1989), is an example of the well-known statistical method of importance sampling .

Token 11283:
Cheng and Druzdzel (2000) describe an adaptive version of likelihood weighting that workswell even when the evidence has very low prior likelihood.

Token 11284:
Markov chain Monte Carlo (MCMC) algorithms began with the Metropolis algorithm, due to Metropolis et al.

Token 11285:
(1953), which was also the source of the simulated annealing algo- rithm described in Chapter 4.

Token 11286:
The Gibbs sampler was devised by Geman and Geman (1984) for inference in undirected Markov networks.

Token 11287:
The application of MCMC to Bayesian net-works is due to Pearl (1987). The papers collected by Gilks et al.

Token 11288:
(1996) cover a wide variety of applications of MCMC, several of which were developed in the well-known B UGS pack- age (Gilks et al. , 1994).

Token 11289:
There are two very important families of approximation methods that we did not cover in the chapter.

Token 11290:
The ﬁrst is the family of variational approximation methods, which can beVARIATIONAL APPROXIMATION used to simplify complex calculations of all kinds.

Token 11291:
The basic idea is to propose a reduced version of the original problem that is simple to work with, but that resembles the originalproblem as closely as possible.

Token 11292:
The reduced problem is described by some variational pa- rameters λthat are adjusted to minimize a distance function Dbetween the original and VARIATIONAL PARAMETER the reduced problem, often by solving the system of equations ∂D/∂ λ=0.

Token 11293:
In many cases, strict upper and lower bounds can be obtained. Variational methods have long been used instatistics (Rustagi, 1976).

Token 11294:
In statistical physics, the mean-ﬁeld method is a particular vari- MEAN FIELD ational approximation in which the individual variables making up the model are assumed

Token 11295:
Bibliographical and Historical Notes 555 to be completely independent.

Token 11296:
This idea was applied to solve large undirected Markov net- works (Peterson and Anderson, 1987; Parisi, 1988). Saul et al.

Token 11297:
(1996) developed the math- ematical foundations for applying variational methods to Bayesian networks and obtainedaccurate lower-bound approximations for sigmoid networks with the use of mean-ﬁeld meth-ods.

Token 11298:
Jaakkola and Jordan (1996) extended the methodology to obtain both lower and upper bounds.

Token 11299:
Since these early papers, variational methods have been applied to many speciﬁc families of models.

Token 11300:
The remarkable paper by Wainwright and Jordan (2008) provides a uni-fying theoretical analysis of the literature on variational methods.

Token 11301:
A second important family of approximation algorithms is based on Pearl’s polytree message-passing algorithm (1982a).

Token 11302:
This algorithm can be applied to general networks, assuggested by Pearl (1988).

Token 11303:
The results might be incorrect, or the algorithm might fail to ter-minate, but in many cases, the values obtained are close to the true values.

Token 11304:
Little attentionwas paid to this so-called belief propagation (or BP) approach until McEliece et al.

Token 11305:
(1998) BELIEF PROPAGATION observed that message passing in a multiply connected Bayesian network was exactly the computation performed by the turbo decoding algorithm (Berrou et al.

Token 11306:
, 1993), which pro- TURBO DECODING vided a major breakthrough in the design of efﬁcient error-correcting codes.

Token 11307:
The implication is that BP is both fast and accurate on the very large and very highly connected networks used for decoding and might therefore be useful more generally.

Token 11308:
Murphy et al.

Token 11309:
(1999) presented a promising empirical study of BP’s performance, and Weiss and Freeman (2001) establishedstrong convergence results for BP on linear Gaussian networks.

Token 11310:
Weiss (2000b) shows how anapproximation called loopy belief propagation works, and when the approximation is correct.Yedidia et al.

Token 11311:
(2005) made further connections between loopy propagation and ideas from statistical physics.

Token 11312:
The connection between probability and ﬁrst-order languages was ﬁrst studied by Car- nap (1950).

Token 11313:
Gaifman (1964) and Scott and Krauss (1966) deﬁned a language in which proba-bilities could be associated with ﬁrst-order sentences and for which models were probabilitymeasures on possible worlds.

Token 11314:
Within AI, this idea was developed for propositional logicby Nilsson (1986) and for ﬁrst-order logic by Halpern (1990).

Token 11315:
The ﬁrst extensive inves- tigation of knowledge representation issues in such languages was carried out by Bacchus (1990).

Token 11316:
The basic idea is that each sentence in the knowledge base expressed a constraint on the distribution over possible worlds; one sentence entails another if it expresses a strongerconstraint.

Token 11317:
For example, the sentence ∀xP(Hungry (x))>0.2rules out distributions in which any object is hungry with probability less than 0.2; thus, it entails the sentence∀xP(Hungry (x))>0.1.

Token 11318:
It turns out that writing a consistent set of sentences in these languages is quite difﬁcult and constructing a unique probability model nearly impossibleunless one adopts the representation approach of Bayesian networks by writing suitable sen-tences about conditional probabilities.

Token 11319:
Beginning in the early 1990s, researchers working on complex applications noticed the expressive limitations of Bayesian networks and developed various languages for writing “templates” with logical variables, from which large networks could be constructed automat- ically for each problem instance (Breese, 1992; Wellman et al.

Token 11320:
, 1992). The most important such language was B UGS (Bayesian inference Using Gibbs Sampling) (Gilks et al.

Token 11321:
, 1994), which combined Bayesian networks with the indexed random variable notation common inINDEXED RANDOM VARIABLE

Token 11322:
556 Chapter 14. Probabilistic Reasoning statistics. (In B UGS, an indexed random variable looks like X[i],w h e r e ihas a deﬁned integer range.)

Token 11323:
These languages inherited the key property of Bayesian networks: every well-formedknowledge base deﬁnes a unique, consistent probability model.

Token 11324:
Languages with well-deﬁnedsemantics based on unique names and domain closure drew on the representational capa-bilities of logic programming (Poole, 1993; Sato and Kameya, 1997; Kersting et al.

Token 11325:
, 2000) and semantic networks (Koller and Pfeffer, 1998; Pfeffer, 2000).

Token 11326:
Pfeffer (2007) went on to develop I BAL, which represents ﬁrst-order probability models as probabilistic programs in a programming language extended with a randomization primitive.

Token 11327:
Another important threadwas the combination of relational and ﬁrst-order notations with (undirected) Markov net-works (Taskar et al.

Token 11328:
, 2002; Domingos and Richardson, 2004), where the emphasis has been less on knowledge representation and more on learning from large data sets.

Token 11329:
Initially, inference in these models was performed by generating an equivalent Bayesian network. Pfeffer et al.

Token 11330:
(1999) introduced a variable elimination algorithm that cached each computed factor for reuse by later computations involving the same relations but differentobjects, thereby realizing some of the computational gains of lifting.

Token 11331:
The ﬁrst truly liftedinference algorithm was a lifted form of variable elimination described by Poole (2003) andsubsequently improved by de Salvo Braz et al.

Token 11332:
(2007). Further advances, including cases where certain aggregate probabilities can be computed in closed form, are described by Milch et al.

Token 11333:
(2008) and Kisynski and Poole (2009).

Token 11334:
Pasula and Russell (2001) studied the application of MCMC to avoid building the complete equivalent Bayes net in cases of relational and identity uncertainty.

Token 11335:
Getoor and Taskar (2007) collect many important papers on ﬁrst-order probability models and their use in machine learning.

Token 11336:
Probabilistic reasoning about identity uncertainty has two distinct origins.

Token 11337:
In statis- tics, the problem of record linkage arises when data records do not contain standard unique RECORD LINKAGE identiﬁers—for example, various citations of this book might name its ﬁrst author “Stuart Russell” or “S.

Token 11338:
J. Russell” or even “Stewart Russle,” and other authors may use the some ofthe same names.

Token 11339:
Literally hundreds of companies exist solely to solve record linkage prob-lems in ﬁnancial, medical, census, and other data.

Token 11340:
Probabilistic analysis goes back to work by Dunn (1946); the Fellegi–Sunter model (1969), which is essentially naive Bayes applied to matching, still dominates current practice.

Token 11341:
The second origin for work on identity uncer-tainty is multitarget tracking (Sittler, 1964), which we cover in Chapter 15.

Token 11342:
For most of itshistory, work in symbolic AI assumed erroneously that sensors could supply sentences withunique identiﬁers for objects.

Token 11343:
The issue was studied in the context of language understandingby Charniak and Goldman (1992) and in the context of surveillance by (Huang and Russell,1998) and Pasula et al.

Token 11344:
(1999). Pasula et al.

Token 11345:
(2003) developed a complex generative model for authors, papers, and citation strings, involving both relational and identity uncertainty,and demonstrated high accuracy for citation information extraction.

Token 11346:
The ﬁrst formally de-ﬁned language for open-universe probability models was B LOG (Milch et al.

Token 11347:
, 2005), which came with a complete (albeit slow) MCMC inference algorithm for all well-deﬁned mdoels.

Token 11348:
(The program code faintly visible on the front cover of this book is part of a B LOG model for detecting nuclear explosions from seismic signals as part of the UN Comprehensive TestBan Treaty veriﬁcation regime.)

Token 11349:
Laskey (2008) describes another open-universe modelinglanguage called multi-entity Bayesian networks .

Token 11350:


Token 11351:
Bibliographical and Historical Notes 557 As explained in Chapter 13, early probabilistic systems fell out of favor in the early 1970s, leaving a partial vacuum to be ﬁlled by alternative methods.

Token 11352:
Certainty factors wereinvented for use in the medical expert system M YCIN (Shortliffe, 1976), which was intended both as an engineering solution and as a model of human judgment under uncertainty.

Token 11353:
Thecollection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete overview of M YCIN and its descendants (see also Steﬁk, 1995).

Token 11354:
David Heckerman (1986) showed that a slightly modiﬁed version of certainty factor calculations gives correct proba-bilistic results in some cases, but results in serious overcounting of evidence in other cases.The P ROSPECTOR expert system (Duda et al.

Token 11355:
, 1979) used a rule-based approach in which the rules were justiﬁed by a (seldom tenable) global independence assumption.

Token 11356:
Dempster–Shafer theory originates with a paper by Arthur Dempster (1968) proposing a generalization of probability to interval values and a combination rule for using them.

Token 11357:
Laterwork by Glenn Shafer (1976) led to the Dempster-Shafer theory’s being viewed as a compet-ing approach to probability.

Token 11358:
Pearl (1988) and Ruspini et al. (1992) analyze the relationship between the Dempster–Shafer theory and standard probability theory.

Token 11359:
Fuzzy sets were developed by Lotﬁ Zadeh (1965) in response to the perceived difﬁculty of providing exact inputs to intelligent systems.

Token 11360:
The text by Zimmermann (2001) provides a thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in Zimmermann (1999).

Token 11361:
As we mentioned in the text, fuzzy logic has often been perceivedincorrectly as a direct competitor to probability theory, whereas in fact it addresses a differentset of issues.

Token 11362:
Possibility theory (Zadeh, 1978) was introduced to handle uncertainty in fuzzy POSSIBILITY THEORY systems and has much in common with probability.

Token 11363:
Dubois and Prade (1994) survey the connections between possibility theory and probability theory.

Token 11364:
The resurgence of probability depended mainly on Pearl’s development of Bayesian networks as a method for representing and using conditional independence information.

Token 11365:
Thisresurgence did not come without a ﬁght; Peter Cheeseman’s (1985) pugnacious “In Defenseof Probability” and his later article “An Inquiry into Computer Understanding” (Cheeseman,1988, with commentaries) give something of the ﬂavor of the debate.

Token 11366:
Eugene Charniak helped present the ideas to AI researchers with a popular article, “Bayesian networks with- out tears” 11(1991), and book (1993).

Token 11367:
The book by Dean and Wellman (1991) also helped introduce Bayesian networks to AI researchers.

Token 11368:
One of the principal philosophical objectionsof the logicists was that the numerical calculations that probability theory was thought to re-quire were not apparent to introspection and presumed an unrealistic level of precision in ouruncertain knowledge.

Token 11369:
The development of qualitative probabilistic networks (Wellman, 1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion ofpositive and negative inﬂuences between variables.

Token 11370:
Wellman shows that in many cases suchinformation is sufﬁcient for optimal decision making without the need for the precise spec-iﬁcation of probability values.

Token 11371:
Goldszmidt and Pearl (1996) take a similar approach.

Token 11372:
Workby Adnan Darwiche and Matt Ginsberg (1992) extracts the basic properties of conditioning and evidence combination from probability theory and shows that they can also be applied in logical and default reasoning.

Token 11373:
Often, programs speak louder than words, and the ready avail- 11The title of the original version of the article was “Pearl for swine.”

Token 11374:
558 Chapter 14.

Token 11375:
Probabilistic Reasoning ability of high-quality software such as the Bayes Net toolkit (Murphy, 2001) accelerated the adoption of the technology.

Token 11376:
The most important single publication in the growth of Bayesian networks was undoubt- edly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988).

Token 11377:
Several excellent texts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche, 2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov- ered in this chapter.

Token 11378:
New research on probabilistic reasoning appears both in mainstreamAI journals, such as Artiﬁcial Intelligence and the Journal of AI Research , and in more spe- cialized journals, such as the International Journal of Approximate Reasoning .

Token 11379:
Many papers on graphical models, which include Bayesian networks, appear in statistical journals.

Token 11380:
Theproceedings of the conferences on Uncertainty in Artiﬁcial Intelligence (UAI), Neural Infor-mation Processing Systems (NIPS), and Artiﬁcial Intelligence and Statistics (AISTATS) areexcellent sources for current research.

Token 11381:
EXERCISES 14.1 We have a bag of three biased coins a,b,a n dcwith probabilities of coming up heads of 20%, 60%, and 80%, respectively.

Token 11382:
One coin is drawn randomly from the bag (with equallikelihood of drawing each of the three coins), and then the coin is ﬂipped three times to generate the outcomes X 1,X2,a n dX3.

Token 11383:
a. Draw the Bayesian network corresponding to this setup and deﬁne the necessary CPTs. b.

Token 11384:
Calculate which coin was most likely to have been drawn from the bag if the observed ﬂips come out heads twice and tails once.

Token 11385:
14.2 Equation (14.1) on page 513 deﬁnes the joint distribution represented by a Bayesian network in terms of the parameters θ(Xi|Parents (Xi)).

Token 11386:
This exercise asks you to derive the equivalence between the parameters and the conditional probabilities P(Xi|Parents (Xi)) from this deﬁnition. a.

Token 11387:
Consider a simple network X→Y→Zwith three Boolean variables.

Token 11388:
Use Equa- tions (13.3) and (13.6) (pages 485 and 492) to express the conditional probabilityP(z|y)as the ratio of two sums, each over entries in the joint distribution P(X,Y,Z ).

Token 11389:
b. Now use Equation (14.1) to write this expression in terms of the network parameters θ(X),θ(Y|X),a n dθ(Z|Y).

Token 11390:
c. Next, expand out the summations in your expression from part (b), writing out explicitly the terms for the true and false values of each summed variable.

Token 11391:
Assuming that allnetwork parameters satisfy the constraint/summationtext xiθ(xi|parents (Xi))= 1 , show that the resulting expression reduces to θ(x|y).

Token 11392:
d. Generalize this derivation to show that θ(Xi|Parents (Xi)) = P(Xi|Parents (Xi)) for any Bayesian network.

Token 11393:


Token 11394:
Exercises 559 14.3 The operation of arc reversal in a Bayesian network allows us to change the direction ARC REVERSAL of an arc X→Ywhile preserving the joint probability distribution that the network repre- sents (Shachter, 1986).

Token 11395:
Arc reversal may require introducing new arcs: all the parents of X also become parents of Y, and all parents of Yalso become parents of X. a.

Token 11396:
Assume that XandYstart with mandnparents, respectively, and that all variables havekvalues.

Token 11397:
By calculating the change in size for the CPTs of XandY, show that the total number of parameters in the network cannot decrease during arc reversal.

Token 11398:
( Hint: the parents of XandYneed not be disjoint.) b. Under what circumstances can the total number remain constant?

Token 11399:
c. Let the parents of XbeU∪Vand the parents of YbeV∪W,w h e r e UandWare disjoint.

Token 11400:
The formulas for the new CPTs after arc reversal are as follows: P(Y|U,V,W)=/summationdisplay xP(Y|V,W,x)P(x|U,V) P(X|U,V,W,Y)= P(Y|X,V,W)P(X|U,V)/P(Y|U,V,W).

Token 11401:
Prove that the new network expresses the same joint distribution over all variables as the original network.

Token 11402:
14.4 Consider the Bayesian network in Figure 14.2. a. If no evidence is observed, are Burglary andEarthquake independent?

Token 11403:
Prove this from the numerical semantics and from the topological semantics. b. If we observe Alarm =true,a r eBurglary andEarthquake independent?

Token 11404:
Justify your answer by calculating whether the probabilities involved satisfy the deﬁnition of condi-tional independence.

Token 11405:
14.5 Suppose that in a Bayesian network containing an unobserved variable Y, all the vari- ables in the Markov blanket MB(Y)have been observed. a.

Token 11406:
Prove that removing the node Yfrom the network will not affect the posterior distribu- tion for any other unobserved variable in the network. b.

Token 11407:
Discuss whether we can remove Yif we are planning to use (i) rejection sampling and (ii) likelihood weighting.

Token 11408:
14.6 Let Hxbe a random variable denoting the handedness of an individual x, with possible values lorr.

Token 11409:
A common hypothesis is that left- or right-handedness is inherited by a simple mechanism; that is, perhaps there is a gene Gx, also with values lorr, and perhaps actual handedness turns out mostly the same (with some probability s) as the gene an individual possesses.

Token 11410:
Furthermore, perhaps the gene itself is equally likely to be inherited from either of an individual’s parents, with a small nonzero probability mof a random mutation ﬂipping the handedness.

Token 11411:
a. Which of the three networks in Figure 14.20 claim that P(Gfather,Gmother ,Gchild)= P(Gfather)P(Gmother )P(Gchild)? b.

Token 11412:
Which of the three networks make independence claims that are consistent with the hypothesis about the inheritance of handedness?

Token 11413:
560 Chapter 14.

Token 11414:
Probabilistic Reasoning HmotherHfather HchildmotherGfatherG childGHmotherHfather HchildmotherGfatherG childGHmotherHfather HchildmotherGfatherG childG (a) (b) (c) Figure 14.20 Three possible structures for a Bayesian network describing genetic inheri- tance of handedness.

Token 11415:
c. Which of the three networks is the best description of the hypothesis? d. Write down the CPT for the Gchild node in network (a), in terms of sandm.

Token 11416:
e. Suppose that P(Gfather=l)=P(Gmother =l)=q.

Token 11417:
In network (a), derive an expres- sion for P(Gchild=l)in terms of mandqonly, by conditioning on its parent nodes.

Token 11418:
f. Under conditions of genetic equilibrium, we expect the distribution of genes to be the same across generations.

Token 11419:
Use this to calculate the value of q, and, given what you know about handedness in humans, explain why the hypothesis described at the beginning ofthis question must be wrong.

Token 11420:
14.7 The Markov blanket of a variable is deﬁned on page 517.

Token 11421:
Prove that a variable is independent of all other variables in the network, given its Markov blanket and deriveEquation (14.12) (page 538).

Token 11422:
RadioBattery Ignition Gas Starts Moves Figure 14.21 A Bayesian network describing some fe atures of a car’s electrical system and engine.

Token 11423:
Each variable is Boolean, and the true value indicates that the corresponding aspect of the vehicle is in working order.

Token 11424:
Exercises 561 14.8 Consider the network for car diagnosis shown in Figure 14.21. a.

Token 11425:
Extend the network with the Boolean variables IcyWeather andStarterMotor . b. Give reasonable conditional probability tables for all the nodes.

Token 11426:
c. How many independent values are contained in the joint probability distribution for eight Boolean nodes, assuming that no conditional independence relations are known to hold among them?

Token 11427:
d. How many independent probability values do your network tables contain?

Token 11428:
e. The conditional distribution for Starts could be described as a noisy-AND distribution.

Token 11429:
Deﬁne this family in general and relate it to the noisy-OR distribution.

Token 11430:
14.9 Consider the family of linear Gaussian networks, as deﬁned on page 520. a.

Token 11431:
In a two-variable network, let X1be the parent of X2,l e tX1have a Gaussian prior, and let P(X2|X1)be a linear Gaussian distribution.

Token 11432:
Show that the joint distribution P(X1,X2)is a multivariate Gaussian, and calculate its covariance matrix. b.

Token 11433:
Prove by induction that the joint distribution for a general linear Gaussian network on X1,...,X nis also a multivariate Gaussian.

Token 11434:
14.10 The probit distribution deﬁned on page 522 describes the probability distribution for a Boolean child, given a single continuous parent. a.

Token 11435:
How might the deﬁnition be extended to cover multiple continuous parents? b. How might it be extended to handle a multivalued child variable?

Token 11436:
Consider both cases where the child’s values are ordered (as in selecting a gear while driving, dependingon speed, slope, desired acceleration, etc.)

Token 11437:
and cases where they are unordered (as inselecting bus, train, or car to get to work).

Token 11438:
( Hint: Consider ways to divide the possible values into two sets, to mimic a Boolean variable.)

Token 11439:
14.11 In your local nuclear power station, there is an alarm that senses when a temperature gauge exceeds a given threshold.

Token 11440:
The gauge measures the temperature of the core.

Token 11441:
Consider the Boolean variables A(alarm sounds), F A(alarm is faulty), and FG(gauge is faulty) and the multivalued nodes G(gauge reading) and T(actual core temperature).

Token 11442:
a. Draw a Bayesian network for this domain, given that the gauge is more likely to fail when the core temperature gets too high. b.

Token 11443:
Is your network a polytree? Why or why not?

Token 11444:
c. Suppose there are just two possible actual and measured temperatures, normal and high; the probability that the gauge gives the correct temperature is xwhen it is working, but ywhen it is faulty.

Token 11445:
Give the conditional probability table associated with G. d. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.

Token 11446:
Give the conditional probability table associated with A. e. Suppose the alarm and gauge are working and the alarm sounds.

Token 11447:
Calculate an expres- sion for the probability that the temperature of the core is too high, in terms of thevarious conditional probabilities in the network.

Token 11448:
562 Chapter 14.

Token 11449:
Probabilistic Reasoning N N (i) (ii) (iii)F1 F1 F1M1M1M1 F2F2 F2M2M2M2 N Figure 14.22 Three possible networks for the telescope problem.

Token 11450:
14.12 Two astronomers in different parts of the world make measurements M1andM2of the number of stars Nin some small region of the sky, using their telescopes.

Token 11451:
Normally, there is a small possibility eof error by up to one star in each direction.

Token 11452:
Each telescope can also (with a much smaller probability f) be badly out of focus (events F1andF2), in which case the scientist will undercount by three or more stars (or if Nis less than 3, fail to detect any stars at all).

Token 11453:
Consider the three networks shown in Figure 14.22. a.

Token 11454:
Which of these Bayesian networks are correct (but not necessarily efﬁcient) represen- tations of the preceding information? b.

Token 11455:
Which is the best network? Explain. c. Write out a conditional distribution for P(M1|N), for the case where N∈{1,2,3}and M1∈{0,1,2,3,4}.

Token 11456:
Each entry in the conditional distribution should be expressed as a function of the parameters eand/or f. d. Suppose M1=1andM2=3.W h a ta r et h e possible numbers of stars if you assume no prior constraint on the values of N?

Token 11457:
e. What is the most likely number of stars, given these observations?

Token 11458:
Explain how to compute this, or if it is not possible to compute, explain what additional information isneeded and how it would affect the result.

Token 11459:
14.13 Consider the network shown in Figure 14.22(ii), and assume that the two telescopes work identically.

Token 11460:
N∈{1,2,3}andM 1,M2∈{0,1,2,3,4}, with the symbolic CPTs as de- scribed in Exercise 14.12.

Token 11461:
Using the enumeration algorithm (Figure 14.9 on page 525), cal- culate the probability distribution P(N|M1=2,M2=2 ).

Token 11462:
14.14 Consider the Bayes net shown in Figure 14.23. a. Which of the following are asserted by the network structure ? (i)P(B,I,M )=P(B)P(I)P(M).

Token 11463:
(ii)P(J|G)=P(J|G, I). (iii) P(M|G, B, I )=P(M|G, B, I, J ).

Token 11464:


Token 11465:
Exercises 563 B IM G JP(B) .9B M P (I) .9 .5 .5 .1 GP (J) .9 .0t .9BM P(G) .0 .0 .0.0.8 .2 .1I tt f tt ft t ff t ftt fft ff t ff ftt ft ft ffP(M) .1 t f Figure 14.23 A simple Bayes net with Boolean variables B=BrokeElectionLaw , I=Indicted ,M=PoliticallyMotivatedProsecutor ,G=FoundGuilty ,J=Jailed .

Token 11466:
b. Calculate the value of P(b,i,¬m,g,j ).

Token 11467:
c. Calculate the probability that someone goes to jail given that they broke the law, have been indicted, and face a politically motivated prosecutor.

Token 11468:
d.Acontext-speciﬁc independence (see page 542) allows a variable to be independent of some of its parents given certain values of others.

Token 11469:
In addition to the usual conditionalindependences given by the graph structure, what context-speciﬁc independences existin the Bayes net in Figure 14.23?

Token 11470:
e. Suppose we want to add the variable P=PresidentialPardon to the network; draw the new network and brieﬂy explain any links you add.

Token 11471:
14.15 Consider the variable elimination algorithm in Figure 14.11 (page 528). a.

Token 11472:
Section 14.4 applies variable elimination to the query P(Burglary|JohnCalls =true,MaryCalls =true).

Token 11473:
Perform the calculations indicated and check that the answer is correct. b.

Token 11474:
Count the number of arithmetic operations performed, and compare it with the number performed by the enumeration algorithm.

Token 11475:
c. Suppose a network has the form of a chain : a sequence of Boolean variables X 1,...,X n where Parents (Xi)={Xi−1}fori=2,...,n .

Token 11476:
What is the complexity of computing P(X1|Xn=true)using enumeration? Using variable elimination?

Token 11477:
d. Prove that the complexity of running variable elimination on a polytree network is linear in the size of the tree for any variable ordering consistent with the network structure.

Token 11478:
14.16 Investigate the complexity of exact inference in general Bayesian networks: a.

Token 11479:
Prove that any 3-SAT problem can be reduced to exact inference in a Bayesian network constructed to represent the particular problem and hence that exact inference is NP-

Token 11480:
564 Chapter 14. Probabilistic Reasoning hard.

Token 11481:
( Hint: Consider a network with one variable for each proposition symbol, one for each clause, and one for the conjunction of clauses.) b.

Token 11482:
The problem of counting the number of satisfying assignments for a 3-SAT problem is #P-complete.

Token 11483:
Show that exact inference is at least as hard as this.

Token 11484:
14.17 Consider the problem of generating a random sample from a speciﬁed distribution on a single variable.

Token 11485:
Assume you have a random number generator that returns a randomnumber uniformly distributed between 0 and 1. a.L e tXbe a discrete variable with P(X=x i)=pifori∈{1,...,k}.T h e cumulative distribution ofXgives the probability that X∈{x1,...,x j}for each possible j.

Token 11486:
( S e eCUMULATIVE DISTRIBUTION also Appendix A.)

Token 11487:
Explain how to calculate the cumulative distribution in O(k)time and how to generate a single sample of Xfrom it.

Token 11488:
Can the latter be done in less than O(k)time? b. Now suppose we want to generate Nsamples of X,w h e r e N/greatermuchk.

Token 11489:
Explain how to do this with an expected run time per sample that is constant (i.e., independent of k).

Token 11490:
c. Now consider a continuous-valued variable with a parameterized distribution (e.g., Gaussian).

Token 11491:
How can samples be generated from such a distribution?

Token 11492:
d. Suppose you want to query a continuous-valued variable and you are using a sampling algorithm such as L IKELIHOOD WEIGHTING to do the inference.

Token 11493:
How would you have to modify the query-answering process?

Token 11494:
14.18 Consider the query P(Rain|Sprinkler =true,WetGrass =true)in Figure 14.12(a) (page 529) and how Gibbs sampling can answer it. a.

Token 11495:
How many states does the Markov chain have? b. Calculate the transition matrix Q containing q(y→y/prime)for all y,y/prime.

Token 11496:
c. What does Q2, the square of the transition matrix, represent? d. What about Qnasn→∞ ?

Token 11497:
e. Explain how to do probabilistic inference in Bayesian networks, assuming that Qnis available. Is this a practical way to do inference?

Token 11498:
14.19 This exercise explores the stationary distribution for Gibbs sampling methods. a.

Token 11499:
The convex composition [α,q1;1−α,q2]ofq1andq2is a transition probability distri- bution that ﬁrst chooses one of q1andq2with probabilities αand1−α, respectively, and then applies whichever is chosen.

Token 11500:
Prove that if q1andq2are in detailed balance withπ, then their convex composition is also in detailed balance with π.

Token 11501:
(Note :t h i s result justiﬁes a variant of G IBBS -ASKin which variables are chosen at random rather than sampled in a ﬁxed sequence.) b.

Token 11502:
Prove that if each of q1andq2hasπas its stationary distribution, then the sequential composition q=q1◦q2also has πas its stationary distribution.

Token 11503:
14.20 TheMetropolis–Hastings algorithm is a member of the MCMC family; as such, it isMETROPOLIS– HASTINGS designed to generate samples x(eventually) according to target probabilities π(x).

Token 11504:
(Typically

Token 11505:
Exercises 565 we are interested in sampling from π(x)=P(x|e).) Like simulated annealing, Metropolis– Hastings operates in two stages.

Token 11506:
First, it samples a new state x/primefrom a proposal distributionPROPOSAL DISTRIBUTION q(x/prime|x), given the current state x.

Token 11507:
Then, it probabilistically accepts or rejects x/primeaccording to theacceptance probabilityACCEPTANCE PROBABILITY α(x/prime|x)=m i n/parenleftbigg 1,π(x/prime)q(x|x/prime) π(x)q(x/prime|x)/parenrightbigg .

Token 11508:
If the proposal is rejected, the state remains at x. a. Consider an ordinary Gibbs sampling step for a speciﬁc variable Xi.

Token 11509:
Show that this step, considered as a proposal, is guaranteed to be accepted by Metropolis–Hastings.

Token 11510:
(Hence, Gibbs sampling is a special case of Metropolis–Hastings.) b.

Token 11511:
Show that the two-step process above, viewed as a transition probability distribution, is in detailed balance with π.

Token 11512:
14.21 Three soccer teams A,B,a n dC, play each other once. Each match is between two teams, and can be won, drawn, or lost.

Token 11513:
Each team has a ﬁxed, unknown degree of quality— an integer ranging from 0 to 3—and the outcome of a match depends probabilistically on thedifference in quality between the two teams.

Token 11514:
a. Construct a relational probability model to describe this domain, and suggest numerical values for all the necessary probability distributions.

Token 11515:
b. Construct the equivalent Bayesian network for the three matches.

Token 11516:
c. Suppose that in the ﬁrst two matches AbeatsBand draws with C. Using an exact inference algorithm of your choice, compute the posterior distribution for the outcomeof the third match.

Token 11517:
d. Suppose there are nteams in the league and we have the results for all but the last match.

Token 11518:
How does the complexity of predicting the last game vary with n? e. Investigate the application of MCMC to this problem.

Token 11519:
How quickly does it converge in practice and how well does it scale?

Token 11520:


Token 11521:
15PROBABILISTIC REASONING OVER TIME In which we try to interpret the present, understand the past, and perhaps predict the future, even when very little is crystal clear.

Token 11522:
Agents in partially observable environments must be able to keep track of the current state, to the extent that their sensors allow.

Token 11523:
In Section 4.4 we showed a methodology for doing that: anagent maintains a belief state that represents which states of the world are currently possible.

Token 11524:
From the belief state and a transition model , the agent can predict how the world might evolve in the next time step.

Token 11525:
From the percepts observed and a sensor model , the agent can update the belief state.

Token 11526:
This is a pervasive idea: in Chapter 4 belief states were represented byexplicitly enumerated sets of states, whereas in Chapters 7 and 11 they were represented by logical formulas.

Token 11527:
Those approaches deﬁned belief states in terms of which world states were possible , but could say nothing about which states were likely orunlikely .

Token 11528:
In this chapter, we use probability theory to quantify the degree of belief in elements of the belief state.

Token 11529:
As we show in Section 15.1, time itself is handled in the same way as in Chapter 7: a changing world is modeled using a variable for each aspect of the world state at each point in time.

Token 11530:
The transition and sensor models may be uncertain: the transition model describes the probability distribution of the variables at time t, given the state of the world at past times, while the sensor model describes the probability of each percept at time t, given the current state of the world.

Token 11531:
Section 15.2 deﬁnes the basic inference tasks and describes the gen-eral structure of inference algorithms for temporal models.

Token 11532:
Then we describe three speciﬁckinds of models: hidden Markov models ,Kalman ﬁlters ,a n d dynamic Bayesian net- works (which include hidden Markov models and Kalman ﬁlters as special cases).

Token 11533:
Finally, Section 15.6 examines the problems faced when keeping track of more than one thing.

Token 11534:
15.1 T IME AND UNCERTAINTY We have developed our techniques for probabilistic reasoning in the context of static worlds, in which each random variable has a single ﬁxed value.

Token 11535:
For example, when repairing a car,we assume that whatever is broken remains broken during the process of diagnosis; our jobis to infer the state of the car from observed evidence, which also remains ﬁxed.

Token 11536:
566

Token 11537:
Section 15.1. Time and Uncertainty 567 Now consider a slightly different problem: treating a diabetic patient.

Token 11538:
As in the case of car repair, we have evidence such as recent insulin doses, food intake, blood sugar measure-ments, and other physical signs.

Token 11539:
The task is to assess the current state of the patient, includingthe actual blood sugar level and insulin level.

Token 11540:
Given this information, we can make a deci-sion about the patient’s food intake and insulin dose.

Token 11541:
Unlike the case of car repair, here the dynamic aspects of the problem are essential.

Token 11542:
Blood sugar levels and measurements thereof can change rapidly over time, depending on recent food intake and insulin doses, metabolicactivity, the time of day, and so on.

Token 11543:
To assess the current state from the history of evidenceand to predict the outcomes of treatment actions, we must model these changes.

Token 11544:
The same considerations arise in many other contexts, such as tracking the location of a robot, tracking the economic activity of a nation, and making sense of a spoken or writtensequence of words.

Token 11545:
How can dynamic situations like these be modeled?

Token 11546:
15.1.1 States and observations We view the world as a series of snapshots, or time slices , each of which contains a set of TIME SLICE random variables, some observable and some not.1For simplicity, we will assume that the same subset of variables is observable in each time slice (although this is not strictly necessaryin anything that follows).

Token 11547:
We will use X tto denote the set of state variables at time t,w h i c h are assumed to be unobservable, and Etto denote the set of observable evidence variables.

Token 11548:
The observation at time tisEt=etfor some set of values et.

Token 11549:
Consider the following example: You are the security guard stationed at a secret under- ground installation.

Token 11550:
You want to know whether it’s raining today, but your only access to the outside world occurs each morning when you see the director coming in with, or without, anumbrella.

Token 11551:
For each day t, the set E tthus contains a single evidence variable Umbrella torUt for short (whether the umbrella appears), and the set Xtcontains a single state variable Rain t orRtfor short (whether it is raining).

Token 11552:
Other problems can involve larger sets of variables.

Token 11553:
In the diabetes example, we might have evidence variables, such as MeasuredBloodSugar tand PulseRate t, and state variables, such as BloodSugar tandStomachContents t. (Notice that BloodSugar tandMeasuredBloodSugar tare not the same variable; this is how we deal with noisy measurements of actual quantities.)

Token 11554:
The interval between time slices also depends on the problem. For diabetes monitoring, a suitable interval might be an hour rather than a day.

Token 11555:
In this chapter we assume the interval between slices is ﬁxed, so we can label times by integers.

Token 11556:
We will assume that the state sequence starts at t=0; for various uninteresting reasons, we will assume that evidence starts arriving at t=1rather than t=0.

Token 11557:
Hence, our umbrella world is represented by state variables R0,R1,R2,... and evidence variables U1,U2,.... We will use the notation a:bto denote the sequence of integers from atob(inclusive), and the notation Xa:bto denote the set of variables from XatoXb.

Token 11558:
For example, U1:3corresponds to the variables U1,U2,U3. 1Uncertainty over continuous time can be modeled by stochastic differential equations (SDEs).

Token 11559:
The models studied in this chapter can be viewed as discrete-time approximations to SDEs.

Token 11560:
568 Chapter 15.

Token 11561:
Probabilistic Reasoning over Time Xt–2 Xt–1 Xt (a) (b)Xt+1 Xt+2 Xt–2 Xt–1 Xt Xt+1 Xt+2 Figure 15.1 (a) Bayesian network structure corresponding to a ﬁrst-order Markov process with state deﬁned by the variables Xt.

Token 11562:
(b) A second-order Markov process.

Token 11563:
15.1.2 Transition and sensor models With the set of state and evidence variables for a given problem decided on, the next step is to specify how the world evolves (the transition model) and how the evidence variables gettheir values (the sensor model).

Token 11564:
The transition model speciﬁes the probability distribution over the latest state variables, given the previous values, that is, P(X t|X0:t−1).

Token 11565:
Now we face a problem: the set X0:t−1is unbounded in size as tincreases.

Token 11566:
We solve the problem by making a Markov assumption —MARKOV ASSUMPTION that the current state depends on only a ﬁnite ﬁxed number of previous states.

Token 11567:
Processes sat- isfying this assumption were ﬁrst studied in depth by the Russian statistician Andrei Markov (1856–1922) and are called Markov processes orMarkov chains .

Token 11568:
They come in various ﬂa- MARKOV PROCESS vors; the simplest is the ﬁrst-order Markov process , in which the current state depends onlyFIRST-ORDER MARKOV PROCESS on the previous state and not on any earlier states.

Token 11569:
In other words, a state provides enough information to make the future conditionally independent of the past, and we have P(Xt|X0:t−1)=P(Xt|Xt−1).

Token 11570:
(15.1) Hence, in a ﬁrst-order Markov process, the transition model is the conditional distribution P(Xt|Xt−1).

Token 11571:
The transition model for a second-order Markov process is the conditional distribution P(Xt|Xt−2,Xt−1).

Token 11572:
Figure 15.1 shows the Bayesian network structures corre- sponding to ﬁrst-order and second-order Markov processes.

Token 11573:
Even with the Markov assumption there is still a problem: there are inﬁnitely many possible values of t. Do we need to specify a different distribution for each time step?

Token 11574:
We avoid this problem by assuming that changes in the world state are caused by a stationary process —that is, a process of change that is governed by laws that do not themselves changeSTATIONARY PROCESS over time.

Token 11575:
(Don’t confuse stationary with static :i na static process, the state itself does not change.)

Token 11576:
In the umbrella world, then, the conditional probability of rain, P(Rt|Rt−1),i st h e same for all t, and we only have to specify one conditional probability table.

Token 11577:
Now for the sensor model.

Token 11578:
The evidence variables Etcould depend on previous vari- ables as well as the current state variables, but any state that’s worth its salt should sufﬁce to generate the current sensor values.

Token 11579:
Thus, we make a sensor Markov assumption as follows:SENSOR MARKOV ASSUMPTION P(Et|X0:t,E0:t−1)=P(Et|Xt).

Token 11580:
(15.2) Thus, P(Et|Xt)is our sensor model (sometimes called the observation model ).

Token 11581:
Figure 15.2 shows both the transition model and the sensor model for the umbrella example. Notice the

Token 11582:
Section 15.1.

Token 11583:
Time and Uncertainty 569 Raint UmbrellatRaint–1 Umbrellat–1Raint+1 Umbrellat+1Rt-1tP(R ) 0.3f0.7t tR tP(U ) 0.9t 0.2f Figure 15.2 Bayesian network structure and conditional distributions describing the umbrella world.

Token 11584:
The transition model is P(Rain t|Rain t−1)and the sensor model is P(Umbrella t|Rain t).

Token 11585:
direction of the dependence between state and sensors: the arrows go from the actual state of the world to sensor values because the state of the world causes the sensors to take on particular values: the rain causes the umbrella to appear.

Token 11586:
(The inference process, of course, goes in the other direction; the distinction between the direction of modeled dependenciesand the direction of inference is one of the principal advantages of Bayesian networks.)

Token 11587:
In addition to specifying the transition and sensor models, we need to say how every- thing gets started—the prior probability distribution at time 0, P(X 0).

Token 11588:
With that, we have a speciﬁcation of the complete joint distribution over all the variables, using Equation (14.2).

Token 11589:
For any t, P(X0:t,E1:t)=P(X0)t/productdisplay i=1P(Xi|Xi−1)P(Ei|Xi).

Token 11590:
(15.3) The three terms on the right-hand side are the initial state model P(X0), the transition model P(Xi|Xi−1), and the sensor model P(Ei|Xi).

Token 11591:
The structure in Figure 15.2 is a ﬁrst-order Markov process—the probability of rain is assumed to depend only on whether it rained the previous day.

Token 11592:
Whether such an assumption is reasonable depends on the domain itself.

Token 11593:
The ﬁrst-order Markov assumption says that thestate variables contain allthe information needed to characterize the probability distribution for the next time slice.

Token 11594:
Sometimes the assumption is exactly true—for example, if a particle is executing a random walk along the x-axis, changing its position by ±1at each time step, then using the x-coordinate as the state gives a ﬁrst-order Markov process.

Token 11595:
Sometimes the assumption is only approximate, as in the case of predicting rain only on the basis of whether it rained the previous day.

Token 11596:
There are two ways to improve the accuracy of the approximation: 1. Increasing the order of the Markov process model.

Token 11597:
For example, we could make a second-order model by adding Rain t−2as a parent of Rain t, which might give slightly more accurate predictions.

Token 11598:
For example, in Palo Alto, California, it very rarely rains more than two days in a row. 2. Increasing the set of state variables.

Token 11599:
For example, we could add Season tto allow

Token 11600:
570 Chapter 15.

Token 11601:
Probabilistic Reasoning over Time us to incorporate historical records of rainy seasons, or we could add Temperature t, Humidity tandPressure t(perhaps at a range of locations) to allow us to use a physical model of rainy conditions.

Token 11602:
Exercise 15.1 asks you to show that the ﬁrst solution—increasing the order—can always be reformulated as an increase in the set of state variables, keeping the order ﬁxed.

Token 11603:
Notice thatadding state variables might improve the system’s predictive power but also increases theprediction requirements : we now have to predict the new variables as well.

Token 11604:
Thus, we are looking for a “self-sufﬁcient” set of variables, which really means that we have to understandthe “physics” of the process being modeled.

Token 11605:
The requirement for accurate modeling of the process is obviously lessened if we can add new sensors (e.g., measurements of temperature and pressure) that provide information directly about the new state variables.

Token 11606:
Consider, for example, the problem of tracking a robot wandering randomly on the X–Y plane.

Token 11607:
One might propose that the position and velocity are a sufﬁcient set of state variables:one can simply use Newton’s laws to calculate the new position, and the velocity may changeunpredictably.

Token 11608:
If the robot is battery-powered, however, then battery exhaustion would tend tohave a systematic effect on the change in velocity.

Token 11609:
Because this in turn depends on how muchpower was used by all previous maneuvers, the Markov property is violated.

Token 11610:
We can restorethe Markov property by including the charge level Battery tas one of the state variables that make up Xt.

Token 11611:
This helps in predicting the motion of the robot, but in turn requires a model for predicting Battery tfromBattery t−1and the velocity.

Token 11612:
In some cases, that can be done reliably, but more often we ﬁnd that error accumulates over time.

Token 11613:
In that case, accuracy can be improved by adding a new sensor for the battery level.

Token 11614:
15.2 I NFERENCE IN TEMPORAL MODELS Having set up the structure of a generic temporal model, we can formulate the basic inference tasks that must be solved: •Filtering : This is the task of computing the belief state —the posterior distribution FILTERING BELIEF STATE over the most recent state—given all evidence to date.

Token 11615:
Filtering2is also called state estimation . In our example, we wish to compute P(Xt|e1:t).

Token 11616:
In the umbrella example, STATE ESTIMATION this would mean computing the probability of rain today, given all the observations of the umbrella carrier made so far.

Token 11617:
Filtering is what a rational agent does to keep track of the current state so that rational decisions can be made.

Token 11618:
It turns out that an almost identical calculation provides the likelihood of the evidence sequence, P(e1:t).

Token 11619:
•Prediction : This is the task of computing the posterior distribution over the future state, PREDICTION given all evidence to date.

Token 11620:
That is, we wish to compute P(Xt+k|e1:t)for some k>0.

Token 11621:
In the umbrella example, this might mean computing the probability of rain three daysfrom now, given all the observations to date.

Token 11622:
Prediction is useful for evaluating possiblecourses of action based on their expected outcomes.

Token 11623:
2The term “ﬁltering” refers to the roots of this problem in early work on signal processing, where the problem is to ﬁlter out the noise in a signal by estimating its underlying properties.

Token 11624:
Section 15.2.

Token 11625:
Inference in Temporal Models 571 •Smoothing : This is the task of computing the posterior distribution over a past state, SMOOTHING given all evidence up to the present.

Token 11626:
That is, we wish to compute P(Xk|e1:t)for some k such that 0≤k<t .

Token 11627:
In the umbrella example, it might mean computing the probability that it rained last Wednesday, given all the observations of the umbrella carrier madeup to today.

Token 11628:
Smoothing provides a better estimate of the state than was available at the time, because it incorporates more evidence.

Token 11629:
3 •Most likely explanation : Given a sequence of observations, we might wish to ﬁnd the sequence of states that is most likely to have generated those observations.

Token 11630:
That is, wewish to compute argmax x1:tP(x1:t|e1:t).

Token 11631:
For example, if the umbrella appears on each of the ﬁrst three days and is absent on the fourth, then the most likely explanation is thatit rained on the ﬁrst three days and did not rain on the fourth.

Token 11632:
Algorithms for this taskare useful in many applications, including speech recognition—where the aim is to ﬁndthe most likely sequence of words, given a series of sounds—and the reconstruction ofbit strings transmitted over a noisy channel.

Token 11633:
In addition to these inference tasks, we also have •Learning : The transition and sensor models, if not yet known, can be learned from observations.

Token 11634:
Just as with static Bayesian networks, dynamic Bayes net learning can bedone as a by-product of inference.

Token 11635:
Inference provides an estimate of what transitionsactually occurred and of what states generated the sensor readings, and these estimatescan be used to update the models.

Token 11636:
The updated models provide new estimates, and theprocess iterates to convergence.

Token 11637:
The overall process is an instance of the expectation-maximization or EM algorithm . (See Section 20.3.)

Token 11638:
Note that learning requires smoothing, rather than ﬁltering, because smoothing provides bet- ter estimates of the states of the process.

Token 11639:
Learning with ﬁltering can fail to converge correctly; consider, for example, the problem of learning to solve murders: unless you are an eyewit- ness, smoothing is always required to infer what happened at the murder scene from the observable variables.

Token 11640:
The remainder of this section describes generic algorithms for the four inference tasks, independent of the particular kind of model employed.

Token 11641:
Improvements speciﬁc to each modelare described in subsequent sections.

Token 11642:
15.2.1 Filtering and prediction As we pointed out in Section 7.7.3, a useful ﬁltering algorithm needs to maintain a current state estimate and update it, rather than going back over the entire history of percepts for each update.

Token 11643:
(Otherwise, the cost of each update increases as time goes by.)

Token 11644:
In other words, giventhe result of ﬁltering up to time t, the agent needs to compute the result for t+1from the new evidence e t+1, P(Xt+1|e1:t+1)=f(et+1,P(Xt|e1:t)), for some function f. This process is called recursive estimation .

Token 11645:
We can view the calculationRECURSIVE ESTIMATION 3In particular, when tracking a moving object with inaccurate position observations, smoothing gives a smoother estimated trajectory than ﬁltering—hence the name.

Token 11646:
572 Chapter 15.

Token 11647:
Probabilistic Reasoning over Time as being composed of two parts: ﬁrst, the current state distribution is projected forward from ttot+1; then it is updated using the new evidence et+1.

Token 11648:
This two-part process emerges quite simply when the formula is rearranged: P(Xt+1|e1:t+1)=P(Xt+1|e1:t,et+1)(dividing up the evidence) =αP(et+1|Xt+1,e1:t)P(Xt+1|e1:t)(using Bayes’ rule) =αP(et+1|Xt+1)P(Xt+1|e1:t)(by the sensor Markov assumption).

Token 11649:
(15.4) Here and throughout this chapter, αis a normalizing constant used to make probabilities sum up to 1.

Token 11650:
The second term, P(Xt+1|e1:t)represents a one-step prediction of the next state, and the ﬁrst term updates this with the new evidence; notice that P(et+1|Xt+1)is obtainable directly from the sensor model.

Token 11651:
Now we obtain the one-step prediction for the next state byconditioning on the current state X t: P(Xt+1|e1:t+1)=αP(et+1|Xt+1)/summationdisplay xtP(Xt+1|xt,e1:t)P(xt|e1:t) =αP(et+1|Xt+1)/summationdisplay xtP(Xt+1|xt)P(xt|e1:t)(Markov assumption).

Token 11652:
(15.5) Within the summation, the ﬁrst factor comes from the transition model and the second comes from the current state distribution.

Token 11653:
Hence, we have the desired recursive formulation.

Token 11654:
We canthink of the ﬁltered estimate P(X t|e1:t)as a “message” f1:tthat is propagated forward along the sequence, modiﬁed by each transition and updated by each new observation.

Token 11655:
The process is given by f1:t+1=αFORW ARD (f1:t,et+1), where F ORW ARD implements the update described in Equation (15.5) and the process begins with f1:0=P(X0).

Token 11656:
When all the state variables are discrete, the time for each update is constant (i.e., independent of t), and the space required is also constant.

Token 11657:
(The constants depend, of course, on the size of the state space and the speciﬁc type of the temporal modelin question.)

Token 11658:
The time and space requirements for updating must be constant if an agent with limited memory is to keep track of the current state distribution over an unbounded sequence of observations.

Token 11659:
Let us illustrate the ﬁltering process for two steps in the basic umbrella example (Fig- ure 15.2.)

Token 11660:
That is, we will compute P(R2|u1:2)as follows: •On day 0, we have no observations, only the security guard’s prior beliefs; let’s assume that consists of P(R0)=/angbracketleft0.5,0.5/angbracketright.

Token 11661:
•On day 1, the umbrella appears, so U1=true.

Token 11662:
The prediction from t=0tot=1is P(R1)=/summationdisplay r0P(R1|r0)P(r0) =/angbracketleft0.7,0.3/angbracketright×0.5+/angbracketleft0.3,0.7/angbracketright×0.5=/angbracketleft0.5,0.5/angbracketright.

Token 11663:
Then the update step simply multiplies by the probability of the evidence for t=1and normalizes, as shown in Equation (15.4): P(R1|u1)=αP(u1|R1)P(R1)=α/angbracketleft0.9,0.2/angbracketright/angbracketleft0.5,0.5/angbracketright =α/angbracketleft0.45,0.1/angbracketright≈/angbracketleft0.818,0.182/angbracketright.

Token 11664:
Section 15.2. Inference in Temporal Models 573 •On day 2, the umbrella appears, so U2=true.

Token 11665:
The prediction from t=1tot=2is P(R2|u1)=/summationdisplay r1P(R2|r1)P(r1|u1) =/angbracketleft0.7,0.3/angbracketright×0.818 +/angbracketleft0.3,0.7/angbracketright×0.182≈/angbracketleft0.627,0.373/angbracketright, and updating it with the evidence for t=2gives P(R2|u1,u2)=αP(u2|R2)P(R2|u1)=α/angbracketleft0.9,0.2/angbracketright/angbracketleft0.627,0.373/angbracketright =α/angbracketleft0.565,0.075/angbracketright≈/angbracketleft0.883,0.117/angbracketright.

Token 11666:
Intuitively, the probability of rain increases from day 1 to day 2 because rain persists.

Token 11667:
Exer- cise 15.2(a) asks you to investigate this tendency further.

Token 11668:
The task of prediction can be seen simply as ﬁltering without the addition of new evidence.

Token 11669:
In fact, the ﬁltering process already incorporates a one-step prediction, and it iseasy to derive the following recursive computation for predicting the state at t+k+1from a prediction for t+k: P(X t+k+1|e1:t)=/summationdisplay xt+kP(Xt+k+1|xt+k)P(xt+k|e1:t).

Token 11670:
(15.6) Naturally, this computation involves only the transition model and not the sensor model.

Token 11671:
It is interesting to consider what happens as we try to predict further and further into the future.

Token 11672:
As Exercise 15.2(b) shows, the predicted distribution for rain converges to aﬁxed point/angbracketleft0.5,0.5/angbracketright, after which it remains constant for all time.

Token 11673:
This is the stationary distribution of the Markov process deﬁned by the transition model. (See also page 537.)

Token 11674:
A great deal is known about the properties of such distributions and about the mixing time — MIXING TIME roughly, the time taken to reach the ﬁxed point.

Token 11675:
In practical terms, this dooms to failure any attempt to predict the actual state for a number of steps that is more than a small fraction of the mixing time, unless the stationary distribution itself is strongly peaked in a small area of the state space.

Token 11676:
The more uncertainty there is in the transition model, the shorter will be themixing time and the more the future is obscured.

Token 11677:
In addition to ﬁltering and prediction, we can use a forward recursion to compute the likelihood of the evidence sequence, P(e 1:t).

Token 11678:
This is a useful quantity if we want to compare different temporal models that might have produced the same evidence sequence (e.g., twodifferent models for the persistence of rain).

Token 11679:
For this recursion, we use a likelihood message/lscript 1:t(Xt)=P(Xt,e1:t).

Token 11680:
It is a simple exercise to show that the message calculation is identical to that for ﬁltering: /lscript1:t+1=FORW ARD (/lscript1:t,et+1).

Token 11681:
Having computed /lscript1:t, we obtain the actual likelihood by summing out Xt: L1:t=P(e1:t)=/summationdisplay xt/lscript1:t(xt).

Token 11682:
(15.7) Notice that the likelihood message represents the probabilities of longer and longer evidence sequences as time goes by and so becomes numerically smaller and smaller, leading to under-ﬂow problems with ﬂoating-point arithmetic.

Token 11683:
This is an important problem in practice, butwe shall not go into solutions here.

Token 11684:
574 Chapter 15.

Token 11685:
Probabilistic Reasoning over Time X1 E1X0 Xk EkXt Et Figure 15.3 Smoothing computes P(Xk|e1:t), the posterior distribution of the state at some past time kgiven a complete sequence of observations from 1tot.

Token 11686:
15.2.2 Smoothing As we said earlier, smoothing is the process of computing the distribution over past states given evidence up to the present; that is, P(Xk|e1:t)for0≤k<t .

Token 11687:
(See Figure 15.3.)

Token 11688:
In anticipation of another recursive message-passing approach, we can split the computationinto two parts—the evidence up to kand the evidence from k+1tot, P(X k|e1:t)= P(Xk|e1:k,ek+1:t) =αP(Xk|e1:k)P(ek+1:t|Xk,e1:k)(using Bayes’ rule) =αP(Xk|e1:k)P(ek+1:t|Xk)(using conditional independence) =αf1:k×bk+1:t. (15.8) where “×” represents pointwise multiplication of vectors.

Token 11689:
Here we have deﬁned a “back- ward” message bk+1:t=P(ek+1:t|Xk), analogous to the forward message f1:k.T h ef o r w a r d message f1:kcan be computed by ﬁltering forward from 1 to k, as given by Equation (15.5).

Token 11690:
It turns out that the backward message bk+1:tcan be computed by a recursive process that runs backward fromt: P(ek+1:t|Xk)=/summationdisplay xk+1P(ek+1:t|Xk,xk+1)P(xk+1|Xk)(conditioning on Xk+1) =/summationdisplay xk+1P(ek+1:t|xk+1)P(xk+1|Xk)(by conditional independence) =/summationdisplay xk+1P(ek+1,ek+2:t|xk+1)P(xk+1|Xk) =/summationdisplay xk+1P(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk), (15.9) where the last step follows by the conditional independence of ek+1andek+2:t,g i v e n Xk+1.

Token 11691:
Of the three factors in this summation, the ﬁrst and third are obtained directly from the model,and the second is the “recursive call.” Using the message notation, we have b k+1:t=BACKWARD (bk+2:t,ek+1), where B ACKWARD implements the update described in Equation (15.9).

Token 11692:
As with the forward recursion, the time and space needed for each update are constant and thus independent of t. We can now see that the two terms in Equation (15.8) can both be computed by recur- sions through time, one running forward from 1tokand using the ﬁltering equation (15.5)

Token 11693:
Section 15.2. Inference in Temporal Models 575 and the other running backward from ttok+1 and using Equation (15.9).

Token 11694:
Note that the backward phase is initialized with bt+1:t=P(et+1:t|Xt)=P(|Xt)1,w h e r e 1is a vector of 1s.

Token 11695:
(Because et+1:tis an empty sequence, the probability of observing it is 1.)

Token 11696:
Let us now apply this algorithm to the umbrella example, computing the smoothed estimate for the probability of rain at time k=1, given the umbrella observations on days 1 and 2.

Token 11697:
From Equation (15.8), this is given by P(R1|u1,u2)=αP(R1|u1)P(u2|R1).

Token 11698:
(15.10) The ﬁrst term we already know to be /angbracketleft.818,.182/angbracketright, from the forward ﬁltering process de- scribed earlier.

Token 11699:
The second term can be computed by applying the backward recursion in Equation (15.9): P(u2|R1)=/summationdisplay r2P(u2|r2)P(|r2)P(r2|R1) =( 0.9×1×/angbracketleft0.7,0.3/angbracketright)+( 0.2×1×/angbracketleft0.3,0.7/angbracketright)=/angbracketleft0.69,0.41/angbracketright.

Token 11700:
Plugging this into Equation (15.10), we ﬁnd that the smoothed estimate for rain on day 1 is P(R1|u1,u2)=α/angbracketleft0.818,0.182/angbracketright×/angbracketleft0.69,0.41/angbracketright≈/angbracketleft0.883,0.117/angbracketright.

Token 11701:
Thus, the smoothed estimate for rain on day 1 is higher than the ﬁltered estimate (0.818) in this case.

Token 11702:
This is because the umbrella on day 2 makes it more likely to have rained on day2; in turn, because rain tends to persist, that makes it more likely to have rained on day 1.

Token 11703:
Both the forward and backward recursions take a constant amount of time per step; hence, the time complexity of smoothing with respect to evidence e 1:tisO(t).

Token 11704:
This is the complexity for smoothing at a particular time step k. If we want to smooth the whole se- quence, one obvious method is simply to run the whole smoothing process once for eachtime step to be smoothed.

Token 11705:
This results in a time complexity of O(t 2).

Token 11706:
A better approach uses a simple application of dynamic programming to reduce the complexity to O(t).Ac l u e appears in the preceding analysis of the umbrella example, where we were able to reuse the results of the forward-ﬁltering phase.

Token 11707:
The key to the linear-time algorithm is to record the results of forward ﬁltering over the whole sequence.

Token 11708:
Then we run the backward recursion fromtdown to 1, computing the smoothed estimate at each step kfrom the computed back- ward message bk+1:tand the stored forward message f1:k. The algorithm, aptly called the forward–backward algorithm , is shown in Figure 15.4.FORWARD– BACKWARD ALGORITHMThe alert reader will have spotted that the Bayesian network structure shown in Fig- ure 15.3 is a polytree as deﬁned on page 528.

Token 11709:
This means that a straightforward application of the clustering algorithm also yields a linear-time algorithm that computes smoothed es-timates for the entire sequence.

Token 11710:
It is now understood that the forward–backward algorithmis in fact a special case of the polytree propagation algorithm used with clustering methods(although the two were developed independently).

Token 11711:
The forward–backward algorithm forms the computational backbone for many applica- tions that deal with sequences of noisy observations.

Token 11712:
As described so far, it has two practical drawbacks.

Token 11713:
The ﬁrst is that its space complexity can be too high when the state space is largeand the sequences are long.

Token 11714:
It uses O(|f|t)space where|f|is the size of the representation of the forward message.

Token 11715:
The space requirement can be reduced to O(|f|logt)with a concomi-

Token 11716:
576 Chapter 15. Probabilistic Reasoning over Time tant increase in the time complexity by a factor of logt, as shown in Exercise 15.3.

Token 11717:
In some cases (see Section 15.3), a constant-space algorithm can be used.

Token 11718:
The second drawback of the basic algorithm is that it needs to be modiﬁed to work in an online setting where smoothed estimates must be computed for earlier time slices as new observations are continuously added to the end of the sequence.

Token 11719:
The most common requirement is for ﬁxed-lag smoothing , which requires computing the smoothed estimateFIXED-LAG SMOOTHING P(Xt−d|e1:t)for ﬁxed d. That is, smoothing is done for the time slice dsteps behind the current time t;a stincreases, the smoothing has to keep up.

Token 11720:
Obviously, we can run the forward–backward algorithm over the d-step “window” as each new observation is added, but this seems inefﬁcient.

Token 11721:
In Section 15.3, we will see that ﬁxed-lag smoothing can, in somecases, be done in constant time per update, independent of the lag d. 15.2.3 Finding the most likely sequence Suppose that [true,true,false,true,true]is the umbrella sequence for the security guard’s ﬁrst ﬁve days on the job.

Token 11722:
What is the weather sequence most likely to explain this?

Token 11723:
Doesthe absence of the umbrella on day 3 mean that it wasn’t raining, or did the director forgetto bring it?

Token 11724:
If it didn’t rain on day 3, perhaps (because weather tends to persist) it didn’train on day 4 either, but the director brought the umbrella just in case.

Token 11725:
In all, there are 2 5 possible weather sequences we could pick. Is there a way to ﬁnd the most likely one, short of enumerating all of them?

Token 11726:
We could try this linear-time procedure: use smoothing to ﬁnd the posterior distribution for the weather at each time step; then construct the sequence, using at each step the weather that is most likely according to the posterior.

Token 11727:
Such an approach should set off alarm bells in the reader’s head, because the posterior distributions computed by smoothing are distri- function FORWARD -BACKWARD (ev,prior )returns a vector of probability distributions inputs :ev, a vector of evidence values for steps 1,...,t prior , the prior distribution on the initial state, P(X0) local variables :fv, a vector of forward messages for steps 0,...,t b, a representation of the backward message, initially all 1s sv, a vector of smoothed estimates for steps 1,...,t fv[0]←prior fori=1 totdo fv[i]←FORWARD (fv[i−1],ev[i]) fori=tdownto 1do sv[i]←NORMALIZE (fv[i]×b) b←BACKWARD (b,ev[i]) return sv Figure 15.4 The forward–backward algorithm for smoothing: computing posterior prob- abilities of a sequence of states given a sequence of observations.

Token 11728:
The F ORWARD and BACKWARD operators are deﬁned by Equations (15.5) and (15.9), respectively.

Token 11729:
Section 15.2.

Token 11730:
Inference in Temporal Models 577 Rain1 m1:1trueRain5 m1:5trueRain4 m1:4trueRain3 m1:3falseRain2 m1:2true Umbrellat(a) (b).8182 .1818.0210 .0024.0334 .0173.0361 .1237.5155 .0491true falsetrue falsetrue falsetrue falsetrue false Figure 15.5 (a) Possible state sequences for Rain tcan be viewed as paths through a graph of the possible states at each time step.

Token 11731:
(States are shown as rectangles to avoid confusionwith nodes in a Bayes net.)

Token 11732:
(b) Operation of the Viterbi algorithm for the umbrella obser- vation sequence [true,true,false,true,true].

Token 11733:
For each t, we have shown the values of the message m 1:t, which gives the probability of the best sequence reaching each state at time t. Also, for each state, the bold arrow leading into it indicates its best predecessor as measured by the product of the preceding sequence probab ility and the transition p robability.

Token 11734:
Following the bold arrows back from the most likely state in m1:5gives the most likely sequence.

Token 11735:
butions over single time steps, whereas to ﬁnd the most likely sequence we must consider joint probabilities over all the time steps.

Token 11736:
The results can in fact be quite different. (See Exercise 15.4.)

Token 11737:
There isa linear-time algorithm for ﬁnding the most likely sequence, but it requires a little more thought.

Token 11738:
It relies on the same Markov property that yielded efﬁcient algorithms forﬁltering and smoothing.

Token 11739:
The easiest way to think about the problem is to view each sequenceas a path through a graph whose nodes are the possible states at each time step.

Token 11740:
Such a graph is shown for the umbrella world in Figure 15.5(a).

Token 11741:
Now consider the task of ﬁndingthe most likely path through this graph, where the likelihood of any path is the product ofthe transition probabilities along the path and the probabilities of the given observations at each state.

Token 11742:
Let’s focus in particular on paths that reach the state Rain 5=true.

Token 11743:
Because of the Markov property, it follows that the most likely path to the state Rain 5=true consists of the most likely path to some state at time 4 followed by a transition to Rain 5=true;a n dt h e state at time 4 that will become part of the path to Rain 5=true is whichever maximizes the likelihood of that path.

Token 11744:
In other words, there is a recursive relationship between most likely paths to each state xt+1and most likely paths to each state xt.We can write this relationship as an equation connecting the probabilities of the paths: max x1...xtP(x1,..., xt,Xt+1|e1:t+1) =αP(et+1|Xt+1)max xt/parenleftbigg P(Xt+1|xt)m a x x1...xt−1P(x1,..., xt−1,xt|e1:t)/parenrightbigg .

Token 11745:
(15.11) Equation (15.11) is identical to the ﬁltering equation (15.5) except that

Token 11746:
578 Chapter 15. Probabilistic Reasoning over Time 1.

Token 11747:
The forward message f1:t=P(Xt|e1:t)is replaced by the message m1:t=m a x x1...xt−1P(x1,..., xt−1,Xt|e1:t), that is, the probabilities of the most likely path to each state xt;a n d 2. the summation over xtin Equation (15.5) is replaced by the maximization over xtin Equation (15.11).

Token 11748:
Thus, the algorithm for computing the most likely sequence is similar to ﬁltering: it runs for- ward along the sequence, computing the mmessage at each time step, using Equation (15.11).

Token 11749:
The progress of this computation is shown in Figure 15.5(b).

Token 11750:
At the end, it will have the probability for the most likely sequence reaching each of the ﬁnal states.

Token 11751:
One can thus easily select the most likely sequence overall (the states outlined in bold).

Token 11752:
In order to identify theactual sequence, as opposed to just computing its probability, the algorithm will also need torecord, for each state, the best state that leads to it; these are indicated by the bold arrows inFigure 15.5(b).

Token 11753:
The optimal sequence is identiﬁed by following these bold arrows backwardsfrom the best ﬁnal state.

Token 11754:
The algorithm we have just described is called the Viterbi algorithm , after its inventor.

Token 11755:
VITERBI ALGORITHM Like the ﬁltering algorithm, its time complexity is linear in t, the length of the sequence.

Token 11756:
Unlike ﬁltering, which uses constant space, its space requirement is also linear in t.T h i s is because the Viterbi algorithm needs to keep the pointers that identify the best sequenceleading to each state.

Token 11757:
15.3 H IDDEN MARKOV MODELS The preceding section developed algorithms for temporal probabilistic reasoning using a gen-eral framework that was independent of the speciﬁc form of the transition and sensor models.In this and the next two sections, we discuss more concrete models and applications that illustrate the power of the basic algorithms and in some cases allow further improvements.

Token 11758:
We begin with the hidden Markov model ,o rHMM .

Token 11759:
An HMM is a temporal proba- HIDDEN MARKOV MODEL bilistic model in which the state of the process is described by a single discrete random vari- able.

Token 11760:
The possible values of the variable are the possible states of the world.

Token 11761:
The umbrellaexample described in the preceding section is therefore an HMM, since it has just one statevariable: Rain t. What happens if you have a model with two or more state variables?

Token 11762:
You can still ﬁt it into the HMM framework by combining the variables into a single “megavariable”whose values are all possible tuples of values of the individual state variables.

Token 11763:
We will seethat the restricted structure of HMMs allows for a simple and elegant matrix implementationof all the basic algorithms.

Token 11764:
4 4The reader unfamiliar with basic operations on vectors and matrices might wish to consult Appendix A before proceeding with this section.

Token 11765:
Section 15.3.

Token 11766:
Hidden Markov Models 579 15.3.1 Simpliﬁed matrix algorithms With a single, discrete state variable Xt, we can give concrete form to the representations of the transition model, the sensor model, and the forward and backward messages.

Token 11767:
Let thestate variable X thave values denoted by integers 1,...,S ,w h e r e Sis the number of possible states.

Token 11768:
The transition model P(Xt|Xt−1)becomes an S×Smatrix T,w h e r e Tij=P(Xt=j|Xt−1=i).

Token 11769:
That is, Tijis the probability of a transition from state ito state j.

Token 11770:
For example, the transition matrix for the umbrella world is T=P(Xt|Xt−1)=/parenleftbigg0.70.3 0.30.7/parenrightbigg .

Token 11771:
We also put the sensor model in matrix form.

Token 11772:
In this case, because the value of the evidence variable Etis known at time t(call it et), we need only specify, for each state, how likely it is that the state causes etto appear: we need P(et|Xt=i)for each state i.

Token 11773:
For mathematical convenience we place these values into an S×Sdiagonal matrix, Otwhose ith diagonal entry is P(et|Xt=i)and whose other entries are 0.

Token 11774:
For example, on day 1 in the umbrella world of Figure 15.5, U1=true, and on day 3, U3=false , so, from Figure 15.2, we have O1=/parenleftbigg0.90 00.2/parenrightbigg ; O3=/parenleftbigg0.10 00.8/parenrightbigg .

Token 11775:
Now, if we use column vectors to represent the forward and backward messages, all the com- putations become simple matrix–vector operations.

Token 11776:
The forward equation (15.5) becomes f1:t+1=αOt+1T/latticetopf1:t (15.12) and the backward equation (15.9) becomes bk+1:t=TOk+1bk+2:t. (15.13) From these equations, we can see that the time complexity of the forward–backward algo- rithm (Figure 15.4) applied to a sequence of length tisO(S2t), because each step requires multiplying an S-element vector by an S×Smatrix.

Token 11777:
The space requirement is O(St),b e - cause the forward pass stores tvectors of size S. Besides providing an elegant description of the ﬁltering and smoothing algorithms for HMMs, the matrix formulation reveals opportunities for improved algorithms.

Token 11778:
The ﬁrst isa simple variation on the forward–backward algorithm that allows smoothing to be carriedout in constant space, independently of the length of the sequence.

Token 11779:
The idea is that smooth- ing for any particular time slice krequires the simultaneous presence of both the forward and backward messages, f 1:kandbk+1:t, according to Equation (15.8).

Token 11780:
The forward–backward al- gorithm achieves this by storing the fs computed on the forward pass so that they are available during the backward pass.

Token 11781:
Another way to achieve this is with a single pass that propagates both fandbin the same direction.

Token 11782:
For example, the “forward” message fcan be propagated backward if we manipulate Equation (15.12) to work in the other direction: f1:t=α/prime(T/latticetop)−1O−1 t+1f1:t+1.

Token 11783:
The modiﬁed smoothing algorithm works by ﬁrst running the standard forward pass to com- pute ft:t(forgetting all the intermediate results) and then running the backward pass for both

Token 11784:
580 Chapter 15.

Token 11785:
Probabilistic Reasoning over Time function FIXED -LAG-SMOOTHING (et,hmm ,d)returns a distribution over Xt−d inputs :et, the current evidence for time step t hmm , a hidden Markov model with S×Stransition matrix T d, the length of the lag for smoothing persistent :t, the current time, initially 1 f, the forward message P(Xt|e1:t), initially hmm.PRIOR B,t h ed-step backward transformation matrix, initially the identity matrix et−d:t, double-ended list of evidence from t−dtot, initially empty local variables :Ot−d,Ot, diagonal matrices containing the sensor model information addetto the end of et−d:t Ot←diagonal matrix containing P(et|Xt) ift>d then f←FORWARD (f,et) remove et−d−1from the beginning of et−d:t Ot−d←diagonal matrix containing P(et−d|Xt−d) B←O−1 t−dT−1BTO t else B←BTO t t←t+1 ift>d then return NORMALIZE (f×B1)else return null Figure 15.6 An algorithm for smoothing with a ﬁxed time lag of dsteps, implemented as an online algorithm that outputs the new smoothed estimate given the observation for anew time step.

Token 11786:
Notice that the ﬁnal output N ORMALIZE (f×B1)is just αf×b, by Equa- tion (15.14).

Token 11787:
bandftogether, using them to compute the smoothed estimate at each step.

Token 11788:
Since only one copy of each message is needed, the storage requirements are constant (i.e., independent oft, the length of the sequence).

Token 11789:
There are two signiﬁcant restrictions on this algorithm: it re- quires that the transition matrix be invertible and that the sensor model have no zeroes—thatis, that every observation be possible in every state.

Token 11790:
A second area in which the matrix formulation reveals an improvement is in online smoothing with a ﬁxed lag.

Token 11791:
The fact that smoothing can be done in constant space suggeststhat there should exist an efﬁcient recursive algorithm for online smoothing—that is, an al- gorithm whose time complexity is independent of the length of the lag.

Token 11792:
Let us suppose that the lag is d; that is, we are smoothing at time slice t−d, where the current time is t.B y Equation (15.8), we need to compute αf 1:t−d×bt−d+1:t for slice t−d.

Token 11793:
Then, when a new observation arrives, we need to compute αf1:t−d+1×bt−d+2:t+1 for slice t−d+1. How can this be done incrementally?

Token 11794:
First, we can compute f1:t−d+1from f1:t−d, using the standard ﬁltering process, Equation (15.5).

Token 11795:
Section 15.3.

Token 11796:
Hidden Markov Models 581 Computing the backward message incrementally is trickier, because there is no simple relationship between the old backward message bt−d+1:tand the new backward message bt−d+2:t+1.

Token 11797:
Instead, we will examine the relationship between the old backward message bt−d+1:tand the backward message at the front of the sequence, bt+1:t. To do this, we apply Equation (15.13) dtimes to get bt−d+1:t=/parenleftBiggt/productdisplay i=t−d+1TOi/parenrightBigg bt+1:t=Bt−d+1:t1, (15.14) where the matrix Bt−d+1:tis the product of the sequence of TandOmatrices.

Token 11798:
Bcan be thought of as a “transformation operator” that transforms a later backward message into anearlier one.

Token 11799:
A similar equation holds for the new backward messages after the next observa- tion arrives: b t−d+2:t+1=/parenleftBiggt+1/productdisplay i=t−d+2TOi/parenrightBigg bt+2:t+1=Bt−d+2:t+11.

Token 11800:
(15.15) Examining the product expressions in Equations (15.14) and (15.15), we see that they have a simple relationship: to get the second product, “divide” the ﬁrst product by the ﬁrst elementTO t−d+1, and multiply by the new last element TOt+1.

Token 11801:
In matrix language, then, there is a simple relationship between the old and new Bmatrices: Bt−d+2:t+1=O−1 t−d+1T−1Bt−d+1:tTOt+1.

Token 11802:
(15.16) This equation provides an incremental update for the Bmatrix, which in turn (through Equa- tion (15.15)) allows us to compute the new backward message bt−d+2:t+1.

Token 11803:
The complete algorithm, which requires storing and updating fandB, is shown in Figure 15.6.

Token 11804:
15.3.2 Hidden Markov model example: Localization On page 145, we introduced a simple form of the localization problem for the vacuum world.

Token 11805:
In that version, the robot had a single nondeterministic Move action and its sensors reported perfectly whether or not obstacles lay immediately to the north, south, east, and west; therobot’s belief state was the set of possible locations it could be in.

Token 11806:
Here we make the problem slightly more realistic by including a simple probability model for the robot’s motion and by allowing for noise in the sensors.

Token 11807:
The state variable X t represents the location of the robot on the discrete grid; the domain of this variable is the set of empty squares {s1,...,s n}.L e tN EIGHBORS (s)be the set of empty squares that are adjacent to sand let N(s)be the size of that set.

Token 11808:
Then the transition model for Move action says that the robot is equally likely to end up at any neighboring square: P(Xt+1=j|Xt=i)=Tij=( 1/N(i)ifj∈NEIGHBORS (i)else0).

Token 11809:
We don’t know where the robot starts, so we will assume a uniform distribution over all the squares; that is, P(X0=i)=1/n.

Token 11810:
For the particular environment we consider (Figure 15.7), n=4 2 and the transition matrix Thas42×42= 1764 entries.

Token 11811:
The sensor variable Ethas 16 possible values, each a four-bit sequence giving the pres- ence or absence of an obstacle in a particular compass direction.

Token 11812:
We will use the notation

Token 11813:
582 Chapter 15.

Token 11814:
Probabilistic Reasoning over Time (a) Posterior distribution over robot location after E1=N S W (b) Posterior distribution over robot location after E1=N S W , E 2=N S Figure 15.7 Posterior distribution over robot location: (a) one observation E1=NSW ; (b) after a second observation E2=NS.

Token 11815:
The size of each disk corresponds to the probability that the robot is at that location. The sensor error rate is /epsilon1=0.2.

Token 11816:
NS, for example, to mean that the north and south sensors report an obstacle and the east and west do not.

Token 11817:
Suppose that each sensor’s error rate is /epsilon1and that errors occur independently for the four sensor directions.

Token 11818:
In that case, the probability of getting all four bits right is (1−/epsilon1)4 and the probability of getting them all wrong is /epsilon14.F u r t h e r m o r e ,i f ditis the discrepancy—the number of bits that are different—between the true values for square iand the actual reading et, then the probability that a robot in square iwould receive a sensor reading etis P(Et=et|Xt=i)=Otii=( 1−/epsilon1)4−dit/epsilon1dit.

Token 11819:
For example, the probability that a square with obstacles to the north and south would produce a sensor reading NSE is(1−/epsilon1)3/epsilon11.

Token 11820:
Given the matrices TandOt, the robot can use Equation (15.12) to compute the pos- terior distribution over locations—that is, to work out where it is.

Token 11821:
Figure 15.7 shows thedistributions P(X 1|E1=NSW )andP(X2|E1=NSW,E 2=NS).

Token 11822:
This is the same maze we saw before in Figure 4.18 (page 146), but there we used logical ﬁltering to ﬁnd the loca- tions that were possible , assuming perfect sensing.

Token 11823:
Those same locations are still the most likely with noisy sensing, but now every location has some nonzero probability.

Token 11824:
In addition to ﬁltering to estimate its current location, the robot can use smoothing (Equation (15.13)) to work out where it was at any given past time—for example, where it began at time 0—and it can use the Viterbi algorithm to work out the most likely path it has

Token 11825:
Section 15.3.

Token 11826:
Hidden Markov Models 583 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 0 5 10 15 20 25 30 35 40Localization error Number of observationsε = 0.20 ε = 0.10 ε = 0.05 ε = 0.02 ε = 0.00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 30 35 40Path accuracy Number of observationsε = 0.00 ε = 0.02 ε = 0.05 ε = 0.10 ε = 0.20 (a) (b) Figure 15.8 Performance of HMM localization as a function of the length of the observa- tion sequence for various different values of the sensor error probability /epsilon1; data averaged over 400 runs.

Token 11827:
(a) The localization error, deﬁned as the Manhattan distance from the true location.

Token 11828:
(b) The Viterbi path accuracy, deﬁned as the fraction of correct states on the Viterbi path. taken to get where it is now.

Token 11829:
Figure 15.8 shows the localization error and Viterbi path accuracy for various values of the per-bit sensor error rate /epsilon1.E v e nw h e n /epsilon1is 20%—which means that the overall sensor reading is wrong 59% of the time—the robot is usually able to work out its location within two squares after 25 observations.

Token 11830:
This is because of the algorithm’s ability to integrate evidence over time and to take into account the probabilistic constraints imposedon the location sequence by the transition model.

Token 11831:
When /epsilon1is 10%, the performance after a half-dozen observations is hard to distinguish from the performance with perfect sensing.Exercise 15.7 asks you to explore how robust the HMM localization algorithm is to errors inthe prior distribution P(X 0)and in the transition model itself.

Token 11832:
Broadly speaking, high levels of localization and path accuracy are maintained even in the face of substantial errors in themodels used.

Token 11833:
The state variable for the example we have considered in this section is a physical location in the world.

Token 11834:
Other problems can, of course, include other aspects of the world.

Token 11835:
Exercise 15.8 asks you to consider a version of the vacuum robot that has the policy of going straight for as long as it can; only when it encounters an obstacle does it change to a new (randomly selected) heading.

Token 11836:
To model this robot, each state in the model consists of a (location, heading) pair.

Token 11837:
For the environment in Figure 15.7, which has 42 empty squares, this leads to 168 states and a transition matrix with 168 2=2 8,224entries—still a manageable number.

Token 11838:
If we add the possibility of dirt in the squares, the number of states is multiplied by2 42and the transition matrix ends up with more than 1029entries—no longer a manageable number; Section 15.5 shows how to use dynamic Bayesian networks to model domains withmany state variables.

Token 11839:
If we allow the robot to move continuously rather than in a discretegrid, the number of states becomes inﬁnite; the next section shows how to handle this case.

Token 11840:
584 Chapter 15.

Token 11841:
Probabilistic Reasoning over Time 15.4 K ALMAN FILTERS Imagine watching a small bird ﬂying through dense jungle foliage at dusk: you glimpse brief, intermittent ﬂashes of motion; you try hard to guess where the bird is and where it willappear next so that you don’t lose it.

Token 11842:
Or imagine that you are a World War II radar operatorpeering at a faint, wandering blip that appears once every 10 seconds on the screen.

Token 11843:
Or, goingback further still, imagine you are Kepler trying to reconstruct the motions of the planetsfrom a collection of highly inaccurate angular observations taken at irregular and impreciselymeasured intervals.

Token 11844:
In all these cases, you are doing ﬁltering: estimating state variables (here, position and velocity) from noisy observations over time.

Token 11845:
If the variables were discrete, we could model the system with a hidden Markov model.

Token 11846:
This section examines methods forhandling continuous variables, using an algorithm called Kalman ﬁltering , after one of its KALMAN FILTERING inventors, Rudolf E. Kalman.

Token 11847:
The bird’s ﬂight might be speciﬁed by six continuous variables at each time point; three for position (Xt,Yt,Zt)and three for velocity (˙Xt,˙Yt,˙Zt).

Token 11848:
We will need suitable conditional densities to represent the transition and sensor models; as in Chapter 14, we will use linear Gaussian distributions.

Token 11849:
This means that the next state Xt+1must be a linear function of the current state Xt, plus some Gaussian noise, a condition that turns out to be quite reasonable in practice.

Token 11850:
Consider, for example, the X-coordinate of the bird, ignoring the other coordinates for now.

Token 11851:
Let the time interval between observations be Δ, and assume constant velocity during the interval; then the position update is given by Xt+Δ=Xt+˙XΔ.

Token 11852:
Adding Gaussian noise (to account for wind variation, etc.

Token 11853:
), we obtain a linear Gaussian transition model: P(Xt+Δ=xt+Δ|Xt=xt,˙Xt=˙xt)=N(xt+˙xtΔ,σ2)(xt+Δ).

Token 11854:
The Bayesian network structure for a system with position vector Xtand velocity ˙Xtis shown in Figure 15.9.

Token 11855:
Note that this is a very speciﬁc form of linear Gaussian model; the generalform will be described later in this section and covers a vast array of applications beyond thesimple motion examples of the ﬁrst paragraph.

Token 11856:
The reader might wish to consult Appendix A for some of the mathematical properties of Gaussian distributions; for our immediate pur- poses, the most important is that a multivariate Gaussian distribution for dvariables is MULTIVARIATE GAUSSIAN speciﬁed by a d-element mean μand ad×dcovariance matrix Σ.

Token 11857:
15.4.1 Updating Gaussian distributions In Chapter 14 on page 521, we alluded to a key property of the linear Gaussian family of dis- tributions: it remains closed under the standard Bayesian network operations.

Token 11858:
Here, we makethis claim precise in the context of ﬁltering in a temporal probability model.

Token 11859:
The requiredproperties correspond to the two-step ﬁltering calculation in Equation (15.5): 1.

Token 11860:
If the current distribution P(X t|e1:t)is Gaussian and the transition model P(Xt+1|xt) is linear Gaussian, then the one-step predicted distribution given by P(Xt+1|e1:t)=/integraldisplay xtP(Xt+1|xt)P(xt|e1:t)dxt (15.17) is also a Gaussian distribution.

Token 11861:
Section 15.4.

Token 11862:
Kalman Filters 585 tZ t+1ZtX t+1X tX t+1X Figure 15.9 Bayesian network structure for a linear dynamical system with position Xt, velocity ˙Xt, and position measurement Zt.

Token 11863:
2.

Token 11864:
If the prediction P(Xt+1|e1:t)is Gaussian and the sensor model P(et+1|Xt+1)is linear Gaussian, then, after conditioning on the new evidence, the updated distribution P(Xt+1|e1:t+1)=αP(et+1|Xt+1)P(Xt+1|e1:t) (15.18) is also a Gaussian distribution.

Token 11865:
Thus, the F ORW ARD operator for Kalman ﬁltering takes a Gaussian forward message f1:t, speciﬁed by a mean μtand covariance matrix Σt, and produces a new multivariate Gaussian forward message f1:t+1, speciﬁed by a mean μt+1and covariance matrix Σt+1.S o , i f w e start with a Gaussian prior f1:0=P(X0)=N(μ0,Σ0), ﬁltering with a linear Gaussian model produces a Gaussian state distribution for all time.

Token 11866:
This seems to be a nice, elegant result, but why is it so important?

Token 11867:
The reason is that, except for a few special cases such as this, ﬁltering with continuous or hybrid (discrete and continuous) networks generates state distributions whose representation grows without bound over time.

Token 11868:
This statement is not easy to prove in general, but Exercise 15.10 shows what happens for a simple example.

Token 11869:
15.4.2 A simple one-dimensional example We have said that the F ORW ARD operator for the Kalman ﬁlter maps a Gaussian into a new Gaussian.

Token 11870:
This translates into computing a new mean and covariance matrix from the previ-ous mean and covariance matrix.

Token 11871:
Deriving the update rule in the general (multivariate) caserequires rather a lot of linear algebra, so we will stick to a very simple univariate case for now;and later give the results for the general case.

Token 11872:
Even for the univariate case, the calculationsare somewhat tedious, but we feel that they are worth seeing because the usefulness of theKalman ﬁlter is tied so intimately to the mathematical properties of Gaussian distributions.

Token 11873:
The temporal model we consider describes a random walk of a single continuous state variable X twith a noisy observation Zt.

Token 11874:
An example might be the “consumer conﬁdence” in- dex, which can be modeled as undergoing a random Gaussian-distributed change each monthand is measured by a random consumer survey that also introduces Gaussian sampling noise.

Token 11875:
586 Chapter 15. Probabilistic Reasoning over Time The prior distribution is assumed to be Gaussian with variance σ2 0: P(x0)=αe−1 2„ (x0−μ0)2 σ2 0« .

Token 11876:
(For simplicity, we use the same symbol αfor all normalizing constants in this section.)

Token 11877:
The transition model adds a Gaussian perturbation of constant variance σ2 xto the current state: P(xt+1|xt)=αe−1 2„ (xt+1−xt)2 σ2x« .

Token 11878:
The sensor model assumes Gaussian noise with variance σ2 z: P(zt|xt)=αe−1 2„ (zt−xt)2 σ2z« .

Token 11879:
Now, given the prior P(X0), the one-step predicted distribution comes from Equation (15.17): P(x1)=/integraldisplay∞ −∞P(x1|x0)P(x0)dx0=α/integraldisplay∞ −∞e−1 2„ (x1−x0)2 σ2x« e−1 2„ (x0−μ0)2 σ2 0« dx0 =α/integraldisplay∞ −∞e−1 2„ σ2 0(x1−x0)2+σ2x(x0−μ0)2 σ2 0σ2x« dx0.

Token 11880:
This integral looks rather complicated.

Token 11881:
The key to progress is to notice that the exponent is the sum of two expressions that are quadratic inx0and hence is itself a quadratic in x0.As i m p l e trick known as completing the square allows the rewriting of any quadratic ax2 0+bx0+cCOMPLETING THE SQUARE as the sum of a squared term a(x0−−b 2a)2and a residual term c−b2 4athat is independent of x0.

Token 11882:
The residual term can be taken outside the integral, giving us P(x1)=αe−1 2“ c−b2 4a”/integraldisplay∞ −∞e−1 2(a(x0−−b 2a)2)dx0.

Token 11883:
Now the integral is just the integral of a Gaussian over its full range, which is simply 1.

Token 11884:
Thus, we are left with only the residual term from the quadratic.

Token 11885:
Then, we notice that the residualterm is a quadratic in x 1; in fact, after simpliﬁcation, we obtain P(x1)=αe−1 2„ (x1−μ0)2 σ2 0+σ2x« .

Token 11886:
That is, the one-step predicted distribution is a Gaussian with the same mean μ0and a variance equal to the sum of the original variance σ2 0and the transition variance σ2 x.

Token 11887:
To complete the update step, we need to condition on the observation at the ﬁrst time step, namely, z1.

Token 11888:
From Equation (15.18), this is given by P(x1|z1)=αP(z1|x1)P(x1) =αe−1 2„ (z1−x1)2 σ2z« e−1 2„ (x1−μ0)2 σ2 0+σ2x« .

Token 11889:
Once again, we combine the exponents and complete the square (Exercise 15.11), obtaining P(x1|z1)=αe−1 20 BB@(x1−(σ2 0+σ2x)z1+σ2zμ0 σ2 0+σ2x+σ2z)2 (σ2 0+σ2x)σ2z/(σ2 0+σ2x+σ2z)1 CCA .

Token 11890:
(15.19)

Token 11891:
Section 15.4.

Token 11892:
Kalman Filters 587 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -10 -5 0 5 10P(x) x positionP(x0) P(x1)P(x1 | z1 = 2.5) *z1 Figure 15.10 Stages in the Kalman ﬁlter update cycle for a random walk with a prior given by μ0=0.0andσ0=1.0, transition noise given by σx=2.0, sensor noise given by σz=1.0, and a ﬁrst observation z1=2.5(marked on the x-axis).

Token 11893:
Notice how the prediction P(x1)is ﬂattened out, relative to P(x0), by the transition noise.

Token 11894:
Notice also that the mean of the posterior distribution P(x1|z1)is slightly to the left of the observation z1because the mean is a weighted average of the prediction and the observation.

Token 11895:
Thus, after one update cycle, we have a new Gaussian distribution for the state variable.

Token 11896:
From the Gaussian formula in Equation (15.19), we see that the new mean and standard deviation can be calculated from the old mean and standard deviation as follows: μt+1=(σ2 t+σ2 x)zt+1+σ2 zμt σ2 t+σ2x+σ2zand σ2 t+1=(σ2 t+σ2 x)σ2 z σ2 t+σ2x+σ2z.

Token 11897:
(15.20) Figure 15.10 shows one update cycle for particular values of the transition and sensor models.

Token 11898:
Equation (15.20) plays exactly the same role as the general ﬁltering equation (15.5) or the HMM ﬁltering equation (15.12).

Token 11899:
Because of the special nature of Gaussian distributions, however, the equations have some interesting additional properties.

Token 11900:
First, we can interpretthe calculation for the new mean μ t+1as simply a weighted mean of the new observation zt+1and the old mean μt.

Token 11901:
If the observation is unreliable, then σ2 zis large and we pay more attention to the old mean; if the old mean is unreliable ( σ2 tis large) or the process is highly unpredictable ( σ2 xis large), then we pay more attention to the observation.

Token 11902:
Second, notice that the update for the variance σ2 t+1isindependent of the observation .

Token 11903:
We can therefore compute in advance what the sequence of variance values will be.

Token 11904:
Third, the sequence ofvariance values converges quickly to a ﬁxed value that depends only on σ 2 xandσ2 z, thereby substantially simplifying the subsequent calculations.

Token 11905:
(See Exercise 15.12.)

Token 11906:
15.4.3 The general case The preceding derivation illustrates the key property of Gaussian distributions that allows Kalman ﬁltering to work: the fact that the exponent is a quadratic form.

Token 11907:
This is true not just for the univariate case; the full multivariate Gaussian distribution has the form N(μ,Σ)(x)=αe−1 2“ (x−μ)/latticetopΣ−1(x−μ)” .

Token 11908:
588 Chapter 15.

Token 11909:
Probabilistic Reasoning over Time Multiplying out the terms in the exponent makes it clear that the exponent is also a quadratic function of the values xiinx.

Token 11910:
As in the univariate case, the ﬁltering update preserves the Gaussian nature of the state distribution.

Token 11911:
Let us ﬁrst deﬁne the general temporal model used with Kalman ﬁltering.

Token 11912:
Both the tran- sition model and the sensor model allow for a linear transformation with additive Gaussian noise.

Token 11913:
Thus, we have P(xt+1|xt)=N(Fxt,Σx)(xt+1) P(zt|xt)=N(Hxt,Σz)(zt),(15.21) where FandΣxare matrices describing the linear transition model and transition noise co- variance, and HandΣzare the corresponding matrices for the sensor model.

Token 11914:
Now the update equations for the mean and covariance, in their full, hairy horribleness, are μt+1=Fμt+Kt+1(zt+1−HFμt) Σt+1=(I−Kt+1H)(FΣtF/latticetop+Σx),(15.22) where Kt+1=(FΣtF/latticetop+Σx)H/latticetop(H(FΣtF/latticetop+Σx)H/latticetop+Σz)−1is called the Kalman gain matrix .

Token 11915:
Believe it or not, these equations make some intuitive sense.

Token 11916:
For example, considerKALMAN GAIN MATRIX the update for the mean state estimate μ.T h e t e r m Fμtis the predicted state at t+1,s o HFμtis the predicted observation.

Token 11917:
Therefore, the term zt+1−HFμtrepresents the error in the predicted observation.

Token 11918:
This is multiplied by Kt+1to correct the predicted state; hence, Kt+1is a measure of how seriously to take the new observation relative to the prediction.

Token 11919:
As in Equation (15.20), we also have the property that the variance update is independent of theobservations.

Token 11920:
The sequence of values for Σ tandKtcan therefore be computed ofﬂine, and the actual calculations required during online tracking are quite modest.

Token 11921:
To illustrate these equations at work, we have applied them to the problem of tracking an object moving on the X–Yplane.

Token 11922:
The state variables are X=(X,Y, ˙X,˙Y)/latticetop,s oF,Σx, H,a n dΣzare4×4matrices.

Token 11923:
Figure 15.11(a) shows the true trajectory, a series of noisy observations, and the trajectory estimated by Kalman ﬁltering, along with the covariancesindicated by the one-standard-deviation contours.

Token 11924:
The ﬁltering process does a good job oftracking the actual motion, and, as expected, the variance quickly reaches a ﬁxed point.

Token 11925:
We can also derive equations for smoothing as well as ﬁltering with linear Gaussian models. The smoothing results are shown in Figure 15.11(b).

Token 11926:
Notice how the variance in the position estimate is sharply reduced, except at the ends of the trajectory (why?

Token 11927:
), and that the estimated trajectory is much smoother.

Token 11928:
15.4.4 Applicability of Kalman ﬁltering The Kalman ﬁlter and its elaborations are used in a vast array of applications.

Token 11929:
The “classical”application is in radar tracking of aircraft and missiles.

Token 11930:
Related applications include acoustictracking of submarines and ground vehicles and visual tracking of vehicles and people.

Token 11931:
In aslightly more esoteric vein, Kalman ﬁlters are used to reconstruct particle trajectories from bubble-chamber photographs and ocean currents from satellite surface measurements.

Token 11932:
The range of application is much larger than just the tracking of motion: any system characterizedby continuous state variables and noisy measurements will do.

Token 11933:
Such systems include pulpmills, chemical plants, nuclear reactors, plant ecosystems, and national economies.

Token 11934:
Section 15.4.

Token 11935:
Kalman Filters 589 8 10 12 14 16 18 20 22 24 266789101112 XY 8 10 12 14 16 18 20 22 24 266789101112 XYtrue observed smoothedtrue observed smoothed (a) (b)2D filtering 2D smoothing Figure 15.11 (a) Results of Kalman ﬁltering for an object moving on the X–Yplane, showing the true trajectory (left to right), a series of noisy observations, and the trajectory estimated by Kalman ﬁltering.

Token 11936:
Variance in the position estimate is indicated by the ovals. (b)The results of Kalman smoothing for the same observation sequence.

Token 11937:
The fact that Kalman ﬁltering can be applied to a system does not mean that the re- sults will be valid or useful.

Token 11938:
The assumptions made—a linear Gaussian transition and sensormodels—are very strong.

Token 11939:
The extended Kalman ﬁlter (EKF) attempts to overcome nonlin- EXTENDED KALMAN FILTER (EKF) earities in the system being modeled.

Token 11940:
A system is nonlinear if the transition model cannot NONLINEAR be described as a matrix multiplication of the state vector, as in Equation (15.21).

Token 11941:
The EKF works by modeling the system as locally linear in xtin the region of xt=μt, the mean of the current state distribution.

Token 11942:
This works well for smooth, well-behaved systems and allows thetracker to maintain and update a Gaussian state distribution that is a reasonable approximation to the true posterior.

Token 11943:
A detailed example is given in Chapter 25. What does it mean for a system to be “unsmooth” or “poorly behaved”?

Token 11944:
Technically, it means that there is signiﬁcant nonlinearity in system response within the region that is“close” (according to the covariance Σ t) to the current mean μt.

Token 11945:
To understand this idea in nontechnical terms, consider the example of trying to track a bird as it ﬂies through the jungle.

Token 11946:
The bird appears to be heading at high speed straight for a tree trunk.

Token 11947:
The Kalman ﬁlter, whether regular or extended, can make only a Gaussian prediction of the location of the bird, and the mean of this Gaussian will be centered on the trunk, as shown in Figure 15.12(a).A reasonable model of the bird, on the other hand, would predict evasive action to one side orthe other, as shown in Figure 15.12(b).

Token 11948:
Such a model is highly nonlinear, because the bird’sdecision varies sharply depending on its precise location relative to the trunk.

Token 11949:
To handle examples like these, we clearly need a more expressive language for repre- senting the behavior of the system being modeled.

Token 11950:
Within the control theory community, forwhich problems such as evasive maneuvering by aircraft raise the same kinds of difﬁculties,the standard solution is the switching Kalman ﬁlter .

Token 11951:
In this approach, multiple Kalman ﬁl- SWITCHINGKALMAN FILTER

Token 11952:
590 Chapter 15. Probabilistic Reasoning over Time (a) (b) Figure 15.12 A bird ﬂying toward a tree (top views).

Token 11953:
(a) A Kalman ﬁlter will predict the location of the bird using a single Gaussian centered on the obstacle.

Token 11954:
(b) A more realisticmodel allows for the bird’s evasive action, predicting that it will ﬂy to one side or the other.

Token 11955:
ters run in parallel, each using a different model of the system—for example, one for straight ﬂight, one for sharp left turns, and one for sharp right turns.

Token 11956:
A weighted sum of predictionsis used, where the weight depends on how well each ﬁlter ﬁts the current data.

Token 11957:
We will seein the next section that this is simply a special case of the general dynamic Bayesian net-work model, obtained by adding a discrete “maneuver” state variable to the network shownin Figure 15.9.

Token 11958:
Switching Kalman ﬁlters are discussed further in Exercise 15.10.

Token 11959:
15.5 D YNAMIC BAYESIAN NETWORKS Adynamic Bayesian network ,o rDBN , is a Bayesian network that represents a temporalDYNAMIC BAYESIAN NETWORK probability model of the kind described in Section 15.1.

Token 11960:
We have already seen examples of DBNs: the umbrella network in Figure 15.2 and the Kalman ﬁlter network in Figure 15.9.

Token 11961:
Ingeneral, each slice of a DBN can have any number of state variables X tand evidence variables Et.

Token 11962:
For simplicity, we assume that the variables and their links are exactly replicated from slice to slice and that the DBN represents a ﬁrst-order Markov process, so that each variablecan have parents only in its own slice or the immediately preceding slice.

Token 11963:
It should be clear that every hidden Markov model can be represented as a DBN with a single state variable and a single evidence variable.

Token 11964:
It is also the case that every discrete- variable DBN can be represented as an HMM; as explained in Section 15.3, we can combine all the state variables in the DBN into a single state variable whose values are all possibletuples of values of the individual state variables.

Token 11965:
Now, if every HMM is a DBN and everyDBN can be translated into an HMM, what’s the difference? The difference is that, by de-

Token 11966:
Section 15.5.

Token 11967:
Dynamic Bayesian Networks 591 composing the state of a complex system into its constituent variables, the can take advantage ofsparseness in the temporal probability model.

Token 11968:
Suppose, for example, that a DBN has 20 Boolean state variables, each of which has three parents in the preceding slice.

Token 11969:
Then theDBN transition model has 20×2 3= 160 probabilities, whereas the corresponding HMM has 220states and therefore 240, or roughly a trillion, probabilities in the transition matrix.

Token 11970:
This is bad for at least three reasons: ﬁrst, the HMM itself requires much more space; second, the huge transition matrix makes HMM inference much more expensive; and third, the prob-lem of learning such a huge number of parameters makes the pure HMM model unsuitablefor large problems.

Token 11971:
The relationship between DBNs and HMMs is roughly analogous to therelationship between ordinary Bayesian networks and full tabulated joint distributions.

Token 11972:
We have already explained that every Kalman ﬁlter model can be represented in a DBN with continuous variables and linear Gaussian conditional distributions (Figure 15.9).It should be clear from the discussion at the end of the preceding section that notevery DBN can be represented by a Kalman ﬁlter model.

Token 11973:
In a Kalman ﬁlter, the current state distributionis always a single multivariate Gaussian distribution—that is, a single “bump” in a particularlocation.

Token 11974:
DBNs, on the other hand, can model arbitrary distributions. For many real-worldapplications, this ﬂexibility is essential.

Token 11975:
Consider, for example, the current location of my keys.

Token 11976:
They might be in my pocket, on the bedside table, on the kitchen counter, dangling from the front door, or locked in the car.

Token 11977:
A single Gaussian bump that included all theseplaces would have to allocate signiﬁcant probability to the keys being in mid-air in the fronthall.

Token 11978:
Aspects of the real world such as purposive agents, obstacles, and pockets introduce“nonlinearities” that require combinations of discrete and continuous variables in order to getreasonable models.

Token 11979:
15.5.1 Constructing DBNs To construct a DBN, one must specify three kinds of information: the prior distribution overthe state variables, P(X 0); the transition model P(Xt+1|Xt); and the sensor model P(Et|Xt).

Token 11980:
To specify the transition and sensor models, one must also specify the topology of the con- nections between successive slices and between the state and evidence variables.

Token 11981:
Becausethe transition and sensor models are assumed to be stationary—the same for all t—it is most convenient simply to specify them for the ﬁrst slice.

Token 11982:
For example, the complete DBN speci-ﬁcation for the umbrella world is given by the three-node network shown in Figure 15.13(a).From this speciﬁcation, the complete DBN with an unbounded number of time slices can beconstructed as needed by copying the ﬁrst slice.

Token 11983:
Let us now consider a more interesting example: monitoring a battery-powered robot moving in the X–Y plane, as introduced at the end of Section 15.1.

Token 11984:
First, we need statevariables, which will include both X t=(Xt,Yt)for position and ˙Xt=(˙Xt,˙Yt)for velocity.

Token 11985:
We assume some method of measuring position—perhaps a ﬁxed camera or onboard GPS (Global Positioning System)—yielding measurements Zt.

Token 11986:
The position at the next time step depends on the current position and velocity, as in the standard Kalman ﬁlter model.

Token 11987:
Thevelocity at the next step depends on the current velocity and the state of the battery.

Token 11988:
WeaddBattery tto represent the actual battery charge level, which has as parents the previous

Token 11989:
592 Chapter 15.

Token 11990:
Probabilistic Reasoning over Time Rain0 Rain1 Umbrella 10.9t 0.2fP(U )1 R10.3f0.7tP(R )1R0 0.7P(R )0 Z1X1 X1 tXX0X01Battery Battery01BMeter (a) (b) Figure 15.13 (a) Speciﬁcation of the prior, transition model, and sensor model for the umbrella DBN.

Token 11991:
All subsequent slices are assumed to be copies of slice 1. (b) A simple DBN for robot motion in the X–Y plane.

Token 11992:
battery level and the velocity, and we add BMeter t, which measures the battery charge level. This gives us the basic model shown in Figure 15.13(b).

Token 11993:
It is worth looking in more depth at the nature of the sensor model for BMeter t.L e t us suppose, for simplicity, that both Battery tandBMeter tcan take on discrete values 0 through 5.

Token 11994:
If the meter is always accurate, then the CPT P(BMeter t|Battery t)should have probabilities of 1.0 “along the diagonal” and probabilities of 0.0 elsewhere.

Token 11995:
In reality, noisealways creeps into measurements. For continuous measurements, a Gaussian distributionwith a small variance might be used.

Token 11996:
5For our discrete variables, we can approximate a Gaussian using a distribution in which the probability of error drops off in the appropriate way, so that the probability of a large error is very small.

Token 11997:
We use the term Gaussian error model to cover both the continuous and discrete versions.GAUSSIAN ERROR MODEL Anyone with hands-on experience of robotics, computerized process control, or other forms of automatic sensing will readily testify to the fact that small amounts of measurement noise are often the least of one’s problems.

Token 11998:
Real sensors fail.

Token 11999:
When a sensor fails, it does not necessarily send a signal saying, “Oh, by the way, the data I’m about to send you is aload of nonsense.” Instead, it simply sends the nonsense.

Token 12000:
The simplest kind of failure iscalled a transient failure , where the sensor occasionally decides to send some nonsense.

Token 12001:
For TRANSIENT FAILURE example, the battery level sensor might have a habit of sending a zero when someone bumps the robot, even if the battery is fully charged.

Token 12002:
Let’s see what happens when a transient failure occurs with a Gaussian error model that doesn’t accommodate such failures.

Token 12003:
Suppose, for example, that the robot is sitting quietly andobserves 20 consecutive battery readings of 5.

Token 12004:
Then the battery meter has a temporary seizure 5Strictly speaking, a Gaussian distribution is problematic because it a ssigns nonzero probab ility to large nega- tive charge levels.

Token 12005:
The beta distribution is sometimes a better choice for a variable whose range is restricted.

Token 12006:
Section 15.5. Dynamic Bayesian Networks 593 and the next reading is BMeter 21=0.

Token 12007:
What will the simple Gaussian error model lead us to believe about Battery 21?

Token 12008:
According to Bayes’ rule, the answer depends on both the sensor model P(BMeter 21=0|Battery 21)and the prediction P(Battery 21|BMeter 1:20).I f t h e probability of a large sensor error is signiﬁcantly less likely than the probability of a transitiontoBattery 21=0, even if the latter is very unlikely, then the posterior distribution will assign a high probability to the battery’s being empty.

Token 12009:
A second reading of 0 at t=2 2 will make this conclusion almost certain.

Token 12010:
If the transient failure then disappears and the reading returnsto 5 from t=2 3 onwards, the estimate for the battery level will quickly return to 5, as if by magic.

Token 12011:
This course of events is illustrated in the upper curve of Figure 15.14(a), which showsthe expected value of Battery tover time, using a discrete Gaussian error model.

Token 12012:
Despite the recovery, there is a time ( t=2 2 ) when the robot is convinced that its battery is empty; presumably, then, it should send out a mayday signal and shut down.

Token 12013:
Alas, itsoversimpliﬁed sensor model has led it astray. How can this be ﬁxed?

Token 12014:
Consider a familiarexample from everyday human driving: on sharp curves or steep hills, one’s “fuel tank empty”warning light sometimes turns on.

Token 12015:
Rather than looking for the emergency phone, one simplyrecalls that the fuel gauge sometimes gives a very large error when the fuel is sloshing aroundin the tank.

Token 12016:
The moral of the story is the following: for the system to handle sensor failure properly, the sensor model must include the possibility of failure.

Token 12017:
The simplest kind of failure model for a sensor allows a certain probability that the sensor will return some completely incorrect value, regardless of the true state of the world.For example, if the battery meter fails by returning 0, we might say that P(BMeter t=0|Battery t=5 )=0 .03, which is presumably much larger than the probability assigned by the simple Gaussian error model.

Token 12018:
Let’s call this the transient failure model . How does it help when we are facedTRANSIENT FAILURE MODEL with a reading of 0?

Token 12019:
Provided that the predicted probability of an empty battery, according to the readings so far, is much less than 0.03, then the best explanation of the observationBMeter 21=0is that the sensor has temporarily failed.

Token 12020:
Intuitively, we can think of the belief about the battery level as having a certain amount of “inertia” that helps to overcome tempo- rary blips in the meter reading.

Token 12021:
The upper curve in Figure 15.14(b) shows that the transientfailure model can handle transient failures without a catastrophic change in beliefs.

Token 12022:
So much for temporary blips. What about a persistent sensor failure? Sadly, failures of this kind are all too common.

Token 12023:
If the sensor returns 20 readings of 5 followed by 20 readingsof 0, then the transient sensor failure model described in the preceding paragraph will resultin the robot gradually coming to believe that its battery is empty when in fact it may be thatthe meter has failed.

Token 12024:
The lower curve in Figure 15.14(b) shows the belief “trajectory” forthis case.

Token 12025:
By t=2 5 —ﬁve readings of 0—the robot is convinced that its battery is empty.

Token 12026:
Obviously, we would prefer the robot to believe that its battery meter is broken—if indeedthis is the more likely event.

Token 12027:
Unsurprisingly, to handle persistent failure, we need a persistent failure model that PERSISTENT FAILUREMODEL describes how the sensor behaves under normal conditions and after failure.

Token 12028:
To do this, we need to augment the state of the system with an additional variable, say, BMBroken ,t h a t describes the status of the battery meter.

Token 12029:
The persistence of failure must be modeled by an

Token 12030:
594 Chapter 15.

Token 12031:
Probabilistic Reasoning over Time -1012345 15 20 25 30E(Batteryt) Time step tE(Batteryt |...5555005555...) E(Batteryt |...5555000000...) -1012345 15 20 25 30E(Batteryt) Time stepE(Batteryt |...5555005555...) E(Batteryt |...5555000000...) (a) (b) Figure 15.14 (a) Upper curve: trajectory of the expected value of Battery tfor an observa- tion sequence consisting of all 5s except for 0s at t=21 andt=22 , using a simple Gaussian error model.

Token 12032:
Lower curve: trajectory when the observation remains at 0 from t=21 onwards. (b) The same experiment run with the transient failure model.

Token 12033:
Notice that the transient fail- ure is handled well, but the persistent failure results in excessive pessimism about the battery charge.

Token 12034:
1Battery Battery01BMeter0 BMBroken 1 BMBrokenft0B 1P(B ) 1.000 0.001 -1012345 15 20 25 30E(Batteryt) Time stepE(Batteryt |...5555005555...) E(Batteryt |...5555000000...) P(BMBrokent |...5555000000...) P(BMBrokent |...5555005555...) (a) (b) Figure 15.15 (a) A DBN fragment showing the sensor status variable required for mod- eling persistent failure of the battery sensor.

Token 12035:
(b) Upper curves: trajectories of the expected value of Battery tfor the “transient failure” and “permanent failure” observations sequences.

Token 12036:
Lower curves: probability trajectories for BMBroken given the two observation sequences.

Token 12037:
arc linking BMBroken 0toBMBroken 1.T h i s persistence arc has a CPT that gives a small PERSISTENCE ARC probability of failure in any given time step, say, 0.001, but speciﬁes that the sensor stays broken once it breaks.

Token 12038:
When the sensor is OK, the sensor model for BMeter is identical to the transient failure model; when the sensor is broken, it says BMeter is always 0, regardless of the actual battery charge.

Token 12039:
Section 15.5.

Token 12040:
Dynamic Bayesian Networks 595 0.3f0.7tP(R )1R0 0.7P(R0) 0.2f0.9tP(U )1R1Umbrella 1Rain0Rain10.7P(R0) 4 0.2f0.9tP(U ) R4ft 0.30.7P(R )4R3 Umbrella 4Rain4 0.2f0.9tP(U )3R3ftR 0.30.7P(R )3 2 Umbrella 3Rain3 0.2f0.9tP(U )2R2ftR 0.30.7P(R )2 1 Umbrella 2Rain2 0.2f0.9tP(U )1R1ftR 0.30.7P(R )1 0 Umbrella 1Rain0Rain1 Figure 15.16 Unrolling a dynamic Bayesian network : slices are replicated to accommo- date the observation sequence Umbrella 1:3.

Token 12041:
Further slices have no effect on inferences within the observation period.

Token 12042:
The persistent failure model for the battery sensor is shown in Figure 15.15(a).

Token 12043:
Its performance on the two data sequences (temporary blip and persistent failure) is shown inFigure 15.15(b).

Token 12044:
There are several things to notice about these curves.

Token 12045:
First, in the caseof the temporary blip, the probability that the sensor is broken rises signiﬁcantly after thesecond 0 reading, but immediately drops back to zero once a 5 is observed.

Token 12046:
Second, in thecase of persistent failure, the probability that the sensor is broken rises quickly to almost 1and stays there.

Token 12047:
Finally, once the sensor is known to be broken, the robot can only assumethat its battery discharges at the “normal” rate, as shown by the gradually descending level ofE(Battery t|...).

Token 12048:
So far, we have merely scratched the surface of the problem of representing complex processes.

Token 12049:
The variety of transition models is huge, encompassing topics as disparate as modeling the human endocrine system and modeling multiple vehicles driving on a freeway.Sensor modeling is also a vast subﬁeld in itself, but even subtle phenomena, such as sensordrift, sudden decalibration, and the effects of exogenous conditions (such as weather) onsensor readings, can be handled by explicit representation within dynamic Bayesian networks.

Token 12050:
15.5.2 Exact inference in DBNs Having sketched some ideas for representing complex processes as DBNs, we now turn tothe question of inference.

Token 12051:
In a sense, this question has already been answered: dynamicBayesian networks areBayesian networks, and we already have algorithms for inference in Bayesian networks.

Token 12052:
Given a sequence of observations, one can construct the full Bayesiannetwork representation of a DBN by replicating slices until the network is large enough toaccommodate the observations, as in Figure 15.16.

Token 12053:
This technique, mentioned in Chapter 14in the context of relational probability models, is called unrolling .

Token 12054:
(Technically, the DBN is equivalent to the semi-inﬁnite network obtained by unrolling forever.

Token 12055:
Slices added beyondthe last observation have no effect on inferences within the observation period and can be omitted.)

Token 12056:
Once the DBN is unrolled, one can use any of the inference algorithms—variable elimination, clustering methods, and so on—described in Chapter 14.

Token 12057:
Unfortunately, a naive application of unrolling would not be particularly efﬁcient.

Token 12058:
If we want to perform ﬁltering or smoothing with a long sequence of observations e 1:t,t h e

Token 12059:
596 Chapter 15.

Token 12060:
Probabilistic Reasoning over Time unrolled network would require O(t)space and would thus grow without bound as more observations were added.

Token 12061:
Moreover, if we simply run the inference algorithm anew eachtime an observation is added, the inference time per update will also increase as O(t).

Token 12062:
Looking back to Section 15.2.1, we see that constant time and space per ﬁltering update can be achieved if the computation can be done recursively.

Token 12063:
Essentially, the ﬁltering update in Equation (15.5) works by summing out the state variables of the previous time step to get the distribution for the new time step.

Token 12064:
Summing out variables is exactly what the variable elimination (Figure 14.11) algorithm does, and it turns out that running variable elimination with the variables in temporal order exactly mimics the operation of the recursive ﬁlteringupdate in Equation (15.5).

Token 12065:
The modiﬁed algorithm keeps at most two slices in memory atany one time: starting with slice 0, we add slice 1, then sum out slice 0, then add slice 2, thensum out slice 1, and so on.

Token 12066:
In this way, we can achieve constant space and time per ﬁlteringupdate.

Token 12067:
(The same performance can be achieved by suitable modiﬁcations to the clusteringalgorithm.)

Token 12068:
Exercise 15.17 asks you to verify this fact for the umbrella network.

Token 12069:
So much for the good news; now for the bad news: It turns out that the “constant” for the per-update time and space complexity is, in almost all cases, exponential in the number ofstate variables.

Token 12070:
What happens is that, as the variable elimination proceeds, the factors grow to include all the state variables (or, more precisely, all those state variables that have parents in the previous time slice).

Token 12071:
The maximum factor size is O(d n+k)and the total update cost per step is O(ndn+k),w h e r e dis the domain size of the variables and kis the maximum number of parents of any state variable.

Token 12072:
Of course, this is much less than the cost of HMM updating, which is O(d2n),b u ti t is still infeasible for large numbers of variables.

Token 12073:
This grim fact is somewhat hard to accept.

Token 12074:
What it means is that even though we can use DBNs to represent very complex temporal processes with many sparsely connected variables, we cannot reason efﬁciently and exactly about those processes.

Token 12075:
The DBN model itself, which represents the prior joint distribution over all the variables, is factorable into its constituent CPTs, but the posterior joint distribu-tion conditioned on an observation sequence—that is, the forward message—is generally not factorable.

Token 12076:
So far, no one has found a way around this problem, despite the fact that many important areas of science and engineering would beneﬁt enormously from its solution.

Token 12077:
Thus,we must fall back on approximate methods.

Token 12078:
15.5.3 Approximate inference in DBNs Section 14.5 described two approximation algorithms: likelihood weighting (Figure 14.15)and Markov chain Monte Carlo (MCMC, Figure 14.16).

Token 12079:
Of the two, the former is most easilyadapted to the DBN context.

Token 12080:
(An MCMC ﬁltering algorithm is described brieﬂy in the notesat the end of the chapter.)

Token 12081:
We will see, however, that several improvements are required overthe standard likelihood weighting algorithm before a practical method emerges.

Token 12082:
Recall that likelihood weighting works by sampling the nonevidence nodes of the net- work in topological order, weighting each sample by the likelihood it accords to the observedevidence variables.

Token 12083:
As with the exact algorithms, we could apply likelihood weighting di-rectly to an unrolled DBN, but this would suffer from the same problems of increasing time

Token 12084:
Section 15.5. Dynamic Bayesian Networks 597 and space requirements per update as the observation sequence grows.

Token 12085:
The problem is that the standard algorithm runs each sample in turn, all the way through the network.

Token 12086:
Instead,we can simply run all Nsamples together through the DBN, one slice at a time.

Token 12087:
The mod- iﬁed algorithm ﬁts the general pattern of ﬁltering algorithms, with the set of Nsamples as the forward message.

Token 12088:
The ﬁrst key innovation, then, is to use the samples themselves as an approximate representation of the current state distribution.

Token 12089:
This meets the requirement of a “constant” time per update, although the constant depends on the number of samples requiredto maintain an accurate approximation.

Token 12090:
There is also no need to unroll the DBN, because weneed to have in memory only the current slice and the next slice.

Token 12091:
In our discussion of likelihood weighting in Chapter 14, we pointed out that the al- gorithm’s accuracy suffers if the evidence variables are “downstream” from the variablesbeing sampled, because in that case the samples are generated without any inﬂuence fromthe evidence.

Token 12092:
Looking at the typical structure of a DBN—say, the umbrella DBN in Fig-ure 15.16—we see that indeed the early state variables will be sampled without the beneﬁt ofthe later evidence.

Token 12093:
In fact, looking more carefully, we see that none of the state variables has anyevidence variables among its ancestors!

Token 12094:
Hence, although the weight of each sample will depend on the evidence, the actual set of samples generated will be completely independent of the evidence.

Token 12095:
For example, even if the boss brings in the umbrella every day, the sam- pling process could still hallucinate endless days of sunshine.

Token 12096:
What this means in practice isthat the fraction of samples that remain reasonably close to the actual series of events (andtherefore have nonnegligible weights) drops exponentially with t, the length of the observa- tion sequence.

Token 12097:
In other words, to maintain a given level of accuracy, we need to increase thenumber of samples exponentially with t. Given that a ﬁltering algorithm that works in real time can use only a ﬁxed number of samples, what happens in practice is that the error blowsup after a very small number of update steps.

Token 12098:
Clearly, we need a better solution. The second key innovation is to focus the set of samples on the high-probability regions of the state space.

Token 12099:
This can be done by throwing away samples that have very low weight, according to the observations, while replicating those that have high weight.

Token 12100:
In that way, the population of samples will stay reasonably close to reality.

Token 12101:
If we think of samples as a resource for modeling the posterior distribution, then itmakes sense to use more samples in regions of the state space where the posterior is higher.

Token 12102:
A family of algorithms called particle ﬁltering is designed to do just that.

Token 12103:
Particle PARTICLE FILTERING ﬁltering works as follows: First, a population of Ninitial-state samples is created by sampling from the prior distribution P(X0).

Token 12104:
Then the update cycle is repeated for each time step: 1.

Token 12105:
Each sample is propagated forward by sampling the next state value xt+1given the current value xtfor the sample, based on the transition model P(Xt+1|xt).

Token 12106:
2. Each sample is weighted by the likelihood it assigns to the new evidence, P(et+1|xt+1). 3.

Token 12107:
The population is resampled to generate a new population of Nsamples.

Token 12108:
Each new sample is selected from the current population; the probability that a particular sample is selected is proportional to its weight.

Token 12109:
The new samples are unweighted.

Token 12110:
The algorithm is shown in detail in Figure 15.17, and its operation for the umbrella DBN is illustrated in Figure 15.18.

Token 12111:
598 Chapter 15.

Token 12112:
Probabilistic Reasoning over Time function PARTICLE -FILTERING (e,N,dbn)returns a set of samples for the next time step inputs :e, the new incoming evidence N, the number of samples to be maintained dbn, a DBN with prior P(X0), transition model P(X1|X0), sensor model P(E1|X1) persistent :S, a vector of samples of size N, initially generated from P(X0) local variables :W, a vector of weights of size N fori=1t oNdo S[i]←sample from P(X1|X0=S[i])/* step 1 */ W[i]←P(e|X1=S[i]) /* step 2 */ S←WEIGHTED -SAMPLE -WITH-REPLACEMENT (N,S,W) / *s t e p3* / return S Figure 15.17 The particle ﬁltering algorithm implemented as a recursive update op- eration with state (the set of samples).

Token 12113:
Each of the sampling operations involves sam-pling the relevant slice variables in topological order, much as in P RIOR -SAMPLE .T h e WEIGHTED -SAMPLE -WITH-REPLACEMENT operation can be implemented to run in O(N) expected time.

Token 12114:
The step numbers refer to the description in the text.

Token 12115:
true false (a) Propagate (c) ResampleRaint Raint+1 Raint+1 Raint+1 (b) Weight Figure 15.18 The particle ﬁltering update cycle for the umbrella DBN with N=1 0 ,s h o w - ing the sample populations of each state.

Token 12116:
(a) At time t, 8 samples indicate rain and 2 indicate ¬rain. Each is propagated forward by sampling the next state through the transition model.

Token 12117:
At time t+1, 6 samples indicate rain and 4 indicate ¬rain. ( b )¬umbrella is observed at t+1.

Token 12118:
Each sample is weighted by its likelihood for the observation, as indicated by the size of the circles.

Token 12119:
(c) A new set of 10 samples is generated by weighted random selection from the current set, resulting in 2 samples that indicate rain and 8 that indicate ¬rain.

Token 12120:
We can show that this algorithm is consistent—gives the correct probabilities as Ntends to inﬁnity—by considering what happens during one update cycle.

Token 12121:
We assume that the samplepopulation starts with a correct representation of the forward message f 1:t=P(Xt|e1:t)at timet.

Token 12122:
Writing N(xt|e1:t)for the number of samples occupying state xtafter observations e1:thave been processed, we therefore have N(xt|e1:t)/N=P(xt|e1:t) (15.23) for large N. Now we propagate each sample forward by sampling the state variables at t+1, given the values for the sample at t. The number of samples reaching state xt+1from each

Token 12123:
Section 15.6.

Token 12124:
Keeping Track of Many Objects 599 xtis the transition probability times the population of xt; hence, the total number of samples reaching xt+1is N(xt+1|e1:t)=/summationdisplay xtP(xt+1|xt)N(xt|e1:t).

Token 12125:
Now we weight each sample by its likelihood for the evidence at t+1. A sample in state xt+1 receives weight P(et+1|xt+1).

Token 12126:
The total weight of the samples in xt+1after seeing et+1is therefore W(xt+1|e1:t+1)=P(et+1|xt+1)N(xt+1|e1:t). Now for the resampling step.

Token 12127:
Since each sample is replicated with probability proportional to its weight, the number of samples in state xt+1after resampling is proportional to the total weight in xt+1before resampling: N(xt+1|e1:t+1)/N=αW(xt+1|e1:t+1) =αP(et+1|xt+1)N(xt+1|e1:t) =αP(et+1|xt+1)/summationdisplay xtP(xt+1|xt)N(xt|e1:t) =αNP (et+1|xt+1)/summationdisplay xtP(xt+1|xt)P(xt|e1:t)(by 15.23) =α/primeP(et+1|xt+1)/summationdisplay xtP(xt+1|xt)P(xt|e1:t) =P(xt+1|e1:t+1)(by 15.5).

Token 12128:
Therefore the sample population after one update cycle correctly represents the forward mes- sage at time t+1.

Token 12129:
Particle ﬁltering is consistent , therefore, but is it efﬁcient ?

Token 12130:
In practice, it seems that the answer is yes: particle ﬁltering seems to maintain a good approximation to the true posterior using a constant number of samples.

Token 12131:
Under certain assumptions—in particular, that the prob- abilities in the transition and sensor models are strictly greater than 0 and less than 1—it is possible to prove that the approximation maintains bounded error with high probability.

Token 12132:
On the practical side, the range of applications has grown to include many ﬁelds of science and engineering; some references are given at the end of the chapter.

Token 12133:
15.6 K EEPING TRACK OF MANY OBJECTS The preceding sections have considered—without mentioning it—state estimation problemsinvolving a single object.

Token 12134:
In this section, we see what happens when two or more objectsgenerate the observations.

Token 12135:
What makes this case different from plain old state estimation is that there is now the possibility of uncertainty about which object generated which observa- tion.

Token 12136:
This is the identity uncertainty problem of Section 14.6.3 (page 544), now viewed in a temporal context.

Token 12137:
In the control theory literature, this is the data association problem—that DATA ASSOCIATION is, the problem of associating observation data with the objects that generated them.

Token 12138:
600 Chapter 15.

Token 12139:
Probabilistic Reasoning over Time 2 1 3 5421 35 4 2 1 3 5421 35 4 2 1 3 5421 35 42 1 3 5421 35 4 (d) (c)(b) (a) track termination false alarmdetection failure track initiation Figure 15.19 (a) Observations made of object locations in 2D space over ﬁve time steps.

Token 12140:
Each observation is labeled with the time step but does not identify the object that producedit.

Token 12141:
(b–c) Possible hypotheses about the underlying object tracks.

Token 12142:
(d) A hypothesis for the case in which false alarms, detection failures, and track initiation/termination are possible.

Token 12143:
The data association problem was studied originally in the context of radar tracking, where reﬂected pulses are detected at ﬁxed time intervals by a rotating radar antenna.

Token 12144:
At each time step, multiple blips may appear on the screen, but there is no direct observation of which blips at time tbelong to which blips at time t−1.

Token 12145:
Figure 15.19(a) shows a simple example with two blips per time step for ﬁve steps.

Token 12146:
Let the two blip locations at time tbee1 tande2 t. (The labeling of blips within a time step as “1” and “2” is completely arbitrary and carries noinformation.)

Token 12147:
Let us assume, for the time being, that exactly two aircraft, AandB, generated the blips; their true positions are X A tandXB t. Just to keep things simple, we’ll also assume that the each aircraft moves independently according to a known transition model—e.g., alinear Gaussian model as used in the Kalman ﬁlter (Section 15.4).

Token 12148:
Suppose we try to write down the overall probability model for this scenario, just as we did for general temporal processes in Equation (15.3) on page 569.

Token 12149:
As usual, the jointdistribution factors into contributions for each time step as follows: P(x A 0:t,xB 0:t,e1 1:t,e2 1:t)= P(xA 0)P(xB 0)t/productdisplay i=1P(xA i|xA i−1)P(xB i|xB i−1)P(e1 i,e2i|xA i,xBi).

Token 12150:
(15.24) We would like to factor the observation term P(e1 i,e2i|xA i,xBi)into a product of two terms, one for each object, but this would require knowing which observation was generated bywhich object.

Token 12151:
Instead, we have to sum over all possible ways of associating the observations

Token 12152:
Section 15.6. Keeping Track of Many Objects 601 with the objects.

Token 12153:
Some of those ways are shown in Figure 15.19(b–c); in general, for n objects and Ttime steps, there are (n!

Token 12154:
)Tways of doing it—an awfully large number.

Token 12155:
Mathematically speaking, the “way of associating the observations with the objects” is a collection of unobserved random variable that identify the source of each observation.We’ll write ω tto denote the one-to-one mapping from objects to observations at time t, with ωt(A)andωt(B)denoting the speciﬁc observations (1 or 2) that ωtassigns to AandB.

Token 12156:
(Fornobjects, ωtwill have n!possible values; here, n!=2 .)

Token 12157:
Because the labels “1” ad “2” on the observations are assigned arbitrarily, the prior on ωtis uniform and ωtis inde- pendent of the states of the objects, xA tandxB t).

Token 12158:
So we can condition the observation term P(e1 i,e2i|xA i,xBi)onωtand then simplify: P(e1 i,e2i|xA i,xBi)=/summationdisplay ωiP(e1 i,e2i|xA i,xBi,ωi)P(ωi|xA i,xBi) =/summationdisplay ωiP(eωi(A) i|xA i)P(eωi(B) i|xB i)P(ωi|xA i,xBi) =1 2/summationdisplay ωiP(eωi(A) i|xA i)P(eωi(B) i|xB i).

Token 12159:
Plugging this into Equation (15.24), we get an expression that is only in terms of transition and sensor models for individual objects and observations.

Token 12160:
As for all probability models, inference means summing out the variables other than the query and the evidence.

Token 12161:
For ﬁltering in HMMs and DBNs, we were able to sum out thestate variables from 1 to t−1by a simple dynamic programming trick; for Kalman ﬁlters, we took advantage of special properties of Gaussians.

Token 12162:
For data association, we are less fortunate.

Token 12163:
There is no (known) efﬁcient exact algorithm, for the same reason that there is none for theswitching Kalman ﬁlter (page 589): the ﬁltering distribution P(x A t|e1 1:t,e2 1:t)for object A ends up as a mixture of exponentially many distributions, one for each way of picking a sequence of observations to assign to A.

Token 12164:
As a result of the complexity of exact inference, many different approximate methods have been used.

Token 12165:
The simplest approach is to choose a single “best” assignment at each time step, given the predicted positions of the objects at the current time step.

Token 12166:
This assignmentassociates observations with objects and enables the track of each object to be updated anda prediction made for the next time step.

Token 12167:
For choosing the “best” assignment, it is commonto use the so-called nearest-neighbor ﬁlter , which repeatedly chooses the closest pairing NEAREST-NEIGHBOR FILTER of predicted position and observation and adds that pairing to the assignment.

Token 12168:
The nearest- neighbor ﬁlter works well when the objects are well separated in state space and the predictionuncertainty and observation error are small—in other words, when there is no possibility ofconfusion.

Token 12169:
When there is more uncertainty as to the correct assignment, a better approachis to choose the assignment that maximizes the joint probability of the current observationsgiven the predicted positions.

Token 12170:
This can be done very efﬁciently using the Hungarian algo- rithm (Kuhn, 1955), even though there are n!assignments to choose from.

Token 12171:
HUNGARIAN ALGORITHM Any method that commits to a single best assignment at each time step fails miserably under more difﬁcult conditions.

Token 12172:
In particular, if the algorithm commits to an incorrect as-signment, the prediction at the next time step may be signiﬁcantly wrong, leading to more

Token 12173:
602 Chapter 15.

Token 12174:
Probabilistic Reasoning over Time (a) (b) Figure 15.20 Images from (a) upstream and (b) downstream surveillance cameras roughly two miles apart on Highway 99 in Sacramento, California.

Token 12175:
The boxed vehicle has been identiﬁed at both cameras. incorrect assignments, and so on. Two modern approaches turn out to be much more effec- tive.

Token 12176:
A particle ﬁltering algorithm (see page 598) for data association works by maintaining a large collection of possible current assignments.

Token 12177:
An MCMC algorithm explores the space of assignment histories—for example, Figure 15.19(b–c) might be states in the MCMC statespace—and can change its mind about previous assignment decisions.

Token 12178:
Current MCMC dataassociation methods can handle many hundreds of objects in real time while giving a goodapproximation to the true posterior distributions.

Token 12179:
The scenario described so far involved nknown objects generating nobservations at each time step.

Token 12180:
Real application of data association are typically much more complicated.

Token 12181:
Often, the reported observations include false alarms (also known as clutter ), which are not FALSE ALARM CLUTTER caused by real objects.

Token 12182:
Detection failures can occur, meaning that no observation is reported DETECTION FAILURE for a real object.

Token 12183:
Finally, new objects arrive and old ones disappear.

Token 12184:
These phenomena, which create even more possible worlds to worry about, are illustrated in Figure 15.19(d).

Token 12185:
Figure 15.20 shows two images from widely separated cameras on a California freeway.

Token 12186:
In this application, we are interested in two goals: estimating the time it takes, under currenttrafﬁc conditions, to go from one place to another in the freeway system; and measuringdemand , i.e., how many vehicles travel between any two points in the system at particular times of the day and on particular days of the week.

Token 12187:
Both goals require solving the dataassociation problem over a wide area with many cameras and tens of thousands of vehiclesper hour.

Token 12188:
With visual surveillance, false alarms are caused by moving shadows, articulatedvehicles, reﬂections in puddles, etc.

Token 12189:
; detection failures are caused by occlusion, fog, darkness,and lack of visual contrast; and vehicles are constantly entering and leaving the freeway system.

Token 12190:
Furthermore, the appearance of any given vehicle can change dramatically between cameras depending on lighting conditions and vehicle pose in the image, and the transitionmodel changes as trafﬁc jams come and go.

Token 12191:
Despite these problems, modern data associationalgorithms have been successful in estimating trafﬁc parameters in real-world settings.

Token 12192:
Section 15.7.

Token 12193:
Summary 603 Data association is an essential foundation for keeping track of a complex world, be- cause without it there is no way to combine multiple observations of any given object.

Token 12194:
Whenobjects in the world interact with each other in complex activities, understanding the worldrequires combining data association with the relational and open-universe probability modelsof Section 14.6.3.

Token 12195:
This is currently an active area of research.

Token 12196:
15.7 S UMMARY This chapter has addressed the general problem of representing and reasoning about proba-bilistic temporal processes.

Token 12197:
The main points are as follows: •The changing state of the world is handled by using a set of random variables to repre- sent the state at each point in time.

Token 12198:
•Representations can be designed to satisfy the Markov property , so that the future is independent of the past given the present.

Token 12199:
Combined with the assumption that theprocess is stationary —that is, the dynamics do not change over time—this greatly simpliﬁes the representation.

Token 12200:
•A temporal probability model can be thought of as containing a transition model de- scribing the state evolution and a sensor model describing the observation process.

Token 12201:
•The principal inference tasks in temporal models are ﬁltering ,prediction ,smooth- ing, and computing the most likely explanation .

Token 12202:
Each of these can be achieved using simple, recursive algorithms whose run time is linear in the length of the sequence.

Token 12203:
•Three families of temporal models were studied in more depth: hidden Markov mod- els,Kalman ﬁlters ,a n d dynamic Bayesian networks (which include the other two as special cases).

Token 12204:
•Unless special assumptions are made, as in Kalman ﬁlters, exact inference with many state variables is intractable.

Token 12205:
In practice, the particle ﬁltering algorithm seems to be an effective approximation algorithm.

Token 12206:
•When trying to keep track of many objects, uncertainty arises as to which observations belong to which objects—the data association problem.

Token 12207:
The number of association hypotheses is typically intractably large, but MCMC and particle ﬁltering algorithms for data association work well in practice.

Token 12208:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Many of the basic ideas for estimating the state of dynamical systems came from the mathe-matician C. F. Gauss (1809), who formulated a deterministic least-squares algorithm for theproblem of estimating orbits from astronomical observations.

Token 12209:
A. A. Markov (1913) devel-oped what was later called the Markov assumption in his analysis of stochastic processes;

Token 12210:
604 Chapter 15. Probabilistic Reasoning over Time he estimated a ﬁrst-order Markov chain on letters from the text of Eugene Onegin .

Token 12211:
The gen- eral theory of Markov chains and their mixing times is covered by Levin et al. (2008).

Token 12212:
Signiﬁcant classiﬁed work on ﬁltering was done during World War II by Wiener (1942) for continuous-time processes and by Kolmogorov (1941) for discrete-time processes.

Token 12213:
Al-though this work led to important technological developments over the next 20 years, its use of a frequency-domain representation made many calculations quite cumbersome.

Token 12214:
Di- rect state-space modeling of the stochastic process turned out to be simpler, as shown byPeter Swerling (1959) and Rudolf Kalman (1960).

Token 12215:
The latter paper described what is nowknown as the Kalman ﬁlter for forward inference in linear systems with Gaussian noise;Kalman’s results had, however, been obtained previously by the Danish statistician ThorvoldThiele (1880) and by the Russian mathematician Ruslan Stratonovich (1959), whom Kalmanmet in Moscow in 1960.

Token 12216:
After a visit to NASA Ames Research Center in 1960, Kalmansaw the applicability of the method to the tracking of rocket trajectories, and the ﬁlter waslater implemented for the Apollo missions.

Token 12217:
Important results on smoothing were derived byRauch et al.

Token 12218:
(1965), and the impressively named Rauch–Tung–Striebel smoother is still a standard technique today. Many early results are gathered in Gelb (1974).

Token 12219:
Bar-Shalom andFortmann (1988) give a more modern treatment with a Bayesian ﬂavor, as well as many ref- erences to the vast literature on the subject.

Token 12220:
Chatﬁeld (1989) and Box et al. (1994) cover the control theory approach to time series analysis.

Token 12221:
The hidden Markov model and associated algorithms for inference and learning, in- cluding the forward–backward algorithm, were developed by Baum and Petrie (1966).

Token 12222:
The Viterbi algorithm ﬁrst appeared in (Viterbi, 1967). Similar ideas also appeared independently in the Kalman ﬁltering community (Rauch et al.

Token 12223:
, 1965). The forward–backward algorithm was one of the main precursors of the general formulation of the EM algorithm (Dempsteret al.

Token 12224:
, 1977); see also Chapter 20. Constant-space smoothing appears in Binder et al.

Token 12225:
(1997b), as does the divide-and-conquer algorithm developed in Exercise 15.3.

Token 12226:
Constant-time ﬁxed-lag smoothing for HMMs ﬁrst appeared in Russell and Norvig (2003).

Token 12227:
HMMs have foundmany applications in language processing (Charniak, 1993), speech recognition (Rabiner and Juang, 1993), machine translation (Och and Ney, 2003), computational biology (Krogh et al.

Token 12228:
, 1994; Baldi et al. , 1994), ﬁnancial economics Bhar and Hamori (2004) and other ﬁelds.

Token 12229:
There have been several extensions to the basic HMM model, for example the Hierarchical HMM(Fine et al. , 1998) and Layered HMM (Oliver et al.

Token 12230:
, 2004) introduce structure back into the model, replacing the single state variable of HMMs.

Token 12231:
Dynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a Markov process and were ﬁrst used in AI by Dean and Kanazawa (1989b), Nicholson and Brady(1992), and Kjaerulff (1992).

Token 12232:
The last work extends the H UGIN Bayes net system to ac- commodate dynamic Bayesian networks.

Token 12233:
The book by Dean and Wellman (1991) helpedpopularize DBNs and the probabilistic approach to planning and control within AI.

Token 12234:
Murphy(2002) provides a thorough analysis of DBNs.

Token 12235:
Dynamic Bayesian networks have become popular for modeling a variety of com- plex motion processes in computer vision (Huang et al.

Token 12236:
, 1994; Intille and Bobick, 1999). Like HMMs, they have found applications in speech recognition (Zweig and Russell, 1998;Richardson et al.

Token 12237:
, 2000; Stephenson et al. , 2000; Neﬁan et al. , 2002; Livescu et al. , 2003), ge-

Token 12238:
Bibliographical and Historical Notes 605 nomics (Murphy and Mian, 1999; Perrin et al.

Token 12239:
, 2003; Husmeier, 2003) and robot localization (Theocharous et al. , 2004).

Token 12240:
The link between HMMs and DBNs, and between the forward– backward algorithm and Bayesian network propagation, was made explicitly by Smyth et al.(1997).

Token 12241:
A further uniﬁcation with Kalman ﬁlters (and other statistical models) appears in Roweis and Ghahramani (1999).

Token 12242:
Procedures exist for learning the parameters (Binder et al. , 1997a; Ghahramani, 1998) and structures (Friedman et al. , 1998) of DBNs.

Token 12243:
The particle ﬁltering algorithm described in Section 15.5 has a particularly interesting history.

Token 12244:
The ﬁrst sampling algorithms for particle ﬁltering (also called sequential Monte Carlomethods) were developed in the control theory community by Handschin and Mayne (1969),and the resampling idea that is the core of particle ﬁltering appeared in a Russian controljournal (Zaritskii et al.

Token 12245:
, 1975).

Token 12246:
It was later reinvented in statistics as sequential importance- sampling resampling ,o rSIR (Rubin, 1988; Liu and Chen, 1998), in control theory as parti- cle ﬁltering (Gordon et al.

Token 12247:
, 1993; Gordon, 1994), in AI as survival of the ﬁttest (Kanazawa et al. , 1995), and in computer vision as condensation (Isard and Blake, 1996).

Token 12248:
The paper by Kanazawa et al.

Token 12249:
(1995) includes an improvement called evidence reversal whereby the state EVIDENCE REVERSAL at time t+1is sampled conditional on both the state at time tand the evidence at time t+1.

Token 12250:
This allows the evidence to inﬂuence sample generation directly and was proved by Doucet (1997) and Liu and Chen (1998) to reduce the approximation error.

Token 12251:
Particle ﬁltering has been applied in many areas, including tracking complex motion patterns in video (Isard and Blake,1996), predicting the stock market (de Freitas et al.

Token 12252:
, 2000), and diagnosing faults on plane- tary rovers (Verma et al. , 2004).

Token 12253:
A variant called the Rao-Blackwellized particle ﬁlter or RAO- BLACKWELLIZED PARTICLE FILTERRBPF (Doucet et al.

Token 12254:
, 2000; Murphy and Russell, 2001) applies particle ﬁltering to a subset of state variables and, for each particle, performs exact inference on the remaining variablesconditioned on the value sequence in the particle.

Token 12255:
In some cases RBPF works well with t hou- sands of state variables.

Token 12256:
An application of RBPF to localization and mapping in r obotics is described in Chapter 25. The book by Doucet et al.

Token 12257:
(2001) collects many important papers on sequential Monte Carlo (SMC) algorithms, of which particle ﬁltering is the most important SEQUENTIAL MONTE CARLO instance.

Token 12258:
Pierre Del Moral and colleagues have performed extensive theoretical analyses of SMC algorithms (Del Moral, 2004; Del Moral et al. , 2006).

Token 12259:
MCMC methods (see Section 14.5.2) can be applied to the ﬁltering problem; for ex- ample, Gibbs sampling can be applied directly to an unrolled DBN.

Token 12260:
To avoid the problem ofincreasing update times as the unrolled network grows, the decayed MCMC ﬁlter (Marthi DECAYED MCMC et al.

Token 12261:
, 2002) prefers to sample more recent state variables, with a probability that decays as 1/k2for a variable ksteps into the past.

Token 12262:
Decayed MCMC is a provably nondivergent ﬁlter.

Token 12263:
Nondivergence theorems can also be obtained for certain types of assumed-density ﬁlter .ASSUMED-DENSITY FILTER An assumed-density ﬁlter assumes that the posterior distribution over states at time tbelongs to a particular ﬁnitely parameterized family; if the projection and update steps take it outsidethis family, the distribution is projected back to give the best approximation within the fam-ily.

Token 12264:
For DBNs, the Boyen–Koller algorithm (Boyen et al.

Token 12265:
, 1999) and the factored frontier FACTORED FRONTIER algorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approxi- mated well by a product of small factors.

Token 12266:
Variational techniques (see Chapter 14) have alsobeen developed for temporal models.

Token 12267:
Ghahramani and Jordan (1997) discuss an approxima-tion algorithm for the factorial HMM , a DBN in which two or more independently evolving FACTORIAL HMM

Token 12268:
606 Chapter 15. Probabilistic Reasoning over Time Markov chains are linked by a shared observation stream. Jordan et al.

Token 12269:
(1998) cover a number of other applications.

Token 12270:
Data association for multitarget tracking was ﬁrst described in a probabilistic setting by Sittler (1964).

Token 12271:
The ﬁrst practical algorithm for large-scale problems was the “multiplehypothesis tracker” or MHT algorithm (Reid, 1979).

Token 12272:
Many important papers are collected by Bar-Shalom and Fortmann (1988) and Bar-Shalom (1992).

Token 12273:
The development of an MCMC algorithm for data association is due to Pasula et al. (1999), who applied it to trafﬁc surveil- lance problems. Oh et al.

Token 12274:
(2009) provide a formal analysis and extensive experimental com- parisons to other methods. Schulz et al.

Token 12275:
(2003) describe a data association method based on particle ﬁltering.

Token 12276:
Ingemar Cox analyzed the complexity of data association (Cox, 1993; Coxand Hingorani, 1994) and brought the topic to the attention of the vision community.

Token 12277:
He alsonoted the applicability of the polynomial-time Hungarian algorithm to the problem of ﬁnd-ing most-likely assignments, which had long been considered an intractable problem in thetracking community.

Token 12278:
The algorithm itself was published by Kuhn (1955), based on transla-tions of papers published in 1931 by two Hungarian mathematicians, D´ enes K¨ onig and Jen¨ o Egerv´ ary.

Token 12279:
The basic theorem had been derived previously, however, in an unpublished Latin manuscript by the famous Prussian mathematician Carl Gustav Jacobi (1804–1851).

Token 12280:
EXERCISES 15.1 Show that any second-order Markov process can be rewritten as a ﬁrst-order Markov process with an augmented set of state variables.

Token 12281:
Can this always be done parsimoniously , i.e., without increasing the number of parameters needed to specify the transition model?

Token 12282:
15.2 In this exercise, we examine what happens to the probabilities in the umbrella world in the limit of long time sequences. a.

Token 12283:
Suppose we observe an unending sequence of days on which the umbrella appears.

Token 12284:
Show that, as the days go by, the probability of rain on the current day increases mono-tonically toward a ﬁxed point. Calculate this ﬁxed point. b.

Token 12285:
Now consider forecasting further and further into the future, given just the ﬁrst two umbrella observations.

Token 12286:
First, compute the probability P(r 2+k|u1,u2)fork=1...20 and plot the results. You should see that the probability converges towards a ﬁxed point.

Token 12287:
Prove that the exact value of this ﬁxed point is 0.5.

Token 12288:
15.3 This exercise develops a space-efﬁcient variant of the forward–backward algorithm described in Figure 15.4 (page 576).

Token 12289:
We wish to compute P(Xk|e1:t)fork=1,...,t .T h i s will be done with a divide-and-conquer approach. a.

Token 12290:
Suppose, for simplicity, that tis odd, and let the halfway point be h=(t+1 )/2.S h o w thatP(Xk|e1:t)can be computed for k=1,...,h given just the initial forward message f1:0, the backward message bh+1:t, and the evidence e1:h. b.

Token 12291:
Show a similar result for the second half of the sequence.

Token 12292:


Token 12293:
Exercises 607 c. Given the results of (a) and (b), a recursive divide-and-conquer algorithm can be con- structed by ﬁrst running forward along the sequence and then backward from the end,storing just the required messages at the middle and the ends.

Token 12294:
Then the algorithm iscalled on each half. Write out the algorithm in detail.

Token 12295:
d. Compute the time and space complexity of the algorithm as a function of t, the length of the sequence.

Token 12296:
How does this change if we divide the input into more than two pieces?

Token 12297:
15.4 On page 577, we outlined a ﬂawed procedure for ﬁnding the most likely state sequence, given an observation sequence.

Token 12298:
The procedure involves ﬁnding the most likely state at eachtime step, using smoothing, and returning the sequence composed of these states.

Token 12299:
Show that,for some temporal probability models and observation sequences, this procedure returns animpossible state sequence (i.e., the posterior probability of the sequence is zero).

Token 12300:
15.5 Equation (15.12) describes the ﬁltering process for the matrix formulation of HMMs.

Token 12301:
Give a similar equation for the calculation of likelihoods, which was described generically inEquation (15.7).

Token 12302:
15.6 Consider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy sensing).

Token 12303:
Suppose that the robot receives an observation sequence such that, with perfect sensing, there is exactly one possible location it could be in.

Token 12304:
Is this location necessarily the most probable location under noisy sensing for sufﬁciently small noise probability /epsilon1?

Token 12305:
Prove your claim or ﬁnd a counterexample.

Token 12306:
15.7 In Section 15.3.2, the prior distribution over locations is uniform and the transition model assumes an equal probability of moving to any neighboring square.

Token 12307:
What if those assumptions are wrong?

Token 12308:
Suppose that the initial location is actually chosen uniformly fromthe northwest quadrant of the room and the Move action actually tends to move southeast.

Token 12309:
Keeping the HMM model ﬁxed, explore the effect on localization and path accuracy as thesoutheasterly tendency increases, for different values of /epsilon1.

Token 12310:
15.8 Consider a version of the vacuum robot (page 582) that has the policy of going straight for as long as it can; only when it encounters an obstacle does it change to a new (randomly selected) heading.

Token 12311:
To model this robot, each state in the model consists of a (location, head- ing)pair.

Token 12312:
Implement this model and see how well the Viterbi algorithm can track a robot with this model.

Token 12313:
The robot’s policy is more constrained than the random-walk robot; does thatmean that predictions of the most likely path are more accurate?

Token 12314:
15.9 This exercise is concerned with ﬁltering in an environment with no landmarks.

Token 12315:
Con- sider a vacuum robot in an empty room, represented by an n×mrectangular grid.

Token 12316:
The robot’s location is hidden; the only evidence available to the observer is a noisy location sensor thatgives an approximation to the robot’s location.

Token 12317:
If the robot is at location (x,y)then with probability .1 the sensor gives the correct location, with probability .05 each it reports one of the 8 locations immediately surrounding (x,y), with probability .025 each it reports one of the 16 locations that surround those 8, and with the remaining probability of .1 it reports“no reading.” The robot’s policy is to pick a direction and follow it with probability .8 oneach step; the robot switches to a randomly selected new heading with probability .2 (or with

Token 12318:
608 Chapter 15. Probabilistic Reasoning over Time XtSt Zt Zt+1Xt+1St+1 Figure 15.21 A Bayesian network representation of a switching Kalman ﬁlter.

Token 12319:
The switching variable Stis a discrete state variable whose value determines the transition model for the continuous state variables Xt.

Token 12320:
For any discrete state i, the transition model P(Xt+1|Xt,St=i)is a linear Gaussian model, just as in a regular Kalman ﬁlter.

Token 12321:
The tran- sition model for the discrete state, P(St+1|St), can be thought of as a matrix, as in a hidden Markov model.

Token 12322:
probability 1 if it encounters a wall). Implement this as an HMM and do ﬁltering to track the robot. How accurately can we track the robot’s path?

Token 12323:
15.10 Often, we wish to monitor a continuous-state system whose behavior switches unpre- dictably among a set of kdistinct “modes.” For example, an aircraft trying to evade a missile can execute a series of distinct maneuvers that the missile may attempt to track.

Token 12324:
A Bayesiannetwork representation of such a switching Kalman ﬁlter model is shown in Figure 15.21. a.

Token 12325:
Suppose that the discrete state S thaskpossible values and that the prior continuous state estimate P(X0)is a multivariate Gaussian distribution.

Token 12326:
Show that the prediction P(X1)is amixture of Gaussians —that is, a weighted sum of Gaussians such that the weights sum to 1. b.

Token 12327:
Show that if the current continuous state estimate P(Xt|e1:t)is a mixture of mGaus- sians, then in the general case the updated state estimate P(Xt+1|e1:t+1)will be a mix- ture of kmGaussians.

Token 12328:
c. What aspect of the temporal process do the weights in the Gaussian mixture represent?

Token 12329:
The results in (a) and (b) show that the representation of the posterior grows without limit even for switching Kalman ﬁlters, which are among the simplest hybrid dynamic models.

Token 12330:
15.11 Complete the missing step in the derivation of Equation (15.19) on page 586, the ﬁrst update step for the one-dimensional Kalman ﬁlter.

Token 12331:
15.12 Let us examine the behavior of the variance update in Equation (15.20) (page 587). a.

Token 12332:
Plot the value of σ2 tas a function of t, given various values for σ2 xandσ2 z. b.

Token 12333:
Show that the update has a ﬁxed point σ2such that σ2 t→σ2ast→∞ , and calculate the value of σ2.

Token 12334:
c. Give a qualitative explanation for what happens as σ2 x→0and as σ2 z→0.

Token 12335:
Exercises 609 15.13 A professor wants to know if students are getting enough sleep.

Token 12336:
Each day, the pro- fessor observes whether the students sleep in class, and whether they have red eyes.

Token 12337:
Theprofessor has the following domain theory: •The prior probability of getting enough sleep, with no observations, is 0.7.

Token 12338:
•The probability of getting enough sleep on night tis 0.8 given that the student got enough sleep the previous night, and 0.3 if not.

Token 12339:
•The probability of having red eyes is 0.2 if the student got enough sleep, and 0.7 if not.

Token 12340:
•The probability of sleeping in class is 0.1 if the student got enough sleep, and 0.3 if not.

Token 12341:
Formulate this information as a dynamic Bayesian network that the professor could use to ﬁlter or predict from a sequence of observations.

Token 12342:
Then reformulate it as a hidden Markov model that has only a single observation variable. Give the complete probability tables forthe model.

Token 12343:
15.14 For the DBN speciﬁed in Exercise 15.13 and for the evidence values e 1=not red eyes, not sleeping in class e2=red eyes, not sleeping in class e3=red eyes, sleeping in class perform the following computations: a.

Token 12344:
State estimation: Compute P(EnoughSleept|e1:t)for each of t=1,2,3. b. Smoothing: Compute P(EnoughSleept|e1:3)for each of t=1,2,3.

Token 12345:
c. Compare the ﬁltered and smoothed probabilities for t=1andt=2.

Token 12346:
15.15 Suppose that a particular student shows up with red eyes and sleeps in class every day.

Token 12347:
Given the model described in Exercise 15.13, explain why the probability that the student had enough sleep the previous night converges to a ﬁxed point rather than continuing to go down as we gather more days of evidence.

Token 12348:
What is the ﬁxed point? Answer this both numerically(by computation) and analytically.

Token 12349:
15.16 This exercise analyzes in more detail the persistent-failure model for the battery sen- sor in Figure 15.15(a) (page 594). a.

Token 12350:
Figure 15.15(b) stops at t=3 2 . Describe qualitatively what should happen as t→∞ if the sensor continues to read 0. b.

Token 12351:
Suppose that the external temperature affects the battery sensor in such a way that tran- sient failures become more likely as temperature increases.

Token 12352:
Show how to augment the DBN structure in Figure 15.15(a), and explain any required changes to the CPTs.

Token 12353:
c. Given the new network structure, can battery readings be used by the robot to infer the current temperature?

Token 12354:
15.17 Consider applying the variable elimination algorithm to the umbrella DBN unrolled for three slices, where the query is P(R 3|u1,u2,u3).

Token 12355:
Show that the space complexity of the algorithm—the size of the largest factor—is the same, regardless of whether the rain variablesare eliminated in forward or backward order.

Token 12356:
16MAKING SIMPLE DECISIONS In which we see how an agent should make decisions so that it gets what it wants— on average, at least.

Token 12357:
In this chapter, we ﬁll in the details of how utility theory combines with probability theory to yield a decision-theoretic agent—an agent that can make rational decisions based on what itbelieves and what it wants.

Token 12358:
Such an agent can make decisions in contexts in which uncertaintyand conﬂicting goals leave a logical agent with no way to decide: a goal-based agent has abinary distinction between good (goal) and bad (non-goal) states, while a decision-theoreticagent has a continuous measure of outcome quality.

Token 12359:
Section 16.1 introduces the basic principle of decision theory: the maximization of expected utility.

Token 12360:
Section 16.2 shows that the behavior of any rational agent can be captured by supposing a utility function that is being maximized.

Token 12361:
Section 16.3 discusses the nature ofutility functions in more detail, and in particular their relation to individual quantities such asmoney.

Token 12362:
Section 16.4 shows how to handle utility functions that depend on several quantities.In Section 16.5, we describe the implementation of decision-making systems.

Token 12363:
In particular,we introduce a formalism called a decision network (also known as an inﬂuence diagram ) that extends Bayesian networks by incorporating actions and utilities.

Token 12364:
The remainder of thechapter discusses issues that arise in applications of decision theory to expert systems.

Token 12365:
16.1 C OMBINING BELIEFS AND DESIRES UNDER UNCERTAINTY Decision theory, in its simplest form, deals with choosing among actions based on the desir-ability of their immediate outcomes; that is, the environment is assumed to be episodic in the sense deﬁned on page 43.

Token 12366:
(This assumption is relaxed in Chapter 17.)

Token 12367:
In Chapter 3 we used the notation R ESULT (s0,a) for the state that is the deterministic outcome of taking action a in state s0.

Token 12368:
In this chapter we deal with nondeterministic partially observable environments.

Token 12369:
Since the agent may not know the current state, we omit it and deﬁne R ESULT (a)as arandom variable whose values are the possible outcome states.

Token 12370:
The probability of outcome s/prime,g i v e n evidence observations e, is written P(RESULT (a)=s/prime|a,e), 610

Token 12371:
Section 16.2.

Token 12372:
The Basis of Utility Theory 611 where the aon the right-hand side of the conditioning bar stands for the event that action ais executed.1 The agent’s preferences are captured by a utility function ,U(s), which assigns a single UTILITYFUNCTION number to express the desirability of a state.

Token 12373:
The expected utility of an action given the evi- EXPECTED UTILITY dence, EU(a|e), is just the average utility value of the outcomes, weighted by the probability that the outcome occurs: EU(a|e)=/summationdisplay s/primeP(RESULT (a)=s/prime|a,e)U(s/prime).

Token 12374:
(16.1) The principle of maximum expected utility (MEU) says that a rational agent should chooseMAXIMUM EXPECTED UTILITY the action that maximizes the agent’s expected utility: action =a r g m a x aEU(a|e) In a sense, the MEU principle could be seen as deﬁning all of AI.

Token 12375:
All an intelligent agent has to do is calculate the various quantities, maximize utility over its actions, and away it goes.But this does not mean that the AI problem is solved by the deﬁnition!

Token 12376:
The MEU principle formalizes the general notion that the agent should “do the right thing,” but goes only a small distance toward a full operationalization of that advice.

Token 12377:
Es- timating the state of the world requires perception, learning, knowledge representation, andinference.

Token 12378:
Computing P(R ESULT (a)|a,e)requires a complete causal model of the world and, as we saw in Chapter 14, NP-hard inference in (very large) Bayesian networks.

Token 12379:
Comput- ing the outcome utilities U(s/prime)often requires searching or planning, because an agent may not know how good a state is until it knows where it can get to from that state.

Token 12380:
So, decisiontheory is not a panacea that solves the AI problem—but it does provide a useful framework.

Token 12381:
The MEU principle has a clear relation to the idea of performance measures introduced in Chapter 2. The basic idea is simple.

Token 12382:
Consider the environments that could lead to anagent having a given percept history, and consider the different agents that we could design.If an agent acts so as to maximize a utility function that correctly reﬂects the performance measure, then the agent will achieve the highest possible performance score (averaged overall the possible environments).

Token 12383:
This is the central justiﬁcation for the MEU principle itself.

Token 12384:
While the claim may seem tautological, it does in fact embody a very important transitionfrom a global, external criterion of rationality—the performance measure over environment histories—to a local, internal criterion involving the maximization of a utility function applied to the next state.

Token 12385:
16.2 T HEBASIS OF UTILITY THEORY Intuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable wayto make decisions, but it is by no means obvious that it is the only rational way.

Token 12386:
After all, why should maximizing the average utility be so special?

Token 12387:
What’s wrong with an agent that 1Classical decision theory leaves the current state S0implicit, but we could make it explicit by writing P(RESULT (a)=s/prime|a,e)=P sP(RESULT (s,a)=s/prime|a)P(S0=s|e).

Token 12388:
612 Chapter 16.

Token 12389:
Making Simple Decisions maximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the worst possible loss?

Token 12390:
Could an agent act rationally just by expressing preferences betweenstates, without giving them numeric values?

Token 12391:
Finally, why should a utility function with therequired properties exist at all? We shall see.

Token 12392:
16.2.1 Constraints on rational preferences These questions can be answered by writing down some constraints on the preferences that a rational agent should have and then showing that the MEU principle can be derived from the constraints.

Token 12393:
We use the following notation to describe an agent’s preferences: A/followsB the agent prefers AoverB. A∼B the agent is indifferent between AandB.

Token 12394:
A/follows∼B the agent prefers AoverBor is indifferent between them. Now the obvious question is, what sorts of things are AandB?

Token 12395:
They could be states of the world, but more often than not there is uncertainty about what is really being offered.

Token 12396:
Forexample, an airline passenger who is offered “the pasta dish or the chicken” does not knowwhat lurks beneath the tinfoil cover.

Token 12397:
2The pasta could be delicious or congealed, the chicken juicy or overcooked beyond recognition.

Token 12398:
We can think of the set of outcomes for each action as alottery —think of each action as a ticket.

Token 12399:
A lottery Lwith possible outcomes S1,...,S nLOTTERY that occur with probabilities p1,...,p nis written L=[p1,S1;p2,S2;... p n,Sn].

Token 12400:
In general, each outcome Siof a lottery can be either an atomic state or another lottery.

Token 12401:
The primary issue for utility theory is to understand how preferences between complex lotteriesare related to preferences between the underlying states in those lotteries.

Token 12402:
To address thisissue we list six constraints that we require any reasonable preference relation to obey: •Orderability : Given any two lotteries, a rational agent must either prefer one to the ORDERABILITY other or else rate the two as equally preferable.

Token 12403:
That is, the agent cannot avoid deciding. As we said on page 490, refusing to bet is like refusing to allow time to pass.

Token 12404:
Exactly one of (A/followsB),(B/followsA),or(A∼B)holds.

Token 12405:
•Transitivity : Given any three lotteries, if an agent prefers AtoBand prefers BtoC, TRANSITIVITY then the agent must prefer AtoC.

Token 12406:
(A/followsB)∧(B/followsC)⇒(A/followsC).

Token 12407:
•Continuity : If some lottery Bis between AandCin preference, then there is some CONTINUITY probability pfor which the rational agent will be indifferent between getting Bfor sure and the lottery that yields Awith probability pandCwith probability 1−p.

Token 12408:
A/followsB/followsC⇒∃p[p,A;1−p,C]∼B.

Token 12409:
•Substituta bility : If an agent is indifferent between two lotteries AandB, then the SUBSTITUTABILITY agent is indifferent between two more complex lotteries that are the same except that B 2We apologize to readers whose local airlines no longer offer food on long ﬂights.

Token 12410:
Section 16.2. The Basis of Utility Theory 613 is substituted for Ain one of them.

Token 12411:
This holds regardless of the probabilities and the other outcome(s) in the lotteries. A∼B⇒[p,A;1−p,C]∼[p,B;1−p,C].

Token 12412:
This also holds if we substitute /followsfor∼in this axiom. •Monotonicity : Suppose two lotteries have the same two possible outcomes, AandB.

Token 12413:
MONOTONICITY If an agent prefers AtoB, then the agent must prefer the lottery that has a higher probability for A(and vice versa).

Token 12414:
A/followsB⇒(p>q⇔[p,A;1−p,B]/follows[q,A;1−q,B]).

Token 12415:
•Decomposability : Compound lotteries can be reduced to simpler ones using the laws DECOMPOSABILITY of probability.

Token 12416:
This has been called the “no fun in gambling” rule because it says that two consecutive lotteries can be compressed into a single equivalent lottery, as shownin Figure 16.1(b).

Token 12417:
3 [p,A;1−p,[q,B;1−q,C]]∼[p,A;( 1−p)q,B;( 1−p)(1−q),C]. These constraints are known as the axioms of utility theory.

Token 12418:
Each axiom can be motivated by showing that an agent that violates it will exhibit patently irrational behavior in somesituations.

Token 12419:
For example, we can motivate transitivity by making an agent with nontransitivepreferences give us all its money.

Token 12420:
Suppose that the agent has the nontransitive preferencesA/followsB/followsC/followsA,w h e r e A,B,a n dCare goods that can be freely exchanged.

Token 12421:
If the agent currently has A, then we could offer to trade CforAplus one cent. The agent prefers C, and so would be willing to make this trade.

Token 12422:
We could then offer to trade BforC, extracting another cent, and ﬁnally trade AforB.

Token 12423:
This brings us back where we started from, except that the agent has given us three cents (Figure 16.1(a)).

Token 12424:
We can keep going around the cycleuntil the agent has no money at all. Clearly, the agent has acted irrationally in this case.

Token 12425:
16.2.2 Preferences lead to utility Notice that the axioms of utility theory are really axioms about preferences—they say nothing about a utility function.

Token 12426:
But in fact from the axioms of utility we can derive the following consequences (for the proof, see von Neumann and Morgenstern, 1944): •Existence o f Utility Func tion: If an agent’s preferences obey the axioms of utility, then there exists a function Usuch that U(A)>U(B)if and only if Ais preferred to B, andU(A)=U(B)if and only if the agent is indifferent between AandB.

Token 12427:
U(A)>U(B)⇔A/followsB U(A)=U(B)⇔A∼B •Expected Utility of a Lottery : The utility of a lottery is the sum of the probability of each outcome times the utility of that outcome.

Token 12428:
U([p1,S1;...;pn,Sn]) =/summationdisplay ipiU(Si).

Token 12429:
3We can account for the enjoyment of gambling by encoding gambling events into the state description; for example, “Have $10 and gambled” could be preferred to “Have $10 and didn’t gamble.”

Token 12430:
614 Chapter 16.

Token 12431:
Making Simple Decisions 1¢ 1¢1¢A BCp qA B C p(1–p) (1–p)(1–q)(1–q) A B Cis equivalent to (a) (b)(1–p)q Figure 16.1 (a) A cycle of exchanges showing that the nontransitive preferences A/follows B/followsC/followsAresult in irrational behavior.

Token 12432:
(b) The decomposability axiom.

Token 12433:
In other words, once the probabilities and utilities of the possible outcome states are speciﬁed, the utility of a compound lottery involving those states is completely determined.

Token 12434:
Because the outcome of a nondeterministic action is a lottery, it follows that an agent can act rationally— that is, consistently with its preferences—only by choosing an action that maximizes expectedutility according to Equation (16.1).

Token 12435:
The preceding theorems establish that a utility function exists for any rational agent, but they do not establish that it is unique .

Token 12436:
It is easy to see, in fact, that an agent’s behavior would not change if its utility function U(S)were transformed according to U /prime(S)=aU(S)+b, (16.2) where aandbare constants and a>0; an afﬁne transformation.4This fact was noted in Chapter 5 for two-player games of chance; here, we see that it is completely general.

Token 12437:
As in game-playing, in a deterministic environment an agent just needs a preference ranking on states—the numbers don’t matter.

Token 12438:
This is called a value function orordinal VALUE FUNCTION utility function .ORDINAL UTILITY FUNCTION It is important to remember that the existence of a utility function that describes an agent’s preference behavior does not necessarily mean that the agent is explicitly maximizing that utility function in its own deliberations.

Token 12439:
As we showed in Chapter 2, rational behavior can be generated in any number of ways.

Token 12440:
By observing a rational agent’s preferences, however, an observer can construct the utility function that represents what the agent is actually trying to achieve (even if the agent doesn’t know it).

Token 12441:
4In this sense, utilities resemble temperatures: a tempera ture in Fahrenheit is 1.8 times the Celsius temperature plus 32.

Token 12442:
You get the same results in either measurement system.

Token 12443:
Section 16.3. Utility Functions 615 16.3 U TILITY FUNCTIONS Utility is a function that maps from lotteries to real numbers.

Token 12444:
We know there are some axioms on utilities that all rational agents must obey.

Token 12445:
Is that all we can say about utility functions?Strictly speaking, that is it: an agent can have any preferences it likes.

Token 12446:
For example, an agentmight prefer to have a prime number of dollars in its bank account; in which case, if it had $16it would give away $3.

Token 12447:
This might be unusual, but we can’t call it irrational. An agent mightprefer a dented 1973 Ford Pinto to a shiny new Mercedes.

Token 12448:
Preferences can also interact: forexample, the agent might prefer prime numbers of dollars only when it owns the Pinto, butwhen it owns the Mercedes, it might prefer more dollars to fewer.

Token 12449:
Fortunately, the preferencesof real agents are usually more systematic, and thus easier to deal with.

Token 12450:
16.3.1 Utility assessment and utility scales If we want to build a decision-theoretic system that helps the agent make decisions or actson his or her behalf, we must ﬁrst work out what the agent’s utility function is.

Token 12451:
This process, often called preference elicitation , involves presenting choices to the agent and using the PREFERENCE ELICITATION observed preferences to pin down the underlying utility function.

Token 12452:
Equation (16.2) says that there is no absolute scale for utilities, but it is helpful, nonethe- less, to establish some scale on which utilities can be recorded and compared for any particu- lar problem.

Token 12453:
A scale can be established by ﬁxing the utilities of any two particular outcomes,just as we ﬁx a temperature scale by ﬁxing the freezing point and boiling point of water.Typically, we ﬁx the utility of a “best possible prize” at U(S)=u /latticetopand a “worst possible catastrophe” at U(S)=u⊥.Normalized utilities use a scale with u⊥=0andu/latticetop=1.NORMALIZED UTILITIES Given a utility scale between u/latticetopandu⊥, we can assess the utility of any particular prizeSby asking the agent to choose between Sand a standard lottery [p,u/latticetop;( 1−p),u⊥].

Token 12454:
STANDARD LOTTERY The probability pis adjusted until the agent is indifferent between Sand the standard lottery.

Token 12455:
Assuming normalized utilities, the utility of Sis given by p. Once this is done for each prize, the utilities for all lotteries involving those prizes are determined.

Token 12456:
In medical, transportation, and environmental decision problems, among others, peo- ple’s lives are at stake.

Token 12457:
In such cases, u⊥is the value assigned to immediate death (or perhaps many deaths).

Token 12458:
Although nobody feels comfortable with putting a value on human life, it is a fact that tradeoffs are made all the time.

Token 12459:
Aircraft are given a complete overhaul at intervals determined by trips and miles ﬂown, rather than after every trip.

Token 12460:
Cars are manufactured ina way that trades off costs against accident survival rates.

Token 12461:
Paradoxically, a refusal to “put amonetary value on life” means that life is often undervalued .

Token 12462:
Ross Shachter relates an ex- perience with a government agency that commissioned a study on removing asbestos fromschools.

Token 12463:
The decision analysts performing the study assumed a particular dollar value for the life of a school-age child, and argued that the rational choice under that assumption was to remove the asbestos.

Token 12464:
The agency, morally outraged at the idea of setting the value of a life,rejected the report out of hand.

Token 12465:
It then decided against asbestos removal—implicitly assertinga lower value for the life of a child than that assigned by the analysts.

Token 12466:
616 Chapter 16. Making Simple Decisions Some attempts have been made to ﬁnd out the value that people place on their own lives.

Token 12467:
One common “currency” used in medical and safety analysis is the micromort ,a MICROMORT one in a million chance of death.

Token 12468:
If you ask people how much they would pay to avoid a risk—for example, to avoid playing Russian roulette with a million-barreled revolver—theywill respond with very large numbers, perhaps tens of thousands of dollars, but their actual behavior reﬂects a much lower monetary value for a micromort.

Token 12469:
For example, driving in a car for 230 miles incurs a risk of one micromort; over the life of your car—say, 92,000 miles—that’s 400 micromorts.

Token 12470:
People appear to be willing to pay about $10,000 (at 2009 prices)more for a safer car that halves the risk of death, or about $50 per micromort.

Token 12471:
A numberof studies have conﬁrmed a ﬁgure in this range across many individuals and risk types. Ofcourse, this argument holds only for small risks.

Token 12472:
Most people won’t agree to kill themselvesfor $50 million. Another measure is the QALY , or quality-adjusted life year.

Token 12473:
Patients with a disability QALY are willing to accept a shorter life expectancy to be restored to full health.

Token 12474:
For example, kidney patients on average are indifferent between living two years on a dialysis machine andone year at full health.

Token 12475:
16.3.2 The utility of money Utility theory has its roots in economics, and economics provides one obvious candidatefor a utility measure: money (or more speciﬁcally, an agent’s total net assets).

Token 12476:
The almost universal exchangeability of money for all kinds of goods and services suggests that money plays a signiﬁcant role in human utility functions.

Token 12477:
It will usually be the case that an agent prefers more money to less, all other things being equal.

Token 12478:
We say that the agent exhibits a monotonic preference for more money.

Token 12479:
This does MONOTONIC PREFERENCE not mean that money behaves as a utility function, because it says nothing about preferences between lotteries involving money.

Token 12480:
Suppose you have triumphed over the other competitors in a television game show.

Token 12481:
The host now offers you a choice: either you can take the $1,000,000 prize or you can gamble iton the ﬂip of a coin.

Token 12482:
If the coin comes up heads, you end up with nothing, but if it comesup tails, you get $2,500,000.

Token 12483:
If you’re like most people, you would decline the gamble andpocket the million. Are you being irrational?

Token 12484:
Assuming the coin is fair, the expected monetary value (EMV) of the gamble is 1 2($0)EXPECTED MONETARY VALUE +1 2($2,500,000) =$1,250,000, which is more than the original $1,000,000.

Token 12485:
But that does not necessarily mean that accepting the gamble is a better decision.

Token 12486:
Suppose we use Snto denote the state of possessing total wealth $ n, and that your current wealth is $ k. Then the expected utilities of the two actions of accepting and declining the gamble are EU(Accept )=1 2U(Sk)+1 2U(Sk+2,500,000), EU(Decline )=U(Sk+1,000,000).

Token 12487:
To determine what to do, we need to assign utilities to the outcome states.

Token 12488:
Utility is not directly proportional to monetary value, because the utility for your ﬁrst million is very high(or so they say), whereas the utility for an additional million is smaller.

Token 12489:
Suppose you assigna utility of 5 to your current ﬁnancial status ( S k), a 9 to the state Sk+2,500,000,a n da n8t ot h e

Token 12490:
Section 16.3. Utility Functions 617 U $ $ -150,000 800,000 (a) (b)oooooooooooooooU Figure 16.2 The utility of money. (a) Empirical data for Mr.

Token 12491:
Beard over a limited range. (b) A typical curve for the full range. stateSk+1,000,000.

Token 12492:
Then the rational action would be to decline, because the expected utility of accepting is only 7 (less than the 8 for declining).

Token 12493:
On the other hand, a billionaire would most likely have a utility function that is locally linear over the range of a few million more, and thus would accept the gamble.

Token 12494:
In a pioneering study of actual utility functions, Grayson (1960) found that the utility of money was almost exactly proportional to the logarithm of the amount.

Token 12495:
(This idea was ﬁrst suggested by Bernoulli (1738); see Exercise 16.3.) One particular utility curve, for a certainMr.

Token 12496:
Beard, is shown in Figure 16.2(a).

Token 12497:
The data obtained for Mr. Beard’s preferences areconsistent with a utility function U(S k+n)=−263.31 + 22 .09 log( n+ 150 ,000) for the range between n=−$150,000andn= $800 ,000.

Token 12498:
We should not assume that this is the deﬁnitive utility function for monetary value, but it is likely that most people have a utility function that is concave for positive wealth.

Token 12499:
Going into debt is bad, but preferences between different levels of debt can display a reversal ofthe concavity associated with positive wealth.

Token 12500:
For example, someone already $10,000,000 indebt might well accept a gamble on a fair coin with a gain of $10,000,000 for heads and aloss of $20,000,000 for tails.

Token 12501:
5This yields the S-shaped curve shown in Figure 16.2(b).

Token 12502:
If we restrict our attention to the positive part of the curves, where the slope is decreas- ing, then for any lottery L, the utility of being faced with that lottery is less than the utility of being handed the expected monetary value of the lottery as a sure thing: U(L)<U(SEMV (L)).

Token 12503:
That is, agents with curves of this shape are risk-averse : they prefer a sure thing with a RISK-AVERSE payoff that is less than the expected monetary value of a gamble.

Token 12504:
On the other hand, in the “desperate” region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking .

Token 12505:
RISK-SEEKING 5Such behavior might be called desperate, but it is rational if one is already in a desperate situation.

Token 12506:
618 Chapter 16.

Token 12507:
Making Simple Decisions The value an agent will accept in lieu of a lottery is called the certainty equivalent of theCERTAINTY EQUIVALENT lottery.

Token 12508:
Studies have shown that most people will accept about $400 in lieu of a gamble that gives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lotteryis $400, while the EMV is $500.

Token 12509:
The difference between the EMV of a lottery and its certaintyequivalent is called the insurance premium .

Token 12510:
Risk aversion is the basis for the insurance INSURANCE PREMIUM industry, because it means that insurance premiums are positive.

Token 12511:
People would rather pay a small insurance premium than gamble the price of their house against the chance of a ﬁre.From the insurance company’s point of view, the price of the house is very small comparedwith the ﬁrm’s total reserves.

Token 12512:
This means that the insurer’s utility curve is approximatelylinear over such a small region, and the gamble costs the company almost nothing.

Token 12513:
Notice that for small changes in wealth relative to the current wealth, almost any curve will be approximately linear.

Token 12514:
An agent that has a linear curve is said to be risk-neutral .F o r RISK-NEUTRAL gambles with small sums, therefore, we expect risk neutrality.

Token 12515:
In a sense, this justiﬁes the simpliﬁed procedure that proposed small gambles to assess probabilities and to justify theaxioms of probability in Section 13.2.3.

Token 12516:
16.3.3 Expected utility and post-decision disappointment The rational way to choose the best action, a∗, is to maximize expected utility: a∗=a r g m a x aEU(a|e).

Token 12517:
If we have calculated the expected utility correctly according to our probability model, and if the probability model correctly reﬂects the underlying stochastic processes that generate theoutcomes, then, on average, we will get the utility we expect if the whole process is repeatedmany times.

Token 12518:
In reality, however, our model usually oversimpliﬁes the real situation, either because we don’t know enough (e.g., when making a complex investment decision) or because the computation of the true expected utility is too difﬁcult (e.g., when estimating the utility ofsuccessor states of the root node in backgammon).

Token 12519:
In that case, we are really working withestimates/hatwidestEU(a|e)of the true expected utility.

Token 12520:
We will assume, kindly perhaps, that the estimates are unbiased , that is, the expected value of the error, E(/hatwidestEU(a|e)−EU(a|e))),i s UNBIASED zero.

Token 12521:
In that case, it still seems reasonable to choose the action with the highest estimated utility and to expect to receive that utility, on average, when the action is executed.

Token 12522:
Unfortunately, the real outcome will usually be signiﬁcantly worse than we estimated, even though the estimate was unbiased!

Token 12523:
To see why, consider a decision problem in whichthere are kchoices, each of which has true estimated utility of 0.

Token 12524:
Suppose that the error in each utility estimate has zero mean and standard deviation of 1, shown as the bold curve inFigure 16.3.

Token 12525:
Now, as we actually start to generate the estimates, some of the errors will benegative (pessimistic) and some will be positive (optimistic).

Token 12526:
Because we select the actionwith the highest utility estimate, we are obviously favoring the overly optimistic estimates, and that is the source of the bias.

Token 12527:
It is a straightforward matter to calculate the distribution of the maximum of the kestimates (see Exercise 16.11) and hence quantify the extent of our disappointment.

Token 12528:
The curve in Figure 16.3 for k=3has a mean around 0.85, so the average disappointment will be about 85% of the standard deviation in the utility estimates.

Token 12529:
Section 16.3.

Token 12530:
Utility Functions 619 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 -5 -4 -3 -2 -1 0 1 2 3 4 5 Error in utility estimatek=3k=10k=30 Figure 16.3 Plot of the error in each of kutility estimates and of the distribution of the maximum of kestimates for k=3, 10, and 30.

Token 12531:
With more choices, extremely optimistic estimates are more likely to arise: for k=3 0 ,t h e disappointment will be around twice the standard deviation in the estimates.

Token 12532:
This tendency for the estimated expected utility of the best choice to be too high is called the optimizer’s curse (Smith and Winkler, 2006).

Token 12533:
It afﬂicts even the most seasoned OPTIMIZER’S CURSE decision analysts and statisticians.

Token 12534:
Serious manifestations include believing that an exciting new drug that has cured 80% patients in a trial will cure 80% of patients (it’s been chosenfromk=thousands of candidate drugs) or that a mutual fund advertised as having above- average returns will continue to have them (it’s been chosen to appear in the advertisementout of k=dozens of funds in the company’s overall portfolio).

Token 12535:
It can even be the case that what appears to be the best choice may not be, if the variance in the utility estimate is high: a drug, selected from thousands tried, that has cured 9 of 10 patients is probably worse than one that has cured 800 of 1000.

Token 12536:
The optimizer’s curse crops up everywhere because of the ubiquity of utility-maximizing selection processes, so taking the utility estimates at face value is a bad idea.

Token 12537:
We can avoid the curse by using an explicit probability model P(/hatwidestEU|EU)of the error in the utility estimates.

Token 12538:
Given this model and a prior P(EU)on what we might reasonably expect the utilities to be, we treat the utility estimate, once obtained, as evidence and compute the posterior distributionfor the true utility using Bayes’ rule.

Token 12539:
16.3.4 Human judgment and irrationality Decision theory is a normative theory : it describes how a rational agent should act.

Token 12540:
A NORMATIVE THEORY descriptive theory , on the other hand, describes how actual agents—for example, humans—DESCRIPTIVE THEORY really do act.

Token 12541:
The application of economic theory would be greatly enhanced if the two coincided, but there appears to be some experimental evidence to the contrary.

Token 12542:
The evidencesuggests that humans are “predictably irrational” (Ariely, 2009).

Token 12543:
620 Chapter 16. Making Simple Decisions The best-known problem is the Allais paradox (Allais, 1953).

Token 12544:
People are given a choice between lotteries AandBand then between CandD, which have the following prizes: A:80% chance of $4000 C:20% chance of $4000 B:100% chance of $3000 D:25% chance of $3000 Most people consistently prefer BoverA(taking the sure thing), and CoverD(taking the higher EMV).

Token 12545:
The normative analysis disagrees! We can see this most easily if we use thefreedom implied by Equation (16.2) to set U($0) = 0 .

Token 12546:
In that case, then B/followsAimplies thatU($3000) >0.8U($4000) , whereas C/followsDimplies exactly the reverse.

Token 12547:
In other words, there is no utility function that is consistent with these choices.

Token 12548:
One explanation forthe apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979): CERTAINTY EFFECT people are strongly attracted to gains that are certain.

Token 12549:
There are several reasons why this may be so.

Token 12550:
First, people may prefer to reduce their computational burden; by choosing certainoutcomes, they don’t have to compute with probabilities.

Token 12551:
But the effect persists even whenthe computations involved are very easy ones. Second, people may distrust the legitimacy of the stated probabilities.

Token 12552:
I trust that a coin ﬂip is roughly 50/50 if I have control over the coin and the ﬂip, but I may distrust the result if the ﬂip is done by someone with a vested interestin the outcome.

Token 12553:
6In the presence of distrust, it might be better to go for the sure thing.7Third, people may be accounting for their emotional state as well as their ﬁnancial state.

Token 12554:
Peopleknow they would experience regret if they gave up a certain reward ( B) for an 80% chance at REGRET a higher reward and then lost.

Token 12555:
In other words, if Ais chosen, there is a 20% chance of getting no money and feeling like a complete idiot , which is worse than just getting no money.

Token 12556:
So perhaps people who choose BoverAandCoverDare not being irrational; they are just saying that they are willing to give up $200 of EMV to avoid a 20% chance of feeling like anidiot.

Token 12557:
A related problem is the Ellsberg paradox. Here the prizes are ﬁxed, but the probabilities are underconstrained.

Token 12558:
Your payoff will depend on the color of a ball chosen from an urn.

Token 12559:
You are told that the urn contains 1/3 red balls, and 2/3 either black or yellow balls, but you don’t know how many black and how many yellow.

Token 12560:
Again, you are asked whether you prefer lottery AorB;a n dt h e n CorD: A:$100 for a red ball C:$100 for a red or yellow ball B:$100 for a black ball D:$100 for a black or yellow ball .

Token 12561:
It should be clear that if you think there are more red than black balls then you should prefer AoverBandCoverD; if you think there are fewer red than black you should prefer the opposite.

Token 12562:
But it turns out that most people prefer AoverBa n da l s op r e f e r DoverC,e v e n though there is no state of the world for which this is rational.

Token 12563:
It seems that people have ambiguity aversion :Agives you a 1/3 chance of winning, while Bcould be anywhere AMBIGUITY AVERSION between 0 and 2/3.

Token 12564:
Similarly, Dgives you a 2/3 chance, while Ccould be anywhere between 1/3 and 3/3.

Token 12565:
Most people elect the known probability rather than the unknown unknowns.

Token 12566:
6For example, the mathematician/magician Persi Diaconis can make a coin ﬂip come out the way he wants every time (Landhuis, 2004).

Token 12567:
7Even the sure thing may not be certain.

Token 12568:
Despite cast-iron promises, we have not yet received that $27,000,000 from the Nigerian bank account of a previously unknown deceased relative.

Token 12569:
Section 16.3.

Token 12570:
Utility Functions 621 Yet another problem is that the exact wording of a decision problem can have a big impact on the agent’s choices; this is called the framing effect .

Token 12571:
Experiments show that people FRAMING EFFECT like a medical procedure that it is described as having a “90% survival rate” about twice as much as one described as having a “10% death rate,” even though these two statements meanexactly the same thing.

Token 12572:
This discrepancy in judgment has been found in multiple experiments and is about the same whether the subjects were patients in a clinic, statistically sophisticated business school students, or experienced doctors.

Token 12573:
People feel more comfortable making relative utility judgments rather than absolute ones.

Token 12574:
I may have little idea how much I might enjoy the various wines offered by a restaurant.The restaurant takes advantage of this by offering a $200 bottle that it knows nobody will buy,but which serves to skew upward the customer’s estimate of the value of all wines and makethe $55 bottle seem like a bargain.

Token 12575:
This is called the anchoring effect .

Token 12576:
ANCHORINGEFFECT If human informants insist on contradictory preference judgments, there is nothing that automated agents can do to be consistent with them.

Token 12577:
Fortunately, preference judgments madeby humans are often open to revision in the light of further consideration.

Token 12578:
Paradoxes likethe Allais paradox are greatly reduced (but not eliminated) if the choices are explained bet-ter.

Token 12579:
In work at the Harvard Business School on assessing the utility of money, Keeney and Raiffa (1976, p. 210) found the following: Subjects tend to be too risk-averse in the small and therefore ...the ﬁtted utility functions exhibit unacceptably large risk premiums for lotteries with a large spread.

Token 12580:
...Most of the subjects, however, can reconcile their inconsistencies and feel that they have learned animportant lesson about how they want to behave.

Token 12581:
As a consequence, some subjects cancel their automobile collision insurance and take out more term insurance on their lives.

Token 12582:
The evidence for human irrationality is also questioned by researchers in the ﬁeld of evo- lutionary psychology , who point to the fact that our brain’s decision-making mechanismsEVOLUTIONARY PSYCHOLOGY did not evolve to solve word problems with probabilities and prizes stated as decimal num- bers.

Token 12583:
Let us grant, for the sake of argument, that the brain has built-in neural mechanismfor computing with probabilities and utilities, or something functionally equivalent; if so, therequired inputs would be obtained through accumulated experience of outcomes and rewardsrather than through linguistic presentations of numerical values.

Token 12584:
It is far from obvious that we can directly access the brain’s built-in neural mechanisms by presenting decision problems in linguistic/numerical form.

Token 12585:
The very fact that different wordings of the same decision prob- lemelicit different choices suggests that the decision problem itself is not getting through.

Token 12586:
Spurred by this observation, psychologists have tried presenting problems in uncertain rea- soning and decision making in “evolutionarily appropriate” forms; for example, instead ofsaying “90% survival rate,” the experimenter might show 100 stick-ﬁgure animations of theoperation, where the patient dies in 10 of them and survives in 90.

Token 12587:
(Boredom is a complicat-ing factor in these experiments!)

Token 12588:
With decision problems posed in this way, people seem tobe much closer to rational behavior than previously suspected.

Token 12589:
622 Chapter 16.

Token 12590:
Making Simple Decisions 16.4 M ULTIATTRIBUTE UTILITY FUNCTIONS Decision making in the ﬁeld of public policy involves high stakes, in both money and lives.

Token 12591:
For example, in deciding what levels of harmful emissions to allow from a power plant, pol-icy makers must weigh the prevention of death and disability against the beneﬁt of the powerand the economic burden of mitigating the emissions.

Token 12592:
Siting a new airport requires consid-eration of the disruption caused by construction; the cost of land; the distance from centersof population; the noise of ﬂight operations; safety issues arising from local topography andweather conditions; and so on.

Token 12593:
Problems like these, in which outcomes are characterized by two or more attributes, are handled by multiattribute utility theory .

Token 12594:
MULTIATTRIBUTE UTILITYTHEORY We will call the attributes X=X1,...,X n; a complete vector of assignments will be x=/angbracketleftx1,...,x n/angbracketright, where each xiis either a numeric value or a discrete value with an assumed ordering on values.

Token 12595:
We will assume that higher values of an attribute correspond to higher utilities, all other things being equal.

Token 12596:
For example, if we choose AbsenceOfNoise as an attribute in the airport problem, then the greater its value, the better the solution.8We begin by examining cases in which decisions can be made without combining the attribute values into a single utility value.

Token 12597:
Then we look at cases in which the utilities of attribute combinationscan be speciﬁed very concisely.

Token 12598:
16.4.1 Dominance Suppose that airport site S1costs less, generates less noise pollution, and is safer than site S2.

Token 12599:
One would not hesitate to reject S2. We then say that there is strict dominance ofS1over STRICT DOMINANCE S2.

Token 12600:
In general, if an option is of lower value on all attributes than some other option, it need not be considered further.

Token 12601:
Strict dominance is often very useful in narrowing down the ﬁeldof choices to the real contenders, although it seldom yields a unique choice.

Token 12602:
Figure 16.4(a)shows a schematic diagram for the two-attribute case.

Token 12603:
That is ﬁne for the deterministic case, in which the attribute values are known for sure.

Token 12604:
What about the general case, where the outcomes are uncertain?

Token 12605:
A direct analog of strictdominance can be constructed, where, despite the uncertainty, all possible concrete outcomesforS 1strictly dominate all possible outcomes for S2.

Token 12606:
(See Figure 16.4(b).) Of course, this will probably occur even less often than in the deterministic case.

Token 12607:
Fortunately, there is a more useful generalization called stochastic dominance ,w h i c hSTOCHASTIC DOMINANCE occurs very frequently in real problems.

Token 12608:
Stochastic dominance is easiest to understand in the context of a single attribute.

Token 12609:
Suppose we believe that the cost of siting the airport at S1is uniformly distributed between $2.8 billion and $4.8 billion and that the cost at S2is uniformly distributed between $3 billion and $5.2 billion.

Token 12610:
Figure 16.5(a) shows these distributions, withcost plotted as a negative value.

Token 12611:
Then, given only the information that utility decreases with 8In some cases, it may be necessary to subdivide the range of values so that utility varies monotonically within each range.

Token 12612:
For example, if the RoomTemperature attribute has a utility peak at 70◦F, we would split it into two attributes measuring the difference from the ideal, one colder and one hotter.

Token 12613:
Utility would then be monotonicallyincreasing in each attribute.

Token 12614:
Section 16.4. Multiattribute Utility Functions 623 (a) ABC DAB C (b) This region dominates A X2 X2 X1 X1 Figure 16.4 Strict dominance.

Token 12615:
(a) Deterministic: Option A is strictly dominated by B but not by C or D. (b) Uncertain: A is strictly dominated by B but not by C. 00.10.20.30.40.50.6 -6-5.5 -5-4.5 -4-3.5 -3-2.5 -2Probability Negative costS1 S2 00.20.40.60.811.2 -6-5.5 -5-4.5 -4-3.5 -3-2.5 -2Probability Negative costS1S2 (a) (b) Figure 16.5 Stochastic dominance.

Token 12616:
(a) S1stochastically dominates S2on cost. (b) Cu- mulative distributions for the negative cost of S1andS2.

Token 12617:
cost, we can say that S1stochastically dominates S2(i.e.,S2can be discarded).

Token 12618:
It is important to note that this does notfollow from comparing the expected costs.

Token 12619:
For example, if we knew the cost of S1to be exactly $3.8 billion, then we would be unable to make a decision without additional information on the utility of money.

Token 12620:
(It might seem odd that more information on the cost of S1could make the agent lessable to decide.

Token 12621:
The paradox is resolved by noting that in the absence of exact cost information, the decision is easier to make but is more likelyto be wrong.)

Token 12622:
The exact relationship between the attribute distributions needed to establish stochastic dominance is best seen by examining the cumulative distributions , shown in Figure 16.5(b).

Token 12623:
(See also Appendix A.)

Token 12624:
The cumulative distribution measures the probability that the cost isless than or equal to any given amount—that is, it integrates the original distribution.

Token 12625:
If thecumulative distribution for S 1is always to the right of the cumulative distribution for S2,

Token 12626:
624 Chapter 16. Making Simple Decisions then, stochastically speaking, S1is cheaper than S2.

Token 12627:
Formally, if two actions A1andA2lead to probability distributions p1(x)andp2(x)on attribute X,t h e nA1stochastically dominates A2onXif ∀xx/integraldisplay −∞p1(x/prime)dx/prime≤x/integraldisplay −∞p2(x/prime)dx/prime.

Token 12628:
The relevance of this deﬁnition to the selection of optimal decisions comes from the following property: ifA1stochastically dominates A2, then for any monotonically nondecreasing utility function U(x), the expected utility of A1is at least as high as the expected utility of A2.

Token 12629:
Hence, if an action is stochastically dominated by another action on all attributes, then it can be discarded.

Token 12630:
The stochastic dominance condition might seem rather technical and perhaps not so easy to evaluate without extensive probability calculations.

Token 12631:
In fact, it can be decided veryeasily in many cases.

Token 12632:
Suppose, for example, that the construction transportation cost dependson the distance to the supplier.

Token 12633:
The cost itself is uncertain, but the greater the distance, thegreater the cost. If S 1is closer than S2,t h e n S1will dominate S2on cost.

Token 12634:
Although we will not present them here, there exist algorithms for propagating this kind of qualitativeinformation among uncertain variables in qualitative probabilistic networks , enabling a QUALITATIVE PROBABILISTIC NETWORKSsystem to make rational decisions based on stochastic dominance, without using any numeric values.

Token 12635:
16.4.2 Preference structure and multiattribute utility Suppose we have nattributes, each of which has ddistinct possible values.

Token 12636:
To specify the complete utility function U(x1,...,x n), we need dnvalues in the worst case.

Token 12637:
Now, the worst case corresponds to a situation in which the agent’s preferences have no regularity at all.

Token 12638:
Mul-tiattribute utility theory is based on the supposition that the preferences of typical agents havemuch more structure than that.

Token 12639:
The basic approach is to identify regularities in the preferencebehavior we would expect to see and to use what are called representation theorems to show REPRESENTATION THEOREM that an agent with a certain kind of preference structure has a utility function U(x1,...,x n)=F[f1(x1),...,f n(xn)], where Fis, we hope, a simple function such as addition.

Token 12640:
Notice the similarity to the use of Bayesian networks to decompose the joint probability of several random variables.

Token 12641:
Preferences without uncertainty Let us begin with the deterministic case.

Token 12642:
Remember that for deterministic environments the agent has a value function V(x1,...,x n); the aim is to represent this function concisely.

Token 12643:
The basic regularity that arises in deterministic preference structures is called preference independence .

Token 12644:
Two attributes X1andX2are preferentially independent of a third attributePREFERENCE INDEPENDENCE X3if the preference between outcomes /angbracketleftx1,x2,x3/angbracketrightand/angbracketleftx/prime 1,x/prime2,x3/angbracketrightdoes not depend on the particular value x3for attribute X3.

Token 12645:
Going back to the airport example, where we have (among other attributes) Noise , Cost ,a n dDeaths to consider, one may propose that Noise andCost are preferentially inde-

Token 12646:
Section 16.4. Multiattribute Utility Functions 625 pendent of Deaths .

Token 12647:
For example, if we prefer a state with 20,000 people residing in the ﬂight path and a construction cost of $4 billion over a state with 70,000 people residing in the ﬂightpath and a cost of $3.7 billion when the safety level is 0.06 deaths per million passenger milesin both cases, then we would have the same preference when the safety level is 0.12 or 0.03;and the same independence would hold for preferences between any other pair of values for Noise andCost .

Token 12648:
It is also apparent that Cost andDeaths are preferentially independent of Noise and that Noise andDeaths are preferentially independent of Cost .

Token 12649:
We say that the set of attributes {Noise ,Cost,Deaths}exhibits mutual preferential independence (MPI).

Token 12650:
MUTUAL PREFERENTIALINDEPENDENCE MPI says that, whereas each attribute may be important, it does not affect the way in which one trades off the other attributes against each other.

Token 12651:
Mutual preferential independence is something of a mouthful, but thanks to a remark- able theorem due to the economist G´ erard Debreu (1960), we can derive from it a very simple form for the agent’s value function: If attributes X1, ...,X nare mutually preferentially in- dependent, then the agent’s preference behavior can be described as maximizing the function V(x1,...,x n)=/summationdisplay iVi(xi), where each Viis a value function referring only to the attribute Xi.For example, it might well be the case that the airport decision can be made using a value function V(noise,cost,deaths )=−noise×104−cost−deaths×1012.

Token 12652:
A value function of this type is called an additive value function .

Token 12653:
Additive functions are anADDITIVE VALUE FUNCTION extremely natural way to describe an agent’s preferences and are valid in many real-world situations.

Token 12654:
For nattributes, assessing an additive value function requires assessing nseparate one-dimensional value functions rather than one n-dimensional function; typically, this repre- sents an exponential reduction in the number of preference experiments that are needed.

Token 12655:
Evenwhen MPI does not strictly hold, as might be the case at extreme values of the attributes, anadditive value function might still provide a good approximation to the agent’s preferences.This is especially true when the violations of MPI occur in portions of the attribute rangesthat are unlikely to occur in practice.

Token 12656:
To understand MPI better, it helps to look at cases where it doesn’t hold.

Token 12657:
Suppose you are at a medieval market, considering the purchase of some hunting dogs, some chickens, and some wicker cages for the chickens.

Token 12658:
The hunting dogs are very valuable, but if you don’t have enough cages for the chickens, the dogs will eat the chickens; hence, the tradeoffbetween dogs and chickens depends strongly on the number of cages, and MPI is violated.The existence of these kinds of interactions among various attributes makes it much harder toassess the overall value function.

Token 12659:
Preferences with uncertainty When uncertainty is present in the domain, we also need to consider the structure of prefer- ences between lotteries and to understand the resulting properties of utility functions, rather than just value functions.

Token 12660:
The mathematics of this problem can become quite complicated,so we present just one of the main results to give a ﬂavor of what can be done.

Token 12661:
The reader isreferred to Keeney and Raiffa (1976) for a thorough survey of the ﬁeld.

Token 12662:
626 Chapter 16.

Token 12663:
Making Simple Decisions The basic notion of utility independence extends preference independence to coverUTILITY INDEPENDENCE lotteries: a set of attributes Xis utility independent of a set of attributes Yif preferences be- tween lotteries on the attributes in Xare independent of the particular values of the attributes inY.

Token 12664:
A set of attributes is mutually utility independent (MUI) if each of its subsets isMUTUALLY UTILITY INDEPENDENT utility-independent of the remaining attributes.

Token 12665:
Again, it seems reasonable to propose that the airport attributes are MUI.

Token 12666:
MUI implies that the agent’s behavior can be described using a multiplicative utility function (Keeney, 1974).

Token 12667:
The general form of a multiplicative utility function is best seen byMULTIPLICATIVE UTILITYFUNCTION looking at the case for three attributes.

Token 12668:
For conciseness, we use Uito mean Ui(xi): U=k1U1+k2U2+k3U3+k1k2U1U2+k2k3U2U3+k3k1U3U1 +k1k2k3U1U2U3.

Token 12669:
Although this does not look very simple, it contains just three single-attribute utility functions and three constants.

Token 12670:
In general, an n-attribute problem exhibiting MUI can be modeled using nsingle-attribute utilities and nconstants.

Token 12671:
Each of the single-attribute utility functions can be developed independently of the other attributes, and this combination will be guaranteedto generate the correct overall preferences.

Token 12672:
Additional assumptions are required to obtain a purely additive utility function.

Token 12673:
16.5 D ECISION NETWORKS In this section, we look at a general mechanism for making rational decisions.

Token 12674:
The notation is often called an inﬂuence diagram (Howard and Matheson, 1984), but we will use the INFLUENCEDIAGRAM more descriptive term decision network .

Token 12675:
Decision networks combine Bayesian networks DECISION NETWORK with additional node types for actions and utilities.

Token 12676:
We use airport siting as an example.

Token 12677:
16.5.1 Representing a decision problem with a decision network In its most general form, a decision network represents information about the agent’s current state, its possible actions, the state that will result from the agent’s action, and the utility ofthat state.

Token 12678:
It therefore provides a substrate for implementing utility-based agents of the typeﬁrst introduced in Section 2.4.5.

Token 12679:
Figure 16.6 shows a decision network for the airport sitingproblem.

Token 12680:
It illustrates the three types of nodes used: •Chance nodes (ovals) represent random variables, just as they do in Bayesian networks.

Token 12681:
CHANCENODES The agent could be uncertain about the construction cost, the level of air trafﬁc and the potential for litigation, and the Deaths ,Noise , and total Cost variables, each of which also depends on the site chosen.

Token 12682:
Each chance node has associated with it a conditional distribution that is indexed by the state of the parent nodes.

Token 12683:
In decision networks, the parent nodes can include decision nodes as well as chance nodes.

Token 12684:
Note that each ofthe current-state chance nodes could be part of a large Bayesian network for assessingconstruction costs, air trafﬁc levels, or litigation potentials.

Token 12685:
•Decision nodes (rectangles) represent points where the decision maker has a choice of DECISION NODES

Token 12686:
Section 16.5.

Token 12687:
Decision Networks 627 UAirport Site Deaths Noise CostLitigation ConstructionAir Traffic Figure 16.6 A simple decision network for the airport-siting problem.

Token 12688:
actions. In this case, the AirportSite action can take on a different value for each site under consideration.

Token 12689:
The choice inﬂuences the cost, safety, and noise that will result.In this chapter, we assume that we are dealing with a single decision node.

Token 12690:
Chapter 17 deals with cases in which more than one decision must be made. •Utility nodes (diamonds) represent the agent’s utility function.

Token 12691:
9The utility node has UTILITYNODES as parents all variables describing the outcome that directly affect utility.

Token 12692:
Associated with the utility node is a description of the agent’s utility as a function of the parent attributes.

Token 12693:
The description could be just a tabulation of the function, or it might be a parameterized additive or linear function of the attribute values.

Token 12694:
A simpliﬁed form is also used in many cases. The notation remains identical, but the chance nodes describing the outcome state are omitted.

Token 12695:
Instead, the utility node is connecteddirectly to the current-state nodes and the decision node.

Token 12696:
In this case, rather than representinga utility function on outcome states, the utility node represents the expected utility associated with each action, as deﬁned in Equation (16.1) on page 611; that is, the node is associatedwith an action-utility function (also known as a Q-function in reinforcement learning, as ACTION-UTILITY FUNCTION described in Chapter 21).

Token 12697:
Figure 16.7 shows the action-utility representation of the airport siting problem.

Token 12698:
Notice that, because the Noise ,Deaths ,a n dCost chance nodes in Figure 16.6 refer to future states, they can never have their values set as evidence variables.

Token 12699:
Thus, the simpliﬁedversion that omits these nodes can be used whenever the more general form can be used.Although the simpliﬁed form contains fewer nodes, the omission of an explicit descriptionof the outcome of the siting decision means that it is less ﬂexible with respect to changes incircumstances.

Token 12700:
For example, in Figure 16.6, a change in aircraft noise levels can be reﬂectedby a change in the conditional probability table associated with the Noise node, whereas a change in the weight accorded to noise pollution in the utility function can be reﬂected by 9These nodes are also called value nodes in the literature.

Token 12701:
628 Chapter 16.

Token 12702:
Making Simple Decisions UAirport Site Litigation ConstructionAir Traffic Figure 16.7 A simpliﬁed representation of the airport-siting problem.

Token 12703:
Chance nodes cor- responding to outcome states have been factored out. a change in the utility table.

Token 12704:
In the action-utility diagram, Figure 16.7, on the other hand, all such changes have to be reﬂected by changes to the action-utility table.

Token 12705:
Essentially, theaction-utility formulation is a compiled version of the original formulation.

Token 12706:
16.5.2 Evaluating decision networks Actions are selected by evaluating the decision network for each possible setting of the deci- sion node.

Token 12707:
Once the decision node is set, it behaves exactly like a chance node that has beenset as an evidence variable.

Token 12708:
The algorithm for evaluating decision networks is the following: 1. Set the evidence variables for the current state. 2.

Token 12709:
For each possible value of the decision node: (a) Set the decision node to that value.

Token 12710:
(b) Calculate the posterior probabilities for the parent nodes of the utility node, using a standard probabilistic inference algorithm.

Token 12711:
(c) Calculate the resulting utility for the action. 3. Return the action with the highest utility.

Token 12712:
This is a straightforward extension of the Bayesian network algorithm and can be incorpo- rated directly into the agent design given in Figure 13.1 on page 484.

Token 12713:
We will see in Chap-ter 17 that the possibility of executing several actions in sequence makes the problem muchmore interesting.

Token 12714:
16.6 T HEVALUE OF INFORMATION In the preceding analysis, we have assumed that all relevant information, or at least all avail-able information, is provided to the agent before it makes its decision.

Token 12715:
In practice, this is

Token 12716:
Section 16.6. The Value of Information 629 hardly ever the case. One of the most important parts of decision making is knowing what questions to ask.

Token 12717:
For example, a doctor cannot expect to be provided with the results of all possible diagnostic tests and questions at the time a patient ﬁrst enters the consulting room.10 Tests are often expensive and sometimes hazardous (both directly and because of associated delays).

Token 12718:
Their importance depends on two factors: whether the test results would lead to a signiﬁcantly better treatment plan, and how likely the various test results are.

Token 12719:
This section describes information value theory , which enables an agent to chooseINFORMATION VALUE THEORY what information to acquire.

Token 12720:
We assume that, prior to selecting a “real” action represented by the decision node, the agent can acquire the value of any of the potentially observablechance variables in the model.

Token 12721:
Thus, information value theory involves a simpliﬁed formof sequential decision making—simpliﬁed because the observation actions affect only theagent’s belief state , not the external physical state.

Token 12722:
The value of any particular observation must derive from the potential to affect the agent’s eventual physical action; and this potentialcan be estimated directly from the decision model itself.

Token 12723:
16.6.1 A simple example Suppose an oil company is hoping to buy one of nindistinguishable blocks of ocean-drilling rights.

Token 12724:
Let us assume further that exactly one of the blocks contains oil worth Cdollars, while the others are worthless.

Token 12725:
The asking price of each block is C/n dollars. If the company is risk-neutral, then it will be indifferent between buying a block and not buying one.

Token 12726:
Now suppose that a seismologist offers the company the results of a survey of block number 3, which indicates deﬁnitively whether the block contains oil.

Token 12727:
How much shouldthe company be willing to pay for the information?

Token 12728:
The way to answer this question is toexamine what the company would do if it had the information: •With probability 1/n, the survey will indicate oil in block 3.

Token 12729:
In this case, the company will buy block 3 for C/n dollars and make a proﬁt of C−C/n=(n−1)C/n dollars.

Token 12730:
•With probability (n−1)/n, the survey will show that the block contains no oil, in which case the company will buy a different block.

Token 12731:
Now the probability of ﬁnding oil in oneof the other blocks changes from 1/nto1/(n−1), so the company makes an expected proﬁt of C/(n−1)−C/n=C/n(n−1)dollars.

Token 12732:
Now we can calculate the expected proﬁt, given the survey information: 1 n×(n−1)C n+n−1 n×C n(n−1)=C/n .

Token 12733:
Therefore, the company should be willing to pay the seismologist up to C/n dollars for the information: the information is worth as much as the block itself.

Token 12734:
The value of information derives from the fact that with the information, one’s course of action can be changed to suit the actual situation.

Token 12735:
One can discriminate according to the situation, whereas without the information, one has to do what’s best on average over thepossible situations.

Token 12736:
In general, the value of a given piece of information is deﬁned to be thedifference in expected value between best actions before and after information is obtained.

Token 12737:
10In the United States, the only question that is always asked beforehand is whether the patient has insurance.

Token 12738:
630 Chapter 16.

Token 12739:
Making Simple Decisions 16.6.2 A general formula for perfect information It is simple to derive a general mathematical formula for the value of information.

Token 12740:
We assume that exact evidence can be obtained about the value of some random variable Ej(that is, we learnEj=ej), so the phrase value of perfect information (VPI) is used.11 VALUE OF PERFECT INFORMATION Let the agent’s initial evidence be e. Then the value of the current best action αis deﬁned by EU(α|e)=m a x a/summationdisplay s/primeP(RESULT (a)=s/prime|a,e)U(s/prime), and the value of the new best action (after the new evidence Ej=ejis obtained) will be EU(αej|e,ej)=m a x a/summationdisplay s/primeP(RESULT (a)=s/prime|a,e,ej)U(s/prime).

Token 12741:
ButEjis a random variable whose value is currently unknown, so to determine the value of discovering Ej, given current information ewe must average over all possible values ejkthat we might discover for Ej, using our current beliefs about its value: VPI e(Ej)=/parenleftBigg/summationdisplay kP(Ej=ejk|e)EU(αejk|e,Ej=ejk)/parenrightBigg −EU(α|e).

Token 12742:
To get some intuition for this formula, consider the simple case where there are only two actions, a1anda2, from which to choose.

Token 12743:
Their current expected utilities are U1andU2.T h e information Ej=ejkwill yield some new expected utilities U/prime 1andU/prime 2for the actions, but before we obtain Ej, we will have some probability distributions over the possible values of U/prime 1andU/prime 2(which we assume are independent).

Token 12744:
Suppose that a1anda2represent two different routes through a mountain range in winter.

Token 12745:
a1is a nice, straight highway through a low pass, and a2is a winding dirt road over the top.

Token 12746:
Just given this information, a1is clearly preferable, because it is quite possible that a2is blocked by avalanches, whereas it is unlikely that anything blocks a1.U1is therefore clearly higher than U2.

Token 12747:
It is possible to obtain satellite reports Ejon the actual state of each road that would give new expectations, U/prime 1andU/prime 2, for the two crossings.

Token 12748:
The distributions for these expectations are shown in Figure 16.8(a).

Token 12749:
Obviously, in this case, it is not worth theexpense of obtaining satellite reports, because it is unlikely that the information derived fromthem will change the plan.

Token 12750:
With no change, information has no value.

Token 12751:
Now suppose that we are choosing between two different winding dirt roads of slightly different lengths and we are carrying a seriously injured passenger.

Token 12752:
Then, even when U 1 andU2are quite close, the distributions of U/prime 1andU/prime 2are very broad.

Token 12753:
There is a signiﬁcant possibility that the second route will turn out to be clear while the ﬁrst is blocked, and in this 11There is no loss of expressiveness in requiring perf ect information.

Token 12754:
Suppose we wanted to model the case in which we become somewhat more certain about a variable.

Token 12755:
We can do that by introducing another variable about which we learn perfect information.

Token 12756:
For example, s uppose we initially have broad uncertainty about the variable Temperature .

Token 12757:
Then we gain the perfect knowledge Thermometer =3 7 ; this gives us imperfect information about the true Temperature , and the uncertainty due to measurement error is encoded in the sensor model P(Thermometer |Temperature ).

Token 12758:
See Exercise 16.17 for another example.

Token 12759:
Section 16.6.

Token 12760:
The Value of Information 631 (c)P(U | Ej) U1U2U (b)P(U | Ej) U1U2U (a)P(U | Ej) U1 U2U Figure 16.8 Three generic cases for the value of information.

Token 12761:
In (a), a1will almost cer- tainly remain superior to a2, so the information is not needed.

Token 12762:
In (b), the choice is unclear and the information is crucial.

Token 12763:
In (c), the choice is unclear, but because it makes little difference, the information is less valuable.

Token 12764:
(Note: The fact that U2has a high peak in (c) means that its expected value is known with higher certainty than U1.)

Token 12765:
case the difference in utilities will be very high. The VPI formula indicates that it might be worthwhile getting the satellite reports.

Token 12766:
Such a situation is shown in Figure 16.8(b).

Token 12767:
Finally, suppose that we are choosing between the two dirt roads in summertime, when blockage by avalanches is unlikely.

Token 12768:
In this case, satellite reports might show one route to be more scenic than the other because of ﬂowering alpine meadows, or perhaps wetter becauseof errant streams.

Token 12769:
It is therefore quite likely that we would change our plan if we had theinformation.

Token 12770:
In this case, however, the difference in value between the two routes is stilllikely to be very small, so we will not bother to obtain the reports.

Token 12771:
This situation is shown inFigure 16.8(c).

Token 12772:
In sum, information has value to the extent that it is likely to cause a change of plan and to the extent that the new plan will be signiﬁcantly better than the old plan.

Token 12773:
16.6.3 Properties of the value of information One might ask whether it is possible for information to be deleterious: can it actually have negative expected value?

Token 12774:
Intuitively, one should expect this to be impossible.

Token 12775:
After all, onecould in the worst case just ignore the information and pretend that one has never received it.This is conﬁrmed by the following theorem, which applies to any decision-theoretic agent: The expected value of information is nonnegative: ∀e,EjVPI e(Ej)≥0.

Token 12776:
The theorem follows directly from the deﬁnition of VPI, and we leave the proof as an exercise (Exercise 16.18).

Token 12777:
It is, of course, a theorem about expected value, not actual value.

Token 12778:
Additional information can easily lead to a plan that turns out to be worse than the original plan if the information happens to be misleading.

Token 12779:
For example, a medical test that gives a false positiveresult may lead to unnecessary surgery; but that does not mean that the test shouldn’t be done.

Token 12780:
632 Chapter 16.

Token 12781:
Making Simple Decisions It is important to remember that VPI depends on the current state of information, which is why it is subscripted.

Token 12782:
It can change as more information is acquired.

Token 12783:
For any given pieceof evidence E j, the value of acquiring it can go down (e.g., if another variable strongly constrains the posterior for Ej) or up (e.g., if another variable provides a clue on which Ej builds, enabling a new and better plan to be devised).

Token 12784:
Thus, VPI is not additive. That is, VPI e(Ej,Ek)/negationslash=VPI e(Ej)+VPI e(Ek) (in general) . VPI is, however, order independent.

Token 12785:
That is, VPI e(Ej,Ek)=VPI e(Ej)+VPI e,ej(Ek)=VPI e(Ek)+VPI e,ek(Ej).

Token 12786:
Order independence distinguishes sensing actions from ordinary actions and simpliﬁes the problem of calculating the value of a sequence of sensing actions.

Token 12787:
16.6.4 Implementation of an information-gathering agent A sensible agent should ask questions in a reasonable order, should avoid asking questionsthat are irrelevant, should take into account the importance of each piece of information inrelation to its cost, and should stop asking questions when that is appropriate.

Token 12788:
All of thesecapabilities can be achieved by using the value of information as a guide.

Token 12789:
Figure 16.9 shows the overall design of an agent that can gather information intel- ligently before acting.

Token 12790:
For now, we assume that with each observable evidence variableE j, there is an associated cost, Cost(Ej), which reﬂects the cost of obtaining the evidence through tests, consultants, questions, or whatever.

Token 12791:
The agent requests what appears to be themost efﬁcient observation in terms of utility gain per unit cost.

Token 12792:
We assume that the result ofthe action Request (E j)is that the next percept provides the value of Ej.

Token 12793:
If no observation is worth its cost, the agent selects a “real” action.

Token 12794:
The agent algorithm we have described implements a form of information gathering that is called myopic .

Token 12795:
This is because it uses the VPI formula shortsightedly, calculating the MYOPIC value of information as if only a single evidence variable will be acquired.

Token 12796:
Myopic control is based on the same heuristic idea as greedy search and often works well in practice.

Token 12797:
(Forexample, it has been shown to outperform expert physicians in selecting diagnostic tests.)

Token 12798:
function INFORMATION -GATHERING -AGENT (percept )returns anaction persistent :D, a decision network integrate percept intoD j←the value that maximizes VPI(Ej)/Cost(Ej) ifVPI(Ej)>Cost(Ej) return REQUEST (Ej) else return the best action from D Figure 16.9 Design of a simple information-gathering agent.

Token 12799:
The agent works by repeat- edly selecting the observation with the highest information value, until the cost of the next observation is greater than its expected beneﬁt.

Token 12800:
Section 16.7.

Token 12801:
Decision-Theoretic Expert Systems 633 However, if there is no single evidence variable that will help a lot, a myopic agent might hastily take an action when it would have been better to request two or more variables ﬁrstand then take action.

Token 12802:
A better approach in this situation would be to construct a conditional plan (as described in Section 11.3.2) that asks for variable values and takes different next steps depending on the answer.

Token 12803:
One ﬁnal consideration is the effect a series of questions will have on a human respon- dent.

Token 12804:
People may respond better to a series of questions if they “make sense,” so some expertsystems are built to take this into account, asking questions in an order that maximizes thetotal utility of the system and human rather than an order that maximizes value of information.

Token 12805:
16.7 D ECISION -THEORETIC EXPERT SYSTEMS The ﬁeld of decision analysis , which evolved in the 1950s and 1960s, studies the application DECISION ANALYSIS of decision theory to actual decision problems.

Token 12806:
It is used to help make rational decisions in important domains where the stakes are high, such as business, government, law, military strategy, medical diagnosis and public health, engineering design, and resource management.

Token 12807:
The process involves a careful study of the possible actions and outcomes, as well as thepreferences placed on each outcome.

Token 12808:
It is traditional in decision analysis to talk about tworoles: the decision maker states preferences between outcomes, and the decision analyst DECISION MAKER DECISION ANALYST enumerates the possible actions and outcomes and elicits preferences from the decision maker to determine the best course of action.

Token 12809:
Until the early 1980s, the main purpose of decisionanalysis was to help humans make decisions that actually reﬂect their own preferences.

Token 12810:
Asmore and more decision processes become automated, decision analysis is increasingly usedto ensure that the automated processes are behaving as desired.

Token 12811:
Early expert system research concentrated on answering questions, rather than on mak- ing decisions.

Token 12812:
Those systems that did recommend actions rather than providing opinions on matters of fact generally did so using condition-action rules, rather than with explicit rep- resentations of outcomes and preferences.

Token 12813:
The emergence of Bayesian networks in the late1980s made it possible to build large-scale systems that generated sound probabilistic infer-ences from evidence.

Token 12814:
The addition of decision networks means that expert systems can bedeveloped that recommend optimal decisions, reﬂecting the preferences of the agent as wellas the available evidence.

Token 12815:
A system that incorporates utilities can avoid one of the most common pitfalls associ- ated with the consultation process: confusing likelihood and importance.

Token 12816:
A common strategyin early medical expert systems, for example, was to rank possible diagnoses in order of like-lihood and report the most likely.

Token 12817:
Unfortunately, this can be disastrous!

Token 12818:
For the majority ofpatients in general practice, the two most likely diagnoses are usually “There’s nothing wrong with you” and “You have a bad cold,” but if the third most likely diagnosis for a given patient is lung cancer, that’s a serious matter.

Token 12819:
Obviously, a testing or treatment plan should dependboth on probabilities and utilities.

Token 12820:
Current medical expert systems can take into account thevalue of information to recommend tests, and then describe a differential diagnosis.

Token 12821:
634 Chapter 16. Making Simple Decisions We now describe the knowledge engineering process for decision-theoretic expert sys- tems.

Token 12822:
As an example we consider the problem of selecting a medical treatment for a kind ofcongenital heart disease in children (see Lucas, 1996).

Token 12823:
About 0.8% of children are born with a heart anomaly, the most common being aortic coarctation (a constriction of the aorta).

Token 12824:
It can be treated with surgery, angioplasty (expand- AORTIC COARCTATION ing the aorta with a balloon placed inside the artery), or medication.

Token 12825:
The problem is to decide what treatment to use and when to do it: the younger the infant, the greater the risks of certaintreatments, but one mustn’t wait too long.

Token 12826:
A decision-theoretic expert system for this problemcan be created by a team consisting of at least one domain expert (a pediatric cardiologist)and one knowledge engineer.

Token 12827:
The process can be broken down into the following steps: Create a causal model . Determine the possible symptoms, disorders, treatments, and outcomes.

Token 12828:
Then draw arcs between them, indicating what disorders cause what symptoms,and what treatments alleviate what disorders.

Token 12829:
Some of this will be well known to the domainexpert, and some will come from the literature.

Token 12830:
Often the model will match well with theinformal graphical descriptions given in medical textbooks. Simplify to a qualitative decision model .

Token 12831:
Since we are using the model to make treatment decisions and not for other purposes (such as determining the joint probability of certain symptom/disorder combinations), we can often simplify by removing variables that are not involved in treatment decisions.

Token 12832:
Sometimes variables will have to be split or joinedto match the expert’s intuitions.

Token 12833:
For example, the original aortic coarctation model had aTreatment variable with values surgery ,angioplasty ,a n d medication , and a separate variable forTiming of the treatment.

Token 12834:
But the expert had a hard time thinking of these separately, so they were combined, with Treatment taking on values such as surgery in 1 month .T h i sg i v e s us the model of Figure 16.10.

Token 12835:
Assign probabilities . Probabilities can come from patient databases, literature studies, or the expert’s subjective assessments.

Token 12836:
Note that a diagnostic system will reason from symp-toms and other observations to the disease or other cause of the problems.

Token 12837:
Thus, in the earlyyears of building these systems, experts were asked for the probability of a cause given an effect.

Token 12838:
In general they found this difﬁcult to do, and were better able to assess the probability of an effect given a cause.

Token 12839:
So modern systems usually assess causal knowledge and encode itdirectly in the Bayesian network structure of the model, leaving the diagnostic reasoning tothe Bayesian network inference algorithms (Shachter and Heckerman, 1987).

Token 12840:
Assign utilities .

Token 12841:
When there are a small number of possible outcomes, they can be enumerated and evaluated individually using the methods of Section 16.3.1.

Token 12842:
We would createa scale from best to worst outcome and give each a numeric value, for example 0 for deathand 1 for complete recovery.

Token 12843:
We would then place the other outcomes on this scale.

Token 12844:
Thiscan be done by the expert, but it is better if the patient (or in the case of infants, the patient’sparents) can be involved, because different people have different preferences.

Token 12845:
If there are ex-ponentially many outcomes, we need some way to combine them using multiattribute utility functions.

Token 12846:
For example, we may say that the costs of various complications are additive. Verify and reﬁne the model .

Token 12847:
To evaluate the system we need a set of correct (input, output) pairs; a so-called gold standard to compare against.

Token 12848:
For medical expert systems GOLD STANDARD this usually means assembling the best available doctors, presenting them with a few cases,

Token 12849:
Section 16.7.

Token 12850:
Decision-Theoretic Expert Systems 635 Tachypnea Dyspnea Heart FailureAgeTachycardia Failure To Thrive Intercostal Recession Hepato- megaly Pulmonary Crepitations CardiomegalyTreatmentIntermediate ResultLate ResultParaplegiaAortic AneurysmParadoxical HypertensionPostcoarctectomy SyndromeSex CVA Aortic Dissection Myocardial Infarction U Figure 16.10 Inﬂuence diagram for aortic coarct ation (courtesy of Peter Lucas).

Token 12851:
and asking them for their diagnosis and recommended treatment plan. We then see how well the system matches their recommendations.

Token 12852:
If it does poorly, we try to isolate the partsthat are going wrong and ﬁx them.

Token 12853:
It can be useful to run the system “backward.” Insteadof presenting the system with symptoms and asking for a diagnosis, we can present it witha diagnosis such as “heart failure,” examine the predicted probability of symptoms such astachycardia, and compare with the medical literature.

Token 12854:
Perform sensitivity analysis .

Token 12855:
This important step checks whether the best decision is SENSITIVITY ANALYSIS sensitive to small changes in the assigned probabilities and utilities by systematically varying those parameters and running the evaluation again.

Token 12856:
If small changes lead to signiﬁcantly different decisions, then it could be worthwhile to spend more resources to collect betterdata.

Token 12857:
If all variations lead to the same decision, then the agent will have more conﬁdence thatit is the right decision.

Token 12858:
Sensitivity analysis is particularly important, because one of the main

Token 12859:
636 Chapter 16.

Token 12860:
Making Simple Decisions criticisms of probabilistic approaches to expert systems is that it is too difﬁcult to assess the numerical probabilities required.

Token 12861:
Sensitivity analysis often reveals that many of the numbersneed be speciﬁed only very approximately.

Token 12862:
For example, we might be uncertain about theconditional probability P(tachycardia|dyspnea ), but if the optimal decision is reasonably robust to small variations in the probability, then our ignorance is less of a concern.

Token 12863:
16.8 S UMMARY This chapter shows how to combine utility theory with probability to enable an agent to selectactions that will maximize its expected performance.

Token 12864:
•Probability theory describes what an agent should believe on the basis of evidence, utility theory describes what an agent wants, and decision theory puts the two together to describe what an agent should do.

Token 12865:
•We can use decision theory to build a system that makes decisions by considering all possible actions and choosing the one that leads to the best expected outcome.

Token 12866:
Such asystem is known as a rational agent .

Token 12867:
•Utility theory shows that an agent whose preferences between lotteries are consistent with a set of simple axioms can be described as possessing a utility function; further- more, the agent selects actions as if maximizing its expected utility.

Token 12868:
•Multiattribute utility theory deals with utilities that depend on several distinct at- tributes of states.

Token 12869:
Stochastic dominance is a particularly useful technique for making unambiguous decisions, even without precise utility values for attributes.

Token 12870:
•Decision networks provide a simple formalism for expressing and solving decision problems.

Token 12871:
They are a natural extension of Bayesian networks, containing decision andutility nodes in addition to chance nodes.

Token 12872:
•Sometimes, solving a problem involves ﬁnding more information before making a de- cision.

Token 12873:
The value of information is deﬁned as the expected improvement in utility compared with making a decision without the information.

Token 12874:
•Expert systems that incorporate utility information have additional capabilities com- pared with pure inference systems.

Token 12875:
In addition to being able to make decisions, theycan use the value of information to decide which questions to ask, if any; they can rec-ommend contingency plans; and they can calculate the sensitivity of their decisions tosmall changes in probability and utility assessments.

Token 12876:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The book L’art de Penser , also known as the Port-Royal Logic (Arnauld, 1662) states: To judge what one must do to obtain a good or avoid an evil, it is necessary to consider not only the good and the evil in itself, but also the probability that it happens or does not happen; and to view geometrically the proportion that all these things have together.

Token 12877:


Token 12878:
Bibliographical and Historical Notes 637 Modern texts talk of utility rather than good and evil, but this statement correctly notes that one should multiply utility by probability (“view geometrically”) to give expected utility,and maximize that over all outcomes (“all these things”) to “judge what one must do.” Itis remarkable how much this got right, 350 years ago, and only 8 years after Pascal andFermat showed how to use probability correctly.

Token 12879:
The Port-Royal Logic also marked the ﬁrst publication of Pascal’s wager.

Token 12880:
Daniel Bernoulli (1738), investigating the St. Petersburg paradox (see Exercise 16.3), was the ﬁrst to realize the importance of preference measurement for lotteries, writing “thevalue of an item must not be based on its price , but rather on the utility that it yields” (ital- ics his).

Token 12881:
Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for weighing “pleasures” and “pains,” arguing that all decisions (not just monetary ones) couldbe reduced to utility comparisons.

Token 12882:
The derivation of numerical utilities from preferences was ﬁrst carried out by Ram- sey (1931); the axioms for preference in the present text are closer in form to those rediscov-ered in Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944).

Token 12883:
A good presentation of these axioms, in the course of a discussion on risk preference, is givenby Howard (1977).

Token 12884:
Ramsey had derived subjective probabilities (not just utilities) from an agent’s preferences; Savage (1954) and Jeffrey (1983) carry out more recent constructions of this kind.

Token 12885:
Von Winterfeldt and Edwards (1986) provide a modern perspective on decisionanalysis and its relationship to human preference structures.

Token 12886:
The micromort utility measureis discussed by Howard (1989).

Token 12887:
A 1994 survey by the Economist set the value of a life at between $750,000 and $2.6 million.

Token 12888:
However, Richard Thaler (1992) found irrational fram- ing effects on the price one is willing to pay to avoid a risk of death versus the price one is willing to be paid to accept a risk.

Token 12889:
For a 1/1000 chance, a respondent wouldn’t pay morethan $200 to remove the risk, but wouldn’t accept $50,000 to take on the risk.

Token 12890:
How much arepeople willing to pay for a QALY?

Token 12891:
When it comes down to a speciﬁc case of saving oneselfor a family member, the number is approximately “whatever I’ve got.” But we can ask at asocietal level: suppose there is a vaccine that would yield XQALYs but costs Ydollars; is it worth it?

Token 12892:
In this case people report a wide range of values from around $10,000 to $150,000 per QALY (Prades et al. , 2008).

Token 12893:
QALYs are much more widely used in medical and social policy decision making than are micromorts; see (Russell, 1990) for a typical example of anargument for a major change in public health policy on grounds of increased expected utilitymeasured in QALYs.

Token 12894:
The optimizer’s curse was brought to the attention of decision analysts in a forceful way by Smith and Winkler (2006), who pointed out that the ﬁnancial beneﬁts to the clientprojected by analysts for their proposed course of action almost never materialized.

Token 12895:
Theytrace this directly to the bias introduced by selecting an optimal action and show that a morecomplete Bayesian analysis eliminates the problem.

Token 12896:
The same underlying concept has beencalled post-decision disappointment by Harrison and March (1984) and was noted in the POST-DECISION DISAPPOINTMENT context of analyzing capital investment projects by Brown (1974).

Token 12897:
The optimizer’s curse is also closely related to the winner’s curse (Capen et al.

Token 12898:
, 1971; Thaler, 1992), which applies WINNER’S CURSE to competitive bidding in auctions: whoever wins the auction is very likely to have overes- timated the value of the object in question.

Token 12899:
Capen et al. quote a petroleum engineer on the

Token 12900:
638 Chapter 16.

Token 12901:
Making Simple Decisions topic of bidding for oil-drilling rights: “If one wins a tract against two or three others he may feel ﬁne about his good fortune.

Token 12902:
But how should he feel if he won against 50 others?

Token 12903:
Ill.”Finally, behind both curses is the general phenomenon of regression to the mean , whereby REGRESSION TO THE MEAN individuals selected on the basis of exceptional characteristics previously exhibited will, with high probability, become less exceptional in future.

Token 12904:
The Allais paradox, due to Nobel Prize-winning economist Maurice Allais (1953) was tested experimentally (Tversky and Kahneman, 1982; Conlisk, 1989) to show that peopleare consistently inconsistent in their judgments.

Token 12905:
The Ellsberg paradox on ambiguity aver-sion was introduced in the Ph.D. thesis of Daniel Ellsberg (Ellsberg, 1962), who went on tobecome a military analyst at the RAND Corporation and to leak documents known as ThePentagon Papers, which contributed to the end of the Vietnam war and the resignation ofPresident Nixon.

Token 12906:
Fox and Tversky (1995) describe a further study of ambiguity aversion.Mark Machina (2005) gives an overview of choice under uncertainty and how it can varyfrom expected utility theory.

Token 12907:
There has been a recent outpouring of more-or-less popular books on human irrational- ity.

Token 12908:
The best known is Predictably Irrational (Ariely, 2009); others include Sway (Brafman and Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We Decide (Lehrer, 2009) and On Being Certain (Burton, 2009).

Token 12909:
They complement the classic (Kahneman et al. , 1982) and the article that started it all (Kahneman and Tversky, 1979).

Token 12910:
The ﬁeld of evolutionary psychology (Buss, 2005), on the other hand, has run counter to this literature, arguing that humans are quite rational in evolutionarily appropriate contexts.

Token 12911:
Its adherents point out that irrationality is penalized by deﬁnition in an evolutionary context and show that in some cases it is an artifact of the experimental setup (Cummins and Allen, 1998).

Token 12912:
There has been a recent resurgence of interest in Bayesian models of cognition, overturningdecades of pessimism (Oaksford and Chater, 1998; Elio, 2002; Chater and Oaksford, 2008).

Token 12913:
Keeney and Raiffa (1976) give a thorough introduction to multiattribute utility the- ory.

Token 12914:
They describe early computer implementations of methods for eliciting the necessaryparameters for a multiattribute utility function and include extensive accounts of real appli- cations of the theory.

Token 12915:
In AI, the principal reference for MAUT is Wellman’s (1985) paper, which includes a system called URP (Utility Reasoning Package) that can use a collectionof statements about preference independence and conditional independence to analyze thestructure of decision problems.

Token 12916:
The use of stochastic dominance together with qualitativeprobability models was investigated extensively by Wellman (1988, 1990a).

Token 12917:
Wellman andDoyle (1992) provide a preliminary sketch of how a complex set of utility-independence re-lationships might be used to provide a structured model of a utility function, in much thesame way that Bayesian networks provide a structured model of joint probability distribu-tions.

Token 12918:
Bacchus and Grove (1995, 1996) and La Mura and Shoham (1999) give further resultsalong these lines.

Token 12919:
Decision theory has been a standard tool in economics, ﬁnance, and management sci- ence since the 1950s.

Token 12920:
Until the 1980s, decision trees were the main tool used for representing simple decision problems.

Token 12921:
Smith (1988) gives an overview of the methodology of deci-sion analysis.

Token 12922:
Inﬂuence diagrams were introduced by Howard and Matheson (1984), basedon earlier work at SRI (Miller et al. , 1976).

Token 12923:
Howard and Matheson’s method involved the

Token 12924:
Bibliographical and Historical Notes 639 derivation of a decision tree from a decision network, but in general the tree is of exponential size.

Token 12925:
Shachter (1986) developed a method for making decisions based directly on a decisionnetwork, without the creation of an intermediate decision tree.

Token 12926:
This algorithm was also oneof the ﬁrst to provide complete inference for multiply connected Bayesian networks. Zhanget al.

Token 12927:
(1994) showed how to take advantage of conditional independence of information to re- duce the size of trees in practice; they use the term decision network for networks that use this approach (although others use it as a synonym for inﬂuence diagram).

Token 12928:
Nilsson and Lauritzen(2000) link algorithms for decision networks to ongoing developments in clustering algo-rithms for Bayesian networks.

Token 12929:
Koller and Milch (2003) show how inﬂuence diagrams can beused to solve games that involve gathering information by opposing players, and Detwarasitiand Shachter (2005) show how inﬂuence diagrams can be used as an aid to decision makingfor a team that shares goals but is unable to share all information perfectly.

Token 12930:
The collectionby Oliver and Smith (1990) has a number of useful articles on decision networks, as does the1990 special issue of the journal Networks .

Token 12931:
Papers on decision networks and utility modeling also appear regularly in the journals Management Science andDecision Analysis .

Token 12932:
The theory of information value was explored ﬁrst in the context of statistical experi- ments, where a quasi-utility (entropy reduction) was used (Lindley, 1956).

Token 12933:
The Russian con- trol theorist Ruslan Stratonovich (1965) developed the more general theory presented here, in which information has value by virtue of its ability to affect decisions.

Token 12934:
Stratonovich’s workwas not known in the West, where Ron Howard (1966) pioneered the same idea.

Token 12935:
His paperends with the remark “If information value theory and associated decision theoretic structuresdo not in the future occupy a large part of the education of engineers, then the engineeringprofession will ﬁnd that its traditional role of managing scientiﬁc and economic resources forthe beneﬁt of man has been forfeited to another profession.” To date, the implied revolutionin managerial methods has not occurred.

Token 12936:
Recent work by Krause and Guestrin (2009) shows that computing the exact non- myopic value of information is intractable even in polytree networks.

Token 12937:
There are other cases—more restricted than general value of information—in which the myopic algorithm does pro- vide a provably good approximation to the optimal sequence of observations (Krause et al.

Token 12938:
, 2008).

Token 12939:
In some cases—for example, looking for treasure buried in one of nplaces—ranking experiments in order of success probability divided by cost gives an optimal solution (Kadaneand Simon, 1977).

Token 12940:
Surprisingly few early AI researchers adopted decision-theoretic tools after the early applications in medical decision making described in Chapter 13.

Token 12941:
One of the few exceptionswas Jerry Feldman, who applied decision theory to problems in vision (Feldman and Yaki-movsky, 1974) and planning (Feldman and Sproull, 1977).

Token 12942:
After the resurgence of interest inprobabilistic methods in AI in the 1980s, decision-theoretic expert systems gained widespreadacceptance (Horvitz et al.

Token 12943:
, 1988; Cowell et al. , 2002).

Token 12944:
In fact, from 1991 onward, the cover design of the journal Artiﬁcial Intelligence has depicted a decision network, although some artistic license appears to have been taken with the direction of the arrows.

Token 12945:
640 Chapter 16. Making Simple Decisions EXERCISES 16.1 (Adapted from David Heckerman.)

Token 12946:
This exercise concerns the Almanac Game ,w h i c h is used by decision analysts to calibrate numeric estimation.

Token 12947:
For each of the questions thatfollow, give your best guess of the answer, that is, a number that you think is as likely to betoo high as it is to be too low.

Token 12948:
Also give your guess at a 25th percentile estimate, that is, anumber that you think has a 25% chance of being too high, and a 75% chance of being toolow.

Token 12949:
Do the same for the 75th percentile. (Thus, you should give three estimates in all—low,median, and high—for each question.) a.

Token 12950:
Number of passengers who ﬂew between New York and Los Angeles in 1989. b.

Token 12951:
Population of Warsaw in 1992. c. Year in which Coronado discovered the Mississippi River.

Token 12952:
d. Number of votes received by Jimmy Carter in the 1976 presidential election.

Token 12953:
e. Age of the oldest living tree, as of 2002. f. Height of the Hoover Dam in feet.

Token 12954:
g. Number of eggs produced in Oregon in 1985. h. Number of Buddhists in the world in 1992. i.

Token 12955:
Number of deaths due to AIDS in the United States in 1981. j. Number of U.S. patents granted in 1901.

Token 12956:
The correct answers appear after the last exercise of this chapter.

Token 12957:
From the point of view of decision analysis, the interesting thing is not how close your median guesses came to the real answers, but rather how often the real answer came within your 25% and 75% bounds.

Token 12958:
If it was about half the time, then your bounds are accurate.

Token 12959:
But if you’re like most people, you will be more sure of yourself than you should be, and fewer than half the answers will fall within the bounds.

Token 12960:
With practice, you can calibrate yourself to give realistic bounds, and thusbe more useful in supplying information for decision making.

Token 12961:
Try this second set of questionsand see if there is any improvement: a. Year of birth of Zsa Zsa Gabor. b.

Token 12962:
Maximum distance from Mars to the sun in miles.

Token 12963:
c. Value in dollars of exports of wheat from the United States in 1992. d. Tons handled by the port of Honolulu in 1991. e. Annual salary in dollars of the governor of California in 1993. f. Population of San Diego in 1990. g. Year in which Roger Williams founded Providence, Rhode Island.

Token 12964:
h. Height of Mt. Kilimanjaro in feet. i. Length of the Brooklyn Bridge in feet. j.

Token 12965:
Number of deaths due to automobile accidents in the United States in 1992.

Token 12966:
Exercises 641 16.2 Chris considers four used cars before buying the one with maximum expected utility. Pat considers ten cars and does the same.

Token 12967:
All other things being equal, which one is morelikely to have the better car?

Token 12968:
Which is more likely to be disappointed with their car’s quality?By how much (in terms of standard deviations of expected quality)?

Token 12969:
16.3 In 1713, Nicolas Bernoulli stated a puzzle, now called the St. Petersburg paradox, which works as follows.

Token 12970:
You have the opportunity to play a game in which a fair coin istossed repeatedly until it comes up heads.

Token 12971:
If the ﬁrst heads appears on the nth toss, you win 2 ndollars. a. Show that the expected monetary value of this game is inﬁnite. b.

Token 12972:
How much would you, personally, pay to play the game?

Token 12973:
c. Nicolas’s cousin Daniel Bernoulli resolved the apparent paradox in 1738 by suggesting that the utility of money is measured on a logarithmic scale (i.e., U(Sn)=alog2n+b, where Snis the state of having $ n).

Token 12974:
What is the expected utility of the game under this assumption?

Token 12975:
d. What is the maximum amount that it would be rational to pay to play the game, assum- ing that one’s initial wealth is $ k?

Token 12976:
16.4 Write a computer program to automate the process in Exercise 16.9.

Token 12977:
Try your pro- gram out on several people of different net worth and political outlook.

Token 12978:
Comment on the consistency of your results, both for an individual and across individuals.

Token 12979:
16.5 The Surprise Candy Company makes candy in two ﬂavors: 70% are strawberry ﬂa- vor and 30% are anchovy ﬂavor.

Token 12980:
Each new piece of candy starts out with a round shape;as it moves along the production line, a machine randomly selects a certain percentage tobe trimmed into a square; then, each piece is wrapped in a wrapper whose color is chosenrandomly to be red or brown.

Token 12981:
80% of the strawberry candies are round and 80% have a redwrapper, while 90% of the anchovy candies are square and 90% have a brown wrapper.

Token 12982:
Allcandies are sold individually in sealed, identical, black boxes.

Token 12983:
Now you, the customer, have just bought a Surprise candy at the store but have not yet opened the box. Consider the three Bayes nets in Figure 16.11.

Token 12984:
(i) (ii) (iii)Flavor Wrapper ShapeWrapper Shape FlavorWrapper Shape Flavor Figure 16.11 Three proposed Bayes nets for the Surprise Candy problem, Exercise 16.5. a.

Token 12985:
Which network(s) can correctly represent P(Flavor ,Wrapper ,Sh ape )? b. Which network is the best representation for this problem?

Token 12986:
642 Chapter 16. Making Simple Decisions c. Does network (i) assert that P(Wrapper|Shape )=P(Wrapper )?

Token 12987:
d. What is the probability that your candy has a red wrapper? e. In the box is a round candy with a red wrapper.

Token 12988:
What is the probability that its ﬂavor is strawberry?

Token 12989:
f. A unwrapped strawberry candy is worth son the open market and an unwrapped an- chovy candy is worth a.

Token 12990:
Write an expression for the value of an unopened candy box.

Token 12991:
g. A new law prohibits trading of unwrapped candies, but it is still legal to trade wrapped candies (out of the box).

Token 12992:
Is an unopened candy box now worth more than less than, or the same as before?

Token 12993:
16.6 Prove that the judgments B/followsAandC/followsDin the Allais paradox (page 620) violate the axiom of substitutability.

Token 12994:
16.7 Consider the Allais paradox described on page 620: an agent who prefers Bover A(taking the sure thing), and CoverD(taking the higher EMV) is not acting rationally, according to utility theory.

Token 12995:
Do you think this indicates a problem for the agent, a problem forthe theory, or no problem at all? Explain. 16.8 Tickets to a lottery cost $1.

Token 12996:
There are two possible prizes: a $10 payoff with probabil- ity 1/50, and a $1,000,000 payoff with probability 1/2,000,000.

Token 12997:
What is the expected mone-tary value of a lottery ticket? When (if ever) is it rational to buy a ticket?

Token 12998:
Be precise—show anequation involving utilities.

Token 12999:
You may assume current wealth of $ kand that U(S k)=0 .Y o u may also assume that U(Sk+10)=1 0×U(Sk+1), but you may not make any assumptions about U(Sk+1,000,000).

Token 13000:
Sociological studies show that people with lower income buy a dis- proportionate number of lottery tickets.

Token 13001:
Do you think this is because they are worse decisionmakers or because they have a different utility function?

Token 13002:
Consider the value of contemplatingthe possibility of winning the lottery versus the value of contemplating becoming an actionhero while watching an adventure movie.

Token 13003:
16.9 Assess your own utility for different incremental amounts of money by running a series of preference tests between some deﬁnite amount M 1and a lottery [p,M2;(1−p),0].

Token 13004:
Choose different values of M1andM2,a n dv a r y puntil you are indifferent between the two choices. Plot the resulting utility function.

Token 13005:
16.10 How much is a micromort worth to you? Devise a protocol to determine this.

Token 13006:
Ask questions based both on paying to avoid risk and being paid to accept risk.

Token 13007:
16.11 Let continuous variables X1,...,X kbe independently distributed according to the same probability density function f(x).

Token 13008:
Prove that the density function for max{X1,...,X k} is given by kf(x)(F(x))k−1,w h e r e Fis the cumulative distribution for f. 16.12 Economists often make use of an exponential utility function for money: U(x)= −ex/R,w h e r e Ris a positive constant representing an individual’s risk tolerance.

Token 13009:
Risk toler- ance reﬂects how likely an individual is to accept a lottery with a particular expected monetaryvalue (EMV) versus some certain payoff.

Token 13010:
As R(which is measured in the same units as x) becomes larger, the individual becomes less risk-averse.

Token 13011:
Exercises 643 a. Assume Mary has an exponential utility function with R= $500 .

Token 13012:
Mary is given the choice between receiving $500 with certainty (probability 1) or participating in a lot-tery which has a 60% probability of winning $5000 and a 40% probability of winningnothing.

Token 13013:
Assuming Marry acts rationally, which option would she choose? Show howyou derived your answer. b.

Token 13014:
Consider the choice between receiving $100 with certainty (probability 1) or participat- ing in a lottery which has a 50% probability of winning $500 and a 50% probability of winning nothing.

Token 13015:
Approximate the value of R (to 3 signiﬁcant digits) in an exponentialutility function that would cause an individual to be indifferent to these two alternatives.

Token 13016:
(You might ﬁnd it helpful to write a short program to help you solve this problem.)

Token 13017:
16.13 Repeat Exercise 16.16, using the action-utility representation shown in Figure 16.7.

Token 13018:
16.14 For either of the airport-siting diagrams from Exercises 16.16 and 16.13, to which conditional probability table entry is the utility most sensitive, given the available evidence?

Token 13019:
16.15 Consider a student who has the choice to buy or not buy a textbook for a course.

Token 13020:
We’ll model this as a decision problem with one Boolean decision node, B, indicating whether the agent chooses to buy the book, and two Boolean chance nodes, M, indicating whether the student has mastered the material in the book, and P, indicating whether the student passes the course.

Token 13021:
Of course, there is also a utility node, U.

Token 13022:
A certain student, Sam, has an additive utility function: 0 for not buying the book and -$100 for buying it; and $2000 for passing the course and 0 for not passing.

Token 13023:
Sam’s conditional probability estimates are as follows: P(p|b,m)=0.9P(m|b)=0.9 P(p|b,¬m)=0.5P(m|¬b)=0.7 P(p|¬b,m)=0.8 P(p|¬b,¬m)=0.3 You might think that Pwould be independent of BgivenM, But this course has an open- book ﬁnal—so having the book helps.

Token 13024:
a. Draw the decision network for this problem. b. Compute the expected utility of buying the book and of not buying it. c. What should Sam do?

Token 13025:
16.16 This exercise completes the analysis of the airport-siting problem in Figure 16.6. a.

Token 13026:
Provide reasonable variable domains, probabilities, and utilities for the network, assum- ing that there are three possible sites. b.

Token 13027:
Solve the decision problem. c. What happens if changes in technology mean that each aircraft generates half the noise?

Token 13028:
d. What if noise avoidance becomes three times more important? e. Calculate the VPI for AirTraﬃc ,Litigation ,a n dConstruction in your model.

Token 13029:
644 Chapter 16. Making Simple Decisions 16.17 (Adapted from Pearl (1988).)

Token 13030:
A used-car buyer can decide to carry out various tests with various costs (e.g., kick the tires, take the car to a qualiﬁed mechanic) and then, depend-ing on the outcome of the tests, decide which car to buy.

Token 13031:
We will assume that the buyer isdeciding whether to buy car c 1, that there is time to carry out at most one test, and that t1is the test of c1and costs $50.

Token 13032:
A car can be in good shape (quality q+) or bad shape (quality q−), and the tests might help indicate what shape the car is in.

Token 13033:
Car c1costs $1,500, and its market value is $2,000 if it is in good shape; if not, $700 in repairs will be needed to make it in good shape.

Token 13034:
The buyer’sestimate is that c 1has a 70% chance of being in good shape. a. Draw the decision network that represents this problem. b.

Token 13035:
Calculate the expected net gain from buying c1, given no test.

Token 13036:
c. Tests can be described by the probability that the car will pass or fail the test given that the car is in good or bad shape.

Token 13037:
We have the following information: P(pass(c1,t1)|q+(c1)) = 0 .8 P(pass(c1,t1)|q−(c1)) = 0 .35 Use Bayes’ theorem to calculate the probability that the car will pass (or fail) its test andhence the probability that it is in good (or bad) shape given each possible test outcome.

Token 13038:
d. Calculate the optimal decisions given either a pass or a fail, and their expected utilities.

Token 13039:
e. Calculate the value of information of the test, and derive an optimal conditional plan for the buyer.

Token 13040:
16.18 Recall the deﬁnition of value of information in Section 16.6. a. Prove that the value of information is nonnegative and order independent. b.

Token 13041:
Explain why it is that some people would prefer not to get some information—for ex- ample, not wanting to know the sex of their baby when an ultrasound is done.

Token 13042:
c. A function fon sets is submodular if, for any element xand any sets AandBsuch SUBMODULARITY thatA⊆B, adding xtoAgives a greater increase in fthan adding xtoB: A⊆B⇒(f(A∪{x})−f(A))≥(f(B∪{x})−f(B)).

Token 13043:
Submodularity captures the intuitive notion of diminishing returns .

Token 13044:
I st h ev a l u eo fi n - formation, viewed as a function fon sets of possible observations, submodular? Prove this or ﬁnd a counterexample.

Token 13045:
The answers to Exercise 16.1 (where M stands for million): First set: 3M, 1.6M, 1541, 41M, 4768, 221, 649M, 295M, 132, 25,546.

Token 13046:
Second set: 1917, 155M, 4,500M, 11M, 120,000, 1.1M, 1636, 19,340, 1,595, 41,710.

Token 13047:
17MAKING COMPLEX DECISIONS In which we examine methods for deciding what to do today, given that we may decide again tomorrow.

Token 13048:
In this chapter, we address the computational issues involved in making decisions in a stochas- tic environment.

Token 13049:
Whereas Chapter 16 was concerned with one-shot or episodic decisionproblems, in which the utility of each action’s outcome was well known, we are concernedhere with sequential decision problems , in which the agent’s utility depends on a sequence SEQUENTIAL DECISION PROBLEM of decisions.

Token 13050:
Sequential decision problems incorporate utilities, uncertainty, and sensing, and include search and planning problems as special cases.

Token 13051:
Section 17.1 explains how se-quential decision problems are deﬁned, and Sections 17.2 and 17.3 explain how they can be solved to produce optimal behavior that balances the risks and rewards of acting in an uncertain environment.

Token 13052:
Section 17.4 extends these ideas to the case of partially observableenvironments, and Section 17.4.3 develops a complete design for decision-theoretic agents inpartially observable environments, combining dynamic Bayesian networks from Chapter 15with decision networks from Chapter 16.

Token 13053:
The second part of the chapter covers environments with multiple agents.

Token 13054:
In such en- vironments, the notion of optimal behavior is complicated by the interactions among theagents.

Token 13055:
Section 17.5 introduces the main ideas of game theory , including the idea that ra- tional agents might need to behave randomly.

Token 13056:
Section 17.6 looks at how multiagent systemscan be designed so that multiple agents can achieve a common goal.

Token 13057:
17.1 S EQUENTIAL DECISION PROBLEMS Suppose that an agent is situated in the 4×3environment shown in Figure 17.1(a).

Token 13058:
Beginning in the start state, it must choose an action at each time step.

Token 13059:
The interaction with the environ-ment terminates when the agent reaches one of the goal states, marked +1 or –1.

Token 13060:
Just as for search problems, the actions available to the agent in each state are given by A CTIONS (s), sometimes abbreviated to A(s);i nt h e 4×3environment, the actions in every state are Up, Down ,Left,a n d Right .

Token 13061:
We assume for now that the environment is fully observable ,s ot h a t the agent always knows where it is. 645

Token 13062:
646 Chapter 17.

Token 13063:
Making Complex Decisions 123 1234START0.8 0.1 0.1 (a) (b)–1+ 1 Figure 17.1 (a) A simple 4×3environment that presents the agent with a sequential decision problem.

Token 13064:
(b) Illustration of the transition model of the environment: the “intended”outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles to the intended direction.

Token 13065:
A collision with a wall results in no movement.

Token 13066:
The two terminal states have reward +1 and –1, respectively, and all other states have a reward of –0.04.

Token 13067:
If the environment were deterministic, a solution would be easy: [ Up, Up, Right, Right, Right ].

Token 13068:
Unfortunately, the environment won’t always go along with this solution, because the actions are unreliable.

Token 13069:
The particular model of stochastic motion that we adopt is illustratedin Figure 17.1(b).

Token 13070:
Each action achieves the intended effect with probability 0.8, but the restof the time, the action moves the agent at right angles to the intended direction.

Token 13071:
Furthermore,if the agent bumps into a wall, it stays in the same square.

Token 13072:
For example, from the start square(1,1), the action Upmoves the agent to (1,2) with probability 0.8, but with probability 0.1, it moves right to (2,1), and with probability 0.1, it moves left, bumps into the wall, and stays in(1,1).

Token 13073:
In such an environment, the sequence [Up,Up,Right,Right,Right]goes up around the barrier and reaches the goal state at (4,3) with probability 0.8 5=0.32768 .

Token 13074:
There is also a small chance of accidentally reaching the goal by going the other way around with probability0.1 4×0.8, for a grand total of 0.32776.

Token 13075:
(See also Exercise 17.1.)

Token 13076:
As in Chapter 3, the transition model (or just “model,” whenever no confusion can arise) describes the outcome of each action in each state.

Token 13077:
Here, the outcome is stochastic,so we write P(s /prime|s,a)to denote the probability of reaching state s/primeif action ais done in states.

Token 13078:
We will assume that transitions are Markovian in the sense of Chapter 15, that is, the probability of reaching s/primefromsdepends only on sand not on the history of earlier states.

Token 13079:
For now, you can think of P(s/prime|s,a)as a big three-dimensional table containing probabilities.

Token 13080:
Later, in Section 17.4.3, we will see that the transition model can be represented as a dynamic Bayesian network , just as in Chapter 15.

Token 13081:
To complete the deﬁnition of the task environment, we must specify the utility function for the agent.

Token 13082:
Because the decision problem is sequential, the utility function will dependon a sequence of states—an environment history —rather than on a single state.

Token 13083:
Later in this section, we investigate how such utility functions can be speciﬁed in general; for now, we simply stipulate that in each state s, the agent receives a reward R(s),w h i c hm a yb e REWARD positive or negative, but must be bounded.

Token 13084:
For our particular example, the reward is −0.04 in all states except the terminal states (which have rewards +1 and –1). The utility of an

Token 13085:
Section 17.1. Sequential Decision Problems 647 environment history is just (for now) the sum of the rewards received.

Token 13086:
For example, if the agent reaches the +1 state after 10 steps, its total utility will be 0.6.

Token 13087:
The negative reward of–0.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochasticgeneralization of the search problems of Chapter 3.

Token 13088:
Another way of saying this is that theagent does not enjoy living in this environment and so wants to leave as soon as possible.

Token 13089:
To sum up: a sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive rewards is called a Markov decision process , MARKOV DECISION PROCESS orMDP , and consists of a set of states (with an initial state s0); a set A CTIONS (s)of actions in each state; a transition model P(s/prime|s,a); and a reward function R(s).1 The next question is, what does a solution to the problem look like?

Token 13090:
We have seen that any ﬁxed action sequence won’t solve the problem, because the agent might end up in a stateother than the goal.

Token 13091:
Therefore, a solution must specify what the agent should do for anystate that the agent might reach. A solution of this kind is called a policy .

Token 13092:
It is traditional to denote POLICY a policy by π,a n dπ(s)is the action recommended by the policy πfor state s. If the agent has a complete policy, then no matter what the outcome of any action, the agent will alwaysknow what to do next.

Token 13093:
Each time a given policy is executed starting from the initial state, the stochastic nature of the environment may lead to a different environment history.

Token 13094:
The quality of a policy is therefore measured by the expected utility of the possible environment histories generated by that policy.

Token 13095:
An optimal policy is a policy that yields the highest expected utility. We OPTIMAL POLICY useπ∗to denote an optimal policy.

Token 13096:
Given π∗, the agent decides what to do by consulting its current percept, which tells it the current state s, and then executing the action π∗(s).A policy represents the agent function explicitly and is therefore a description of a simple reﬂexagent, computed from the information used for a utility-based agent.

Token 13097:
An optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a).

Token 13098:
Notice that, because the cost of taking a step is fairly small compared with the penalty for ending up in (4,2) by accident, the optimal policy for the state (3,1) is conservative.

Token 13099:
The policy recommends taking the long way round, rather than taking the shortcut and thereby risking entering (4,2).

Token 13100:
The balance of risk and reward changes depending on the value of R(s)for the nonter- minal states.

Token 13101:
Figure 17.2(b) shows optimal policies for four different ranges of R(s).W h e n R(s)≤−1.6284 , life is so painful that the agent heads straight for the nearest exit, even if the exit is worth –1.

Token 13102:
When −0.4278≤R(s)≤−0.0850 , life is quite unpleasant; the agent takes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci-dent.

Token 13103:
In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary(−0.0221<R(s)<0), the optimal policy takes no risks at all .

Token 13104:
In (4,1) and (3,2), the agent heads directly away from the –1 state so that it cannot fall in by accident, even though thismeans banging its head against the wall quite a few times.

Token 13105:
Finally, if R(s)>0, then life is positively enjoyable and the agent avoids both exits.

Token 13106:
As long as the actions in (4,1), (3,2), 1Some deﬁnitions of MDPs allow the reward to depend on the action and outcome too, so the reward function isR(s,a,s/prime).

Token 13107:
This simpliﬁes the description of some environments but does not change the problem in any fundamental way, as shown in Exercise 17.4.

Token 13108:
648 Chapter 17.

Token 13109:
Making Complex Decisions 123123+ 1 –1 4–1+1 R(s) < –1.6284 (a) (b)– 0.0221 < R(s) < 0 –1+1 –1+1 –1+1 R(s) > 0 – 0.4278 < R(s) < – 0.0850 Figure 17.2 (a) An optimal policy for the stochastic environment with R(s)=−0.04in the nonterminal states.

Token 13110:
(b) Optimal policies for four different ranges of R(s).

Token 13111:
and (3,3) are as shown, every policy is optimal, and the agent obtains inﬁnite total reward be- cause it never enters a terminal state.

Token 13112:
Surprisingly, it turns out that there are six other optimalpolicies for various ranges of R(s); Exercise 17.5 asks you to ﬁnd them.

Token 13113:
The careful balancing of risk and reward is a characteristic of MDPs that does not arise in deterministic search problems; moreover, it is a characteristic of many real-worlddecision problems.

Token 13114:
For this reason, MDPs have been studied in several ﬁelds, includingAI, operations research, economics, and control theory.

Token 13115:
Dozens of algorithms have beenproposed for calculating optimal policies.

Token 13116:
In sections 17.2 and 17.3 we describe two of themost important algorithm families.

Token 13117:
First, however, we must complete our investigation ofutilities and policies for sequential decision problems.

Token 13118:
17.1.1 Utilities over time In the MDP example in Figure 17.1, the performance of the agent was measured by a sum ofrewards for the states visited.

Token 13119:
This choice of performance measure is not arbitrary, but it isnot the only possibility for the utility function on environment histories, which we write asU h([s0,s1,...,s n]).

Token 13120:
Our analysis draws on multiattribute utility theory (Section 16.4) and is somewhat technical; the impatient reader may wish to skip to the next section.

Token 13121:
The ﬁrst question to answer is whether there is a ﬁnite horizon or an inﬁnite horizon FINITE HORIZON INFINITEHORIZON for decision making.

Token 13122:
A ﬁnite horizon means that there is a ﬁxed timeNafter which nothing matters—the game is over, so to speak.

Token 13123:
Thus, Uh([s0,s1,...,s N+k])=Uh([s0,s1,...,s N]) for all k>0.

Token 13124:
For example, suppose an agent starts at (3,1) in the 4×3world of Figure 17.1, and suppose that N=3.

Token 13125:
Then, to have any chance of reaching the +1 state, the agent must head directly for it, and the optimal action is to go Up.

Token 13126:
On the other hand, if N= 100 , then there is plenty of time to take the safe route by going Left.So, with a ﬁnite horizon,

Token 13127:
Section 17.1. Sequential Decision Problems 649 the optimal action in a given state could change over time.

Token 13128:
We say that the optimal policy for a ﬁnite horizon is nonstationary .

Token 13129:
With no ﬁxed time limit, on the other hand, there isNONSTATIONARY POLICY no reason to behave differently in the same state at different times.

Token 13130:
Hence, the optimal ac- tion depends only on the current state, and the optimal policy is stationary .

Token 13131:
Policies for the STATIONARY POLICY inﬁnite-horizon case are therefore simpler than those for the ﬁnite-horizon case, and we deal mainly with the inﬁnite-horizon case in this chapter.

Token 13132:
(We will see later that for partially ob- servable environments, the inﬁnite-horizon case is not so simple.)

Token 13133:
Note that “inﬁnite horizon”does not necessarily mean that all state sequences are inﬁnite; it just means that there is noﬁxed deadline.

Token 13134:
In particular, there can be ﬁnite state sequences in an inﬁnite-horizon MDPcontaining a terminal state.

Token 13135:
The next question we must decide is how to calculate the utility of state sequences.

Token 13136:
In the terminology of multiattribute utility theory, each state s ican be viewed as an attribute of the state sequence [s0,s1,s2...].

Token 13137:
To obtain a simple expression in terms of the attributes, we will need to make some sort of preference-independence assumption.

Token 13138:
The most natural as-sumption is that the agent’s preferences between state sequences are stationary .

Token 13139:
Stationarity STATIONARY PREFERENCE for preferences means the following: if two state sequences [s0,s1,s2,...]and[s/prime 0,s/prime1,s/prime2,...] begin with the same state (i.e., s0=s/prime 0), then the two sequences should be preference-ordered the same way as the sequences [s1,s2,...]and[s/prime 1,s/prime2,...].

Token 13140:
In English, this means that if you prefer one future to another starting tomorrow, then you should still prefer that future if itwere to start today instead.

Token 13141:
Stationarity is a fairly innocuous-looking assumption with verystrong consequences: it turns out that under stationarity there are just two coherent ways toassign utilities to sequences: 1.Additive rewards : The utility of a state sequence is ADDITIVE REWARD Uh([s0,s1,s2,...]) =R(s0)+R(s1)+R(s2)+···.

Token 13142:
The4×3world in Figure 17.1 uses additive rewards.

Token 13143:
Notice that additivity was used implicitly in our use of path cost functions in heuristic search algorithms (Chapter 3).

Token 13144:
2.Discounted rewards : The utility of a state sequence isDISCOUNTED REWARD Uh([s0,s1,s2,...]) =R(s0)+γR(s1)+γ2R(s2)+···, where the discount factor γis a number between 0 and 1.

Token 13145:
The discount factor describes DISCOUNT FACTOR the preference of an agent for current rewards over future rewards.

Token 13146:
When γis close to 0, rewards in the distant future are viewed as insigniﬁcant.

Token 13147:
When γis 1, discounted rewards are exactly equivalent to additive rewards, so additive rewards are a special case of discounted rewards.

Token 13148:
Discounting appears to be a good model of both animaland human preferences over time.

Token 13149:
A discount factor of γis equivalent to an interest rate of(1/γ)−1.

Token 13150:
For reasons that will shortly become clear, we assume discounted rewards in the remainder of the chapter, although sometimes we allow γ=1.

Token 13151:
Lurking beneath our choice of inﬁnite horizons is a problem: if the environment does not contain a terminal state, or if the agent never reaches one, then all environment historieswill be inﬁnitely long, and utilities with additive, undiscounted rewards will generally be

Token 13152:
650 Chapter 17. Making Complex Decisions inﬁnite.

Token 13153:
While we can agree that +∞is better than−∞, comparing two state sequences with +∞utility is more difﬁcult.

Token 13154:
There are three solutions, two of which we have seen already: 1. With discounted rewards, the utility of an inﬁnite sequence is ﬁnite .

Token 13155:
In fact, if γ<1 and rewards are bounded by ±Rmax,w eh a v e Uh([s0,s1,s2,...]) =∞/summationdisplay t=0γtR(st)≤∞/summationdisplay t=0γtRmax=Rmax/(1−γ), (17.1) using the standard formula for the sum of an inﬁnite geometric series.

Token 13156:
2.

Token 13157:
If the environment contains terminal states and if the agent is guaranteed to get to one eventually , then we will never need to compare inﬁnite sequences.

Token 13158:
A policy that is guaranteed to reach a terminal state is called a proper policy .

Token 13159:
With proper policies, we PROPER POLICY can use γ=1(i.e., additive rewards).

Token 13160:
The ﬁrst three policies shown in Figure 17.2(b) are proper, but the fourth is improper.

Token 13161:
It gains inﬁnite total reward by staying away fromthe terminal states when the reward for the nonterminal states is positive.

Token 13162:
The existenceof improper policies can cause the standard algorithms for solving MDPs to fail with additive rewards, and so provides a good reason for using discounted rewards.

Token 13163:
3. Inﬁnite sequences can be compared in terms of the average reward obtained per time AVERAGE REWARD step.

Token 13164:
Suppose that square (1,1) in the 4×3world has a reward of 0.1 while the other nonterminal states have a reward of 0.01.

Token 13165:
Then a policy that does its best to stay in(1,1) will have higher average reward than one that stays elsewhere.

Token 13166:
Average reward isa useful criterion for some problems, but the analysis of average-reward algorithms isbeyond the scope of this book.

Token 13167:
In sum, discounted rewards present the fewest difﬁculties in evaluating state sequences.

Token 13168:
17.1.2 Optimal policies and the utilities of states Having decided that the utility of a given state sequence is the sum of discounted rewards obtained during the sequence, we can compare policies by comparing the expected utilities obtained when executing them.

Token 13169:
We assume the agent is in some initial state sand deﬁne St (a random variable) to be the state the agent reaches at time twhen executing a particular policy π.

Token 13170:
(Obviously, S0=s, the state the agent is in now.)

Token 13171:
The probability distribution over state sequences S1,S2,..., is determined by the initial state s, the policy π, and the transition model for the environment.

Token 13172:
The expected utility obtained by executing πstarting in sis given by Uπ(s)=E/bracketleftBigg∞/summationdisplay t=0γtR(St)/bracketrightBigg , (17.2) where the expectation is with respect to the probability distribution over state sequences de- termined by sandπ.

Token 13173:
Now, out of all the policies the agent could choose to execute starting in s, one (or more) will have higher expected utilities than all the others.

Token 13174:
We’ll use π∗ sto denote one of these policies: π∗ s=a r g m a x πUπ(s). (17.3)

Token 13175:
Section 17.1.

Token 13176:
Sequential Decision Problems 651 Remember that π∗ sis a policy, so it recommends an action for every state; its connection withsin particular is that it’s an optimal policy when sis the starting state.

Token 13177:
A remarkable consequence of using discounted utilities with inﬁnite horizons is that the optimal policy isindependent of the starting state.

Token 13178:
(Of course, the action sequence won’t be independent; remember that a policy is a function specifying an action for each state.)

Token 13179:
This fact seems intuitively obvious: if policy π ∗ ais optimal starting in aand policy π∗ bis optimal starting in b, then, when they reach a third state c, there’s no good reason for them to disagree with each other, or with π∗ c, about what to do next.2So we can simply write π∗for an optimal policy.

Token 13180:
Given this deﬁnition, the true utility of a state is just Uπ∗(s)—that is, the expected sum of discounted rewards if the agent executes an optimal policy.

Token 13181:
We write this as U(s), matching the notation used in Chapter 16 for the utility of an outcome.

Token 13182:
Notice that U(s)and R(s)are quite different quantities; R(s)is the “short term” reward for being in s, whereas U(s)is the “long term” total reward from sonward.

Token 13183:
Figure 17.3 shows the utilities for the 4×3world.

Token 13184:
Notice that the utilities are higher for states closer to the +1 exit, because fewer steps are required to reach the exit.

Token 13185:
123123 –1+ 1 40.6110.812 0.6550.7620.918 0.7050.6600.868 0.388 Figure 17.3 The utilities of the states in the 4×3world, calculated with γ=1 and R(s)=−0.04for nonterminal states.

Token 13186:
The utility function U(s)allows the agent to select actions by using the principle of maximum expected utility from Chapter 16—that is, choose the action that maximizes theexpected utility of the subsequent state: π ∗(s) = argmax a∈A(s)/summationdisplay s/primeP(s/prime|s,a)U(s/prime).

Token 13187:
(17.4) The next two sections describe algorithms for ﬁnding optimal policies.

Token 13188:
2Although this seems obvious, it does not hold for ﬁnite-horizon policies or for other ways of combining rewards over time.

Token 13189:
The proof follows directly from the uniqueness of the utility function on states, as shown inSection 17.2.

Token 13190:
652 Chapter 17.

Token 13191:
Making Complex Decisions 17.2 V ALUE ITERATION In this section, we present an algorithm, called value iteration , for calculating an optimal VALUE ITERATION policy.

Token 13192:
The basic idea is to calculate the utility of each state and then use the state utilities to select an optimal action in each state.

Token 13193:
17.2.1 The Bellman equation for utilities Section 17.1.2 deﬁned the utility of being in a state as the expected sum of discounted rewardsfrom that point onwards.

Token 13194:
From this, it follows that there is a direct relationship between theutility of a state and the utility of its neighbors: the utility of a state is the immediate reward for that state plus the expected discounted utility of the next state, assuming that the agent chooses the optimal action.

Token 13195:
That is, the utility of a state is given by U(s)=R(s)+γmax a∈A(s)/summationdisplay s/primeP(s/prime|s,a)U(s/prime).

Token 13196:
(17.5) This is called the Bellman equation , after Richard Bellman (1957).

Token 13197:
The utilities of the BELLMAN EQUATION states—deﬁned by Equation (17.2) as the expected utility of subsequent state sequences—are solutions of the set of Bellman equations.

Token 13198:
In fact, they are the unique solutions, as we show in Section 17.2.3. Let us look at one of the Bellman equations for the 4×3world.

Token 13199:
The equation for the state (1,1) is U(1,1) =−0.04 +γmax[ 0 .8U(1,2) + 0 .1U(2,1) + 0 .1U(1,1), (Up) 0.9U(1,1) + 0 .1U(1,2), (Left) 0.9U(1,1) + 0 .1U(2,1), (Down ) 0.8U(2,1) + 0 .1U(1,2) + 0 .1U(1,1) ].

Token 13200:
(Right) When we plug in the numbers from Figure 17.3, we ﬁnd that Upis the best action.

Token 13201:
17.2.2 The value iteration algorithm The Bellman equation is the basis of the value iteration algorithm for solving MDPs.

Token 13202:
If there arenpossible states, then there are nBellman equations, one for each state. The nequations contain nunknowns—the utilities of the states.

Token 13203:
So we would like to solve these simultaneous equations to ﬁnd the utilities.

Token 13204:
There is one problem: the equations are nonlinear , because the “max” operator is not a linear operator.

Token 13205:
Whereas systems of linear equations can be solved quickly using linear algebra techniques, systems of nonlinear equations are more problematic.One thing to try is an iterative approach.

Token 13206:
We start with arbitrary initial values for the utilities, calculate the right-hand side of the equation, and plug it into the left-hand side—therebyupdating the utility of each state from the utilities of its neighbors.

Token 13207:
We repeat this until we reach an equilibrium. Let U i(s)be the utility value for state sat theith iteration.

Token 13208:
The iteration step, called a Bellman update , looks like this: BELLMAN UPDATE Ui+1(s)←R(s)+γmax a∈A(s)/summationdisplay s/primeP(s/prime|s,a)Ui(s/prime), (17.6)

Token 13209:
Section 17.2.

Token 13210:
Value Iteration 653 function VALUE -ITERATION (mdp ,/epsilon1)returns a utility function inputs :mdp , an MDP with states S, actions A(s), transition model P(s/prime|s, a), rewards R(s), discount γ /epsilon1, the maximum error allowed in the utility of any state local variables :U,U/prime, vectors of utilities for states in S, initially zero δ, the maximum change in the utility of any state in an iteration repeat U←U/prime;δ←0 for each statesinSdo U/prime[s]←R(s)+γmax a∈A(s)/summationdisplay s/primeP(s/prime|s, a)U[s/prime] if|U/prime[s]−U[s]|>δ thenδ←|U/prime[s]−U[s]| untilδ</epsilon1 (1−γ)/γ return U Figure 17.4 The value iteration algorithm for calculating utilities of states.

Token 13211:
The termina- tion condition is from Equation (17.8).

Token 13212:
-0.200.20.40.60.81 0 5 10 15 20 25 30Utility estimates Number of iterations(4,3) (3,3) (1,1) (3,1) (4,1) 1101001000100001000001e+061e+07 0.50.55 0.60.65 0.70.75 0.80.85 0.90.95 1Iterations required Discount factor γc = 0.0001 c = 0.001 c = 0.01 c = 0.1 (a) (b) Figure 17.5 (a) Graph showing the evolution of the utilities of selected states using value iteration.

Token 13213:
(b) The number of value iterations krequired to guarantee an error of at most /epsilon1=c·Rmax, for different values of c, as a function of the discount factor γ. where the update is assumed to be applied simultaneously to all the states at each iteration.

Token 13214:
If we apply the Bellman update inﬁnitely often, we are guaranteed to reach an equilibrium(see Section 17.2.3), in which case the ﬁnal utility values must be solutions to the Bellmanequations.

Token 13215:
In fact, they are also the unique solutions, and the corresponding policy (obtained using Equation (17.4)) is optimal.

Token 13216:
The algorithm, called V ALUE -ITERATION ,i ss h o w ni n Figure 17.4. We can apply value iteration to the 4×3world in Figure 17.1(a).

Token 13217:
Starting with initial values of zero, the utilities evolve as shown in Figure 17.5(a). Notice how the states at differ-

Token 13218:
654 Chapter 17.

Token 13219:
Making Complex Decisions ent distances from (4,3) accumulate negative reward until a path is found to (4,3), whereupon the utilities start to increase.

Token 13220:
We can think of the value iteration algorithm as propagating information through the state space by means of local updates.

Token 13221:
17.2.3 Convergence of value iteration We said that value iteration eventually converges to a unique set of solutions of the Bellman equations.

Token 13222:
In this section, we explain why this happens.

Token 13223:
We introduce some useful mathe-matical ideas along the way, and we obtain some methods for assessing the error in the utilityfunction returned when the algorithm is terminated early; this is useful because it means thatwe don’t have to run forever.

Token 13224:
This section is quite technical. The basic concept used in showing that value iteration converges is the notion of a con- traction .

Token 13225:
Roughly speaking, a contraction is a function of one argument that, when applied to CONTRACTION two different inputs in turn, produces two output values that are “closer together,” by at least some constant factor, than the original inputs.

Token 13226:
For example, the function “divide by two” is a contraction, because, after we divide any two numbers by two, their difference is halved.Notice that the “divide by two” function has a ﬁxed point, namely zero, that is unchanged bythe application of the function.

Token 13227:
From this example, we can discern two important propertiesof contractions: •A contraction has only one ﬁxed point; if there were two ﬁxed points they would not get closer together when the function was applied, so it would not be a contraction.

Token 13228:
•When the function is applied to any argument, the value must get closer to the ﬁxed point (because the ﬁxed point does not move), so repeated application of a contraction always reaches the ﬁxed point in the limit.

Token 13229:
Now, suppose we view the Bellman update (Equation (17.6)) as an operator Bthat is applied simultaneously to update the utility of every state.

Token 13230:
Let U idenote the vector of utilities for all the states at the ith iteration. Then the Bellman update equation can be written as Ui+1←BUi.

Token 13231:
Next, we need a way to measure distances between utility vectors.

Token 13232:
We will use the max norm , MAX NORM which measures the “length” of a vector by the absolute value of its biggest component: ||U||=m a x s|U(s)|.

Token 13233:
With this deﬁnition, the “distance” between two vectors, ||U−U/prime||, is the maximum dif- ference between any two corresponding elements.

Token 13234:
The main result of this section is thefollowing: LetU iandU/prime ibe any two utility vectors. Then we have ||BUi−BU/prime i||≤γ||Ui−U/prime i||.

Token 13235:
(17.7) That is, the Bellman update is a contraction by a factor of γon the space of utility vectors.

Token 13236:
(Exercise 17.6 provides some guidance on proving this claim.)

Token 13237:
Hence, from the properties ofcontractions in general, it follows that value iteration always converges to a unique solutionof the Bellman equations whenever γ<1.

Token 13238:
Section 17.2. Value Iteration 655 We can also use the contraction property to analyze the rate of convergence to a solu- tion.

Token 13239:
In particular, we can replace U/prime iin Equation (17.7) with the true utilities U,f o rw h i c h BU=U.

Token 13240:
Then we obtain the inequality ||BUi−U||≤γ||Ui−U||.

Token 13241:
So, if we view ||Ui−U||as the error in the estimate Ui, we see that the error is reduced by a factor of at least γon each iteration.

Token 13242:
This means that value iteration converges exponentially fast.

Token 13243:
We can calculate the number of iterations required to reach a speciﬁed error bound /epsilon1 as follows: First, recall from Equation (17.1) that the utilities of all states are bounded by±R max/(1−γ).

Token 13244:
This means that the maximum initial error ||U0−U||≤2Rmax/(1−γ). Suppose we run for Niterations to reach an error of at most /epsilon1.

Token 13245:
Then, because the error is reduced by at least γeach time, we require γN·2Rmax/(1−γ)≤/epsilon1.

Token 13246:
Taking logs, we ﬁnd N=⌈log(2Rmax//epsilon1(1−γ))/log(1/γ)⌉ iterations sufﬁce.

Token 13247:
Figure 17.5(b) shows how Nvaries with γ, for different values of the ratio /epsilon1/R max.

Token 13248:
The good news is that, because of the exponentially fast convergence, Ndoes not depend much on the ratio /epsilon1/R max.

Token 13249:
The bad news is that Ngrows rapidly as γbecomes close to 1.

Token 13250:
We can get fast convergence if we make γsmall, but this effectively gives the agent a short horizon and could miss the long-term effects of the agent’s actions.

Token 13251:
The error bound in the preceding paragraph gives some idea of the factors inﬂuencing the run time of the algorithm, but is sometimes overly conservative as a method of decidingwhen to stop the iteration.

Token 13252:
For the latter purpose, we can use a bound relating the errorto the size of the Bellman update on any given iteration.

Token 13253:
From the contraction property(Equation (17.7)), it can be shown that if the update is small (i.e., no state’s utility changes bymuch), then the error, compared with the true utility function, also is small.

Token 13254:
More precisely, if||U i+1−Ui||</epsilon1(1−γ)/γ then||Ui+1−U||</epsilon1.

Token 13255:
(17.8) This is the termination condition used in the V ALUE -ITERATION algorithm of Figure 17.4.

Token 13256:
So far, we have analyzed the error in the utility function returned by the value iteration algorithm.

Token 13257:
What the agent really cares about, however, is how well it will do if it makes its decisions on the basis of this utility function.

Token 13258:
Suppose that after iiterations of value iteration, the agent has an estimate Uiof the true utility Uand obtains the MEU policy πibased on one-step look-ahead using Ui(as in Equation (17.4)).

Token 13259:
Will the resulting behavior be nearly as good as the optimal behavior?

Token 13260:
This is a crucial question for any real agent, and it turns outthat the answer is yes.

Token 13261:
U πi(s)is the utility obtained if πiis executed starting in s,a n dt h e policy loss||Uπi−U||is the most the agent can lose by executing πiinstead of the optimal POLICY LOSS policy π∗.

Token 13262:
The policy loss of πiis connected to the error in Uiby the following inequality: if||Ui−U||</epsilon1 then||Uπi−U||<2/epsilon1γ/(1−γ).

Token 13263:
(17.9) In practice, it often occurs that πibecomes optimal long before Uihas converged.

Token 13264:
Figure 17.6 shows how the maximum error in Uiand the policy loss approach zero as the value iteration process proceeds for the 4×3environment with γ=0.9.

Token 13265:
The policy πiis optimal when i=4, even though the maximum error in Uiis still 0.46. Now we have everything we need to use value iteration in practice.

Token 13266:
We know that it converges to the correct utilities, we can bound the error in the utility estimates if we

Token 13267:
656 Chapter 17.

Token 13268:
Making Complex Decisions stop after a ﬁnite number of iterations, and we can bound the policy loss that results from executing the corresponding MEU policy.

Token 13269:
As a ﬁnal note, all of the results in this sectiondepend on discounting with γ<1.I fγ=1and the environment contains terminal states, then a similar set of convergence results and error bounds can be derived whenever certaintechnical conditions are satisﬁed.

Token 13270:
17.3 P OLICY ITERATION In the previous section, we observed that it is possible to get an optimal policy even whenthe utility function estimate is inaccurate.

Token 13271:
If one action is clearly better than all others, thenthe exact magnitude of the utilities on the states involved need not be precise.

Token 13272:
This insightsuggests an alternative way to ﬁnd optimal policies.

Token 13273:
The policy iteration algorithm alternates POLICY ITERATION the following two steps, beginning from some initial policy π0: •Policy evaluation : given a policy πi, calculate Ui=Uπi, the utility of each state if πiPOLICY EVALUATION were to be executed.

Token 13274:
•Policy improvement : Calculate a new MEU policy πi+1, using one-step look-aheadPOLICY IMPROVEMENT based on Ui(as in Equation (17.4)).

Token 13275:
The algorithm terminates when the policy improvement step yields no change in the utilities.

Token 13276:
At this point, we know that the utility function Uiis a ﬁxed point of the Bellman update, so it is a solution to the Bellman equations, and πimust be an optimal policy.

Token 13277:
Because there are only ﬁnitely many policies for a ﬁnite state space, and each iteration can be shown to yield abetter policy, policy iteration must terminate.

Token 13278:
The algorithm is shown in Figure 17.7.

Token 13279:
The policy improvement step is obviously straightforward, but how do we implement the P OLICY -EVA LUATI O N routine?

Token 13280:
It turns out that doing so is much simpler than solving the standard Bellman equations (which is what value iteration does), because the action in each state is ﬁxed by the policy.

Token 13281:
At the ith iteration, the policy πispeciﬁes the action πi(s)in 00.20.40.60.81 0 2 4 6 8 10 12 14Max error/Policy loss Number of iterationsMax error Policy loss Figure 17.6 The maximum error ||Ui−U||of the utility estimates and the policy loss ||Uπi−U||, as a function of the number of iterations of value iteration.

Token 13282:
Section 17.3. Policy Iteration 657 states.

Token 13283:
This means that we have a simpliﬁed version of the Bellman equation (17.5) relating the utility of s(under πi) to the utilities of its neighbors: Ui(s)=R(s)+γ/summationdisplay s/primeP(s/prime|s,πi(s))Ui(s/prime).

Token 13284:
(17.10) For example, suppose πiis the policy shown in Figure 17.2(a).

Token 13285:
Then we have πi(1,1)=Up, πi(1,2)=Up, and so on, and the simpliﬁed Bellman equations are Ui(1,1) =−0.04 + 0 .8Ui(1,2) + 0 .1Ui(1,1) + 0 .1Ui(2,1), Ui(1,2) =−0.04 + 0 .8Ui(1,3) + 0 .2Ui(1,2), ...

Token 13286:
The important point is that these equations are linear , because the “ max” operator has been removed.

Token 13287:
For nstates, we have nlinear equations with nunknowns, which can be solved exactly in time O(n3)by standard linear algebra methods.

Token 13288:
For small state spaces, policy evaluation using exact solution methods is often the most efﬁcient approach.

Token 13289:
For large state spaces, O(n3)time might be prohibitive. Fortunately, it is not necessary to do exact policy evaluation.

Token 13290:
Instead, we can perform some number of simpliﬁed value iteration steps (simpliﬁed because the policy is ﬁxed) to give a reasonablygood approximation of the utilities.

Token 13291:
The simpliﬁed Bellman update for this process is U i+1(s)←R(s)+γ/summationdisplay s/primeP(s/prime|s,πi(s))Ui(s/prime), and this is repeated ktimes to produce the next utility estimate.

Token 13292:
The resulting algorithm is called modiﬁed policy iteration .

Token 13293:
It is often much more efﬁcient than standard policy iterationMODIFIED POLICY ITERATION or value iteration.

Token 13294:
function POLICY -ITERATION (mdp )returns a policy inputs :mdp , an MDP with states S, actions A(s), transition model P(s/prime|s, a) local variables :U, a vector of utilities for states in S, initially zero π, a policy vector indexed by state, initially random repeat U←POLICY -EVA L UAT I O N (π,U,mdp ) unchanged ?←true for each statesinSdo ifmax a∈A(s)/summationdisplay s/primeP(s/prime|s, a)U[s/prime]>/summationdisplay s/primeP(s/prime|s, π[s])U[s/prime]then do π[s]←argmax a∈A(s)/summationdisplay s/primeP(s/prime|s, a)U[s/prime] unchanged ?←false untilunchanged ?

Token 13295:
return π Figure 17.7 The policy iteration algorithm for calculating an optimal policy.

Token 13296:
658 Chapter 17. Making Complex Decisions The algorithms we have described so far require updating the utility or policy for all states at once.

Token 13297:
It turns out that this is not strictly necessary.

Token 13298:
In fact, on each iteration, wecan pick any subset of states and apply either kind of updating (policy improvement or sim- pliﬁed value iteration) to that subset.

Token 13299:
This very general algorithm is called asynchronous policy iteration .

Token 13300:
Given certain conditions on the initial policy and initial utility function, ASYNCHRONOUS POLICY ITERATION asynchronous policy iteration is guaranteed to converge to an optimal policy.

Token 13301:
The freedom to choose any states to work on means that we can design much more efﬁcient heuristicalgorithms—for example, algorithms that concentrate on updating the values of states thatare likely to be reached by a good policy.

Token 13302:
This makes a lot of sense in real life: if one has nointention of throwing oneself off a cliff, one should not spend time worrying about the exactvalue of the resulting states.

Token 13303:
17.4 P ARTIALLY OBSERV ABLE MDP S The description of Markov decision processes in Section 17.1 assumed that the environmentwas fully observable .

Token 13304:
With this assumption, the agent always knows which state it is in.

Token 13305:
This, combined with the Markov assumption for the transition model, means that the optimalpolicy depends only on the current state.

Token 13306:
When the environment is only partially observable , the situation is, one might say, much less clear.

Token 13307:
The agent does not necessarily know whichstate it is in, so it cannot execute the action π(s)recommended for that state.

Token 13308:
Furthermore, the utility of a state sand the optimal action in sdepend not just on s, but also on how much the agent knows when it is in s. For these reasons, partially observable MDPs (or POMDPs— PARTIALLY OBSERVABLE MDP pronounced “pom-dee-pees”) are usually viewed as much more difﬁcult than ordinary MDPs.

Token 13309:
We cannot avoid POMDPs, however, because the real world is one.

Token 13310:
17.4.1 Deﬁnition of POMDPs To get a handle on POMDPs, we must ﬁrst deﬁne them properly.

Token 13311:
A POMDP has the same elements as an MDP—the transition model P(s/prime|s,a), actions A(s), and reward function R(s)—but, like the partially observable search problems of Section 4.4, it also has a sensor model P(e|s).

Token 13312:
Here, as in Chapter 15, the sensor model speciﬁes the probability of perceiv- ing evidence ein state s.3For example, we can convert the 4×3world of Figure 17.1 into a POMDP by adding a noisy or partial sensor instead of assuming that the agent knows itslocation exactly.

Token 13313:
Such a sensor might measure the number of adjacent walls , which happens to be 2 in all the nonterminal squares except for those in the third column, where the valueis 1; a noisy version might give the wrong value with probability 0.1.

Token 13314:
In Chapters 4 and 11, we studied nondeterministic and partially observable planning problems and identiﬁed the belief state —the set of actual states the agent might be in—as a key concept for describing and calculating solutions.

Token 13315:
In POMDPs, the belief state bbecomes a probability distribution over all possible states, just as in Chapter 15.

Token 13316:
For example, the initial 3As with the reward function for MDPs, the sensor model can also depend on the action and outcome state, but again this change is not fundamental.

Token 13317:
Section 17.4.

Token 13318:
Partially Observable MDPs 659 belief state for the 4×3POMDP could be the uniform distribution over the nine nonterminal states, i.e.,/angbracketleft1 9,1 9,1 9,1 9,1 9,1 9,1 9,1 9,1 9,0,0/angbracketright.

Token 13319:
We write b(s)for the probability assigned to the actual state sby belief state b.

Token 13320:
The agent can calculate its current belief state as the conditional probability distribution over the actual states given the sequence of percepts and actions sofar.

Token 13321:
This is essentially the ﬁltering task described in Chapter 15.

Token 13322:
The basic recursive ﬁltering equation (15.5 on page 572) shows how to calculate the new belief state from the previous belief state and the new evidence.

Token 13323:
For POMDPs, we also have an action to consider, but theresult is essentially the same.

Token 13324:
If b(s)was the previous belief state, and the agent does action aand then perceives evidence e, then the new belief state is given by b /prime(s/prime)=αP(e|s/prime)/summationdisplay sP(s/prime|s,a)b(s), where αis a normalizing constant that makes the belief state sum to 1.

Token 13325:
By analogy with the update operator for ﬁltering (page 572), we can write this as b/prime=FORW ARD (b,a,e).

Token 13326:
(17.11) In the4×3POMDP, suppose the agent moves Leftand its sensor reports 1 adjacent wall; then it’s quite likely (although not guaranteed, because both the motion and the sensor are noisy) that the agent is now in (3,1).

Token 13327:
Exercise 17.13 asks you to calculate the exact probability values for the new belief state.

Token 13328:
The fundamental insight required to understand POMDPs is this: the optimal action depends only on the agent’s current belief state.

Token 13329:
That is, the optimal policy can be described by a mapping π∗(b)from belief states to actions. It does notdepend on the actual state the agent is in.

Token 13330:
This is a good thing, because the agent does not know its actual state; all it knowsis the belief state.

Token 13331:
Hence, the decision cycle of a POMDP agent can be broken down into thefollowing three steps: 1.

Token 13332:
Given the current belief state b, execute the action a=π ∗(b). 2. Receive percept e. 3. Set the current belief state to F ORW ARD (b,a,e)and repeat.

Token 13333:
Now we can think of POMDPs as requiring a search in belief-state space, just like the meth- ods for sensorless and contingency problems in Chapter 4.

Token 13334:
The main difference is that thePOMDP belief-state space is continuous , because a POMDP belief state is a probability dis- tribution.

Token 13335:
For example, a belief state for the 4×3world is a point in an 11-dimensional continuous space.

Token 13336:
An action changes the belief state, not just the physical state.

Token 13337:
Hence, the action is evaluated at least in part according to the information the agent acquires as a result.

Token 13338:
POMDPs therefore include the value of information (Section 16.6) as one component of thedecision problem.

Token 13339:
Let’s look more carefully at the outcome of actions.

Token 13340:
In particular, let’s calculate the probability that an agent in belief state breaches belief state b /primeafter executing action a.N o w , if we knew the action and the subsequent percept , then Equation (17.11) would provide a deterministic update to the belief state: b/prime=FORW ARD (b,a,e).

Token 13341:
Of course, the subsequent percept is not yet known, so the agent might arrive in one of several possible belief states b/prime, depending on the percept that is received.

Token 13342:
The probability of perceiving e, given that awas

Token 13343:
660 Chapter 17.

Token 13344:
Making Complex Decisions performed starting in belief state b, is given by summing over all the actual states s/primethat the agent might reach: P(e|a,b)=/summationdisplay s/primeP(e|a,s/prime,b)P(s/prime|a,b) =/summationdisplay s/primeP(e|s/prime)P(s/prime|a,b) =/summationdisplay s/primeP(e|s/prime)/summationdisplay sP(s/prime|s,a)b(s).

Token 13345:
Let us write the probability of reaching b/primefromb, given action a,a sP(b/prime|b,a)).

Token 13346:
Then that gives us P(b/prime|b,a)=P(b/prime|a,b)=/summationdisplay eP(b/prime|e,a,b)P(e|a,b) =/summationdisplay eP(b/prime|e,a,b)/summationdisplay s/primeP(e|s/prime)/summationdisplay sP(s/prime|s,a)b(s), (17.12) where P(b/prime|e,a,b)is 1 if b/prime=FORW ARD (b,a,e)and 0 otherwise.

Token 13347:
Equation (17.12) can be viewed as deﬁning a transition model for the belief-state space.

Token 13348:
We can also deﬁne a reward function for belief states (i.e., the expected reward for the actualstates the agent might be in): ρ(b)=/summationdisplay sb(s)R(s).

Token 13349:
Together, P(b/prime|b,a)andρ(b)deﬁne an observable MDP on the space of belief states.

Token 13350:
Fur- thermore, it can be shown that an optimal policy for this MDP, π∗(b), is also an optimal policy for the original POMDP.

Token 13351:
In other words, solving a POMDP on a physical state space can be reduced to solving an MDP on the corresponding belief-state space.

Token 13352:
This fact is perhaps less surprising if we remember that the belief state is always observable to the agent, by deﬁnition.

Token 13353:
Notice that, although we have reduced POMDPs to MDPs, the MDP we obtain has a continuous (and usually high-dimensional) state space.

Token 13354:
None of the MDP algorithms de-scribed in Sections 17.2 and 17.3 applies directly to such MDPs.

Token 13355:
The next two subsec- tions describe a value iteration algorithm designed speciﬁcally for POMDPs and an online decision-making algorithm, similar to those developed for games in Chapter 5.

Token 13356:
17.4.2 Value iteration for POMDPs Section 17.2 described a value iteration algorithm that computed one utility value for eachstate.

Token 13357:
With inﬁnitely many belief states, we need to be more creative.

Token 13358:
Consider an optimalpolicy π ∗and its application in a speciﬁc belief state b: the policy generates an action, then, for each subsequent percept, the belief state is updated and a new action is generated, and so on.

Token 13359:
For this speciﬁc b, therefore, the policy is exactly equivalent to a conditional plan ,a sd e - ﬁned in Chapter 4 for nondeterministic and partially observable problems.

Token 13360:
Instead of thinking about policies, let us think about conditional plans and how the expected utility of executing a ﬁxed conditional plan varies with the initial belief state.

Token 13361:
We make two observations:

Token 13362:
Section 17.4. Partially Observable MDPs 661 1. Let the utility of executing a ﬁxed conditional plan pstarting in physical state sbeαp(s).

Token 13363:
Then the expected utility of executing pin belief state bis just/summationtext sb(s)αp(s),o rb·αp if we think of them both as vectors.

Token 13364:
Hence, the expected utility of a ﬁxed conditional plan varies linearly withb; that is, it corresponds to a hyperplane in belief space. 2.

Token 13365:
At any given belief state b, the optimal policy will choose to execute the conditional plan with highest expected utility; and the expected utility of bunder the optimal policy is just the utility of that conditional plan: U(b)=Uπ∗(b)=m a x pb·αp.

Token 13366:
If the optimal policy π∗chooses to execute pstarting at b, then it is reasonable to expect that it might choose to execute pin belief states that are very close to b; in fact, if we bound the depth of the conditional plans, then there are only ﬁnitely many such plansand the continuous space of belief states will generally be divided into regions , each corresponding to a particular conditional plan that is optimal in that region.

Token 13367:
From these two observations, we see that the utility function U(b)on belief states, being the maximum of a collection of hyperplanes, will be piecewise linear andconvex .

Token 13368:
To illustrate this, we use a simple two-state world. The states are labeled 0 and 1, with R(0)= 0 andR(1)= 1 .

Token 13369:
There are two actions: Stay stays put with probability 0.9 and Go switches to the other state with probability 0.9.

Token 13370:
For now we will assume the discount factorγ=1. The sensor reports the correct state with probability 0.6.

Token 13371:
Obviously, the agent should Stay when it thinks it’s in state 1 and Gowhen it thinks it’s in state 0.

Token 13372:
The advantage of a two-state world is that the belief space can be viewed as one- dimensional, because the two probabilities must sum to 1.

Token 13373:
In Figure 17.8(a), the x-axis represents the belief state, deﬁned by b(1), the probability of being in state 1.

Token 13374:
Now let us con- sider the one-step plans [Stay]and[Go], each of which receives the reward for the current state followed by the (discounted) reward for the state reached after the action: α [Stay](0) = R(0) +γ(0.9R(0) + 0 .1R(1)) = 0 .1 α[Stay](1) = R(1) +γ(0.9R(1) + 0 .1R(0)) = 1 .9 α[Go](0) = R(0) +γ(0.9R(1) + 0 .1R(0)) = 0 .9 α[Go](1) = R(1) +γ(0.9R(0) + 0 .1R(1)) = 1 .1 The hyperplanes (lines, in this case) for b·α[Stay]andb·α[Go]are shown in Figure 17.8(a) and their maximum is shown in bold.

Token 13375:
The bold line therefore represents the utility function for the ﬁnite-horizon problem that allows just one action, and in each “piece” of the piecewiselinear utility function the optimal action is the ﬁrst action of the corresponding conditionalplan.

Token 13376:
In this case, the optimal one-step policy is to Stay whenb(1)>0.5andGootherwise.

Token 13377:
Once we have utilities α p(s)for all the conditional plans pof depth 1 in each physical states, we can compute the utilities for conditional plans of depth 2 by considering each possible ﬁrst action, each possible subsequent percept, and then each way of choosing adepth-1 plan to execute for each percept: [Stay;ifPercept =0thenStay elseStay] [Stay;ifPercept =0thenStay elseGo]...

Token 13378:
662 Chapter 17.

Token 13379:
Making Complex Decisions 0 0.5 1 1.5 2 2.5 3 0 0.2 0.4 0.6 0.8 1Utility Probability of state 1[Stay] [Go] 0 0.5 1 1.5 2 2.5 3 0 0.2 0.4 0.6 0.8 1Utility Probability of state 1 (a) (b) 0 0.5 1 1.5 2 2.5 3 0 0.2 0.4 0.6 0.8 1Utility Probability of state 1 4.5 5 5.5 6 6.5 7 7.5 0 0.2 0.4 0.6 0.8 1Utility Probability of state 1 (c) (d) Figure 17.8 (a) Utility of two one-step plans as a function of the initial belief state b(1) for the two-state world, with the corres ponding u tility function shown in bold.

Token 13380:
(b) Utilities for 8 distinct two-step plans. (c) Utilities f or four undominated two- step plans. (d) Utility function for optimal eight-step plans.

Token 13381:
There are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).

Token 13382:
Notice that four of the plans, shown as dashed lines, are suboptimal across the entire beliefspace—we say these plans are dominated , and they need not be considered further.

Token 13383:
There DOMINATED PLAN are four undominated plans, each of which is optimal in a speciﬁc region, as shown in Fig- ure 17.8(c).

Token 13384:
The regions partition the belief-state space. We repeat the process for depth 3, and so on.

Token 13385:
In general, let pbe a depth- dconditional plan whose initial action is aand whose depth- d−1subplan for percept eisp.e;t h e n αp(s)=R(s)+γ/parenleftBigg/summationdisplay s/primeP(s/prime|s,a)/summationdisplay eP(e|s/prime)αp.e(s/prime)/parenrightBigg .

Token 13386:
(17.13) This recursion naturally gives us a value iteration algorithm, which is sketched in Figure 17.9.

Token 13387:
The structure of the algorithm and its error analysis are similar to those of the basic value iter-ation algorithm in Figure 17.4 on page 653; the main difference is that instead of computingone utility number for each state, POMDP-V ALUE -ITERATION maintains a collection of

Token 13388:
Section 17.4.

Token 13389:
Partially Observable MDPs 663 function POMDP-V ALUE -ITERATION (pomdp ,/epsilon1)returns a utility function inputs :pomdp , a POMDP with states S, actions A(s), transition model P(s/prime|s, a), sensor model P(e|s),r e w a r d s R(s), discount γ /epsilon1, the maximum error allowed in the utility of any state local variables :U,U/prime,s e t so fp l a n s pwith associated utility vectors αp U/prime←a set containing just the empty plan [], with α[](s)=R(s) repeat U←U/prime U/prime←the set of all plans consisting of an action and, for each possible next percept, ap l a ni n Uwith utility vectors computed according to Equation (17.13) U/prime←REMOVE -DOMINATED -PLANS (U/prime) until MAX-DIFFERENCE (U,U/prime)</epsilon1(1−γ)/γ return U Figure 17.9 A high-level sketch of the value iteration algorithm for POMDPs.

Token 13390:
The REMOVE -DOMINATED -PLANS step and M AX-DIFFERENCE test are typically implemented as linear programs.

Token 13391:
undominated plans with their utility hyperplanes. The algorithm’s complexity depends pri- marily on how many plans get generated.

Token 13392:
Given |A|actions and|E|possible observations, it is easy to show that there are |A|O(|E|d−1)distinct depth- dplans.

Token 13393:
Even for the lowly two-state world with d=8, the exact number is 2255.

Token 13394:
The elimination of dominated plans is essential for reducing this doubly exponential growth: the number of undominated plans with d=8is just 144.

Token 13395:
The utility function for these 144 plans is shown in Figure 17.8(d).

Token 13396:
Notice that even though state 0 has lower utility than state 1, the intermediate belief states have even lower utility because the agent lacks the information needed to choose agood action.

Token 13397:
This is why information has value in the sense deﬁned in Section 16.6 and optimal policies in POMDPs often include information-gathering actions.

Token 13398:
Given such a utility function, an executable policy can be extracted by looking at which hyperplane is optimal at any given belief state band executing the ﬁrst action of the corre- sponding plan.

Token 13399:
In Figure 17.8(d), the corresponding optimal policy is still the same as for depth-1 plans: Stay whenb(1)>0.5andGootherwise.

Token 13400:
In practice, the value iteration algorithm in Figure 17.9 is hopelessly inefﬁcient for larger problems—even the 4×3POMDP is too hard.

Token 13401:
The main reason is that, given ncon- ditional plans at level d, the algorithm constructs |A|·n |E|conditional plans at level d+1 before eliminating the dominated ones.

Token 13402:
Since the 1970s, when this algorithm was developed,there have been several advances including more efﬁcient forms of value iteration and variouskinds of policy iteration algorithms.

Token 13403:
Some of these are discussed in the notes at the end of the chapter.

Token 13404:
For general POMDPs, however, ﬁnding optimal policies is very difﬁcult (PSPACE- hard, in fact—i.e., very hard indeed).

Token 13405:
Problems with a few dozen states are often infeasible.The next section describes a different, approximate method for solving POMDPs, one basedon look-ahead search.

Token 13406:
664 Chapter 17.

Token 13407:
Making Complex Decisions Xt–1At–1 Rt–1At RtAt+2 Rt+2At+1 Rt+1At–2 Et–1Xt+1 Et+1Xt+2 Et+2Xt+3 Et+3Ut+3 Xt Et Figure 17.10 The generic structure of a dynamic decision network.

Token 13408:
Variables with known values are shaded. The current time is tand the agent must decide what to do—that is, choose av a l u ef o r At.

Token 13409:
The network has been unrolled into the future for three steps and represents future rewards, as well as the utility of the state at the look-ahead horizon.

Token 13410:
17.4.3 Online agents for POMDPs In this section, we outline a simple approach to agent design for partially observable, stochas- tic environments.

Token 13411:
The basic elements of the design are already familiar: •The transition and sensor models are represented by a dynamic Bayesian network (DBN), as described in Chapter 15.

Token 13412:
•The dynamic Bayesian network is extended with decision and utility nodes, as used in decision networks in Chapter 16.

Token 13413:
The resulting model is called a dynamic decision network , or DDN.DYNAMIC DECISION NETWORK •A ﬁltering algorithm is used to incorporate each new percept and action and to update the belief state representation.

Token 13414:
•Decisions are made by projecting forward possible action sequences and choosing the best one.

Token 13415:
DBNs are factored representations in the terminology of Chapter 2; they typically have an exponential complexity advantage over atomic representations and can model quite sub-stantial real-world problems.

Token 13416:
The agent design is therefore a practical implementation of theutility-based agent sketched in Chapter 2.

Token 13417:
In the DBN, the single state S tbecomes a set of state variables Xt, and there may be multiple evidence variables Et.

Token 13418:
We will use Atto refer to the action at time t, so the transition model becomes P(Xt+1|Xt,At)and the sensor model becomes P(Et|Xt).

Token 13419:
We will use Rtto refer to the reward received at time tandUtto refer to the utility of the state at time t.( B o t h of these are random variables.)

Token 13420:
With this notation, a dynamic decision network looks like theone shown in Figure 17.10.

Token 13421:
Dynamic decision networks can be used as inputs for any POMDP algorithm, including those for value and policy iteration methods.

Token 13422:
In this section, we focus on look-ahead methods that project action sequences forward from the current belief state in much the same way as dothe game-playing algorithms of Chapter 5.

Token 13423:
The network in Figure 17.10 has been projectedthree steps into the future; the current and future decisions Aand the future observations

Token 13424:
Section 17.4. Partially Observable MDPs 665 . . . ... ... ... ... ... .... . . ... ... ...... ... .... . . ... . . . ... .... . . ... . .

Token 13425:
.At in P(Xt | E1:t) At+1 in P(Xt+1 | E1:t+1) At+2 in P(Xt+2 | E1:t+2) U(Xt+3)Et+1 Et+2 Et+3 10 4 6 3 Figure 17.11 Part of the look-ahead solution of the DDN in Figure 17.10.

Token 13426:
Each decision will be taken in the belief state indicated. Eand rewards Rare all unknown.

Token 13427:
Notice that the network includes nodes for the rewards forXt+1andXt+2,b u tt h e utility forXt+3.

Token 13428:
This is because the agent must maximize the (discounted) sum of all future rewards, and U(Xt+3)represents the reward for Xt+3and all subsequent rewards.

Token 13429:
As in Chapter 5, we assume that Uis available only in some approximate form: if exact utility values were available, look-ahead beyond depth 1 would be unnecessary.

Token 13430:
Figure 17.11 shows part of the search tree corresponding to the three-step look-ahead DDN in Figure 17.10.

Token 13431:
Each of the triangular nodes is a belief state in which the agent makesa decision A t+ifori=0,1,2,....

Token 13432:
The round (chance) nodes correspond to choices by the environment, namely, what evidence Et+iarrives.

Token 13433:
Notice that there are no chance nodes corresponding to the action outcomes; this is because the belief-state update for an action is deterministic regardless of the actual outcome.

Token 13434:
The belief state at each triangular node can be computed by applying a ﬁltering al- gorithm to the sequence of percepts and actions leading to it.

Token 13435:
In this way, the algorithm takes into account the fact that, for decision At+i, the agent will have available percepts Et+1, ..., Et+i, even though at time tit does not know what those percepts will be.

Token 13436:
In this way, a decision-theoretic agent automatically takes into account the value of information and will execute information-gathering actions where appropriate.

Token 13437:
A decision can be extracted from the search tree by backing up the utility values from the leaves, taking an average at the chance nodes and taking the maximum at the decisionnodes.

Token 13438:
This is similar to the E XPECTIMINIMAX algorithm for game trees with chance nodes, except that (1) there can also be rewards at non-leaf states and (2) the decision nodes corre- spond to belief states rather than actual states.

Token 13439:
The time complexity of an exhaustive searchto depth disO(|A| d·|E|d),w h e r e|A|is the number of available actions and |E|is the num- ber of possible percepts.

Token 13440:
(Notice that this is far less than the number of depth- dconditional

Token 13441:
666 Chapter 17. Making Complex Decisions plans generated by value iteration.)

Token 13442:
For problems in which the discount factor γis not too close to 1, a shallow search is often good enough to give near-optimal decisions.

Token 13443:
It is alsopossible to approximate the averaging step at the chance nodes, by sampling from the set ofpossible percepts instead of summing over all possible percepts.

Token 13444:
There are various other waysof ﬁnding good approximate solutions quickly, but we defer them to Chapter 21.

Token 13445:
Decision-theoretic agents based on dynamic decision networks have a number of advan- tages compared with other, simpler agent designs presented in earlier chapters.

Token 13446:
In particular,they handle partially observable, uncertain environments and can easily revise their “plans” tohandle unexpected evidence.

Token 13447:
With appropriate sensor models, they can handle sensor failureand can plan to gather information.

Token 13448:
They exhibit “graceful degradation” under time pressureand in complex environments, using various approximation techniques.

Token 13449:
So what is missing?One defect of our DDN-based algorithm is its reliance on forward search through state space,rather than using the hierarchical and other advanced planning techniques described in Chap-ter 11.

Token 13450:
There have been attempts to extend these techniques into the probabilistic domain, butso far they have proved to be inefﬁcient.

Token 13451:
A second, related problem is the basically proposi-tional nature of the DDN language.

Token 13452:
We would like to be able to extend some of the ideas forﬁrst-order probabilistic languages to the problem of decision making.

Token 13453:
Current research has shown that this extension is possible and has signiﬁcant beneﬁts, as discussed in the notes at the end of the chapter.

Token 13454:
17.5 D ECISIONS WITH MULTIPLE AGENTS :GAME THEORY This chapter has concentrated on making decisions in uncertain environments.

Token 13455:
But what ifthe uncertainty is due to other agents and the decisions they make?

Token 13456:
And what if the decisionsof those agents are in turn inﬂuenced by our decisions?

Token 13457:
We addressed this question oncebefore, when we studied games in Chapter 5.

Token 13458:
There, however, we were primarily concerned with turn-taking games in fully observable environments, for which minimax search can be used to ﬁnd optimal moves.

Token 13459:
In this section we study the aspects of game theory that analyze GAME THEORY games with simultaneous moves and other sources of partial observability.

Token 13460:
(Game theorists use the terms perfect information andimperfect information rather than fully and partially observable.)

Token 13461:
Game theory can be used in at least two ways: 1.Agent design : Game theory can analyze the agent’s decisions and compute the expected utility for each decision (under the assumption that other agents are acting optimallyaccording to game theory).

Token 13462:
For example, in the game two-ﬁnger Morra , two players, OandE, simultaneously display one or two ﬁngers.

Token 13463:
Let the total number of ﬁngers bef.I ffis odd, Ocollects fdollars from E;a n di f fis even, Ecollects fdollars fromO.

Token 13464:
Game theory can determine the best strategy against a rational player and the expected return for each player.

Token 13465:
4 4Morra is a recreational version of an inspection game .

Token 13466:
In such games, an inspector chooses a day to inspect a facility (such as a restaurant or a bio logical weapons plant), and the facilit y operator chooses a day to hide all the nasty stuff.

Token 13467:
The inspector wins if the days are different, and the facility operator wins if they are the same.

Token 13468:
Section 17.5.

Token 13469:
Decisions with Multiple Agents: Game Theory 667 2.Mechanism design : When an environment is inhabited by many agents, it might be possible to deﬁne the rules of the environment (i.e., the game that the agents mustplay) so that the collective good of all agents is maximized when each agent adopts thegame-theoretic solution that maximizes its own utility.

Token 13470:
For example, game theory canhelp design the protocols for a collection of Internet trafﬁc routers so that each router has an incentive to act in such a way that global throughput is maximized.

Token 13471:
Mechanism design can also be used to construct intelligent multiagent systems that solve complex problems in a distributed fashion.

Token 13472:
17.5.1 Single-move games We start by considering a restricted set of games: ones where all players take action simulta-neously and the result of the game is based on this single set of actions.

Token 13473:
(Actually, it is notcrucial that the actions take place at exactly the same time; what matters is that no player hasknowledge of the other players’ choices.)

Token 13474:
The restriction to a single move (and the very useof the word “game”) might make this seem trivial, but in fact, game theory is serious busi-ness.

Token 13475:
It is used in decision-making situations including the auctioning of oil drilling rightsand wireless frequency spectrum rights, bankruptcy proceedings, product development andpricing decisions, and national defense—situations involving billions of dollars and hundredsof thousands of lives.

Token 13476:
A single-move game is deﬁned by three components: •Players or agents who will be making decisions.

Token 13477:
Two-player games have received the PLAYER most attention, although n-player games for n>2are also common.

Token 13478:
We give players capitalized names, like Alice andBoborOandE. •Actions that the players can choose.

Token 13479:
We will give actions lowercase names, like oneor ACTION testify . The players may or may not have the same set of actions available.

Token 13480:
•Apayoff function that gives the utility to each player for each combination of actions PAYOFF FUNCTION by all the players.

Token 13481:
For single-move games the payoff function can be represented by a matrix, a representation known as the strategic form (also called normal form ).

Token 13482:
The STRATEGIC FORM payoff matrix for two-ﬁnger Morra is as follows: O: one O: two E: one E=+ 2,O=−2 E=−3,O=+ 3 E: two E=−3,O=+ 3 E=+ 4,O=−4 For example, the lower-right corner shows that when player Ochooses action twoand Ealso chooses two, the payoff is +4 for Eand−4forO.

Token 13483:
Each player in a game must adopt and then execute a strategy (which is the name used in STRATEGY game theory for a policy ).

Token 13484:
A pure strategy is a deterministic policy; for a single-move game, PURE STRATEGY a pure strategy is just a single action.

Token 13485:
For many games an agent can do better with a mixed strategy , which is a randomized policy that selects actions according to a probability distri- MIXED STRATEGY bution.

Token 13486:
The mixed strategy that chooses action awith probability pand action botherwise is written [p:a;(1−p):b].

Token 13487:
For example, a mixed strategy for two-ﬁnger Morra might be [0.5:one;0.5:two].Astrategy proﬁle is an assignment of a strategy to each player; given STRATEGY PROFILE the strategy proﬁle, the game’s outcome is a numeric value for each player.

Token 13488:
OUTCOME

Token 13489:
668 Chapter 17. Making Complex Decisions Asolution to a game is a strategy proﬁle in which each player adopts a rational strategy.

Token 13490:
SOLUTION We will see that the most important issue in game theory is to deﬁne what “rational” means when each agent chooses only part of the strategy proﬁle that determines the outcome.

Token 13491:
It isimportant to realize that outcomes are actual results of playing a game, while solutions aretheoretical constructs used to analyze a game.

Token 13492:
We will see that some games have a solution only in mixed strategies.

Token 13493:
But that does not mean that a player must literally be adopting a mixed strategy to be rational.

Token 13494:
Consider the following story: Two alleged burglars, Alice and Bob, are caught red- handed near the scene of a burglary and are interrogated separately.

Token 13495:
A prosecutor offers each a deal: if you testify against your partner as the leader of a burglary ring, you’ll go free forbeing the cooperative one, while your partner will serve 10 years in prison.

Token 13496:
However, if youboth testify against each other, you’ll both get 5 years.

Token 13497:
Alice and Bob also know that if bothrefuse to testify they will serve only 1 year each for the lesser charge of possessing stolenproperty.

Token 13498:
Now Alice and Bob face the so-called prisoner’s dilemma : should they testify PRISONER’S DILEMMA or refuse?

Token 13499:
Being rational agents, Alice and Bob each want to maximize their own expected utility.

Token 13500:
Let’s assume that Alice is callously unconcerned about her partner’s fate, so her utilitydecreases in proportion to the number of years she will spend in prison, regardless of whathappens to Bob.

Token 13501:
Bob feels exactly the same way.

Token 13502:
To help reach a rational decision, they bothconstruct the following payoff matrix: Alice :testify Alice :refuse Bob:testify A=−5,B=−5 A=−10,B=0 Bob:refuse A=0,B=−10 A=−1,B=−1 Alice analyzes the payoff matrix as follows: “Suppose Bob testiﬁes.

Token 13503:
Then I get 5 years if I testify and 10 years if I don’t, so in that case testifying is better.

Token 13504:
On the other hand, if Bobrefuses, then I get 0 years if I testify and 1 year if I refuse, so in that case as well testifying isbetter.

Token 13505:
So in either case, it’s better for me to testify, so that’s what I must do.” Alice has discovered that testify is adominant strategy for the game.

Token 13506:
We say that a DOMINANT STRATEGY strategy sfor player pstrongly dominates strategy s/primeif the outcome for sis better for pthanSTRONG DOMINATION the outcome for s/prime, for every choice of strategies by the other player(s).

Token 13507:
Strategy sweakly dominates s/primeifsis better than s/primeon at least one strategy proﬁle and no worse on any other.

Token 13508:
WEAK DOMINATION A dominant strategy is a strategy that dominates all others.

Token 13509:
It is irrational to play a dominated strategy, and irrational not to play a dominant strategy if one exists.

Token 13510:
Being rational, Alice chooses the dominant strategy.

Token 13511:
We need just a bit more terminology: we say that an outcome isPareto optimal5if there is no other outcome that all players would prefer.

Token 13512:
An outcome is PARETO OPTIMAL Pareto dominated by another outcome if all players would prefer the other outcome.

Token 13513:
PARETO DOMINATED If Alice is clever as well as rational, she will continue to reason as follows: Bob’s dominant strategy is also to testify.

Token 13514:
Therefore, he will testify and we will both get ﬁve years.When each player has a dominant strategy, the combination of those strategies is called adominant strategy equilibrium .

Token 13515:
In general, a strategy proﬁle forms an equilibrium if no DOMINANT STRATEGY EQUILIBRIUM EQUILIBRIUM player can beneﬁt by switching strategies, given that every other player sticks with the same 5Pareto optimality is named after the economist Vilfredo Pareto (1848–1923).

Token 13516:
Section 17.5. Decisions with Multiple Agents: Game Theory 669 strategy.

Token 13517:
An equilibrium is essentially a local optimum in the space of policies; it is the top of a peak that slopes downward along every dimension, where a dimension corresponds to aplayer’s strategy choices.

Token 13518:
The mathematician John Nash (1928–) proved that every game has at least one equi- librium.

Token 13519:
The general concept of equilibrium is now called Nash equilibrium in his honor.

Token 13520:
Clearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but some NASH EQUILIBRIUM games have Nash equilibria but no dominant strategies.

Token 13521:
The dilemma in the prisoner’s dilemma is that the equilibrium outcome is worse for both players than the outcome they would get if they both refused to testify.

Token 13522:
In other words,(testify ,testify )is Pareto dominated by the (-1, -1) outcome of (refuse ,refuse ).I st h e r ea n y way for Alice and Bob to arrive at the (-1, -1) outcome?

Token 13523:
It is certainly an allowable option for both of them to refuse to testify, but is is hard to see how rational agents can get there,given the deﬁnition of the game.

Token 13524:
Either player contemplating playing refuse will realize that he or she would do better by playing testify .

Token 13525:
That is the attractive power of an equilibrium point.

Token 13526:
Game theorists agree that being a Nash equilibrium is a necessary condition for beinga solution—although they disagree whether it is a sufﬁcient condition.

Token 13527:
It is easy enough to get to the (refuse ,refuse )solution if we modify the game.

Token 13528:
For example, we could change to a repeated game in which the players know that they will meet again.

Token 13529:
Or the agents might have moral beliefs that encourage cooperation and fairness.

Token 13530:
Thatmeans they have a different utility function, necessitating a different payoff matrix, makingit a different game.

Token 13531:
We will see later that agents with limited computational powers, ratherthan the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can anagent that knows that the other agent has limited rationality.

Token 13532:
In each case, we are consideringa different game than the one described by the payoff matrix above.

Token 13533:
Now let’s look at a game that has no dominant strategy.

Token 13534:
Acme, a video game console manufacturer, has to decide whether its next game machine will use Blu-ray discs or DVDs.Meanwhile, the video game software producer Best needs to decide whether to produce itsnext game on Blu-ray or DVD.

Token 13535:
The proﬁts for both will be positive if they agree and negative if they disagree, as shown in the following payoff matrix: Acme :bluray Acme :dvd Best :bluray A=+ 9,B=+ 9 A=−4,B=−1 Best :dvd A=−3,B=−1 A=+ 5,B=+ 5 There is no dominant strategy equilibrium for this game, but there are twoNash equilibria: (bluray, bluray )a n d( dvd, dvd ).

Token 13536:
We know these are Nash equilibria because if either player unilaterally moves to a different strategy, that player will be worse off.

Token 13537:
Now the agents havea problem: there are multiple acceptable solutions, but if each agent aims for a different solution, then both agents will suffer.

Token 13538:
How can they agree on a solution?

Token 13539:
One answer is that both should choose the Pareto-optimal solution ( bluray, bluray ); that is, we can restrict the deﬁnition of “solution” to the unique Pareto-optimal Nash equilibrium provided that one exists .

Token 13540:
Every game has at least one Pareto-optimal solution, but a game might have several, or they might not be equilibrium points.

Token 13541:
For example, if ( bluray, bluray ) had payoff (5, 5), then there would be two equal Pareto-optimal equilibrium points. To choose between

Token 13542:
670 Chapter 17.

Token 13543:
Making Complex Decisions them the agents can either guess or communicate , which can be done either by establishing a convention that orders the solutions before the game begins or by negotiating to reach amutually beneﬁcial solution during the game (which would mean including communicativeactions as part of a sequential game).

Token 13544:
Communication thus arises in game theory for exactlythe same reasons that it arose in multiagent planning in Section 11.4.

Token 13545:
Games in which players need to communicate like this are called coordination games .

Token 13546:
COORDINATION GAME A game can have more than one Nash equilibrium; how do we know that every game must have at least one?

Token 13547:
Some games have no pure-strategy Nash equilibria. Consider, for example, any pure-strategy proﬁle for two-ﬁnger Morra (page 666).

Token 13548:
If the total number ofﬁngers is even, then Owill want to switch; on the other hand (so to speak), if the total is odd, thenEwill want to switch.

Token 13549:
Therefore, no pure strategy proﬁle can be an equilibrium and we must look to mixed strategies instead. Butwhich mixed strategy?

Token 13550:
In 1928, von Neumann developed a method for ﬁnding the optimal mixed strategy for two-player, zero-sum games —games in which the sum of the ZERO-SUM GAME payoffs is always zero.6Clearly, Morra is such a game.

Token 13551:
For two-player, zero-sum games, we know that the payoffs are equal and opposite, so we need consider the payoffs of only oneplayer, who will be the maximizer (just as in Chapter 5).

Token 13552:
For Morra, we pick the even player Eto be the maximizer, so we can deﬁne the payoff matrix by the values U E(e,o)—the payoff toEifEdoeseandOdoeso.

Token 13553:
(For convenience we call player E“her” and O“him.”) Von Neumann’s method is called the the maximin technique, and it works as follows: MAXIMIN •Suppose we change the rules as follows: ﬁrst Epicks her strategy and reveals it to O.T h e n Opicks his strategy, with knowledge of E’s strategy.

Token 13554:
Finally, we evaluate the expected payoff of the game based on the chosen strategies.

Token 13555:
This gives us a turn-taking game to which we can apply the standard minimax algorithm from Chapter 5. Let’s suppose this gives an outcome U E,O.

Token 13556:
Clearly, this game favors O, so the true utility Uof the original game (from E’s point of view) is at least UE,O.

Token 13557:
For example, if we just look at pure strategies, the minimax game tree has a root value of −3(see Figure 17.12(a)), so we know that U≥−3.

Token 13558:
•Now suppose we change the rules to force Oto reveal his strategy ﬁrst, followed by E. Then the minimax value of this game is UO,E, and because this game favors Ewe know thatUisat most UO,E.

Token 13559:
With pure strategies, the value is +2(see Figure 17.12(b)), so we know U≤+2.

Token 13560:
Combining these two arguments, we see that the true utility Uof the solution to the original game must satisfy UE,O≤U≤UO,E or in this case, −3≤U≤2.

Token 13561:
To pinpoint the value of U, we need to turn our analysis to mixed strategies.

Token 13562:
First, observe the following: once the ﬁrst player has revealed his or her strategy, the second player might as well choose a pure strategy.

Token 13563:
The reason is simple: if the second player plays a mixed strategy, [p:one;(1−p):two], its expected utility is a linear combination (p·uone+(1−p)·utwo)of 6or a constant—see page 162.

Token 13564:
Section 17.5.

Token 13565:
Decisions with Multiple Agents: Game Theory 671 one one onetwo two twoE Oone one onetwo two twoO E one twoE O one twoO E +4 +3 +2 +1 0 –1 –2–31two oneU p+4 +3 +2 +1 0 –1 –2–31two oneU q(a) (b) (c) (d) (e) (f)[p: one; (1 – p): two][ q: one; (1 – q): two] 2p – 3(1 – p)2 q – 3(1 – q) 3p + 4(1 – p)3 q + 4(1 – q)2- 3-3-3 -3-3 4 222 -3 -3 44 Figure 17.12 (a) and (b): Minimax game trees for two-ﬁnger Morra if the players take turns playing pure strategies.

Token 13566:
(c) and (d): Parameterized game trees where the ﬁrst playerplays a mixed strategy.

Token 13567:
The payoffs depend on the probability parameter ( porq)i nt h e mixed strategy.

Token 13568:
(e) and (f): For any particular value of the probability parameter, the second player will choose the “better” of the two actions, so the value of the ﬁrst player’s mixedstrategy is given by the heavy lines.

Token 13569:
The ﬁrst player will choose the probability parameter for the mixed strategy at the intersection point.

Token 13570:
the utilities of the pure strategies, uoneandutwo.

Token 13571:
This linear combination can never be better than the better of uoneandutwo, so the second player can just choose the better one.

Token 13572:
With this observation in mind, the minimax trees can be thought of as having inﬁnitely many branches at the root, corresponding to the inﬁnitely many mixed strategies the ﬁrst

Token 13573:
672 Chapter 17. Making Complex Decisions player can choose.

Token 13574:
Each of these leads to a node with two branches corresponding to the pure strategies for the second player.

Token 13575:
We can depict these inﬁnite trees ﬁnitely by having one“parameterized” choice at the root: •IfEchooses ﬁrst, the situation is as shown in Figure 17.12(c).

Token 13576:
Echooses the strategy [p:one;(1−p):two]at the root, and then Ochooses a pure strategy (and hence a move) given the value of p.I fOchooses one, the expected payoff (to E)i s2p−3(1−p)=5p− 3;i fOchooses two, the expected payoff is −3p+4 ( 1−p)=4−7p.

Token 13577:
We can draw these two payoffs as straight lines on a graph, where pranges from 0 to 1 on the x-axis, as shown in Figure 17.12(e).

Token 13578:
O, the minimizer, will always choose the lower of the two lines, as shown by the heavy lines in the ﬁgure.

Token 13579:
Therefore, the best that Ecan do at the root is to choose pto be at the intersection point, which is where 5p−3=4−7p⇒ p=7/12.

Token 13580:
The utility for Eat this point is U E,O=−1/12. •IfOmoves ﬁrst, the situation is as shown in Figure 17.12(d).

Token 13581:
Ochooses the strategy [q:one;(1−q):two]at the root, and then Echooses a move given the value of q.T h e payoffs are 2q−3(1−q)=5q−3and−3q+4(1−q)=4−7q.7Again, Figure 17.12(f) shows that the best Ocan do at the root is to choose the intersection point: 5q−3=4−7q⇒ q=7/12.

Token 13582:
The utility for Eat this point is UO,E=−1/12.

Token 13583:
Now we know that the true utility of the original game lies between −1/12and−1/12,t h a t is, it is exactly −1/12!

Token 13584:
(The moral is that it is better to be OthanEif you are playing this game.)

Token 13585:
Furthermore, the true utility is attained by the mixed strategy [7/12:one;5/12:two], which should be played by both players.

Token 13586:
This strategy is called the maximin equilibrium ofMAXIMIN EQUILIBRIUM the game, and is a Nash equilibrium.

Token 13587:
Note that each component strategy in an equilibrium mixed strategy has the same expected utility.

Token 13588:
In this case, both one andtwohave the same expected utility, −1/12, as the mixed strategy itself.

Token 13589:
Our result for two-ﬁnger Morra is an example of the general result by von Neumann: every two-player zero-sum game has a maximin equilibrium when you allow mixed strategies.

Token 13590:
Furthermore, every Nash equilibrium in a zero-sum game is a maximin for both players.

Token 13591:
Aplayer who adopts the maximin strategy has two guarantees: First, no other strategy can dobetter against an opponent who plays well (although some other strategies might be better atexploiting an opponent who makes irrational mistakes).

Token 13592:
Second, the player continues to do just as well even if the strategy is revealed to the opponent.

Token 13593:
The general algorithm for ﬁnding maximin equilibria in zero-sum games is somewhat more involved than Figures 17.12(e) and (f) might suggest.

Token 13594:
When there are npossible actions, a mixed strategy is a point in n-dimensional space and the lines become hyperplanes.

Token 13595:
It’s also possible for some pure strategies for the second player to be dominated by others, sothat they are not optimal against anystrategy for the ﬁrst player.

Token 13596:
After removing all such strategies (which might have to be done repeatedly), the optimal choice at the root is the 7It is a coincidence that these equations are the same as those for p; the coincidence arises because UE(one,two)=UE(two,one)=−3.

Token 13597:
This also explains why the optimal strategy is the same for both players.

Token 13598:
Section 17.5. Decisions with Multiple Agents: Game Theory 673 highest (or lowest) intersection point of the remaining hyperplanes.

Token 13599:
Finding this choice is an example of a linear programming problem: maximizing an objective function subject to linear constraints.

Token 13600:
Such problems can be solved by standard techniques in time polynomialin the number of actions (and in the number of bits used to specify the reward function, if youwant to get technical).

Token 13601:
The question remains, what should a rational agent actually doin playing a single game of Morra?

Token 13602:
The rational agent will have derived the fact that [7/12:one;5/12:two]is the maximin equilibrium strategy, and will assume that this is mutual knowledge with a rationalopponent.

Token 13603:
The agent could use a 12-sided die or a random number generator to pick randomlyaccording to this mixed strategy, in which case the expected payoff would be -1/12 for E.O r the agent could just decide to play one,o rtwo.

Token 13604:
In either case, the expected payoff remains -1/12 for E. Curiously, unilaterally choosing a particular action does not harm one’s expected payoff, but allowing the other agent to know that one has made such a unilateral decision does affect the expected payoff, because then the opponent can adjust his strategy accordingly.

Token 13605:
Finding equilibria in non-zero-sum games is somewhat more complicated.

Token 13606:
The general approach has two steps: (1) Enumerate all possible subsets of actions that might form mixedstrategies.

Token 13607:
For example, ﬁrst try all strategy proﬁles where each player uses a single action, then those where each player uses either one or two actions, and so on.

Token 13608:
This is exponential in the number of actions, and so only applies to relatively small games.

Token 13609:
(2) For each strategyproﬁle enumerated in (1), check to see if it is an equilibrium.

Token 13610:
This is done by solving a set ofequations and inequalities that are similar to the ones used in the zero-sum case.

Token 13611:
For two play-ers these equations are linear and can be solved with basic linear programming techniques,but for three or more players they are nonlinear and may be very difﬁcult to solve.

Token 13612:
17.5.2 Repeated games So far we have looked only at games that last a single move.

Token 13613:
The simplest kind of multiple-move game is the repeated game , in which players face the same choice repeatedly, but each REPEATED GAME time with knowledge of the history of all players’ previous choices.

Token 13614:
A strategy proﬁle for a repeated game speciﬁes an action choice for each player at each time step for every possiblehistory of previous choices.

Token 13615:
As with MDPs, payoffs are additive over time. Let’s consider the repeated version of the prisoner’s dilemma.

Token 13616:
Will Alice and Bob work together and refuse to testify, knowing they will meet again? The answer depends on thedetails of the engagement.

Token 13617:
For example, suppose Alice and Bob know that they must playexactly 100 rounds of prisoner’s dilemma.

Token 13618:
Then they both know that the 100th round will notbe a repeated game—that is, its outcome can have no effect on future rounds—and thereforethey will both choose the dominant strategy, testify , in that round.

Token 13619:
But once the 100th round is determined, the 99th round can have no effect on subsequent rounds, so it too will havea dominant strategy equilibrium at (testify ,testify ).

Token 13620:
By induction, both players will choose testify on every round, earning a total jail sentence of 500 years each.

Token 13621:
We can get different solutions by changing the rules of the interaction.

Token 13622:
For example, suppose that after each round there is a 99% chance that the players will meet again.

Token 13623:
Thenthe expected number of rounds is still 100, but neither player knows for sure which round

Token 13624:
674 Chapter 17. Making Complex Decisions will be the last. Under these conditions, more cooperative behavior is possible.

Token 13625:
For example, one equilibrium strategy is for each player to refuse unless the other player has ever played testify .

Token 13626:
This strategy could be called perpetual punishment .

Token 13627:
Suppose both players havePERPETUAL PUNISHMENT adopted this strategy, and this is mutual knowledge.

Token 13628:
Then as long as neither player has played testify , then at any point in time the expected future total payoff for each player is ∞/summationdisplay t=00.99t·(−1) =−100.

Token 13629:
A player who deviates from the strategy and chooses testify will gain a score of 0 rather than −1on the very next move, but from then on both players will play testify and the player’s total expected future payoff becomes 0+∞/summationdisplay t=10.99t·(−5) =−495.

Token 13630:
Therefore, at every step, there is no incentive to deviate from (refuse ,refuse ).

Token 13631:
Perpetual punishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: onceeither player decides to testify , it ensures that both players suffer a great deal.

Token 13632:
But it works as a deterrent only if the other player believes you have adopted this strategy—or at least thatyou might have adopted it.

Token 13633:
Other strategies are more forgiving.

Token 13634:
The most famous, called tit-for-tat , calls for start- TIT-FOR-TAT ing with refuse and then echoing the other player’s previous move on all subsequent moves.

Token 13635:
So Alice would refuse as long as Bob refuses and would testify the move after Bob testiﬁed,but would go back to refusing if Bob did.

Token 13636:
Although very simple, this strategy has proven tobe highly robust and effective against a wide variety of strategies.

Token 13637:
We can also get different solutions by changing the agents, rather than changing the rules of engagement.

Token 13638:
Suppose the agents are ﬁnite-state machines with nstates and they a r ep l a y i n gag a m ew i t h m>n total steps.

Token 13639:
The agents are thus incapable of representing the number of remaining steps, and must treat it as an unknown.

Token 13640:
Therefore, they cannot dothe induction, and are free to arrive at the more favorable ( refuse, refuse ) equilibrium.

Token 13641:
In this case, ignorance isbliss—or rather, having your opponent believe that you are ignorant is bliss.

Token 13642:
Your success in these repeated games depends on the other player’s perception of you as a bully or a simpleton, and not on your actual characteristics.

Token 13643:
17.5.3 Sequential games In the general case, a game consists of a sequence of turns that need not be all the same.

Token 13644:
Such games are best represented by a game tree, which game theorists call the extensive form .T h e EXTENSIVE FORM tree includes all the same information we saw in Section 5.1: an initial state S0, a function PLAYER (s)that tells which player has the move, a function A CTIONS (s)enumerating the possible actions, a function R ESULT (s,a)that deﬁnes the transition to a new state, and a partial function U TILITY (s,p), which is deﬁned only on terminal states, to give the payoff for each player.

Token 13645:
To represent stochastic games, such as backgammon, we add a distinguished player, chance , that can take random actions.

Token 13646:
Chance ’s “strategy” is part of the deﬁnition of the

Token 13647:
Section 17.5.

Token 13648:
Decisions with Multiple Agents: Game Theory 675 game, speciﬁed as a probability distribution over actions (the other players get to choose their own strategy).

Token 13649:
To represent games with nondeterministic actions, such as billiards, webreak the action into two pieces: the player’s action itself has a deterministic result, and thenchance has a turn to react to the action in its own capricious way.

Token 13650:
To represent simultaneous moves, as in the prisoner’s dilemma or two-ﬁnger Morra, we impose an arbitrary order on the players, but we have the option of asserting that the earlier player’s actions are not observable to the subsequent players: e.g., Alice must choose refuse ortestify ﬁrst, then Bob chooses, but Bob does not know what choice Alice made at that time (we can also represent the factthat the move is revealed later).

Token 13651:
However, we assume the players always remember all theirown previous actions; this assumption is called perfect recall .

Token 13652:
The key idea of extensive form that sets it apart from the game trees of Chapter 5 is the representation of partial observability.

Token 13653:
We saw in Section 5.6 that a player in a partiallyobservable game such as Kriegspiel can create a game tree over the space of belief states .

Token 13654:
With that tree, we saw that in some cases a player can ﬁnd a sequence of moves (a strategy)that leads to a forced checkmate regardless of what actual state we started in, and regardless ofwhat strategy the opponent uses.

Token 13655:
However, the techniques of Chapter 5 could not tell a playerwhat to do when there is no guaranteed checkmate.

Token 13656:
If the player’s best strategy depends on the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot ﬁnd a solution.

Token 13657:
The extensive form does allow us to ﬁnd solutions because it represents the belief states (game theorists call them information sets )o f allplayers at once.

Token 13658:
From that INFORMATION SETS representation we can ﬁnd equilibrium solutions, just as we did with normal-form games.

Token 13659:
As a simple example of a sequential game, place two agents in the 4×3world of Fig- ure 17.1 and have them move simultaneously until one agent reaches an exit square, and gets the payoff for that square.

Token 13660:
If we specify that no movement occurs when the two agents tryto move into the same square simultaneously (a common problem at many trafﬁc intersec-tions), then certain pure strategies can get stuck forever.

Token 13661:
Thus, agents need a mixed strategyto perform well in this game: randomly choose between moving ahead and staying put.

Token 13662:
Thisis exactly what is done to resolve packet collisions in Ethernet networks. Next we’ll consider a very simple variant of poker.

Token 13663:
The deck has only four cards, two aces and two kings. One card is dealt to each player.

Token 13664:
The ﬁrst player then has the optiontoraise the stakes of the game from 1 point to 2, or to check . If player 1 checks, the game is over.

Token 13665:
If he raises, then player 2 has the option to call, accepting that the game is worth 2 points, or fold, conceding the 1 point.

Token 13666:
If the game does not end with a fold, then the payoff depends on the cards: it is zero for both players if they have the same card; otherwise theplayer with the king pays the stakes to the player with the ace.

Token 13667:
The extensive-form tree for this game is shown in Figure 17.13.

Token 13668:
Nonterminal states are shown as circles, with the player to move inside the circle; player 0 is chance .

Token 13669:
Each action is depicted as an arrow with a label, corresponding to a raise, check, call, orfold,o r ,f o r chance , the four possible deals (“AK” means that player 1 gets an ace and player 2 a king).

Token 13670:
Terminal states are rectangles labeled by their payoff to player 1 and player 2.

Token 13671:
Information sets are shown as labeled dashed boxes; for example, I 1,1is the information set where it is player 1’s turn, and he knows he has an ace (but does not know what player 2 has).

Token 13672:
In informationsetI 2,1, it is player 2’s turn and she knows that she has an ace and that player 1 has raised,

Token 13673:
676 Chapter 17. Making Complex Decisions 0 1 1 1 1 2 2 2 2 0,0 ! +1,-1 ! 0,0 ! -1,+1 ! 1/6: AAr k r k r k r k +1,-1 ! +1,-1 ! +1,-1 ! +1,-1 !0,0 !

Token 13674:
+2,-2 0,0 -2,+2c f c f c f c f 1/3: KA1/3: AK 1/6: KK 2I1,1 I1,2 I2,1 I2,2 I2,1 Figure 17.13 Extensive form of a simpliﬁed version of poker.

Token 13675:
but does not know what card player 1 has. (Due to the limits of two-dimensional paper, this information set is shown as two boxes rather than one.)

Token 13676:
One way to solve an extensive game is to convert it to a normal-form game.

Token 13677:
Recall that the normal form is a matrix, each row of which is labeled with a pure strategy for player 1, andeach column by a pure strategy for player 2.

Token 13678:
In an extensive game a pure strategy for playericorresponds to an action for each information set involving that player.

Token 13679:
So in Figure 17.13, one pure strategy for player 1 is “raise when in I 1,1(that is, when I have an ace), and check when in I1,2(when I have a king).” In the payoff matrix below, this strategy is called rk.

Token 13680:
Similarly, strategy cffor player 2 means “call when I have an ace and fold when I have a king.” Since this is a zero-sum game, the matrix below gives only the payoff for player 1; player 2 always has the opposite payoff: 2:cc 2:cf 2:ff 2:fc 1:rr 0 -1/6 1 7/6 1:kr -1/3 -1/6 5/6 2/3 1:rk 1/3 0 1/6 1/2 1:kk 0 0 0 0 This game is so simple that it has two pure-strategy equilibria, shown in bold: cffor player 2a n d rkorkkfor player 1.

Token 13681:
But in general we can solve extensive games by converting to normal form and then ﬁnding a solution (usually a mixed strategy) using standard linear programming methods.

Token 13682:
That works in theory. But if a player has Iinformation sets and aactions per set, then that player will have aIpure strategies.

Token 13683:
In other words, the size of the normal-form matrix is exponential in the number of information sets, so in practice the

Token 13684:
Section 17.5. Decisions with Multiple Agents: Game Theory 677 approach works only for very small game trees, on the order of a dozen states.

Token 13685:
A game like Texas hold’em poker has about 1018states, making this approach completely infeasible. What are the alternatives?

Token 13686:
In Chapter 5 we saw how alpha–beta search could handle games of perfect information with huge game trees by generating the tree incrementally, bypruning some branches, and by heuristically evaluating nonterminal nodes.

Token 13687:
But that approach does not work well for games with imperfect information, for two reasons: ﬁrst, it is harder to prune, because we need to consider mixed strategies that combine multiple branches, not apure strategy that always chooses the best branch.

Token 13688:
Second, it is harder to heuristically evaluatea nonterminal node, because we are dealing with information sets, not individual states. Koller et al.

Token 13689:
(1996) come to the rescue with an alternative representation of extensive games, called the sequence form , that is only linear in the size of the tree, rather than ex- SEQUENCE FORM ponential.

Token 13690:
Rather than represent strategies, it represents paths through the tree; the number of paths is equal to the number of terminal nodes.

Token 13691:
Standard linear programming methodscan again be applied to this representation.

Token 13692:
The resulting system can solve poker variantswith 25,000 states in a minute or two.

Token 13693:
This is an exponential speedup over the normal-formapproach, but still falls far short of handling full poker, with 10 18states.

Token 13694:
If we can’t handle 1018states, perhaps we can simplify the problem by changing the game to a simpler form.

Token 13695:
For example, if I hold an ace and am considering the possibility that the next card will give me a pair of aces, then I don’t care about the suit of the next card; anysuit will do equally well.

Token 13696:
This suggests forming an abstraction of the game, one in which ABSTRACTION suits are ignored.

Token 13697:
The resulting game tree will be smaller by a factor of 4! = 24 .

Token 13698:
Suppose I can solve this smaller game; how will the solution to that game relate to the original game?

Token 13699:
If no player is going for a ﬂush (or blufﬁng so), then the suits don’t matter to any player, and the solution for the abstraction will also be a solution for the original game.

Token 13700:
However, if anyplayer is contemplating a ﬂush, then the abstraction will be only an approximate solution (butit is possible to compute bounds on the error).

Token 13701:
There are many opportunities for abstraction.

Token 13702:
For example, at the point in a game where each player has two cards, if I hold a pair of queens, then the other players’ hands could be abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of queens) or worse (everything else).

Token 13703:
However, this abstraction might be too coarse.

Token 13704:
A better abstraction would divide worse into, say, medium pair (nines through jacks), low pair ,a n d no pair .

Token 13705:
These examples are abstractions of states; it is also possible to abstract actions.

Token 13706:
For example, instead of having a bet action for each integer from 1 to 1000, we could restrict thebets to 10 0,101,102and103.

Token 13707:
Or we could cut out one of the rounds of betting altogether.

Token 13708:
We can also abstract over chance nodes, by considering only a subset of the possible deals.This is equivalent to the rollout technique used in Go programs.

Token 13709:
Putting all these abstractionstogether, we can reduce the 10 18states of poker to 107states, a size that can be solved with current techniques.

Token 13710:
Poker programs based on this approach can easily defeat novice and some experienced human players, but are not yet at the level of master players.

Token 13711:
Part of the problem is that the solution these programs approximate—the equilibrium solution—is optimal only againstan opponent who also plays the equilibrium strategy.

Token 13712:
Against fallible human players it isimportant to be able to exploit an opponent’s deviation from the equilibrium strategy. As

Token 13713:
678 Chapter 17. Making Complex Decisions Gautam Rao (aka “The Count”), the world’s leading online poker player, said (Billings et al.

Token 13714:
, 2003), “You have a very strong program.

Token 13715:
Once you add opponent modeling to it, it will killeveryone.” However, good models of human fallability remain elusive.

Token 13716:
In a sense, extensive game form is the one of the most complete representations we have seen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic environments—most of the hard cases from the list of environment properties on page 42.

Token 13717:
However, there are two limitations of game theory.

Token 13718:
First, it does not deal well with continuousstates and actions (although there have been some extensions to the continuous case; forexample, the theory of Cournot competition uses game theory to solve problems where two COURNOT COMPETITION companies choose prices for their products from a continuous space).

Token 13719:
Second, game theory assumes the game is known .

Token 13720:
Parts of the game may be speciﬁed as unobservable to some of the players, but it must be known what parts are unobservable.

Token 13721:
In cases in which the playerslearn the unknown structure of the game over time, the model begins to break down.

Token 13722:
Let’sexamine each source of uncertainty, and whether each can be represented in game theory.

Token 13723:
Actions: There is no easy way to represent a game where the players have to discover what actions are available.

Token 13724:
Consider the game between computer virus writers and securityexperts. Part of the problem is anticipating what action the virus writers will try next.

Token 13725:
Strategies: Game theory is very good at representing the idea that the other players’ strategies are initially unknown—as long as we assume all agents are rational.

Token 13726:
The theoryitself does not say what to do when the other players are less than fully rational.

Token 13727:
The notionof aBayes–Nash equilibrium partially addresses this point: it is an equilibrium with respect BAYES–NASH EQUILIBRIUM to a player’s prior probability distribution over the other players’ strategies—in other words, it expresses a player’s beliefs about the other players’ likely strategies.

Token 13728:
Chance: If a game depends on the roll of a die, it is easy enough to model a chance node with uniform distribution over the outcomes.

Token 13729:
But what if it is possible that the die is unfair?We can represent that with another chance node, higher up in the tree, with two branches for“die is fair” and “die is unfair,” such that the corresponding nodes in each branch are in thesame information set (that is, the players don’t know if the die is fair or not).

Token 13730:
And what if we suspect the other opponent does know?

Token 13731:
Then we add another chance node, with one branch representing the case where the opponent does know, and one where he doesn’t.

Token 13732:
Utilities: What if we don’t know our opponent’s utilities?

Token 13733:
Again, that can be modeled with a chance node, such that the other agent knows its own utilities in each branch, but wedon’t.

Token 13734:
But what if we don’t know our own utilities?

Token 13735:
For example, how do I know if it is rational to order the Chef’s salad if I don’t know how much I will like it?

Token 13736:
We can model thatwith yet another chance node specifying an unobservable “intrinsic quality” of the salad.

Token 13737:
Thus, we see that game theory is good at representing most sources of uncertainty—but at the cost of doubling the size of the tree every time we add another node; a habit which quickly leads to intractably large trees.

Token 13738:
Because of these and other problems, game theory has been used primarily to analyze environments that are at equilibrium, rather than to control agents within an environment.

Token 13739:
Next we shall see how it can help design environments.

Token 13740:
Section 17.6.

Token 13741:
Mechanism Design 679 17.6 M ECHANISM DESIGN In the previous section, we asked, “Given a game, what is a rational strategy?” In this sec- tion, we ask, “Given that agents pick rational strategies, what game should we design?” Morespeciﬁcally, we would like to design a game whose solutions, consisting of each agent pursu-ing its own rational strategy, result in the maximization of some global utility function.

Token 13742:
Thisproblem is called mechanism design , or sometimes inverse game theory .

Token 13743:
Mechanism de- MECHANISM DESIGN sign is a staple of economics and political science.

Token 13744:
Capitalism 101 says that if everyone tries to get rich, the total wealth of society will increase.

Token 13745:
But the examples we will discuss showthat proper mechanism design is necessary to keep the invisible hand on track.

Token 13746:
For collections of agents, mechanism design allows us to construct smart systems out of a collection of more limited systems—even uncooperative systems—in much the same way that teams of humanscan achieve goals beyond the reach of any individual.

Token 13747:
Examples of mechanism design include auctioning off cheap airline tickets, routing TCP packets between computers, deciding how medical interns will be assigned to hospitals,and deciding how robotic soccer players will cooperate with their teammates.

Token 13748:
Mechanismdesign became more than an academic subject in the 1990s when several nations, faced withthe problem of auctioning off licenses to broadcast in various frequency bands, lost hundredsof millions of dollars in potential revenue as a result of poor mechanism design.

Token 13749:
Formally,amechanism consists of (1) a language for describing the set of allowable strategies that MECHANISM agents may adopt, (2) a distinguished agent, called the center , that collects reports of strategy CENTER choices from the agents in the game, and (3) an outcome rule, known to all agents, that the center uses to determine the payoffs to each agent, given their strategy choices.

Token 13750:
17.6.1 Auctions Let’s consider auctions ﬁrst. An auction is a mechanism for selling some goods to members AUCTION of a pool of bidders.

Token 13751:
For simplicity, we concentrate on auctions with a single item for sale. Each bidder ihas a utility value vifor having the item.

Token 13752:
In some cases, each bidder has a private value for the item.

Token 13753:
For example, the ﬁrst item sold on eBay was a broken laser pointer, which sold for $14.83 to a collector of broken laser pointers.

Token 13754:
Thus, we know that thecollector has v i≥$14.83, but most other people would have vj/lessmuch$14.83.

Token 13755:
In other cases, such as auctioning drilling rights for an oil tract, the item has a common value —the tract will produce some amount of money, X, and all bidders value a dollar equally—but there is uncertainty as to what the actual value of Xis.

Token 13756:
Different bidders have different information, and hence different estimates of the item’s true value.

Token 13757:
In either case, bidders end up with their ownvi.G i v e n vi, each bidder gets a chance, at the appropriate time or times in the auction, to make a bid bi.

Token 13758:
The highest bid, bmaxwins the item, but the price paid need not be bmax; that’s part of the mechanism design.

Token 13759:
The best-known auction mechanism is the ascending-bid ,8orEnglish auction ,i n ASCENDING-BID ENGLISH AUCTION which the center starts by asking for a minimum (or reserve )b i dbmin.

Token 13760:
If some bidder is 8The word “auction” comes from the Latin augere , to increase.

Token 13761:
680 Chapter 17.

Token 13762:
Making Complex Decisions willing to pay that amount, the center then asks for bmin+d, for some increment d,a n d continues up from there.

Token 13763:
The auction ends when nobody is willing to bid anymore; then thelast bidder wins the item, paying the price he bid.

Token 13764:
How do we know if this is a good mechanism? One goal is to maximize expected revenue for the seller.

Token 13765:
Another goal is to maximize a notion of global utility.

Token 13766:
These goals overlap to some extent, because one aspect of maximizing global utility is to ensure that the winner of the auction is the agent who values the item the most (and thus is willing to paythe most).

Token 13767:
We say an auction is efﬁcient if the goods go to the agent who values them most.

Token 13768:
EFFICIENT The ascending-bid auction is usually both efﬁcient and revenue maximizing, but if the reserve price is set too high, the bidder who values it most may not bid, and if the reserve is set toolow, the seller loses net revenue.

Token 13769:
Probably the most important things that an auction mechanism can do is encourage a sufﬁcient number of bidders to enter the game and discourage them from engaging in collu- sion.

Token 13770:
Collusion is an unfair or illegal agreement by two or more bidders to manipulate prices.

Token 13771:
COLLUSION It can happen in secret backroom deals or tacitly, within the rules of the mechanism.

Token 13772:
For example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a simultaneous auction (bids were taken on all ten blocks at the same time), using the rule that any bid must be a minimum of a 10% raise over the previous bid on a block.

Token 13773:
There were only two credible bidders, and the ﬁrst, Mannesman, entered the bid of 20 million deutschmarkon blocks 1-5 and 18.18 million on blocks 6-10.

Token 13774:
Why 18.18M?

Token 13775:
One of T-Mobile’s managerssaid they “interpreted Mannesman’s ﬁrst bid as an offer.” Both parties could compute thata 10% raise on 18.18M is 19.99M; thus Mannesman’s bid was interpreted as saying “wecan each get half the blocks for 20M; let’s not spoil it by bidding the prices up higher.”And in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding.

Token 13776:
TheGerman government got less than they expected, because the two competitors were able touse the bidding mechanism to come to a tacit agreement on how not to compete.

Token 13777:
Fromthe government’s point of view, a better result could have been obtained by any of thesechanges to the mechanism: a higher reserve price; a sealed-bid ﬁrst-price auction, so that the competitors could not communicate through their bids; or incentives to bring in a third bidder.

Token 13778:
Perhaps the 10% rule was an error in mechanism design, because it facilitated the precise signaling from Mannesman to T-Mobile.

Token 13779:
In general, both the seller and the global utility function beneﬁt if there are more bid- ders, although global utility can suffer if you count the cost of wasted time of bidders thathave no chance of winning.

Token 13780:
One way to encourage more bidders is to make the mechanismeasier for them.

Token 13781:
After all, if it requires too much research or computation on the part of thebidders, they may decide to take their money elsewhere.

Token 13782:
So it is desirable that the biddershave a dominant strategy .

Token 13783:
Recall that “dominant” means that the strategy works against all other strategies, which in turn means that an agent can adopt it without regard for the otherstrategies.

Token 13784:
An agent with a dominant strategy can just bid, without wasting time contemplat- ing other agents’ possible strategies.

Token 13785:
A mechanism where agents have a dominant strategy is called a strategy-proof mechanism.

Token 13786:
If, as is usually the case, that strategy involves the STRATEGY-PROOF bidders revealing their true value, vi,t h e ni ti sc a l l e da truth-revealing ,o rtruthful , auction; TRUTH-REVEALING the term incentive compatible is also used.

Token 13787:
The revelation principle states that any mecha-REVELATION PRINCIPLE

Token 13788:
Section 17.6.

Token 13789:
Mechanism Design 681 nism can be transformed into an equivalent truth-revealing mechanism, so part of mechanism design is ﬁnding these equivalent mechanisms.

Token 13790:
It turns out that the ascending-bid auction has most of the desirable properties.

Token 13791:
The bidder with the highest value vigets the goods at a price of bo+d,w h e r e bois the highest bid among all the other agents and dis the auctioneer’s increment.9Bidders have a simple dominant strategy: keep bidding as long as the current cost is below your vi.

Token 13792:
The mechanism is not quite truth-revealing, because the winning bidder reveals only that his vi≥bo+d;w e have a lower bound on vibut not an exact amount.

Token 13793:
A disadvantage (from the point of view of the seller) of the ascending-bid auction is that it can discourage competition.

Token 13794:
Suppose that in a bid for cell-phone spectrum there isone advantaged company that everyone agrees would be able to leverage existing customersand infrastructure, and thus can make a larger proﬁt than anyone else.

Token 13795:
Potential competitorscan see that they have no chance in an ascending-bid auction, because the advantaged com-pany can always bid higher.

Token 13796:
Thus, the competitors may not enter at all, and the advantagedcompany ends up winning at the reserve price.

Token 13797:
Another negative property of the English auction is its high communication costs.

Token 13798:
Either the auction takes place in one room or all bidders have to have high-speed, secure communi- cation lines; in either case they have to have the time available to go through several rounds of bidding.

Token 13799:
An alternative mechanism, which requires much less communication, is the sealed- bid auction .

Token 13800:
Each bidder makes a single bid and communicates it to the auctioneer, without SEALED-BID AUCTION the other bidders seeing it.

Token 13801:
With this mechanism, there is no longer a simple dominant strat- egy.

Token 13802:
If your value is viand you believe that the maximum of all the other agents’ bids will bebo, then you should bid bo+/epsilon1, for some small /epsilon1, if that is less than vi.

Token 13803:
Thus, your bid depends on your estimation of the other agents’ bids, requiring you to do more work.

Token 13804:
Also,note that the agent with the highest v imight not win the auction.

Token 13805:
This is offset by the fact that the auction is more competitive, reducing the bias toward an advantaged bidder.

Token 13806:
A small change in the mechanism for sealed-bid auctions produces the sealed-bid second-price auction , also known as a Vickrey auction .10In such auctions, the winner paysSEALED-BID SECOND-PRICEAUCTION VICKREY AUCTIONthe price of the second -highest bid, bo, rather than paying his own bid.

Token 13807:
This simple modiﬁ- cation completely eliminates the complex deliberations required for standard (or ﬁrst-price ) sealed-bid auctions, because the dominant strategy is now simply to bid vi; the mechanism is truth-revealing.

Token 13808:
Note that the utility of agent iin terms of his bid bi, his value vi, and the best bid among the other agents, bo,i s ui=/braceleftbigg(vi−bo)ifbi>bo 0otherwise.

Token 13809:
To see that bi=viis a dominant strategy, note that when (vi−bo)is positive, any bid that wins the auction is optimal, and bidding viin particular wins the auction.

Token 13810:
On the other hand, when (vi−bo)is negative, any bid that loses the auction is optimal, and bidding viin 9There is actually a small chance that the agent with highest vifails to get the goods, in the case in which bo<vi<bo+d.

Token 13811:
The chance of this can be made arbitrarily small by decreasing the increment d. 10Named after William Vickrey (1914–1996), who won the 1996 Nobel Prize in economics for this work and died of a heart attack three days later.

Token 13812:
682 Chapter 17. Making Complex Decisions particular loses the auction.

Token 13813:
So bidding viis optimal for all possible values of bo, and in fact, viis the only bid that has this property.

Token 13814:
Because of its simplicity and the minimal computation requirements for both seller and bidders, the Vickrey auction is widely used in constructingdistributed AI systems.

Token 13815:
Also, Internet search engines conduct over a billion auctions a dayto sell advertisements along with their search results, and online auction sites handle $100 billion a year in goods, all using variants of the Vickrey auction.

Token 13816:
Note that the expected value to the seller is b o, which is the same expected return as the limit of the English auction as the increment dgoes to zero.

Token 13817:
This is actually a very general result: the revenue equivalence theorem states that, with a few minor caveats, any auction mechanism where risk-neutralREVENUE EQUIVALENCETHEOREM bidders have values viknown only to themselves (but know a probability distribution from which those values are sampled), will yield the same expected revenue.

Token 13818:
This principle meansthat the various mechanisms are not competing on the basis of revenue generation, but ratheron other qualities.

Token 13819:
Although the second-price auction is truth-revealing, it turns out that extending the idea to multiple goods and using a next-price auction is not truth-revealing.

Token 13820:
Many Internet searchengines use a mechanism where they auction kslots for ads on a page.

Token 13821:
The highest bidder wins the top spot, the second highest gets the second spot, and so on.

Token 13822:
Each winner pays the price bid by the next-lower bidder, with the understanding that payment is made only if the searcher actually clicks on the ad.

Token 13823:
The top slots are considered more valuable because theyare more likely to be noticed and clicked on.

Token 13824:
Imagine that three bidders, b 1,b2andb3,h a v e valuations for a click of v1= 200,v2= 180,andv3=100 ,a n dt h a t k=2slots are available, where it is known that the top spot is clicked on 5% of the time and the bottom spot 2%.

Token 13825:
Ifall bidders bid truthfully, then b 1wins the top slot and pays 180, and has an expected return of(200−180)×0.05= 1 .

Token 13826:
The second slot goes to b2.B u tb1can see that if she were to bid anything in the range 101–179, she would concede the top slot to b2, win the second slot, and yield an expected return of (200−100)×.02= 2 .

Token 13827:
Thus, b1can double her expected return by bidding less than her true value in this case.

Token 13828:
In general, bidders in this multislot auction mustspend a lot of energy analyzing the bids of others to determine their best strategy; there is no simple dominant strategy.

Token 13829:
Aggarwal et al.

Token 13830:
(2006) show that there is a unique truthful auction mechanism for this multislot problem, in which the winner of slot jpays the full price for slotjjust for those additional clicks that are available at slot jand not at slot j+1.T h e winner pays the price for the lower slot for the remaining clicks.

Token 13831:
In our example, b 1would bid 200 truthfully, and would pay 180 for the additional .05−.02=.03clicks in the top slot, but would pay only the cost of the bottom slot, 100, for the remaining .02 clicks.

Token 13832:
Thus, the total return to b1would be (200−180)×.03 + (200−100)×.02= 2 .6.

Token 13833:
Another example of where auctions can come into play within AI is when a collection of agents are deciding whether to cooperate on a joint plan.

Token 13834:
Hunsberger and Grosz (2000)show that this can be accomplished efﬁciently with an auction in which the agents bid forroles in the joint plan.

Token 13835:
Section 17.6.

Token 13836:
Mechanism Design 683 17.6.2 Common goods Now let’s consider another type of game, in which countries set their policy for controlling air pollution.

Token 13837:
Each country has a choice: they can reduce pollution at a cost of -10 points forimplementing the necessary changes, or they can continue to pollute, which gives them a netutility of -5 (in added health costs, etc.)

Token 13838:
and also contributes -1 points to every other country(because the air is shared across countries).

Token 13839:
Clearly, the dominant strategy for each countryis “continue to pollute,” but if there are 100 countries and each follows this policy, then eachcountry gets a total utility of -104, whereas if every country reduced pollution, they would each have a utility of -10.

Token 13840:
This situation is called the tragedy of the commons : if nobody TRAGEDY OF THE COMMONS has to pay for using a common resource, then it tends to be exploited in a way that leads to a lower total utility for all agents.

Token 13841:
It is similar to the prisoner’s dilemma: there is anothersolution to the game that is better for all parties, but there appears to be no way for rationalagents to arrive at that solution.

Token 13842:
The standard approach for dealing with the tragedy of the commons is to change the mechanism to one that charges each agent for using the commons.

Token 13843:
More generally, we needto ensure that all externalities —effects on global utility that are not recognized in the in- EXTERNALITIES dividual agents’ transactions—are made explicit.

Token 13844:
Setting the prices correctly is the difﬁcult part.

Token 13845:
In the limit, this approach amounts to creating a mechanism in which each agent iseffectively required to maximize global utility, but can do so by making a local decision.

Token 13846:
Forthis example, a carbon tax would be an example of a mechanism that charges for use of the commons in a way that, if implemented well, maximizes global utility.

Token 13847:
As a ﬁnal example, consider the problem of allocating some common goods.

Token 13848:
Suppose a city decides it wants to install some free wireless Internet transceivers.

Token 13849:
However, the number of transceivers they can afford is less than the number of neighborhoods that want them.

Token 13850:
The city wants to allocate the goods efﬁciently, to the neighborhoods that would value them themost.

Token 13851:
That is, they want to maximize the global utility V=/summationtext ivi.

Token 13852:
The problem is that if they just ask each neighborhood council “how much do you value this free gift?” they wouldall have an incentive to lie, and report a high value.

Token 13853:
It turns out there is a mechanism, knownas the Vickrey-Clarke-Groves ,o rVCG , mechanism, that makes it a dominant strategy for VICKREY-CLARKE- GROVES VCG each agent to report its true utility and that achieves an efﬁcient allocation of the goods.

Token 13854:
The trick is that each agent pays a tax equivalent to the loss in global utility that occurs because of the agent’s presence in the game.

Token 13855:
The mechanism works like this: 1. The center asks each agent to report its value for receiving an item. Call this bi. 2.

Token 13856:
The center allocates the goods to a subset of the bidders.

Token 13857:
We call this subset A, and use the notation bi(A)to mean the result to iunder this allocation: biifiis inA(that is, i is a winner), and 0 otherwise.

Token 13858:
The center chooses Ato maximize total reported utility B=/summationtext ibi(A). 3.

Token 13859:
The center calculates (for each i) the sum of the reported utilities for all the winners except i.

Token 13860:
We use the notation B−i=/summationtext j/negationslash=ibj(A).

Token 13861:
The center also computes (for each i) the allocation that would maximize total global utility if iwere not in the game; call that sum W−i. 4.

Token 13862:
Each agent ipays a tax equal to W−i−B−i.

Token 13863:
684 Chapter 17.

Token 13864:
Making Complex Decisions In this example, the VCG rule means that each winner would pay a tax equal to the highest reported value among the losers.

Token 13865:
That is, if I report my value as 5, and that causes someonewith value 2 to miss out on an allocation, then I pay a tax of 2.

Token 13866:
All winners should be happybecause they pay a tax that is less than their value, and all losers are as happy as they can be,because they value the goods less than the required tax.

Token 13867:
Why is it that this mechanism is truth-revealing?

Token 13868:
First, consider the payoff to agent i, which is the value of getting an item, minus the tax: v i(A)−(W−i−B−i).

Token 13869:
(17.14) Here we distinguish the agent’s true utility, vi, from his reported utility bi(but we are trying to show that a dominant strategy is bi=vi).

Token 13870:
Agent iknows that the center will maximize global utility using the reported values, /summationdisplay jbj(A)=bi(A)+/summationdisplay j/negationslash=ibj(A) whereas agent iwants the center to maximize (17.14), which can be rewritten as vi(A)+/summationdisplay j/negationslash=ibj(A)−W−i.

Token 13871:
Since agent icannot affect the value of W−i(it depends only on the other agents), the only wayican make the center optimize what iwants is to report the true utility, bi=vi.

Token 13872:
17.7 S UMMARY This chapter shows how to use knowledge about the world to make decisions even when the outcomes of an action are uncertain and the rewards for acting might not be reaped until manyactions have passed.

Token 13873:
The main points are as follows: •Sequential decision problems in uncertain environments, also called Markov decision processes , or MDPs, are deﬁned by a transition model specifying the probabilistic outcomes of actions and a reward function specifying the reward in each state.

Token 13874:
•The utility of a state sequence is the sum of all the rewards over the sequence, possibly discounted over time.

Token 13875:
The solution of an MDP is a policy that associates a decision with every state that the agent might reach.

Token 13876:
An optimal policy maximizes the utility ofthe state sequences encountered when it is executed.

Token 13877:
•The utility of a state is the expected utility of the state sequences encountered when an optimal policy is executed, starting in that state.

Token 13878:
The value iteration algorithm for solving MDPs works by iteratively solving the equations relating the utility of each stateto those of its neighbors.

Token 13879:
•Policy iteration alternates between calculating the utilities of states under the current policy and improving the current policy with respect to the current utilities.

Token 13880:
•Partially observable MDPs, or POMDPs, are much more difﬁcult to solve than are MDPs.

Token 13881:
They can be solved by conversion to an MDP in the continuous space of belief

Token 13882:
Bibliographical and Historical Notes 685 states; both value iteration and policy iteration algorithms have been devised.

Token 13883:
Optimal behavior in POMDPs includes information gathering to reduce uncertainty and there-fore make better decisions in the future.

Token 13884:
•A decision-theoretic agent can be constructed for POMDP environments.

Token 13885:
The agent uses a dynamic decision network to represent the transition and sensor models, to update its belief state, and to project forward possible action sequences.

Token 13886:
•Game theory describes rational behavior for agents in situations in which multiple agents interact simultaneously.

Token 13887:
Solutions of games are Nash equilibria —strategy pro- ﬁles in which no agent has an incentive to deviate from the speciﬁed strategy.

Token 13888:
•Mechanism design can be used to set the rules by which agents will interact, in order to maximize some global utility through the operation of individually rational agents.

Token 13889:
Sometimes, mechanisms exist that achieve this goal without requiring each agent to consider the choices made by other agents.

Token 13890:
We shall return to the world of MDPs and POMDP in Chapter 21, when we study rein- forcement learning methods that allow an agent to improve its behavior from experience in sequential, uncertain environments.

Token 13891:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Richard Bellman developed the ideas underlying the modern approach to sequential decisionproblems while working at the RAND Corporation beginning in 1949.

Token 13892:
According to his au-tobiography (Bellman, 1984), he coined the exciting term “dynamic programming” to hidefrom a research-phobic Secretary of Defense, Charles Wilson, the fact that his group wasdoing mathematics.

Token 13893:
(This cannot be strictly true, because his ﬁrst paper using the term (Bell-man, 1952) appeared before Wilson became Secretary of Defense in 1953.)

Token 13894:
Bellman’s book,Dynamic Programming (1957), gave the new ﬁeld a solid foundation and introduced the basic algorithmic approaches.

Token 13895:
Ron Howard’s Ph.D. thesis (1960) introduced policy iteration and the idea of average reward for solving inﬁnite-horizon problems.

Token 13896:
Several additional resultswere introduced by Bellman and Dreyfus (1962).

Token 13897:
Modiﬁed policy iteration is due to vanNunen (1976) and Puterman and Shin (1978).

Token 13898:
Asynchronous policy iteration was analyzedby Williams and Baird (1993), who also proved the policy loss bound in Equation (17.9).

Token 13899:
Theanalysis of discounting in terms of stationary preferences is due to Koopmans (1972).

Token 13900:
Thetexts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide arigorous introduction to sequential decision problems.

Token 13901:
Papadimitriou and Tsitsiklis (1987)describe results on the computational complexity of MDPs.

Token 13902:
Seminal work by Sutton (1988) and Watkins (1989) on reinforcement learning methods for solving MDPs played a signiﬁcant role in introducing MDPs into the AI community, as did the later survey by Barto et al.

Token 13903:
(1995). (Earlier work by Werbos (1977) contained many similar ideas, but was not taken up to the same extent.)

Token 13904:
The connection between MDPs andAI planning problems was made ﬁrst by Sven Koenig (1991), who showed how probabilistic S TRIPS operators provide a compact representation for transition models (see also Wellman,

Token 13905:
686 Chapter 17. Making Complex Decisions 1990b). Work by Dean et al.

Token 13906:
(1993) and Tash and Russell (1994) attempted to overcome the combinatorics of large state spaces by using a limited search horizon and abstract states.Heuristics based on the value of information can be used to select areas of the state spacewhere a local expansion of the horizon will yield a signiﬁcant improvement in decision qual-ity.

Token 13907:
Agents using this approach can tailor their effort to handle time pressure and generate some interesting behaviors such as using familiar “beaten paths” to ﬁnd their way around the state space quickly without having to recompute optimal decisions at each point.

Token 13908:
As one might expect, AI researchers have pushed MDPs in the direction of more ex- pressive representations that can accommodate much larger problems than the traditionalatomic representations based on transition matrices.

Token 13909:
The use of a dynamic Bayesian networkto represent transition models was an obvious idea, but work on factored MDPs (Boutilier FACTORED MDP et al.

Token 13910:
, 2000; Koller and Parr, 2000; Guestrin et al.

Token 13911:
, 2003b) extends the idea to structured representations of the value function with provable improvements in complexity.

Token 13912:
Relational MDPs (Boutilier et al. , 2001; Guestrin et al.

Token 13913:
, 2003a) go one step further, using structured RELATIONAL MDP representations to handle domains with many related objects.

Token 13914:
The observation that a partially observable MDP can be transformed into a regular MDP over belief states is due to Astrom (1965) and Aoki (1965).

Token 13915:
The ﬁrst complete algorithm for the exact solution of POMDPs—essentially the value iteration algorithm presented in this chapter—was proposed by Edward Sondik (1971) in his Ph.D. thesis.

Token 13916:
(A later journal paperby Smallwood and Sondik (1973) contains some errors, but is more accessible.)

Token 13917:
Lovejoy(1991) surveyed the ﬁrst twenty-ﬁve years of POMDP research, reaching somewhat pes-simistic conclusions about the feasibility of solving large problems.

Token 13918:
The ﬁrst signiﬁcantcontribution within AI was the Witness algorithm (Cassandra et al. , 1994; Kaelbling et al.

Token 13919:
, 1998), an improved version of POMDP value iteration.

Token 13920:
Other algorithms soon followed, in-cluding an approach due to Hansen (1998) that constructs a policy incrementally in the formof a ﬁnite-state automaton.

Token 13921:
In this policy representation, the belief state corresponds directlyto a particular state in the automaton.

Token 13922:
More recent work in AI has focused on point-based value iteration methods that, at each iteration, generate conditional plans and α-vectors for a ﬁnite set of belief states rather than for the entire belief space.

Token 13923:
Lovejoy (1991) proposed such an algorithm for a ﬁxed grid of points, an approach taken also by Bonet (2002). Aninﬂuential paper by Pineau et al.

Token 13924:
(2003) suggested generating reachable points by simulat- ing trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that oneneed generate plans for only a small, randomly selected subset of points to improve on theplans from the previous iteration for all points in the set.

Token 13925:
Current point-based methods—such as point-based policy iteration (Ji et al.

Token 13926:
, 2007)—can generate near-optimal solutions for POMDPs with thousands of states.

Token 13927:
Because POMDPs are PSPACE-hard (Papadimitriou andTsitsiklis, 1987), further progress may require taking advantage of various kinds of structurewithin a factored representation.

Token 13928:
The online approach—using look-ahead search to select an action for the current belief state—was ﬁrst examined by Satia and Lave (1973).

Token 13929:
The use of sampling at chance nodes was explored analytically by Kearns et al. (2000) and Ng and Jordan (2000).

Token 13930:
The basic ideas for an agent architecture using dynamic decision networks were proposed by Deanand Kanazawa (1989a).

Token 13931:
The book Planning and Control by Dean and Wellman (1991) goes

Token 13932:


Token 13933:
Bibliographical and Historical Notes 687 into much greater depth, making connections between DBN/DDN models and the classical control literature on ﬁltering.

Token 13934:
Tatman and Shachter (1990) showed how to apply dynamicprogramming algorithms to DDN models.

Token 13935:
Russell (1998) explains various ways in whichsuch agents can be scaled up and identiﬁes a number of open research issues.

Token 13936:
The roots of game theory can be traced back to proposals made in the 17th century by Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human interactions scientiﬁcally and mathematically.

Token 13937:
Throughout the 19th century, several leadingeconomists created simple mathematical examples to analyze particular examples of com-petitive situations.

Token 13938:
The ﬁrst formal results in game theory are due to Zermelo (1913) (whohad, the year before, suggested a form of minimax search for games, albeit an incorrect one).Emile Borel (1921) introduced the notion of a mixed strategy.

Token 13939:
John von Neumann (1928)proved that every two-person, zero-sum game has a maximin equilibrium in mixed strategiesand a well-deﬁned value.

Token 13940:
Von Neumann’s collaboration with the economist Oskar Morgen-stern led to the publication in 1944 of the Theory of Games and Economic Behavior ,t h e deﬁning book for game theory.

Token 13941:
Publication of the book was delayed by the wartime papershortage until a member of the Rockefeller family personally subsidized its publication.

Token 13942:
In 1950, at the age of 21, John Nash published his ideas concerning equilibria in general (non-zero-sum) games.

Token 13943:
His deﬁnition of an equilibrium solution, although originating in the work of Cournot (1838), became known as Nash equilibrium.

Token 13944:
After a long delay becauseof the schizophrenia he suffered from 1959 onward, Nash was awarded the Nobel MemorialPrize in Economics (along with Reinhart Selten and John Harsanyi) in 1994.

Token 13945:
The Bayes–Nashequilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).Some issues in the use of game theory for agent control are covered by Binmore (1982).

Token 13946:
The prisoner’s dilemma was invented as a classroom exercise by Albert W. Tucker in 1950 (based on an example by Merrill Flood and Melvin Dresher) and is covered extensivelyby Axelrod (1985) and Poundstone (1993).

Token 13947:
Repeated games were introduced by Luce andRaiffa (1957), and games of partial information in extensive form by Kuhn (1953).

Token 13948:
The ﬁrstpractical algorithm for sequential, partial-information games was developed within AI by Koller et al.

Token 13949:
(1996); the paper by Koller and Pfeffer (1997) provides a readable introduction to the ﬁeld and describe a working system for representing and solving sequential games.

Token 13950:
The use of abstraction to reduce a game tree to a size that can be solved with Koller’s technique is discussed by Billings et al. (2003).

Token 13951:
Bowling et al. (2008) show how to use importance sampling to get a better estimate of the value of a strategy. Waugh et al.

Token 13952:
(2009) show that the abstraction approach is vulnerable to making systematic errors in approximatingthe equilibrium solution, meaning that the whole approach is on shaky ground: it works forsome games but not others.

Token 13953:
Korb et al. (1999) experiment with an opponent model in the form of a Bayesian network. It plays ﬁve-card stud about as well as experienced humans.

Token 13954:
(Zinkevich et al.

Token 13955:
, 2008) show how an approach that minimizes regret can ﬁnd approximate equilibria for abstractions with 10 12states, 100 times more than previous methods.

Token 13956:
Game theory and MDPs are combined in the theory of Markov games, also called stochastic games (Littman, 1994; Hu and Wellman, 1998).

Token 13957:
Shapley (1953) actually describedthe value iteration algorithm independently of Bellman, but his results were not widely ap-preciated, perhaps because they were presented in the context of Markov games.

Token 13958:
Evolu-

Token 13959:
688 Chapter 17.

Token 13960:
Making Complex Decisions tionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your opponent’s strategy is changing, how should you react?

Token 13961:
Textbooks on game theory froman economics point of view include those by Myerson (1991), Fudenberg and Tirole (1991),Osborne (2004), and Osborne and Rubinstein (1994); Mailath and Samuelson (2006) concen-trate on repeated games.

Token 13962:
From an AI perspective we have Nisan et al. (2007), Leyton-Brown and Shoham (2008), and Shoham and Leyton-Brown (2009).

Token 13963:
The 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and Myerson “for having laid the foundations of mechanism design theory” (Hurwicz, 1973).

Token 13964:
The tragedyof the commons, a motivating problem for the ﬁeld, was presented by Hardin (1968).

Token 13965:
The rev-elation principle is due to Myerson (1986), and the revenue equivalence theorem was devel-oped independently by Myerson (1981) and Riley and Samuelson (1981).

Token 13966:
Two economists,Milgrom (1997) and Klemperer (2002), write about the multibillion-dollar spectrum auctionsthey were involved in.

Token 13967:
Mechanism design is used in multiagent planning (Hunsberger and Grosz, 2000; Stone et al. , 2009) and scheduling (Rassenti et al. , 1982).

Token 13968:
Varian (1995) gives a brief overview with connections to the computer science literature, and Rosenschein and Zlotkin (1994) present abook-length treatment with applications to distributed AI.

Token 13969:
Related work on distributed AI also goes under other names, including collective intelligence (Tumer and Wolpert, 2000; Segaran, 2007) and market-based control (Clearwater, 1996).

Token 13970:
Since 2001 there has been an annualTrading Agents Competition (TAC), in which agents try to make the best proﬁt on a seriesof auctions (Wellman et al.

Token 13971:
, 2001; Arunachalam and Sadeh, 2005). Papers on computational issues in auctions often appear in the ACM Conferences on Electronic Commerce.

Token 13972:
EXERCISES 17.1 For the 4×3world shown in Figure 17.1, calculate which squares can be reached from (1,1) by the action sequence [Up,Up,Right,Right,Right]and with what probabilities.

Token 13973:
Explain how this computation is related to the prediction task (see Section 15.2.1) for a hiddenMarkov model.

Token 13974:
17.2 Select a speciﬁc member of the set of policies that are optimal for R(s)>0as shown in Figure 17.2(b), and calculate the fraction of time the agent spends in each state, in the limit,if the policy is executed forever.

Token 13975:
( Hint: Construct the state-to-state transition probability matrix corresponding to the policy and see Exercise 15.2.)

Token 13976:
17.3 Suppose that we deﬁne the utility of a state sequence to be the maximum reward ob- tained in any state in the sequence.

Token 13977:
Show that this utility function does not result in stationary preferences between state sequences.

Token 13978:
Is it still possible to deﬁne a utility function on states such that MEU decision making gives optimal behavior?

Token 13979:
17.4 Sometimes MDPs are formulated with a reward function R(s,a)that depends on the action taken or with a reward function R(s,a,s /prime)that also depends on the outcome state.

Token 13980:
a. Write the Bellman equations for these formulations.

Token 13981:
Exercises 689 b.

Token 13982:
Show how an MDP with reward function R(s,a,s/prime)can be transformed into a different MDP with reward function R(s,a), such that optimal policies in the new MDP corre- spond exactly to optimal policies in the original MDP.

Token 13983:
c. Now do the same to convert MDPs with R(s,a)into MDPs with R(s).

Token 13984:
17.5 For the environment shown in Figure 17.1, ﬁnd all the threshold values for R(s)such that the optimal policy changes when the threshold is crossed.

Token 13985:
You will need a way to calcu- late the optimal policy and its value for ﬁxed R(s).

Token 13986:
(Hint: Prove that the value of any ﬁxed policy varies linearly with R(s).)

Token 13987:
17.6 Equation (17.7) on page 654 states that the Bellman operator is a contraction. a.

Token 13988:
Show that, for any functions fandg, |max af(a)−max ag(a)|≤max a|f(a)−g(a)|. b.

Token 13989:
Write out an expression for |(BUi−BU/prime i)(s)|and then apply the result from (a) to complete the proof that the Bellman operator is a contraction.

Token 13990:
17.7 This exercise considers two-player MDPs that correspond to zero-sum, turn-taking games like those in Chapter 5.

Token 13991:
Let the players be AandB,a n dl e t R(s)be the reward for player Ain state s.( T h er e w a r df o r Bis always equal and opposite.)

Token 13992:
a.L e tUA(s)be the utility of state swhen it is A’s turn to move in s,a n dl e t UB(s)be the utility of state swhen it is B’s turn to move in s. All rewards and utilities are calculated fromA’s point of view (just as in a minimax game tree).

Token 13993:
Write down Bellman equations deﬁning UA(s)andUB(s). b.

Token 13994:
Explain how to do two-player value iteration with these equations, and deﬁne a suitable termination criterion.

Token 13995:
c. Consider the game described in Figure 5.17 on page 197.

Token 13996:
Draw the state space (rather than the game tree), showing the moves by Aas solid lines and moves by Bas dashed lines. Mark each state with R(s).

Token 13997:
You will ﬁnd it helpful to arrange the states (sA,sB) on a two-dimensional grid, using sAandsBas “coordinates.” d. Now apply two-player value iteration to solve this game, and derive the optimal policy.

Token 13998:
17.8 Consider the 3×3world shown in Figure 17.14(a).

Token 13999:
The transition model is the same as in the 4×3Figure 17.1: 80% of the time the agent goes in the direction it selects; the rest of the time it moves at right angles to the intended direction.

Token 14000:
Implement value iteration for this world for each value of rbelow. Use discounted rewards with a discount factor of 0.99.

Token 14001:
Show the policy obtained in each case. Explainintuitively why the value of rleads to each policy. a.r= 100 b.r=−3 c.r=0 d.r=+ 3

Token 14002:
690 Chapter 17.

Token 14003:
Making Complex Decisions -50 +1 +1 +1 +1 +1 +1 +1+50 -1 -1 -1 -1 -1 -1 -1 ······ ···Startr -1 -1 -1+10 -1 -1-1 -1 (a) (b) Figure 17.14 (a)3×3world for Exercise 17.8.

Token 14004:
The reward for each state is indicated. The upper right square is a terminal state.

Token 14005:
(b) 101×3world for Exercise 17.9 (omitting 93 identical columns in the middle). The start state has reward 0.

Token 14006:
17.9 Consider the 101×3world shown in Figure 17.14(b).

Token 14007:
In the start state the agent has a choice of two deterministic actions, UporDown , but in the other states the agent has one deterministic action, Right .

Token 14008:
Assuming a discounted reward function, for what values of the discount γshould the agent choose Upand for which Down ?

Token 14009:
Compute the utility of each action as a function of γ.

Token 14010:
(Note that this simple example actually reﬂects many real-world situations in which one must weigh the value of an immediate action versus the potentialcontinual long-term consequences, such as choosing to dump pollutants into a lake.)

Token 14011:
17.10 Consider an undiscounted MDP having three states, (1, 2, 3), with rewards −1,−2, 0, respectively. State 3 is a terminal state.

Token 14012:
In states 1 and 2 there are two possible actions: a andb.

Token 14013:
The transition model is as follows: •In state 1, action amoves the agent to state 2 with probability 0.8 and makes the agent stay put with probability 0.2.

Token 14014:
•In state 2, action amoves the agent to state 1 with probability 0.8 and makes the agent stay put with probability 0.2.

Token 14015:
•In either state 1 or state 2, action bmoves the agent to state 3 with probability 0.1 and makes the agent stay put with probability 0.9.

Token 14016:
Answer the following questions: a. What can be determined qualitatively about the optimal policy in states 1 and 2? b.

Token 14017:
Apply policy iteration, showing each step in full, to determine the optimal policy and the values of states 1 and 2.

Token 14018:
Assume that the initial policy has action bin both states. c. What happens to policy iteration if the initial policy has action ain both states?

Token 14019:
Does discounting help? Does the optimal policy depend on the discount factor? 17.11 Consider the 4×3world shown in Figure 17.1. a.

Token 14020:
Implement an environment simulator for this environment, such that the speciﬁc geog- raphy of the environment is easily altered.

Token 14021:
Some code for doing this is already in theonline code repository.

Token 14022:
Exercises 691 b. Create an agent that uses policy iteration, and measure its performance in the environ- ment simulator from various starting states.

Token 14023:
Perform several experiments from eachstarting state, and compare the average total reward received per run with the utility ofthe state, as determined by your algorithm.

Token 14024:
c. Experiment with increasing the size of the environment. How does the run time for policy iteration vary with the size of the environment?

Token 14025:
17.12 How can the value determination algorithm be used to calculate the expected loss experienced by an agent using a given set of utility estimates Uand an estimated model P, compared with an agent using correct values?

Token 14026:
17.13 Let the initial belief state b 0for the 4×3POMDP on page 658 be the uniform dis- tribution over the nonterminal states, i.e., /angbracketleft1 9,1 9,1 9,1 9,1 9,1 9,1 9,1 9,1 9,0,0/angbracketright.

Token 14027:
Calculate the exact belief state b1after the agent moves Leftand its sensor reports 1 adjacent wall.

Token 14028:
Also calculate b2assuming that the same thing happens again.

Token 14029:
17.14 What is the time complexity of dsteps of POMDP value iteration for a sensorless environment?

Token 14030:
17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is 90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 withequal probability).

Token 14031:
Analyze, either qualitatively or quantitatively, the utility function and theoptimal policy for this problem.

Token 14032:
17.16 Show that a dominant strategy equilibrium is a Nash equilibrium, but not vice versa.

Token 14033:
17.17 In the children’s game of rock–paper–scissors each player reveals at the same time a choice of rock, paper, or scissors.

Token 14034:
Paper wraps rock, rock blunts scissors, and scissors cutpaper.

Token 14035:
In the extended version rock–paper–scissors–ﬁre–water, ﬁre beats rock, paper, andscissors; rock, paper, and scissors beat water; and water beats ﬁre.

Token 14036:
Write out the payoffmatrix and ﬁnd a mixed-strategy solution to this game.

Token 14037:
17.18 The following payoff matrix, from Blinder (1983) by way of Bernstein (1996), shows a game between politicians and the Federal Reserve.

Token 14038:
Fed: contract Fed: do nothing Fed: expand Pol: contract F=7,P=1 F=9,P=4 F=6,P=6 Pol: do nothing F=8,P=2 F=5,P=5 F=4,P=9 Pol: expand F=3,P=3 F=2,P=7 F=1,P=8 Politicians can expand or contract ﬁscal policy, while the Fed can expand or contract mon- etary policy.

Token 14039:
(And of course either side can choose to do nothing.)

Token 14040:
Each side also has pref- erences for who should do what—neither side wants to look like the bad guys.

Token 14041:
The payoffs shown are simply the rank orderings: 9 for ﬁrst choice through 1 for last choice. Find theNash equilibrium of the game in pure strategies.

Token 14042:
Is this a Pareto-optimal solution? You mightwish to analyze the policies of recent administrations in this light.

Token 14043:
692 Chapter 17.

Token 14044:
Making Complex Decisions 17.19 A Dutch auction is similar in an English auction, but rather than starting the bidding at a low price and increasing, in a Dutch auction the seller starts at a high price and graduallylowers the price until some buyer is willing to accept that price.

Token 14045:
(If multiple bidders acceptthe price, one is arbitrarily chosen as the winner.)

Token 14046:
More formally, the seller begins with apricepand gradually lowers pby increments of duntil at least one buyer accepts the price.

Token 14047:
Assuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will always result in the bidder with the highest value for the item obtaining the item?

Token 14048:
If so, showmathematically why. If not, explain how it may be possible for the bidder with highest valuefor the item not to obtain it.

Token 14049:
17.20 Imagine an auction mechanism that is just like an ascending-bid auction, except that at the end, the winning bidder, the one who bid b max, pays only bmax/2rather than bmax.

Token 14050:
Assuming all agents are rational, what is the expected revenue to the auctioneer for thismechanism, compared with a standard ascending-bid auction?

Token 14051:
17.21 Teams in the National Hockey League historically received 2 points for winning a game and 0 for losing.

Token 14052:
If the game is tied, an overtime period is played; if nobody wins inovertime, the game is a tie and each team gets 1 point.

Token 14053:
But league ofﬁcials felt that teamswere playing too conservatively in overtime (to avoid a loss), and it would be more excitingif overtime produced a winner.

Token 14054:
So in 1999 the ofﬁcials experimented in mechanism design:the rules were changed, giving a team that loses in overtime 1 point, not 0.

Token 14055:
It is still 2 pointsfor a win and 1 for a tie. a. Was hockey a zero-sum game before the rule change? After? b.

Token 14056:
Suppose that at a certain time tin a game, the home team has probability pof winning in regulation time, probability 0.78−pof losing, and probability 0.22 of going into overtime, where they have probability qof winning, .9−qof losing, and .1 of tying.

Token 14057:
Give equations for the expected value for the home and visiting teams.

Token 14058:
c. Imagine that it were legal and ethical for the two teams to enter into a pact where they agree that they will skate to a tie in regulation time, and then both try in earnest to win in overtime.

Token 14059:
Under what conditions, in terms of pandq, would it be rational for both teams to agree to this pact?

Token 14060:
d. Longley and Sankaran (2005) report that since the rule change, the percentage of games with a winner in overtime went up 18.2%, as desired, but the percentage of overtimegames also went up 3.6%.

Token 14061:
What does that suggest about possible collusion or conser-vative play after the rule change?

Token 14062:
18LEARNING FROM EXAMPLES In which we describe agents that can improve their behavior through diligent study of their own experiences.

Token 14063:
An agent is learning if it improves its performance on future tasks after making observations LEARNING about the world.

Token 14064:
Learning can range from the trivial, as exhibited by jotting down a phone number, to the profound, as exhibited by Albert Einstein, who inferred a new theory of theuniverse.

Token 14065:
In this chapter we will concentrate on one class of learning problem, which seemsrestricted but actually has vast applicability: from a collection of input–output pairs, learn afunction that predicts the output for new inputs.

Token 14066:
Why would we want an agent to learn?

Token 14067:
If the design of the agent can be improved, why wouldn’t the designers just program in that improvement to begin with? There are three main reasons.

Token 14068:
First, the designers cannot anticipate all possible situations that the agentmight ﬁnd itself in.

Token 14069:
For example, a robot designed to navigate mazes must learn the layoutof each new maze it encounters.

Token 14070:
Second, the designers cannot anticipate all changes overtime; a program designed to predict tomorrow’s stock market prices must learn to adapt whenconditions change from boom to bust.

Token 14071:
Third, sometimes human programmers have no ideahow to program a solution themselves.

Token 14072:
For example, most people are good at recognizing thefaces of family members, but even the best programmers are unable to program a computerto accomplish that task, except by using learning algorithms.

Token 14073:
This chapter ﬁrst gives anoverview of the various forms of learning, then describes one popular approach, decision-tree learning, in Section 18.3, followed by a theoretical analysis of learning in Sections 18.4 and 18.5.

Token 14074:
We look at various learning systems used in practice: linear models, nonlinear models (in particular, neural networks), nonparametric models, and support vector machines.Finally we show how ensembles of models can outperform a single model.

Token 14075:
18.1 F ORMS OF LEARNING Any component of an agent can be improved by learning from data.

Token 14076:
The improvements, andthe techniques used to make them, depend on four major factors: •Which component is to be improved. 693

Token 14077:
694 Chapter 18. Learning from Examples •What prior knowledge the agent already has. •What representation is used for the data and the component.

Token 14078:
•What feedback is available to learn from. Components to be learned Chapter 2 described several agent designs.

Token 14079:
The components of these agents include: 1. A direct mapping from conditions on the current state to actions. 2.

Token 14080:
A means to infer relevant properties of the world from the percept sequence.3.

Token 14081:
Information about the way the world evolves and about the results of possible actions the agent can take. 4.

Token 14082:
Utility information indicating the desirability of world states. 5. Action-value information indicating the desirability of actions.6.

Token 14083:
Goals that describe classes of states whose achievement maximizes the agent’s utility. Each of these components can be learned.

Token 14084:
Consider, for example, an agent training to become a taxi driver.

Token 14085:
Every time the instructor shouts “Brake!” the agent might learn a condition–action rule for when to brake (component 1); the agent also learns every time the instructordoes not shout.

Token 14086:
By seeing many camera images that it is told contain buses, it can learnto recognize them (2).

Token 14087:
By trying actions and observing the results—for example, braking hard on a wet road—it can learn the effects of its actions (3).

Token 14088:
Then, when it receives no tip from passengers who have been thoroughly shaken up during the trip, it can learn a usefulcomponent of its overall utility function (4).

Token 14089:
Representation and prior knowledge We have seen several examples of representations for agent components: propositional and ﬁrst-order logical sentences for the components in a logical agent; Bayesian networks forthe inferential components of a decision-theoretic agent, and so on.

Token 14090:
Effective learning algo-rithms have been devised for all of these representations.

Token 14091:
This chapter (and most of current machine learning research) covers inputs that form a factored representation —a vector of attribute values—and outputs that can be either a continuous numerical value or a discretevalue.

Token 14092:
Chapter 19 covers functions and prior knowledge composed of ﬁrst-order logic sen-tences, and Chapter 20 concentrates on Bayesian networks.

Token 14093:
There is another way to look at the various types of learning.

Token 14094:
We say that learning a (possibly incorrect) general function or rule from speciﬁc input–output pairs is called in- ductive learning .

Token 14095:
We will see in Chapter 19 that we can also do analytical ordeductive INDUCTIVE LEARNING learning : going from a known general rule to a new rule that is logically entailed, but isDEDUCTIVE LEARNING useful because it allows more efﬁcient processing.

Token 14096:
Feedback to learn from There are three types of feedback that determine the three main types of learning: Inunsupervised learning the agent learns patterns in the input even though no explicitUNSUPERVISED LEARNING feedback is supplied.

Token 14097:
The most common unsupervised learning task is clustering : detecting CLUSTERING

Token 14098:
Section 18.2. Supervised Learning 695 potentially useful clusters of input examples.

Token 14099:
For example, a taxi agent might gradually develop a concept of “good trafﬁc days” and “bad trafﬁc days” without ever being givenlabeled examples of each by a teacher.

Token 14100:
Inreinforcement learning the agent learns from a series of reinforcements—rewards REINFORCEMENT LEARNING or punishments.

Token 14101:
For example, the lack of a tip at the end of the journey gives the taxi agent an indication that it did something wrong.

Token 14102:
The two points for a win at the end of a chess game tells the agent it did something right.

Token 14103:
It is up to the agent to decide which of the actions priorto the reinforcement were most responsible for it.

Token 14104:
Insupervised learning the agent observes some example input–output pairs and learns SUPERVISED LEARNING a function that maps from input to output.

Token 14105:
In component 1 above, the inputs are percepts and the output are provided by a teacher who says “Brake!” or “Turn left.” In component 2, theinputs are camera images and the outputs again come from a teacher who says “that’s a bus.”In 3, the theory of braking is a function from states and braking actions to stopping distancein feet.

Token 14106:
In this case the output value is available directly from the agent’s percepts (after thefact); the environment is the teacher.

Token 14107:
In practice, these distinction are not always so crisp.

Token 14108:
In semi-supervised learning we SEMI-SUPERVISED LEARNING are given a few labeled examples and must make what we can of a large collection of un- labeled examples.

Token 14109:
Even the labels themselves may not be the oracular truths that we hope for.

Token 14110:
Imagine that you are trying to build a system to guess a person’s age from a photo.

Token 14111:
Yougather some labeled examples by snapping pictures of people and asking their age. That’ssupervised learning.

Token 14112:
But in reality some of the people lied about their age.

Token 14113:
It’s not justthat there is random noise in the data; rather the inaccuracies are systematic, and to uncoverthem is an unsupervised learning problem involving images, self-reported ages, and true (un-known) ages.

Token 14114:
Thus, both noise and lack of labels create a continuum between supervised andunsupervised learning.

Token 14115:
18.2 S UPERVISED LEARNING The task of supervised learning is this: Given a training set ofNexample input–output pairs TRAINING SET (x1,y1),(x2,y2),...(xN,yN), where each yjwas generated by an unknown function y=f(x), discover a function hthat approximates the true function f. Herexandycan be any value; they need not be numbers.

Token 14116:
The function his ahypothesis .1HYPOTHESIS Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set.

Token 14117:
To measure the accuracy of a hypothesis wegive it a test set of examples that are distinct from the training set.

Token 14118:
We say a hypothesis TEST SET 1A note on notation: except where noted, we will use jto index the Nexamples; xjwill always be the input and yjthe output.

Token 14119:
In cases where the input is speciﬁcally a vector of attribute values (beginning with Section 18.3), we will use xjfor the jth example and we will use ito index the nattributes of each example.

Token 14120:
The elements of xjare written xj,1,xj,2,...,x j,n.

Token 14121:
696 Chapter 18.

Token 14122:
Learning from Examples (c) (a) (b) (d)x x x xf(x) f(x) f(x) f(x) Figure 18.1 (a) Example (x, f(x))pairs and a consistent, linear hypothesis.

Token 14123:
(b) A con- sistent, degree-7 polynomial hypothesis for the same data set.

Token 14124:
(c) A different data set, whichadmits an exact degree-6 polynomial ﬁt or an approximate linear ﬁt.

Token 14125:
(d) A simple, exact sinusoidal ﬁt to the same data set. generalizes well if it correctly predicts the value of yfor novel examples.

Token 14126:
Sometimes the GENERALIZATION function fis stochastic—it is not strictly a function of x, and what we have to learn is a conditional probability distribution, P(Y|x).

Token 14127:
When the output yis one of a ﬁnite set of values (such as sunny, cloudy orrainy ), the learning problem is called classiﬁcation , and is called Boolean or binary classiﬁcation CLASSIFICATION if there are only two values.

Token 14128:
When yis a number (such as tomorrow’s temperature), the learning problem is called regression .

Token 14129:
(Technically, solving a regression problem is ﬁnding REGRESSION a conditional expectation or average value of y, because the probability that we have found exactly the right real-valued number for yis 0.)

Token 14130:
Figure 18.1 shows a familiar example: ﬁtting a function of a single variable to some data points.

Token 14131:
The examples are points in the (x,y)plane, where y=f(x).

Token 14132:
We don’t know what f is, but we will approximate it with a function hselected from a hypothesis space ,H,w h i c h HYPOTHESIS SPACE for this example we will take to be the set of polynomials, such as x5+3x2+2.

Token 14133:
Figure 18.1(a) shows some data with an exact ﬁt by a straight line (the polynomial 0.4x+3).

Token 14134:
The line is called a consistent hypothesis because it agrees with all the data.

Token 14135:
Figure 18.1(b) shows a high- CONSISTENT degree polynomial that is also consistent with the same data.

Token 14136:
This illustrates a fundamental problem in inductive learning: how do we choose from among multiple consistent hypotheses?

Token 14137:
One answer is to prefer the simplest hypothesis consistent with the data.

Token 14138:
This principle is called Ockham’s razor , after the 14th-century English philosopher William of Ockham, who OCKHAM’S RAZOR used it to argue sharply against all sorts of complications.

Token 14139:
Deﬁning simplicity is not easy, but it seems clear that a degree-1 polynomial is simpler than a degree-7 polynomial, and thus (a)should be preferred to (b).

Token 14140:
We will make this intuition more precise in Section 18.4.3. Figure 18.1(c) shows a second data set.

Token 14141:
There is no consistent straight line for this data set; in fact, it requires a degree-6 polynomial for an exact ﬁt.

Token 14142:
There are just 7 datapoints, so a polynomial with 7 parameters does not seem to be ﬁnding any pattern in the data and we do not expect it to generalize well.

Token 14143:
A straight line that is not consistent with any of the data points, but might generalize fairly well for unseen values of x,i sa l s os h o w n in (c).

Token 14144:
In general, there is a tradeoff between complex hypotheses that ﬁt the training data well and simpler hypotheses that may generalize better.

Token 14145:
In Figure 18.1(d) we expand the

Token 14146:
Section 18.3.

Token 14147:
Learning Decision Trees 697 hypothesis space Hto allow polynomials over both xandsin(x), and ﬁnd that the data in (c) can be ﬁtted exactly by a simple function of the form ax+b+csin(x).

Token 14148:
This shows the importance of the choice of hypothesis space.

Token 14149:
We say that a learning problem is realizable if REALIZABLE the hypothesis space contains the true function.

Token 14150:
Unfortunately, we cannot always tell whether a given learning problem is realizable, because the true function is not known.

Token 14151:
In some cases, an analyst looking at a problem is willing to make more ﬁne-grained distinctions about the hypothesis space, to say—even before seeing any data—not just that ahypothesis is possible or impossible, but rather how probable it is.

Token 14152:
Supervised learning canbe done by choosing the hypothesis h ∗that is most probable given the data: h∗=a r g m a x h∈HP(h|data).

Token 14153:
By Bayes’ rule this is equivalent to h∗=a r g m a x h∈HP(data|h)P(h).

Token 14154:
Then we can say that the prior probability P(h)is high for a degree-1 or -2 polynomial, lower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,sharp spikes as in Figure 18.1(b).

Token 14155:
We allow unusual-looking functions when the data say wereally need them, but we discourage them by giving them a low prior probability.

Token 14156:
Why not letHbe the class of all Java programs, or Turing machines?

Token 14157:
After all, every computable function can be represented by some Turing machine, and that is the best wecan do.

Token 14158:
One problem with this idea is that it does not take into account the computationalcomplexity of learning.

Token 14159:
There is a tradeoff between the expressiveness of a hypothesis space and the complexity of ﬁnding a good hypothesis within that space.

Token 14160:
For example, ﬁtting a straight line to data is an easy computation; ﬁtting high-degree polynomials is somewhatharder; and ﬁtting Turing machines is in general undecidable.

Token 14161:
A second reason to prefer simple hypothesis spaces is that presumably we will want to use hafter we have learned it, and computing h(x)whenhis a linear function is guaranteed to be fast, while computing an arbitrary Turing machine program is not even guaranteed to terminate.

Token 14162:
For these reasons,most work on learning has focused on simple representations.

Token 14163:
We will see that the expressiveness–complexity tradeoff is not as simple as it ﬁrst seems: it is often the case, as we saw with ﬁrst-order logic in Chapter 8, that an expressive languagemakes it possible for a simple hypothesis to ﬁt the data, whereas restricting the expressiveness of the language means that any consistent hypothesis must be very complex.

Token 14164:
For example,the rules of chess can be written in a page or two of ﬁrst-order logic, but require thousands ofpages when written in propositional logic.

Token 14165:
18.3 L EARNING DECISION TREES Decision tree induction is one of the simplest and yet most successful forms of machinelearning.

Token 14166:
We ﬁrst describe the representation—the hypothesis space—and then show how tolearn a good hypothesis.

Token 14167:
698 Chapter 18.

Token 14168:
Learning from Examples 18.3.1 The decision tree representation Adecision tree represents a function that takes as input a vector of attribute values and DECISION TREE returns a “decision”—a single output value.

Token 14169:
The input and output values can be discrete or continuous.

Token 14170:
For now we will concentrate on problems where the inputs have discrete valuesand the output has exactly two possible values; this is Boolean classiﬁcation, where eachexample input will be classiﬁed as true (a positive example) or false (a negative example).

Token 14171:
POSITIVE NEGATIVE A decision tree reaches its decision by performing a sequence of tests.

Token 14172:
Each internal node in the tree corresponds to a test of the value of one of the input attributes, Ai,a n d the branches from the node are labeled with the possible values of the attribute, Ai=vik.

Token 14173:
Each leaf node in the tree speciﬁes a value to be returned by the function.

Token 14174:
The decision treerepresentation is natural for humans; indeed, many “How To” manuals (e.g., for car repair)are written entirely as a single decision tree stretching over hundreds of pages.

Token 14175:
As an example, we will build a decision tree to decide whether to wait for a table at a restaurant.

Token 14176:
The aim here is to learn a deﬁnition for the goal predicate WillWait .F i r s t w e GOAL PREDICATE list the attributes that we will consider as part of the input: 1.Alternate : whether there is a suitable alternative restaurant nearby.

Token 14177:
2.Bar: whether the restaurant has a comfortable bar area to wait in. 3.Fri/Sat: true on Fridays and Saturdays. 4.Hungry : whether we are hungry.

Token 14178:
5.Patrons : how many people are in the restaurant (values are None ,Some ,a n dFull). 6.Price : the restaurant’s price range ($, $$, $$$).

Token 14179:
7.Raining : whether it is raining outside. 8.Reservation : whether we made a reservation.

Token 14180:
9.Type : the kind of restaurant (French, Italian, Thai, or burger).

Token 14181:
10.WaitEstimate : the wait estimated by the host (0–10 minutes, 10–30, 30–60, or >60).

Token 14182:
Note that every variable has a small set of possible values; the value of WaitEstimate ,f o r example, is not an integer, rather it is one of the four discrete values 0–10, 10–30, 30–60, or>60.

Token 14183:
The decision tree usually used by one of us (SR) for this domain is shown in Figure 18.2. Notice that the tree ignores the Price andType attributes.

Token 14184:
Examples are processed by the tree starting at the root and following the appropriate branch until a leaf is reached.

Token 14185:
For instance,an example with Patrons =Full andWaitEstimate =0–10 will be classiﬁed as positive (i.e., yes, we will wait for a table).

Token 14186:
18.3.2 Expressiveness of decision trees A Boolean decision tree is logically equivalent to the assertion that the goal attribute is trueif and only if the input attributes satisfy one of the paths leading to a leaf with value true.

Token 14187:
Writing this out in propositional logic, we have Goal⇔(Path 1∨Path 2∨···), where each Path is a conjunction of attribute-value tests required to follow that path.

Token 14188:
Thus, the whole expression is equivalent to disjunctive normal form (see page 283), which means

Token 14189:
Section 18.3. Learning Decision Trees 699 that any function in propositional logic can be expressed as a decision tree.

Token 14190:
As an example, the rightmost path in Figure 18.2 is Path=(Patrons =Full∧WaitEstimate =0–10).

Token 14191:
For a wide variety of problems, the decision tree format yields a nice, concise result. But some functions cannot be represented concisely.

Token 14192:
For example, the majority function, whichreturns true if and only if more than half of the inputs are true, requires an exponentiallylarge decision tree.

Token 14193:
In other words, decision trees are good for some kinds of functions andbad for others.

Token 14194:
Is there anykind of representation that is efﬁcient for allkinds of functions? Unfortunately, the answer is no. We can show this in a general way.

Token 14195:
Consider the set of allBoolean functions on nattributes. How many different functions are in this set?

Token 14196:
This is just the number of different truth tables that we can write down, because the function is deﬁned by its truth table.

Token 14197:
A truth table over nattributes has 2 nrows, one for each combination of values of the attributes.

Token 14198:
We can consider the “answer” column of the table as a 2n-bit number that deﬁnes the function.

Token 14199:
That means there are 22ndifferent functions (and there will be more than that number of trees, since more than one tree can compute the same function).

Token 14200:
This isa scary number.

Token 14201:
For example, with just the ten Boolean attributes of our restaurant problemthere are 2 1024or about 10308different functions to choose from, and for 20 attributes there are over 10300,000.

Token 14202:
We will need some ingenious algorithms to ﬁnd good hypotheses in such a large space.

Token 14203:
18.3.3 Inducing decision trees from examples An example for a Boolean decision tree consists of an (x,y)pair, where xis a vector of values for the input attributes, and yis a single Boolean output value.

Token 14204:
A training set of 12 examples No Ye s No Ye sNo Ye s No Ye sNone Some Full >60 30-60 10-30 0-10 No Ye sAlternate?Hungry? Reservation? Bar?

Token 14205:
Raining?Alternate?Patrons? Fri/Sat?No Yes No Yes Yes Yes No Yes No YesYes No Yes No Ye s Yes NoWaitEstimate?

Token 14206:
Figure 18.2 A decision tree for deciding whether to wait for a table.

Token 14207:
700 Chapter 18.

Token 14208:
Learning from Examples Example Input Attributes Goal Alt Bar Fri Hun Pat Price Rain Res Type Est WillWait x1 Yes No No Yes Some $$$ No Yes French 0–10 y1=Yes x2 Yes No No Yes Full $ No No Thai 30–60 y2=No x3 No Yes No No Some $ No No Burger 0–10 y3=Yes x4 Yes No Yes Yes Full $ Yes No Thai 10–30 y4=Yes x5 Yes No Yes No Full $$$ No Yes French >60 y5=No x6 No Yes No Yes Some $$ Yes Yes Italian 0–10 y6=Yes x7 No Yes No No None $ Yes No Burger 0–10 y7=No x8 No No No Yes Some $$ Yes Yes Thai 0–10 y8=Yes x9 No Yes Yes No Full $ Yes No Burger >60 y9=No x10 Yes Yes Yes Yes Full $$$ No Yes Italian 10–30 y10=No x11 No No No No None $ No No Thai 0–10 y11=No x12 Yes Yes Yes Yes Full $ No No Burger 30–60 y12=Yes Figure 18.3 Examples for the restaurant domain.

Token 14209:
is shown in Figure 18.3.

Token 14210:
The positive examples are the ones in which the goal WillWait is true(x1,x3,...); the negative examples are the ones in which it is false (x2,x5,...).

Token 14211:
We want a tree that is consistent with the examples and is as small as possible.

Token 14212:
Un- fortunately, no matter how we measure size, it is an intractable problem to ﬁnd the smallestconsistent tree; there is no way to efﬁciently search through the 2 2ntrees.

Token 14213:
With some simple heuristics, however, we can ﬁnd a good approximate solution: a small (but not smallest) con-sistent tree.

Token 14214:
The D ECISION -TREE-LEARNING algorithm adopts a greedy divide-and-conquer strategy: always test the most important attribute ﬁrst.

Token 14215:
This test divides the problem up into smaller subproblems that can then be solved recursively.

Token 14216:
By “most important attribute,” we mean the one that makes the most difference to the classiﬁcation of an example.

Token 14217:
That way, we hope to get to the correct classiﬁcation with a small number of tests, meaning that all paths in the tree will be short and the tree as a whole will be shallow.

Token 14218:
Figure 18.4(a) shows that Type is a poor attribute, because it leaves us with four possible outcomes, each of which has the same number of positive as negative examples.

Token 14219:
On the otherhand, in (b) we see that Patrons is a fairly important attribute, because if the value is None or Some , then we are left with example sets for which we can answer deﬁnitively ( NoandYes, respectively).

Token 14220:
If the value is Full, we are left with a mixed set of examples.

Token 14221:
In general, after the ﬁrst attribute test splits up the examples, each outcome is a new decision tree learningproblem in itself, with fewer examples and one less attribute.

Token 14222:
There are four cases to considerfor these recursive problems: 1.

Token 14223:
If the remaining examples are all positive (or all negative), then we are done: we can answer Yes orNo.

Token 14224:
Figure 18.4(b) shows examples of this happening in the None and Some branches. 2.

Token 14225:
If there are some positive and some negative examples, then choose the best attribute to split them.

Token 14226:
Figure 18.4(b) shows Hungry being used to split the remaining examples. 3.

Token 14227:
If there are no examples left, it means that no example has been observed for this com-

Token 14228:
Section 18.3. Learning Decision Trees 701 (a)None Some FullPatrons? Yes No Hungry?

Token 14229:
(b)No Yes12 13468 25791011 French Italian Thai BurgerType?12 13468 25791011 1 56 1048 211123 79 7111368 124 25910 124 210 59 Figure 18.4 Splitting the examples by testing on attributes.

Token 14230:
At each node we show the positive (light boxes) and negative (dark boxes) examples remaining.

Token 14231:
(a) Splitting on Type brings us no nearer to distingui shing between positive and neg ative examples.

Token 14232:
(b) Splitting onPatrons does a good job of separating positive and n egative examples.

Token 14233:
After splitting on Patrons ,Hungry is a fairly good second test.

Token 14234:
bination of attribute values, and we return a default value calculated from the plurality classiﬁcation of all the examples that were used in constructing the node’s parent.

Token 14235:
These are passed along in the variable parent examples . 4.

Token 14236:
If there are no attributes left, but both positive and negative examples, it means that these examples have exactly the same description, but different classiﬁcations.

Token 14237:
This canhappen because there is an error or noise in the data; because the domain is nondeter- NOISE ministic; or because we can’t observe an attribute that would distinguish the examples.

Token 14238:
The best we can do is return the plurality classiﬁcation of the remaining examples. The D ECISION -TREE-LEARNING algorithm is shown in Figure 18.5.

Token 14239:
Note that the set of examples is crucial for constructing the tree, but nowhere do the examples appear in the tree itself.

Token 14240:
A tree consists of just tests on attributes in the interior nodes, values of attributes onthe branches, and output values on the leaf nodes.

Token 14241:
The details of the I MPORTANCE function are given in Section 18.3.4.

Token 14242:
The output of the learning algorithm on our sample trainingset is shown in Figure 18.6.

Token 14243:
The tree is clearly different from the original tree shown inFigure 18.2.

Token 14244:
One might conclude that the learning algorithm is not doing a very good jobof learning the correct function.

Token 14245:
This would be the wrong conclusion to draw, however.

Token 14246:
Thelearning algorithm looks at the examples , not at the correct function, and in fact, its hypothesis (see Figure 18.6) not only is consistent with all the examples, but is considerably simpler than the original tree!

Token 14247:
The learning algorithm has no reason to include tests for Raining and Reservation , because it can classify all the examples without them.

Token 14248:
It has also detected an interesting and previously unsuspected pattern: the ﬁrst author will wait for Thai food on weekends.

Token 14249:
It is also bound to make some mistakes for cases where it has seen no examples.

Token 14250:
For example, it has never seen a case where the wait is 0–10 minutes but the restaurant is full.

Token 14251:
702 Chapter 18.

Token 14252:
Learning from Examples function DECISION -TREE-LEARNING (examples ,attributes ,parent examples )returns at r e e ifexamples is empty then return PLURALITY -VALUE (parent examples ) else if allexamples have the same classiﬁcation then return the classiﬁcation else ifattributes is empty then return PLURALITY -VALUE (examples ) else A←argmaxa∈attributes IMPORTANCE (a,examples ) tree←a new decision tree with root test A for each valuevkofAdo exs←{e:e∈examples ande.A=vk} subtree←DECISION -TREE-LEARNING (exs,attributes−A,examples ) add a branch to tree with label (A=vk)and subtree subtree return tree Figure 18.5 The decision-tree learning algorithm.

Token 14253:
The function I MPORTANCE is de- scribed in Section 18.3.4.

Token 14254:
The function P LURALITY -VALUE selects the most common output value among a set of examples, breaking ties randomly. None Some FullPatrons?

Token 14255:
No Yes No YesHungry? No No YesFri/Sat? Yes NoYesType?

Token 14256:
French Italian Thai Burger Yes No Figure 18.6 The decision tree induced from the 12-example training set.

Token 14257:
In that case it says not to wait when Hungry is false, but I (SR) would certainly wait.

Token 14258:
With more training examples the learning program could correct this mistake.

Token 14259:
We note there is a danger of over-interpreting the tree that the algorithm selects.

Token 14260:
When there are several variables of similar importance, the choice between them is somewhat arbi-trary: with slightly different input examples, a different variable would be chosen to split on ﬁrst, and the whole tree would look completely different.

Token 14261:
The function computed by the tree would still be similar, but the structure of the tree can vary widely.

Token 14262:
We can evaluate the accuracy of a learning algorithm with a learning curve ,a ss h o w n LEARNINGCURVE in Figure 18.7.

Token 14263:
We have 100 examples at our disposal, which we split into a training set and

Token 14264:
Section 18.3.

Token 14265:
Learning Decision Trees 703 0.40.50.60.70.80.91 0 20 40 60 80 100Proportion correct on test set Training set size Figure 18.7 A learning curve for the decision tree learning algorithm on 100 randomly generated examples in the restaurant domain.

Token 14266:
Each data point is the average of 20 trials. a test set. We learn a hypothesis hwith the training set and measure its accuracy with the test set.

Token 14267:
We do this starting with a training set of size 1 and increasing one at a time up to size99.

Token 14268:
For each size we actually repeat the process of randomly splitting 20 times, and average the results of the 20 trials.

Token 14269:
The curve shows that as the training set size grows, the accuracy increases. (For this reason, learning curves are also called happy graphs .)

Token 14270:
In this graph we reach 95% accuracy, and it looks like the curve might continue to increase with more data.

Token 14271:
18.3.4 Choosing attribute tests The greedy search used in decision tree learning is designed to approximately minimize thedepth of the ﬁnal tree.

Token 14272:
The idea is to pick the attribute that goes as far as possible towardproviding an exact classiﬁcation of the examples.

Token 14273:
A perfect attribute divides the examplesinto sets, each of which are all positive or all negative and thus will be leaves of the tree.

Token 14274:
ThePatrons attribute is not perfect, but it is fairly good.

Token 14275:
A really useless attribute, such as Type , leaves the example sets with roughly the same proportion of positive and negative examples as the original set.

Token 14276:
All we need, then, is a formal measure of “fairly good” and “really useless” and we can implement the I MPORTANCE function of Figure 18.5.

Token 14277:
We will use the notion of information gain, which is deﬁned in terms of entropy , the fundamental quantity in information theory ENTROPY (Shannon and Weaver, 1949).

Token 14278:
Entropy is a measure of the uncertainty of a random variable; acquisition of information corresponds to a reduction in entropy.

Token 14279:
A random variable with only one value—a coin thatalways comes up heads—has no uncertainty and thus its entropy is deﬁned as zero; thus, wegain no information by observing its value.

Token 14280:
A ﬂip of a fair coin is equally likely to come upheads or tails, 0 or 1, and we will soon show that this counts as “1 bit” of entropy.

Token 14281:
The roll of a fair four-sided die has 2 bits of entropy, because it takes two bits to describe one of four equally probable choices.

Token 14282:
Now consider an unfair coin that comes up heads 99% of the time.Intuitively, this coin has less uncertainty than the fair coin—if we guess heads we’ll be wrongonly 1% of the time—so we would like it to have an entropy measure that is close to zero, but

Token 14283:
704 Chapter 18. Learning from Examples positive.

Token 14284:
In general, the entropy of a random variable Vwith values vk, each with probability P(vk),i sd e ﬁ n e da s Entropy: H(V)=/summationdisplay kP(vk)log21 P(vk)=−/summationdisplay kP(vk)log2P(vk).

Token 14285:
We can check that the entropy of a fair coin ﬂip is indeed 1 bit: H(Fair)=−(0.5log20.5+0.5log20.5) = 1 .

Token 14286:
If the coin is loaded to give 99% heads, we get H(Loaded )=−(0.99log20.99 + 0 .01log20.01)≈0.08bits.

Token 14287:
It will help to deﬁne B(q)as the entropy of a Boolean random variable that is true with probability q: B(q)=−(qlog2q+( 1−q)log2(1−q)).

Token 14288:
Thus, H(Loaded )=B(0.99)≈0.08. Now let’s get back to decision tree learning.

Token 14289:
If a training set contains ppositive examples and nnegative examples, then the entropy of the goal attribute on the whole set is H(Goal)=B/parenleftbiggp p+n/parenrightbigg .

Token 14290:
The restaurant training set in Figure 18.3 has p=n=6, so the corresponding entropy is B(0.5)or exactly 1 bit.

Token 14291:
A test on a single attribute Amight give us only part of this 1 bit.

Token 14292:
We can measure exactly how much by looking at the entropy remaining after the attribute test.

Token 14293:
An attribute Awithddistinct values divides the training set Einto subsets E1,...,E d. Each subset Ekhaspkpositive examples and nknegative examples, so if we go along that branch, we will need an additional B(pk/(pk+nk))bits of information to answer the ques- tion.

Token 14294:
A randomly chosen example from the training set has the kth value for the attribute with probability (pk+nk)/(p+n), so the expected entropy remaining after testing attribute Ais Remainder (A)=d/summationdisplay k=1pk+nk p+nB(pk pk+nk).

Token 14295:
Theinformation gain from the attribute test on Ais the expected reduction in entropy: INFORMATION GAIN Gain(A)=B(p p+n)−Remainder (A).

Token 14296:
In fact Gain(A)is just what we need to implement the I MPORTANCE function.

Token 14297:
Returning to the attributes considered in Figure 18.4, we have Gain(Patrons )=1−/bracketleftbig2 12B(0 2)+4 12B(4 4)+6 12B(2 6)/bracketrightbig ≈0.541bits, Gain(Type)=1−/bracketleftbig2 12B(1 2)+2 12B(1 2)+4 12B(2 4)+4 12B(2 4)/bracketrightbig =0bits, conﬁrming our intuition that Patrons is a better attribute to split on.

Token 14298:
In fact, Patrons has the maximum gain of any of the attributes and would be chosen by the decision-tree learningalgorithm as the root.

Token 14299:
Section 18.3.

Token 14300:
Learning Decision Trees 705 18.3.5 Generalization and overﬁtting On some problems, the D ECISION -TREE-LEARNING algorithm will generate a large tree when there is actually no pattern to be found.

Token 14301:
Consider the problem of trying to predictwhether the roll of a die will come up as 6 or not.

Token 14302:
Suppose that experiments are carried outwith various dice and that the attributes describing each training example include the colorof the die, its weight, the time when the roll was done, and whether the experimenters hadtheir ﬁngers crossed.

Token 14303:
If the dice are fair, the right thing to learn is a tree with a single nodethat says “no,” But the D ECISION -TREE-LEARNING algorithm will seize on any pattern it can ﬁnd in the input.

Token 14304:
If it turns out that there are 2 rolls of a 7-gram blue die with ﬁngers crossed and they both come out 6, then the algorithm may construct a path that predicts 6 inthat case.

Token 14305:
This problem is called overﬁtting .

Token 14306:
A general phenomenon, overﬁtting occurs with OVERFITTING all types of learners, even when the target function is not at all random.

Token 14307:
In Figure 18.1(b) and (c), we saw polynomial functions overﬁtting the data.

Token 14308:
Overﬁtting becomes more likely as thehypothesis space and the number of input attributes grows, and less likely as we increase thenumber of training examples.

Token 14309:
For decision trees, a technique called decision tree pruning combats overﬁtting.

Token 14310:
Prun- DECISION TREE PRUNING ing works by eliminating nodes that are not clearly relevant.

Token 14311:
We start with a full tree, as generated by D ECISION -TREE-LEARNING . We then look at a test node that has only leaf nodes as descendants.

Token 14312:
If the test appears to be irrelevant—detecting only noise in the data— then we eliminate the test, replacing it with a leaf node.

Token 14313:
We repeat this process, considering each test with only leaf descendants, until each one has either been pruned or accepted as is.

Token 14314:
The question is, how do we detect that a node is testing an irrelevant attribute?

Token 14315:
Suppose we are at a node consisting of ppositive and nnegative examples.

Token 14316:
If the attribute is irrelevant, we would expect that it would split the examples into subsets that each have roughly the sameproportion of positive examples as the whole set, p/(p+n), and so the information gain will be close to zero.

Token 14317:
2Thus, the information gain is a good clue to irrelevance.

Token 14318:
Now the question is, how large a gain should we require in order to split on a particular attribute?

Token 14319:
We can answer this question by using a statistical signiﬁcance test .

Token 14320:
Such a test begins SIGNIFICANCE TEST by assuming that there is no underlying pattern (the so-called null hypothesis ).

Token 14321:
Then the ac- NULL HYPOTHESIS tual data are analyzed to calculate the extent to which they deviate from a perfect absence of pattern.

Token 14322:
If the degree of deviation is statistically unlikely (usually taken to mean a 5% prob- ability or less), then that is considered to be good evidence for the presence of a signiﬁcantpattern in the data.

Token 14323:
The probabilities are calculated from standard distributions of the amountof deviation one would expect to see in random sampling.

Token 14324:
In this case, the null hypothesis is that the attribute is irrelevant and, hence, that the information gain for an inﬁnitely large sample would be zero.

Token 14325:
We need to calculate theprobability that, under the null hypothesis, a sample of size v=n+pwould exhibit the observed deviation from the expected distribution of positive and negative examples.

Token 14326:
We canmeasure the deviation by comparing the actual numbers of positive and negative examples in 2The gain will be strictly positive except for the unlikely case where all the proportions are exactly the same.

Token 14327:
(See Exercise 18.5.)

Token 14328:
706 Chapter 18.

Token 14329:
Learning from Examples each subset, pkandnk, with the expected numbers, ˆpkandˆnk, assuming true irrelevance: ˆpk=p×pk+nk p+nˆnk=n×pk+nk p+n.

Token 14330:
A convenient measure of the total deviation is given by Δ=d/summationdisplay k=1(pk−ˆpk)2 ˆpk+(nk−ˆnk)2 ˆnk.

Token 14331:
Under the null hypothesis, the value of Δis distributed according to the χ2(chi-squared) distribution with v−1degrees of freedom.

Token 14332:
We can use a χ2table or a standard statistical library routine to see if a particular Δvalue conﬁrms or rejects the null hypothesis.

Token 14333:
For example, consider the restaurant type attribute, with four values and thus three degrees offreedom.

Token 14334:
A value of Δ=7.82or more would reject the null hypothesis at the 5% level (and a value of Δ=1 1 .35or more would reject at the 1% level).

Token 14335:
Exercise 18.8 asks you to extend the D ECISION -TREE-LEARNING algorithm to implement this form of pruning, which is known asχ2pruning .

Token 14336:
χ2PRUNING With pruning, noise in the examples can be tolerated.

Token 14337:
Errors in the example’s label (e.g., an example (x,Yes)that should be (x,No)) give a linear increase in prediction error, whereas errors in the descriptions of examples (e.g., Price =$when it was actually Price =$ $ )h a v e an asymptotic effect that gets worse as the tree shrinks down to smaller sets.

Token 14338:
Pruned trees perform signiﬁcantly better than unpruned trees when the data contain a large amount ofnoise.

Token 14339:
Also, the pruned trees are often much smaller and hence easier to understand.

Token 14340:
One ﬁnal warning: You might think that χ 2pruning and information gain look similar, so why not combine them using an approach called early stopping —have the decision tree EARLY STOPPING algorithm stop generating nodes when there is no good attribute to split on, rather than going to all the trouble of generating nodes and then pruning them away.

Token 14341:
The problem with earlystopping is that it stops us from recognizing situations where there is no one good attribute,but there are combinations of attributes that are informative.

Token 14342:
For example, consider the XORfunction of two binary attributes.

Token 14343:
If there are roughly equal number of examples for all fourcombinations of input values, then neither attribute will be informative, yet the correct thing to do is to split on one of the attributes (it doesn’t matter which one), and then at the second level we will get splits that are informative.

Token 14344:
Early stopping would miss this, but generate-and-then-prune handles it correctly.

Token 14345:
18.3.6 Broadening the applicability of decision trees In order to extend decision tree induction to a wider variety of problems, a number of issuesmust be addressed.

Token 14346:
We will brieﬂy mention several, suggesting that a full understanding is best obtained by doing the associated exercises: •Missing data : In many domains, not all the attribute values will be known for every example.

Token 14347:
The values might have gone unrecorded, or they might be too expensive toobtain.

Token 14348:
This gives rise to two problems: First, given a complete decision tree, howshould one classify an example that is missing one of the test attributes?

Token 14349:
Second, how

Token 14350:
Section 18.3. Learning Decision Trees 707 should one modify the information-gain formula when some examples have unknown values for the attribute?

Token 14351:
These questions are addressed in Exercise 18.9.

Token 14352:
•Multivalued attributes : When an attribute has many possible values, the information gain measure gives an inappropriate indication of the attribute’s usefulness.

Token 14353:
In the ex-treme case, an attribute such as ExactTime has a different value for every example, which means each subset of examples is a singleton with a unique classiﬁcation, andthe information gain measure would have its highest value for this attribute.

Token 14354:
But choos-ing this split ﬁrst is unlikely to yield the best tree. One solution is to use the gain ratio GAIN RATIO (Exercise 18.10).

Token 14355:
Another possibility is to allow a Boolean test of the form A=vk,t h a t is, picking out just one of the possible values for an attribute, leaving the remainingvalues to possibly be tested later in the tree.

Token 14356:
•Continuous and integer-valued input attributes : Continuous or integer-valued at- tributes such as Height andWeight , have an inﬁnite set of possible values.

Token 14357:
Rather than generate inﬁnitely many branches, decision-tree learning algorithms typically ﬁnd thesplit point that gives the highest information gain.

Token 14358:
For example, at a given node in SPLIT POINT the tree, it might be the case that testing on Weight >160gives the most informa- tion.

Token 14359:
Efﬁcient methods exist for ﬁnding good split points: start by sorting the values of the attribute, and then consider only split points that are between two examples in sorted order that have different classiﬁcations, while keeping track of the running totalsof positive and negative examples on each side of the split point.

Token 14360:
Splitting is the mostexpensive part of real-world decision tree learning applications.

Token 14361:
•Continuous-valued output attributes : If we are trying to predict a numerical output value, such as the price of an apartment, then we need a regression tree rather than a REGRESSION TREE classiﬁcation tree.

Token 14362:
A regression tree has at each leaf a linear function of some subset of numerical attributes, rather than a single value.

Token 14363:
For example, the branch for two-bedroom apartments might end with a linear function of square footage, number ofbathrooms, and average income for the neighborhood.

Token 14364:
The learning algorithm mustdecide when to stop splitting and begin applying linear regression (see Section 18.6)over the attributes.

Token 14365:
A decision-tree learning system for real-world applications must be able to handle all of these problems.

Token 14366:
Handling continuous-valued variables is especially important, because both physical and ﬁnancial processes provide numerical data.

Token 14367:
Several commercial packages havebeen built that meet these criteria, and they have been used to develop thousands of ﬁeldedsystems.

Token 14368:
In many areas of industry and commerce, decision trees are usually the ﬁrst methodtried when a classiﬁcation method is to be extracted from a data set.

Token 14369:
One important property of decision trees is that it is possible for a human to understand the reason for the output of the learning algorithm.

Token 14370:
(Indeed, this is a legal requirement for ﬁnancial decisions that are subject to anti-discrimination laws.)

Token 14371:
This is a property not shared by some other representations,such as neural networks.

Token 14372:
708 Chapter 18. Learning from Examples 18.4 E V ALUATING AND CHOOSING THE BESTHYPOTHESIS We want to learn a hypothesis that ﬁts the future data best.

Token 14373:
To make that precise we need to deﬁne “future data” and “best.” We make the stationarity assumption : that there is aSTATIONARITY ASSUMPTION probability distribution over examples that remains stationary over time.

Token 14374:
Each example data point (before we see it) is a random variable Ejwhose observed value ej=(xj,yj)is sampled from that distribution, and is independent of the previous examples: P(Ej|Ej−1,Ej−2,...)=P(Ej), and each example has an identical prior probability distribution: P(Ej)=P(Ej−1)=P(Ej−2)=···.

Token 14375:
Examples that satisfy these assumptions are called independent and identically distributed or i.i.d.. An i.i.d.

Token 14376:
assumption connects the past to the future; without some such connection, all I.I.D. bets are off—the future could be anything.

Token 14377:
(We will see later that learning can still occur if there are slow changes in the distribution.)

Token 14378:
The next step is to deﬁne “best ﬁt.” We deﬁne the error rate of a hypothesis as the ERROR RATE proportion of mistakes it makes—the proportion of times that h(x)/negationslash=yfor an (x,y)example.

Token 14379:
Now, just because a hypothesis hhas a low error rate on the training set does not mean that it will generalize well.

Token 14380:
A professor knows that an exam will not accurately evaluate students if they have already seen the exam questions.

Token 14381:
Similarly, to get an accurate evaluation of a hypothesis, we need to test it on a set of examples it has not seen yet.

Token 14382:
The simplest approach is the one we have seen already: randomly split the available data into a training set from which the learning algorithm produces hand a test set on which the accuracy of his evaluated.

Token 14383:
This method, sometimes called holdout cross-validation , has the disadvantage that it fails to useHOLDOUT CROSS-VALIDATION all the available data; if we use half the data for the test set, then we are only training on half the data, and we may get a poor hypothesis.

Token 14384:
On the other hand, if we reserve only 10% ofthe data for the test set, then we may, by statistical chance, get a poor estimate of the actual accuracy.

Token 14385:
We can squeeze more out of the data and still get an accurate estimate using a technique called k-fold cross-validation .

Token 14386:
The idea is that each example serves double duty—as training K-FOLD CROSS-VALIDATION data and test data. First we split the data into kequal subsets.

Token 14387:
We then perform krounds of learning; on each round 1/kof the data is held out as a test set and the remaining examples are used as training data.

Token 14388:
The average test set score of the krounds should then be a better estimate than a single score.

Token 14389:
Popular values for kare 5 and 10—enough to give an estimate that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.The extreme is k=n, also known as leave-one-out cross-validation orLOOCV .

Token 14390:
LEAVE-ONE-OUT CROSS-VALIDATION LOOCV Despite the best efforts of statistical methodologists, users frequently invalidate their results by inadvertently peeking at the test data.

Token 14391:
Peeking can happen like this: A learning PEEKING algorithm has various “knobs” that can be twiddled to tune its behavior—for example, various different criteria for choosing the next attribute in decision tree learning.

Token 14392:
The researchergenerates hypotheses for various different settings of the knobs, measures their error rates onthe test set, and reports the error rate of the best hypothesis.

Token 14393:
Alas, peeking has occurred! The

Token 14394:
Section 18.4.

Token 14395:
Evaluating and Choosing the Best Hypothesis 709 reason is that the hypothesis was selected on the basis of its test set error rate , so information about the test set has leaked into the learning algorithm.

Token 14396:
Peeking is a consequence of using test-set performance to both choose a hypothesis and evaluate i t .

Token 14397:
T h ew a yt oa v o i dt h i si st o really hold the test set out—lock it away until you are completely done with learning and simply wish to obtain an independent evaluation of the ﬁnal hypothesis.

Token 14398:
(And then, if you don’t like the results ...you have to obtain, and lock away, a completely new test set if you want to go back and ﬁnd a better hypothesis.)

Token 14399:
If thetest set is locked away, but you still want to measure performance on unseen data as a way ofselecting a good hypothesis, then divide the available data (without the test set) into a trainingset and a validation set .

Token 14400:
The next section shows how to use validation sets to ﬁnd a good VALIDATION SET tradeoff between hypothesis complexity and goodness of ﬁt.

Token 14401:
18.4.1 Model selection: Complexity versus goodness of ﬁt In Figure 18.1 (page 696) we showed that higher-degree polynomials can ﬁt the training data better, but when the degree is too high they will overﬁt, and perform poorly on validation data.

Token 14402:
Choosing the degree of the polynomial is an instance of the problem of model selection .Y o u MODEL SELECTION can think of the task of ﬁnding the best hypothesis as two tasks: model selection deﬁnes the hypothesis space and then optimization ﬁnds the best hypothesis within that space.

Token 14403:
OPTIMIZATION In this section we explain how to select among models that are parameterized by size.

Token 14404:
For example, with polynomials we have size=1for linear functions, size=2for quadratics, and so on.

Token 14405:
For decision trees, the size could be the number of nodes in the tree.

Token 14406:
In all caseswe want to ﬁnd the value of the size parameter that best balances underﬁtting and overﬁtting to give the best test set accuracy.

Token 14407:
An algorithm to perform model selection and optimization is shown in Figure 18.8.

Token 14408:
It is awrapper that takes a learning algorithm as an argument (D ECISION -TREE-LEARNING , WRAPPER for example).

Token 14409:
The wrapper enumerates models according to a parameter, size.

Token 14410:
For each size, it uses cross validation on Learner to compute the average error rate on the training and test sets.

Token 14411:
We start with the smallest, simplest models (which probably underﬁt the data), anditerate, considering more complex models at each step, until the models start to overﬁt.

Token 14412:
InFigure 18.9 we see typical curves: the training set error decreases monotonically (althoughthere may in general be slight random variation), while the validation set error decreases atﬁrst, and then increases when the model begins to overﬁt.

Token 14413:
The cross-validation procedurepicks the value of size with the lowest validation set error; the bottom of the U-shaped curve.

Token 14414:
We then generate a hypothesis of that size, using all the data (without holding out any of it).

Token 14415:
Finally, of course, we should evaluate the returned hypothesis on a separate test set.

Token 14416:
This approach requires that the learning algorithm accept a parameter, size, and deliver a hypothesis of that size.

Token 14417:
As we said, for decision tree learning, the size can be the number of nodes.

Token 14418:
We can modify D ECISION -TREE-LEARNER so that it takes the number of nodes as an input, builds the tree breadth-ﬁrst rather than depth-ﬁrst (but at each level it still choosesthe highest gain attribute ﬁrst), and stops when it reaches the desired number of nodes.

Token 14419:
710 Chapter 18.

Token 14420:
Learning from Examples function CROSS -VALIDATION -WRAPPER (Learner ,k,examples )returns a hypothesis local variables :errT , an array, indexed by size, storing training-set error rates errV , an array, indexed by size, storing validation-set error rates forsize =1to∞do errT [size],errV [size]←CROSS -VALIDATION (Learner ,size,k,examples ) iferrT has converged then do best size←the value of size with minimum errV [size] return Learner (best size,examples ) function CROSS -VALIDATION (Learner ,size,k,examples )returns two values: average training set error rate, av erage validation set error rate fold errT←0;fold errV←0 forfold =1to k do training set,validation set←PARTITION (examples ,fold,k) h←Learner (size,training set) fold errT←fold errT +E RROR -RATE(h,training set) fold errV←fold errV +ERROR -RATE(h,validation set) return fold errT /k,fold errV /k Figure 18.8 An algorithm to select the model that has the lowest error rate on validation data by building models of increasing complexity, and choosing the one with best empir- ical error rate on validation data.

Token 14421:
Here errT means error rate on the training data, and errV means error rate on the validation data.

Token 14422:
Learner (size,examples )returns a hypoth- esis whose complexity is set by the parameter size, and which is trained on the examples .

Token 14423:
PARTITION (examples ,fold,k) splits examples into two subsets: a validation set of size N/k and a training set with all the other examples.

Token 14424:
The split is different for each value of fold. 18.4.2 From error rates to loss So far, we have been trying to minimize error rate.

Token 14425:
This is clearly better than maximizing error rate, but it is not the full story.

Token 14426:
Consider the problem of classifying email messages as spam or non-spam.

Token 14427:
It is worse to classify non-spam as spam (and thus potentially miss an important message) then to classify spam as non-spam (and thus suffer a few seconds ofannoyance).

Token 14428:
So a classiﬁer with a 1% error rate, where almost all the errors were classifyingspam as non-spam, would be better than a classiﬁer with only a 0.5% error rate, if most ofthose errors were classifying non-spam as spam.

Token 14429:
We saw in Chapter 16 that decision-makersshould maximize expected utility, and utility is what learners should maximize as well.

Token 14430:
Inmachine learning it is traditional to express utilities by means of a loss function .T h e l o s s LOSS FUNCTION function L(x,y,ˆy)is deﬁned as the amount of utility lost by predicting h(x)=ˆywhen the correct answer is f(x)=y: L(x,y,ˆy)=Utility (result of using ygiven an input x) −Utility (result of using ˆygiven an input x)

Token 14431:
Section 18.4.

Token 14432:
Evaluating and Choosing the Best Hypothesis 711 0 10 20 30 40 50 60 1 2 3 4 5 6 7 8 9 10Error rate Tree sizeValidation Set Error Training Set Error Figure 18.9 Error rates on training data (lower, dashed line) and validation data (upper, solid line) for different size decision trees.

Token 14433:
We stop when the training set error rate asymp- totes, and then choose the tree with minimal error on the validation set; in this case the tree of size 7 nodes.

Token 14434:
This is the most general formulation of the loss function. Often a simpliﬁed version is used, L(y,ˆy), that is independent of x.

Token 14435:
We will use the simpliﬁed version for the rest of this chapter, which means we can’t say that it is worse to misclassify a letter from Mom than it is to misclassify a letter from our annoying cousin, but we can say it is 10 times worse toclassify non-spam as spam than vice-versa: L(spam,nospam )=1,L(nospam ,spam)=1 0 .

Token 14436:
Note that L(y,y)is always zero; by deﬁnition there is no loss when you guess exactly right.

Token 14437:
For functions with discrete outputs, we can enumerate a loss value for each possible mis- classiﬁcation, but we can’t enumerate all the possibilities for real-valued data.

Token 14438:
If f(x)is 137.035999, we would be fairly happy with h(x) = 137 .036, but just how happy should we be?

Token 14439:
In general small errors are better than large ones; two functions that implement that ideaare the absolute value of the difference (called the L 1loss), and the square of the difference (called the L2loss).

Token 14440:
If we are content with the idea of minimizing error rate, we can use theL0/1loss function, which has a loss of 1 for an incorrect answer and is appropriate for discrete-valued outputs: Absolute value loss: L1(y,ˆy)=|y−ˆy| Squared error loss: L2(y,ˆy)=(y−ˆy)2 0/1 loss: L0/1(y,ˆy)=0 ify=ˆy,else1 The learning agent can theoretically maximize its expected utility by choosing the hypoth- esis that minimizes expected loss over all input–output pairs it will see.

Token 14441:
It is meaningless to talk about this expectation without deﬁning a prior probability distribution, P(X,Y)over examples.

Token 14442:
Let Ebe the set of all possible input–output examples.

Token 14443:
Then the expected gener- alization loss for a hypothesis h(with respect to loss function L)i sGENERALIZATION LOSS

Token 14444:
712 Chapter 18.

Token 14445:
Learning from Examples GenLoss L(h)=/summationdisplay (x,y)∈EL(y,h(x))P(x,y), and the best hypothesis, h∗, is the one with the minimum expected generalization loss: h∗=a r g m i n h∈HGenLoss L(h).

Token 14446:
Because P(x,y)is not known, the learning agent can only estimate generalization loss with empirical loss on a set of examples, E: EMPIRICAL LOSS EmpLossL,E(h)=1 N/summationdisplay (x,y)∈EL(y,h(x)).

Token 14447:
The estimated best hypothesis ˆh∗is then the one with minimum empirical loss: ˆh∗=a r g m i n h∈HEmpLossL,E(h).

Token 14448:
There are four reasons why ˆh∗may differ from the true function, f: unrealizability, variance, noise, and computational complexity.

Token 14449:
First, fmay not be realizable—may not be in H—or may be present in such a way that other hypotheses are preferred.

Token 14450:
Second, a learning algo-rithm will return different hypotheses for different sets of examples, even if those sets aredrawn from the same true function f, and those hypotheses will make different predictions on new examples.

Token 14451:
The higher the variance among the predictions, the higher the probabilityof signiﬁcant error.

Token 14452:
Note that even when the problem is realizable, there will still be random variance, but that variance decreases towards zero as the number of training examples in- creases.

Token 14453:
Third, fmay be nondeterministic or noisy —it may return different values for f(x) NOISE each time xoccurs.

Token 14454:
By deﬁnition, noise cannot be predicted; in many cases, it arises because the observed labels yare the result of attributes of the environment not listed in x.

Token 14455:
And ﬁnally, whenHis complex, it can be computationally intractable to systematically search the whole hypothesis space.

Token 14456:
The best we can do is a local search (hill climbing or greedy search) thatexplores only part of the space. That gives us an approximation error.

Token 14457:
Combining the sourcesof error, we’re left with an estimation of an approximation of the true function f. Traditional methods in statistics and the early years of machine learning concentrated onsmall-scale learning , where the number of training examples ranged from dozens to the SMALL-SCALE LEARNING low thousands.

Token 14458:
Here the generalization error mostly comes from the approximation error of not having the true fin the hypothesis space, and from estimation error of not having enough training examples to limit variance.

Token 14459:
In recent years there has been more emphasis on large- scale learning , often with millions of examples.

Token 14460:
Here the generalization error is dominatedLARGE-SCALE LEARNING by limits of computation: there is enough data and a rich enough model that we could ﬁnd an hthat is very close to the true f, but the computation to ﬁnd it is too complex, so we settle for a sub-optimal approximation.

Token 14461:
18.4.3 Regularization In Section 18.4.1, we saw how to do model selection with cross-validation on model size.

Token 14462:
Analternative approach is to search for a hypothesis that directly minimizes the weighted sum of

Token 14463:
Section 18.5.

Token 14464:
The Theory of Learning 713 empirical loss and the complexity of the hypothesis, which we will call the total cost: Cost(h)=EmpLoss (h)+λComplexity (h) ˆh∗=a r g m i n h∈HCost(h).

Token 14465:
Hereλis a parameter, a positive number that serves as a conversion rate between loss and hypothesis complexity (which after all are not measured on the same scale).

Token 14466:
This approachcombines loss and complexity into one metric, allowing us to ﬁnd the best hypothesis all at once.

Token 14467:
Unfortunately we still need to do a cross-validation search to ﬁnd the hypothesis that generalizes best, but this time it is with different values of λrather than size.

Token 14468:
We select the value of λthat gives us the best validation set score.

Token 14469:
This process of explicitly penalizing complex hypotheses is called regularization (be- REGULARIZATION cause it looks for a function that is more regular, or less complex).

Token 14470:
Note that the cost function requires us to make two choices: the loss function and the complexity measure, which iscalled a regularization function.

Token 14471:
The choice of regularization function depends on the hy-pothesis space.

Token 14472:
For example, a good regularization function for polynomials is the sum ofthe squares of the coefﬁcients—keeping the sum small would guide us away from the wigglypolynomials in Figure 18.1(b) and (c).

Token 14473:
We will show an example of this type of regularizationin Section 18.6.

Token 14474:
Another way to simplify models is to reduce the dimensions that the models work with.

Token 14475:
A process of feature selection can be performed to discard attributes that appear to be irrel- FEATURE SELECTION evant.

Token 14476:
χ2pruning is a kind of feature selection.

Token 14477:
It is in fact possible to have the empirical loss and the complexity measured on the same scale, without the conversion factor λ: they can both be measured in bits.

Token 14478:
First encode the hypothesis as a Turing machine program, and count the number of bits.

Token 14479:
Then countthe number of bits required to encode the data, where a correctly predicted example costszero bits and the cost of an incorrectly predicted example depends on how large the error is.The minimum description length or MDL hypothesis minimizes the total number of bits MINIMUM DESCRIPTION LENGTHrequired.

Token 14480:
This works well in the limit, but for smaller problems there is a difﬁculty in that the choice of encoding for the program—for example, how best to encode a decision tree as a bit string—affects the outcome.

Token 14481:
In Chapter 20 (page 805), we describe a probabilistic interpretation of the MDL approach.

Token 14482:
18.5 T HETHEORY OF LEARNING The main unanswered question in learning is this: How can we be sure that our learning algorithm has produced a hypothesis that will predict the correct value for previously unseeninputs?

Token 14483:
In formal terms, how do we know that the hypothesis his close to the target function fif we don’t know what fis?

Token 14484:
These questions have been pondered for several centuries.

Token 14485:
In more recent decades, other questions have emerged: how many examples do we needto get a good h? What hypothesis space should we use?

Token 14486:
If the hypothesis space is very complex, can we even ﬁnd the best h, or do we have to settle for a local maximum in the

Token 14487:
714 Chapter 18. Learning from Examples space of hypotheses? How complex should hbe? How do we avoid overﬁtting? This section examines these questions.

Token 14488:
We’ll start with the question of how many examples are needed for learning.

Token 14489:
We saw from the learning curve for decision tree learning on the restaurant problem (Figure 18.7 onpage 703) that improves with more training data.

Token 14490:
Learning curves are useful, but they are speciﬁc to a particular learning algorithm on a particular problem.

Token 14491:
Are there some more gen- eral principles governing the number of examples needed in general?

Token 14492:
Questions like this areaddressed by computational learning theory , which lies at the intersection of AI, statistics, COMPUTATIONAL LEARNINGTHEORY and theoretical computer science.

Token 14493:
The underlying principle is that any hypothesis that is seri- ously wrong will almost certainly be “found out” with high probability after a small number of examples, because it will make an incorrect prediction.

Token 14494:
Thus, any hypothesis that is consis-tent with a sufﬁciently large set of training examples is unlikely to be seriously wrong: that is,it must be probably approximately correct .Any learning algorithm that returns hypotheses PROBABLY APPROXIMATELYCORRECT that are probably approximately correct is called a PAC learning algorithm; we can use this PAC LEARNING approach to provide bounds on the performance of various learning algorithms.

Token 14495:
PAC-learning theorems, like all theorems, are logical consequences of axioms.

Token 14496:
When atheorem (as opposed to, say, a political pundit) states something about the future based on the past, the axioms have to provide the “juice” to make that connection.

Token 14497:
For PAC learning, the juice is provided by the stationarity assumption introduced on page 708, which says thatfuture examples are going to be drawn from the same ﬁxed distribution P(E)=P(X,Y) as past examples.

Token 14498:
(Note that we do not have to know what distribution that is, just that itdoesn’t change.)

Token 14499:
In addition, to keep things simple, we will assume that the true function f is deterministic and is a member of the hypothesis class Hthat is being considered.

Token 14500:
The simplest PAC theorems deal with Boolean functions, for which the 0/1 loss is ap- propriate.

Token 14501:
The error rate of a hypothesis h, deﬁned informally earlier, is deﬁned formally here as the expected generalization error for examples drawn from the stationary distribution: error(h)=GenLoss L0/1(h)=/summationdisplay x,yL0/1(y,h(x))P(x,y).

Token 14502:
In other words, error (h)is the probability that hmisclassiﬁes a new example.

Token 14503:
This is the same quantity being measured experimentally by the learning curves shown earlier.

Token 14504:
A hypothesis his called approximately correct if error (h)≤/epsilon1,w h e r e /epsilon1is a small constant.

Token 14505:
We will show that we can ﬁnd an Nsuch that, after seeing Nexamples, with high probability, all consistent hypotheses will be approximately correct.

Token 14506:
One can think of an approximately correct hypothesis as being “close” to the true function in hypothesis space: itlies inside what is called the /epsilon1-ball around the true function f. The hypothesis space outside /epsilon1 -BALL this ball is called Hbad.

Token 14507:
We can calculate the probability that a “seriously wrong” hypothesis hb∈H badis consistent with the ﬁrst Nexamples as follows.

Token 14508:
We know that error (hb)>/epsilon1. Thus, the probability that it agrees with a given example is at most 1−/epsilon1.

Token 14509:
Since the examples are independent, the bound for Nexamples is P(hbagrees with Nexamples )≤(1−/epsilon1)N.

Token 14510:
Section 18.5.

Token 14511:
The Theory of Learning 715 The probability that Hbadcontains at least one consistent hypothesis is bounded by the sum of the individual probabilities: P(Hbadcontains a consistent hypothesis )≤|H bad|(1−/epsilon1)N≤|H|(1−/epsilon1)N, where we have used the fact that |Hbad|≤|H| .

Token 14512:
We would like to reduce the probability of this event below some small number δ: |H|(1−/epsilon1)N≤δ.

Token 14513:
Given that 1−/epsilon1≤e−/epsilon1, we can achieve this if we allow the algorithm to see N≥1 /epsilon1/parenleftbigg ln1 δ+l n|H|/parenrightbigg (18.1) examples.

Token 14514:
Thus, if a learning algorithm returns a hypothesis that is consistent with this many examples, then with probability at least 1−δ,i th a se r r o ra tm o s t /epsilon1.

Token 14515:
In other words, it is probably approximately correct.

Token 14516:
The number of required examples, as a function of /epsilon1andδ, is called the sample complexity of the hypothesis space.SAMPLE COMPLEXITY As we saw earlier, if His the set of all Boolean functions on nattributes, then |H|= 22n.

Token 14517:
Thus, the sample complexity of the space grows as 2n.

Token 14518:
Because the number of possible examples is also 2n, this suggests that PAC-learning in the class of all Boolean functions requires seeing all, or nearly all, of the possible examples.

Token 14519:
A moment’s thought reveals thereason for this: Hcontains enough hypotheses to classify any given set of examples in all possible ways.

Token 14520:
In particular, for any set of Nexamples, the set of hypotheses consistent with those examples contains equal numbers of hypotheses that predict x N+1to be positive and hypotheses that predict xN+1to be negative.

Token 14521:
To obtain real generalization to unseen examples, then, it seems we need to restrict the hypothesis space Hin some way; but of course, if we do restrict the space, we might eliminate the true function altogether.

Token 14522:
There are three ways to escape this dilemma. The ﬁrst, which we will cover in Chapter 19, is to bring prior knowledge to bear on the problem.

Token 14523:
The second, which we introduced in Section 18.4.3, is to insist that the algorithm return not just any consistent hypothesis, but preferably a simple one (as is done in decision tree learning).

Token 14524:
Incases where ﬁnding simple consistent hypotheses is tractable, the sample complexity resultsare generally better than for analyses based only on consistency.

Token 14525:
The third escape, whichwe pursue next, is to focus on learnable subsets of the entire hypothesis space of Booleanfunctions.

Token 14526:
This approach relies on the assumption that the restricted language contains ahypothesis hthat is close enough to the true function f; the beneﬁts are that the restricted hypothesis space allows for effective generalization and is typically easier to search.

Token 14527:
We nowexamine one such restricted language in more detail.

Token 14528:
18.5.1 PAC learning example: Learning decision lists We now show how to apply PAC learning to a new hypothesis space: decision lists .A DECISION LISTS decision list consists of a series of tests, each of which is a conjunction of literals.

Token 14529:
If a test succeeds when applied to an example description, the decision list speciﬁes the valueto be returned.

Token 14530:
If the test fails, processing continues with the next test in the list.

Token 14531:
Decisionlists resemble decision trees, but their overall structure is simpler: they branch only in one

Token 14532:
716 Chapter 18.

Token 14533:
Learning from Examples Patrons (x, Some )No Yes YesNoPatrons (x, Full) Fri/Sat (x) YesNo Yes^ Figure 18.10 A decision list for the restaurant problem.

Token 14534:
direction. In contrast, the individual tests are more complex.

Token 14535:
Figure 18.10 shows a decision list that represents the following hypothesis: WillWait⇔(Patrons =Some)∨(Patrons =Full∧Fri/Sat).

Token 14536:
If we allow tests of arbitrary size, then decision lists can represent any Boolean function (Exercise 18.14).

Token 14537:
On the other hand, if we restrict the size of each test to at most kliterals, then it is possible for the learning algorithm to generalize successfully from a small numberof examples.

Token 14538:
We call this language k -DL. The example in Figure 18.10 is in 2-DL.

Token 14539:
It is easy to k-DL show (Exercise 18.14) that k-DLincludes as a subset the language k-DT, the set of all decision k-DT trees of depth at most k. It is important to remember that the particular language referred to byk-DLdepends on the attributes used to describe the examples.

Token 14540:
We will use the notation k-DL(n)to denote a k-DLlanguage using nBoolean attributes.

Token 14541:
The ﬁrst task is to show that k-DLis learnable—that is, that any function in k-DLcan be approximated accurately after training on a reasonable number of examples.

Token 14542:
To do this,we need to calculate the number of hypotheses in the language.

Token 14543:
Let the language of tests—conjunctions of at most kliterals using nattributes—be Conj(n,k).

Token 14544:
Because a decision list is constructed of tests, and because each test can be attached to either a Yesor aNooutcome or can be absent from the decision list, there are at most 3 |Conj(n,k)|distinct sets of component tests.

Token 14545:
Each of these sets of tests can be in any order, so |k-DL(n)|≤3|Conj(n,k)||Conj(n,k)|!.

Token 14546:
The number of conjunctions of kliterals from nattributes is given by |Conj(n,k)|=k/summationdisplay i=0/parenleftbigg2n i/parenrightbigg =O(nk).

Token 14547:
Hence, after some work, we obtain |k-DL(n)|=2O(nklog2(nk)).

Token 14548:
We can plug this into Equation (18.1) to show that the number of examples needed for PAC- learning a k-DLfunction is polynomial in n: N≥1 /epsilon1/parenleftbigg ln1 δ+O(nklog2(nk))/parenrightbigg .

Token 14549:
Therefore, any algorithm that returns a consistent decision list will PAC-learn a k-DLfunction in a reasonable number of examples, for small k. The next task is to ﬁnd an efﬁcient algorithm that returns a consistent decision list.

Token 14550:
We will use a greedy algorithm called D ECISION -LIST-LEARNING that repeatedly ﬁnds a

Token 14551:
Section 18.6.

Token 14552:
Regression and Classiﬁcation with Linear Models 717 function DECISION -LIST-LEARNING (examples )returns a decision list, or failure ifexamples is empty then return the trivial decision list No t←a test that matches a nonempty subset examplestofexamples such that the members of examplestare all positive or all negative ifthere is no such tthen return failure ifthe examples in examplestare positive theno←Yes elseo←No return a decision list with initial test tand outcome oand remaining tests given by DECISION -LIST-LEARNING (examples−examplest) Figure 18.11 An algorithm for learning decision lists.

Token 14553:
0.40.50.60.70.80.91 0 20 40 60 80 100Proportion correct on test set Training set sizeDecision tree Decision list Figure 18.12 Learning curve for D ECISION -LIST-LEARNING algorithm on the restaurant data.

Token 14554:
The curve for D ECISION -TREE-LEARNING is shown for comparison. test that agrees exactly with some subset of the training set.

Token 14555:
Once it ﬁnds such a test, it adds it to the decision list under construction and removes the corresponding examples.

Token 14556:
Itthen constructs the remainder of the decision list, using just the remaining examples. This isrepeated until there are no examples left.

Token 14557:
The algorithm is shown in Figure 18.11. This algorithm does not specify the method for selecting the next test to add to the decision list.

Token 14558:
Although the formal results given earlier do not depend on the selection method,it would seem reasonable to prefer small tests that match large sets of uniformly classiﬁed examples, so that the overall decision list will be as compact as possible.

Token 14559:
The simplest strategy is to ﬁnd the smallest test tthat matches any uniformly classiﬁed subset, regardless of the size of the subset.

Token 14560:
Even this approach works quite well, as Figure 18.12 suggests.

Token 14561:
18.6 R EGRESSION AND CLASSIFICATION WITH LINEAR MODELS Now it is time to move on from decision trees and lists to a different hypothesis space, onethat has been used for hundred of years: the class of linear functions of continuous-valued LINEAR FUNCTION

Token 14562:
718 Chapter 18.

Token 14563:
Learning from Examples 300 400 500 600 700 800 900 1000 500 1000 1500 2000 2500 3000 3500House price in $1000 House size in square feetw0 w1Loss (a) (b) Figure 18.13 (a) Data points of price versus ﬂoor space of houses for sale in Berkeley, CA, in July 2009, along with the linear function hypothesis that minimizes squared error loss:y=0.232x+ 246 .

Token 14564:
(b) Plot of the loss function/summationtext j(w1xj+w0−yj)2for various values of w0,w1.

Token 14565:
Note that the loss function is convex, with a single global minimum. inputs.

Token 14566:
We’ll start with the simplest case: regression with a univariate linear function, oth- erwise known as “ﬁtting a straight line.” Section 18.6.2 covers the multivariate case.

Token 14567:
Sec-tions 18.6.3 and 18.6.4 show how to turn linear functions into classiﬁers by applying hard and soft thresholds.

Token 14568:
18.6.1 Univariate linear regression A univariate linear function (a straight line) with input xand output yhas the form y=w1x+ w0,w h e r e w0andw1are real-valued coefﬁcients to be learned.

Token 14569:
We use the letter wbecause we think of the coefﬁcients as weights ;t h ev a l u eo f yis changed by changing the relative WEIGHT weight of one term or another.

Token 14570:
We’ll deﬁne wto be the vector [w0,w1], and deﬁne hw(x)=w1x+w0.

Token 14571:
Figure 18.13(a) shows an example of a training set of npoints in the x,yplane, each point representing the size in square feet and the price of a house offered for sale.

Token 14572:
The task ofﬁnding the h wthat best ﬁts these data is called linear regression .

Token 14573:
To ﬁt a line to the data, all LINEAR REGRESSION we have to do is ﬁnd the values of the weights [w0,w1]that minimize the empirical loss.

Token 14574:
It is traditional (going back to Gauss3) to use the squared loss function, L2, summed over all the training examples: Loss(hw)=N/summationdisplay j=1L2(yj,hw(xj)) =N/summationdisplay j=1(yj−hw(xj))2=N/summationdisplay j=1(yj−(w1xj+w0))2.

Token 14575:
3Gauss showed that if the yjvalues have normally distributed noise, then the most likely values of w1andw0 are obtained by minimizing the sum of the squares of the errors.

Token 14576:
Section 18.6.

Token 14577:
Regression and Classiﬁcation with Linear Models 719 We would like to ﬁnd w∗=a r g m i nwLoss(hw).T h e s u m/summationtextN j=1(yj−(w1xj+w0))2is minimized when its partial derivatives with respect to w0andw1are zero: ∂ ∂w0N/summationdisplay j=1(yj−(w1xj+w0))2=0and∂ ∂w1N/summationdisplay j=1(yj−(w1xj+w0))2=0.

Token 14578:
(18.2) These equations have a unique solution: w1=N(/summationtextxjyj)−(/summationtextxj)(/summationtextyj) N(/summationtextx2 j)−(/summationtextxj)2;w0=(/summationdisplay yj−w1(/summationdisplay xj))/N .

Token 14579:
(18.3) For the example in Figure 18.13(a), the solution is w1=0.232,w0= 246 , and the line with those weights is shown as a dashed line in the ﬁgure.

Token 14580:
Many forms of learning involve adjusting weights to minimize a loss, so it helps to have a mental picture of what’s going on in weight space —the space deﬁned by all possible WEIGHT SPACE settings of the weights.

Token 14581:
For univariate linear regression, the weight space deﬁned by w0and w1is two-dimensional, so we can graph the loss as a function of w0andw1in a 3D plot (see Figure 18.13(b)).

Token 14582:
We see that the loss function is convex , as deﬁned on page 133; this is true forevery linear regression problem with an L2loss function, and implies that there are no local minima.

Token 14583:
In some sense that’s the end of the story for linear models; if we need to ﬁtlines to data, we apply Equation (18.3).

Token 14584:
4 To go beyond linear models, we will need to face the fact that the equations deﬁning minimum loss (as in Equation (18.2)) will often have no closed-form solution.

Token 14585:
Instead, wewill face a general optimization search problem in a continuous weight space.

Token 14586:
As indicatedin Section 4.2 (page 129), such problems can be addressed by a hill-climbing algorithm thatfollows the gradient of the function to be optimized.

Token 14587:
In this case, because we are trying to minimize the loss, we will use gradient descent .

Token 14588:
We choose any starting point in weight GRADIENT DESCENT space—here, a point in the ( w0,w1) plane—and then move to a neighboring point that is downhill, repeating until we converge on the minimum possible loss: w←any point in the parameter space loop until convergence do for each wiin w do wi←wi−α∂ ∂wiLoss(w) (18.4) The parameter α, which we called the step size in Section 4.2, is usually called the learning rate when we are trying to minimize loss in a learning problem.

Token 14589:
It can be a ﬁxed constant, or LEARNINGRATE it can decay over time as the learning process proceeds.

Token 14590:
For univariate regression, the loss function is a quadratic function, so the partial deriva- tive will be a linear function.

Token 14591:
(The only calculus you need to know is that∂ ∂xx2=2xand ∂ ∂xx=1.)

Token 14592:
Let’s ﬁrst work out the partial derivatives—the slopes—in the simpliﬁed case of 4With some caveats: the L2loss function is appropriate when there is normally-distributed noise that is inde- pendent of x; all results rely on the stationarity assumption; etc.

Token 14593:
720 Chapter 18.

Token 14594:
Learning from Examples only one training example, (x,y): ∂ ∂wiLoss(w)=∂ ∂wi(y−hw(x))2 =2 (y−hw(x))×∂ ∂wi(y−hw(x)) =2 (y−hw(x))×∂ ∂wi(y−(w1x+w0)), (18.5) applying this to both w0andw1we get: ∂ ∂w0Loss(w)=−2(y−hw(x));∂ ∂w1Loss(w)=−2(y−hw(x))×x Then, plugging this back into Equation (18.4), and folding the 2 into the unspeciﬁed learning rateα, we get the following learning rule for the weights: w0←w0+α(y−hw(x));w1←w1+α(y−hw(x))×x These updates make intuitive sense: if hw(x)>y, i.e., the output of the hypothesis is too large, reduce w0a bit, and reduce w1ifxwas a positive input but increase w1ifxwas a negative input.

Token 14595:
The preceding equations cover one training example. For Ntraining examples, we want to minimize the sum of the individual losses for each example.

Token 14596:
The derivative of a sum is thesum of the derivatives, so we have: w 0←w0+α/summationdisplay j(yj−hw(xj));w1←w1+α/summationdisplay j(yj−hw(xj))×xj.

Token 14597:
These updates constitute the batch gradient descent learning rule for univariate linear re-BATCH GRADIENT DESCENT gression.

Token 14598:
Convergence to the unique global minimum is guaranteed (as long as we pick α small enough) but may be very slow: we have to cycle through all the training data for every step, and there may be many steps.

Token 14599:
There is another possibility, called stochastic gradient descent , where we considerSTOCHASTIC GRADIENT DESCENT only a single training point at a time, taking a step after each one using Equation (18.5).

Token 14600:
Stochastic gradient descent can be used in an online setting, where new data are coming inone at a time, or ofﬂine, where we cycle through the same data as many times as is neces-sary, taking a step after considering each single example.

Token 14601:
It is often faster than batch gradientdescent.

Token 14602:
With a ﬁxed learning rate α, however, it does not guarantee convergence; it can os- cillate around the minimum without settling down.

Token 14603:
In some cases, as we see later, a scheduleof decreasing learning rates (as in simulated annealing) does guarantee convergence.

Token 14604:
18.6.2 Multivariate linear regression We can easily extend to multivariate linear regression problems, in which each example xjMULTIVARIATE LINEAR REGRESSION is ann-element vector.5Our hypothesis space is the set of functions of the form hsw(xj)=w0+w1xj,1+···+wnxj,n=w0+/summationdisplay iwixj,i.

Token 14605:
5The reader may wish to consult Appendix A for a brief summary of linear algebra.

Token 14606:
Section 18.6. Regression and Classiﬁcation with Linear Models 721 Thew0term, the intercept, stands out as different from the others.

Token 14607:
We can ﬁx that by inventing a dummy input attribute, xj,0, which is deﬁned as always equal to 1.

Token 14608:
Then his simply the dot product of the weights and the input vector (or equivalently, the matrix product of thetranspose of the weights and the input vector): h sw(xj)=w·xj=w/latticetopxj=/summationdisplay iwixj,i.

Token 14609:
The best vector of weights, w∗, minimizes squared-error loss over the examples: w∗=a r g m i n w/summationdisplay jL2(yj,w·xj).

Token 14610:
Multivariate linear regression is actually not much more complicated than the univariate case we just covered.

Token 14611:
Gradient descent will reach the (unique) minimum of the loss function; theupdate equation for each weight w iis wi←wi+α/summationdisplay jxj,i(yj−hw(xj)).

Token 14612:
(18.6) It is also possible to solve analytically for the wthat minimizes loss.

Token 14613:
Let ybe the vector of outputs for the training examples, and Xbe the data matrix , i.e., the matrix of inputs with DATA MATRIX onen-dimensional example per row.

Token 14614:
Then the solution w∗=(X/latticetopX)−1X/latticetopy minimizes the squared error.

Token 14615:
With univariate linear regression we didn’t have to worry about overﬁtting.

Token 14616:
But with multivariate linear regression in high-dimensional spaces it is possible that some dimension that is actually irrelevant appears by chance to be useful, resulting in overﬁtting .

Token 14617:
Thus, it is common to use regularization on multivariate linear functions to avoid over- ﬁtting.

Token 14618:
Recall that with regularization we minimize the total cost of a hypothesis, countingboth the empirical loss and the complexity of the hypothesis: Cost(h)=EmpLoss (h)+λComplexity (h).

Token 14619:
For linear functions the complexity can be speciﬁed as a function of the weights.

Token 14620:
We can consider a family of regularization functions: Complexity (h w)=Lq(w)=/summationdisplay i|wi|q.

Token 14621:
As with loss functions,6withq=1we have L1regularization, which minimizes the sum of the absolute values; with q=2,L2regularization minimizes the sum of squares.

Token 14622:
Which reg- ularization function should you pick?

Token 14623:
That depends on the speciﬁc problem, but L1regular- ization has an important advantage: it tends to produce a sparse model .

Token 14624:
That is, it often sets SPARSE MODEL many weights to zero, effectively declaring the corresponding attributes to be irrelevant—just as D ECISION -TREE-LEARNING does (although by a different mechanism).

Token 14625:
Hypotheses that discard attributes can be easier for a human to understand, and may be less likely to overﬁt.

Token 14626:
6It is perhaps confusing that L1andL2are used for both loss functions and regularization functions.

Token 14627:
They need not be used in pairs: you could use L2loss with L1regularization, or vice versa.

Token 14628:
722 Chapter 18. Learning from Examples w1w2 w* w1w2 w* Figure 18.14 WhyL1regularization tends to produce a sparse model.

Token 14629:
(a) With L1regu- larization (box), the minimal achievable loss (concentric contours) often occurs on an axis, meaning a weight of zero.

Token 14630:
(b) With L2regularization (circle), the minimal loss is likely to o c c u ra n y w h e r eo nt h ec i r c l e ,g i v i n gn op r e f e r e n c et oz e r ow e i g h t s .

Token 14631:
Figure 18.14 gives an intuitive explanation of why L1regularization leads to weights of zero, while L2regularization does not.

Token 14632:
Note that minimizing Loss(w)+λComplexity (w) is equivalent to minimizing Loss(w)subject to the constraint that Complexity (w)≤c,f o r some constant cthat is related to λ.

Token 14633:
Now, in Figure 18.14(a) the diamond-shaped box repre- sents the set of points win two-dimensional weight space that have L1complexity less than c; our solution will have to be somewhere inside this box.

Token 14634:
The concentric ovals represent contours of the loss function, with the minimum loss at the center.

Token 14635:
We want to ﬁnd the pointin the box that is closest to the minimum; you can see from the diagram that, for an arbitraryposition of the minimum and its contours, it will be common for the corner of the box to ﬁndits way closest to the minimum, just because the corners are pointy.

Token 14636:
And of course the corners are the points that have a value of zero in some dimension.

Token 14637:
In Figure 18.14(b), we’ve done the same for the L 2complexity measure, which represents a circle rather than a diamond.

Token 14638:
Here you can see that, in general, there is no reason for the intersection to appear on one ofthe axes; thus L 2regularization does not tend to produce zero weights.

Token 14639:
The result is that the number of examples required to ﬁnd a good his linear in the number of irrelevant features for L2regularization, but only logarithmic with L1regularization.

Token 14640:
Empirical evidence on many problems supports this analysis.

Token 14641:
Another way to look at it is that L1regularization takes the dimensional axes seriously, while L2treats them as arbitrary.

Token 14642:
The L2function is spherical, which makes it rotationally invariant: Imagine a set of points in a plane, measured by their xandycoordinates.

Token 14643:
Now imagine rotating the axes by 45o. You’d get a different set of (x/prime,y/prime)values representing the same points.

Token 14644:
If you apply L2regularization before and after rotating, you get exactly the same point as the answer (although the point would be described with the new (x/prime,y/prime) coordinates).

Token 14645:
That is appropriate when the choice of axes really is arbitrary—when it doesn’tmatter whether your two dimensions are distances north and east; or distances north-east and

Token 14646:
Section 18.6. Regression and Classiﬁcation with Linear Models 723 south-east.

Token 14647:
With L1regularization you’d get a different answer, because the L1function is not rotationally invariant.

Token 14648:
That is appropriate when the axes are not interchangeable; it doesn’tmake sense to rotate “number of bathrooms” 45 otowards “lot size.” 18.6.3 Linear classiﬁers with a hard threshold Linear functions can be used to do classiﬁcation as well as regression.

Token 14649:
For example, Fig- ure 18.15(a) shows data points of two classes: earthquakes (which are of interest to seismolo-gists) and underground explosions (which are of interest to arms control experts).

Token 14650:
Each pointis deﬁned by two input values, x 1andx2, that refer to body and surface wave magnitudes computed from the seismic signal.

Token 14651:
Given these training data, the task of classiﬁcation is tolearn a hypothesis hthat will take new (x 1,x2)points and return either 0 for earthquakes or 1 for explosions.

Token 14652:
2.5 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 4.5 5 5.5 6 6.5 7x2 x1 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 4.5 5 5.5 6 6.5 7x2 x1 (a) (b) Figure 18.15 (a) Plot of two seismic data parameters, body wave magnitude x1and sur- face wave magnitude x2, for earthquakes (white circles) and nuclear explosions (black cir- cles) occurring between 1982 and 1990 in Asia and the Middle East (Kebeasy et al.

Token 14653:
, 1998). Also shown is a decision boundary between the classes. (b) The same domain with more data points.

Token 14654:
The earthquakes and explosions are no longer linearly separable.

Token 14655:
Adecision boundary is a line (or a surface, in higher dimensions) that separates theDECISION BOUNDARY two classes.

Token 14656:
In Figure 18.15(a), the decision boundary is a straight line.

Token 14657:
A linear decision boundary is called a linear separator and data that admit such a separator are called linearly LINEAR SEPARATOR separable .

Token 14658:
The linear separator in this case is deﬁned byLINEAR SEPARABILITY x2=1.7x1−4.9or−4.9+1.7x1−x2=0.

Token 14659:
The explosions, which we want to classify with value 1, are to the right of this line with higher values of x1and lower values of x2, so they are points for which −4.9+1.7x1−x2>0, while earthquakes have −4.9+1.7x1−x2<0.

Token 14660:
Using the convention of a dummy input x0=1, we can write the classiﬁcation hypothesis as hw(x)=1 ifw·x≥0and0otherwise.

Token 14661:
724 Chapter 18.

Token 14662:
Learning from Examples Alternatively, we can think of has the result of passing the linear function w·xthrough a threshold function :THRESHOLD FUNCTION hw(x)=Threshold (w·x)whereThreshold (z)=1 ifz≥0and0otherwise.

Token 14663:
The threshold function is shown in Figure 18.17(a).

Token 14664:
Now that the hypothesis hw(x)has a well-deﬁned mathematical form, we can think about choosing the weights wto minimize the loss.

Token 14665:
In Sections 18.6.1 and 18.6.2, we did this both in closed form (by setting the gradient to zero and solving for the weights) andby gradient descent in weight space.

Token 14666:
Here, we cannot do either of those things because thegradient is zero almost everywhere in weight space except at those points where w·x=0, and at those points the gradient is undeﬁned.

Token 14667:
There is, however, a simple weight update rule that converges to a solution—that is, a linear separator that classiﬁes the data perfectly–provided the data are linearly separable.

Token 14668:
Fora single example (x,y),w eh a v e w i←wi+α(y−hw(x))×xi (18.7) which is essentially identical to the Equation (18.6), the update rule for linear regression!

Token 14669:
This rule is called the perceptron learning rule , for reasons that will become clear in Section 18.7.PERCEPTRON LEARNINGRULE Because we are considering a 0/1 classiﬁcation problem, however, the behavior is somewhat different.

Token 14670:
Both the true value yand the hypothesis output hw(x)are either 0 or 1, so there are three possibilities: •If the output is correct, i.e., y=hw(x), then the weights are not changed.

Token 14671:
•Ifyis 1 but hw(x)is 0, then wiisincreased when the corresponding input xiis positive anddecreased whenxiis negative.

Token 14672:
This makes sense, because we want to make w·x bigger so that hw(x)outputs a 1.

Token 14673:
•Ifyis 0 but hw(x)is 1, then wiisdecreased when the corresponding input xiis positive andincreased whenxiis negative.

Token 14674:
This makes sense, because we want to make w·x smaller so that hw(x)outputs a 0.

Token 14675:
Typically the learning rule is applied one example at a time, choosing examples at random (as in stochastic gradient descent).

Token 14676:
Figure 18.16(a) shows a training curve for this learning TRAININGCURVE rule applied to the earthquake/explosion data shown in Figure 18.15(a).

Token 14677:
A training curve measures the classiﬁer performance on a ﬁxed training set as the learning process proceeds on that same training set.

Token 14678:
The curve shows the update rule converging to a zero-error linear separator. The “convergence” process isn’t exactly pretty, but it always works.

Token 14679:
This particular run takes 657 steps to converge, for a data set with 63 examples, so each example is presented roughly 10 times on average.

Token 14680:
Typically, the variation across runs is very large.

Token 14681:
We have said that the perceptron learning rule converges to a perfect linear separator when the data points are linearly separable, but what if they are not?

Token 14682:
This situation is alltoo common in the real world. For example, Figure 18.15(b) adds back in the data points left out by Kebeasy et al.

Token 14683:
(1998) when they plotted the data shown in Figure 18.15(a).

Token 14684:
In Figure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000steps: even though it hits the minimum-error solution (three errors) many times, the algo-rithm keeps changing the weights.

Token 14685:
In general, the perceptron rule may not converge to a

Token 14686:
Section 18.6.

Token 14687:
Regression and Classiﬁcation with Linear Models 725 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700Proportion correct Number of weight updates 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20000 40000 60000 80000 100000Proportion correct Number of weight updates 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20000 40000 60000 80000 100000Proportion correct Number of weight updates (a) (b) (c) Figure 18.16 (a) Plot of total training-set accuracy vs. number of iterations through the training set for the perceptron learning rule, given the earthquake/explosion data in Fig- ure 18.15(a).

Token 14688:
(b) The same plot for the noisy, non-separable data in Figure 18.15(b); note the change in scale of the x-axis.

Token 14689:
(c) The same plot as in (b), with a learning rate schedule α(t)= 1000 /(1000 + t).

Token 14690:
stable solution for ﬁxed learning rate α,b u ti f αdecays as O(1/t)where tis the iteration number, then the rule can be shown to converge to a minimum-error solution when examplesare presented in a random sequence.

Token 14691:
7It can also be shown that ﬁnding the minimum-error solution is NP-hard, so one expects that many presentations of the examples will be required for convergence to be achieved.

Token 14692:
Figure 18.16(b) shows the training process with a learning rate schedule α(t)= 1000 /(1000 + t): convergence is not perfect after 100,000 iterations, but it is much better than the ﬁxed- αcase.

Token 14693:
18.6.4 Linear classiﬁcation with logistic regression We have seen that passing the output of a linear function through the threshold function creates a linear classiﬁer; yet the hard nature of the threshold causes some problems: the hypothesis hw(x)is not differentiable and is in fact a discontinuous function of its inputs and its weights; this makes learning with the perceptron rule a very unpredictable adventure.

Token 14694:
Fur-thermore, the linear classiﬁer always announces a completely conﬁdent prediction of 1 or 0,even for examples that are very close to the boundary; in many situations, we really needmore gradated predictions.

Token 14695:
All of these issues can be resolved to a large extent by softening the threshold function— approximating the hard threshold with a continuous, differentiable function.

Token 14696:
In Chapter 14(page 522), we saw two functions that look like soft thresholds: the integral of the standardnormal distribution (used for the probit model) and the logistic function (used for the logitmodel).

Token 14697:
Although the two functions are very similar in shape, the logistic function Logistic (z)=1 1+e−z 7Technically, we require thatP∞ t=1α(t)=∞andP∞ t=1α2(t)<∞.

Token 14698:
The decay α(t)=O(1/t)satisﬁes these conditions.

Token 14699:
726 Chapter 18.

Token 14700:
Learning from Examples 0 0.5 1 -8-6-4-2 0 2 4 6 8 0 0.5 1 -6-4-2 0 2 4 6-2 0 2 4 6-4-2 0 2 4 6 8 10 0 0.2 0.4 0.6 0.8 1 x1x2 (a) (b) (c) Figure 18.17 (a) The hard threshold function Threshold (z)with 0/1 output.

Token 14701:
Note that the function is nondifferentiable at z=0. (b) The logistic function, Logistic (z)= 1 1+e−z, also known as the sigmoid function.

Token 14702:
(c) Plot of a logistic regression hypothesis hw(x)=Logistic (w·x)for the data shown in Figure 18.15(b). has more convenient mathematical properties.

Token 14703:
The function is shown in Figure 18.17(b). With the logistic function replacing the threshold function, we now have hw(x)=Logistic (w·x)=1 1+e−w·x.

Token 14704:
An example of such a hypothesis for the two-input earthquake/explosion problem is shown in Figure 18.17(c).

Token 14705:
Notice that the output, being a number between 0 and 1, can be interpretedas a probability of belonging to the class labeled 1.

Token 14706:
The hypothesis forms a soft boundary in the input space and gives a probability of 0.5 for any input at the center of the boundaryregion, and approaches 0 or 1 as we move away from the boundary.

Token 14707:
The process of ﬁtting the weights of this model to minimize loss on a data set is called logistic regression .

Token 14708:
There is no easy closed-form solution to ﬁnd the optimal value of wwith LOGISTIC REGRESSION this model, but the gradient descent computation is straightforward.

Token 14709:
Because our hypotheses no longer output just 0 or 1, we will use the L2loss function; also, to keep the formulas readable, we’ll use gto stand for the logistic function, with g/primeits derivative.

Token 14710:
For a single example (x,y), the derivation of the gradient is the same as for linear regression (Equation (18.5)) up to the point where the actual form of his inserted.

Token 14711:
(For this derivation, we will need the chain rule :∂g(f(x))/∂x=g/prime(f(x))∂f(x)/∂x.)

Token 14712:
We have CHAIN RULE ∂ ∂wiLoss(w)=∂ ∂wi(y−hw(x))2 =2 (y−hw(x))×∂ ∂wi(y−hw(x)) =−2(y−hw(x))×g/prime(w·x)×∂ ∂wiw·x =−2(y−hw(x))×g/prime(w·x)×xi.

Token 14713:
Section 18.7.

Token 14714:
Artiﬁcial Neural Networks 727 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1000 2000 3000 4000 5000Squared error per example Number of weight updates 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20000 40000 60000 80000 100000Squared error per example Number of weight updates 0.4 0.5 0.6 0.7 0.8 0.9 1 0 20000 40000 60000 80000 100000Squared error per example Number of weight updates (a) (b) (c) Figure 18.18 Repeat of the experiments in Figure 18.16 using logistic regression and squared error.

Token 14715:
The plot in (a) covers 5000 iterations rather than 1000, while (b) and (c) uset h es a m es c a l e .

Token 14716:
The derivative g/primeof the logistic function satisﬁes g/prime(z)=g(z)(1−g(z)),s ow eh a v e g/prime(w·x)=g(w·x)(1−g(w·x)) =hw(x)(1−hw(x)) so the weight update for minimizing the loss is wi←wi+α(y−hw(x))×hw(x)(1−hw(x))×xi.

Token 14717:
(18.8) Repeating the experiments of Figure 18.16 with logistic regression instead of the linear threshold classiﬁer, we obtain the results shown in Figure 18.18.

Token 14718:
In (a), the linearly sep-arable case, logistic regression is somewhat slower to converge, but behaves much morepredictably.

Token 14719:
In (b) and (c), where the data are noisy and nonseparable, logistic regression converges far more quickly and reliably.

Token 14720:
These advantages tend to carry over into real-world applications and logistic regression has become one of the most popular classiﬁcation tech-niques for problems in medicine, marketing and survey analysis, credit scoring, public health,and other applications.

Token 14721:
18.7 A RTIFICIAL NEURAL NETWORKS We turn now to what seems to be a somewhat unrelated topic: the brain.

Token 14722:
In fact, as wewill see, the technical ideas we have discussed so far in this chapter turn out to be useful inbuilding mathematical models of the brain’s activity; conversely, thinking about the brain hashelped in extending the scope of the technical ideas.

Token 14723:
Chapter 1 touched brieﬂy on the basic ﬁndings of neuroscience—in particular, the hy- pothesis that mental activity consists primarily of electrochemical activity in networks ofbrain cells called neurons .

Token 14724:
(Figure 1.2 on page 11 showed a schematic diagram of a typical neuron.)

Token 14725:
Inspired by this hypothesis, some of the earliest AI work aimed to create artiﬁcial neural networks .

Token 14726:
(Other names for the ﬁeld include connectionism ,parallel distributed NEURAL NETWORK processing ,a n d neural computation .)

Token 14727:
Figure 18.19 shows a simple mathematical model of the neuron devised by McCulloch and Pitts (1943).

Token 14728:
Roughly speaking, it “ﬁres” when alinear combination of its inputs exceeds some (hard or soft) threshold—that is, it implements

Token 14729:
728 Chapter 18.

Token 14730:
Learning from Examples OutputΣ Input LinksActivation FunctionInput FunctionOutput Linksa0 = 1 aj= g(inj) ajginj wi,jw0,jBias Weight ai Figure 18.19 A simple mathematical model for a neuron.

Token 14731:
The unit’s output activation is aj=g(/summationtextn i=0wi,jai),w h e r e aiis the output activation of unit iandwi,jis the weight on the link from unit ito this unit.

Token 14732:
a linear classiﬁer of the kind described in the preceding section.

Token 14733:
A neural network is just a collection of units connected together; the properties of the network are determined by its topology and the properties of the “neurons.” Since 1943, much more detailed and realistic models have been developed, both for neurons and for larger systems in the brain, leading to the modern ﬁeld of computational neuroscience .

Token 14734:
On the other hand, researchers in AI and statistics became interested in theCOMPUTATIONAL NEUROSCIENCE more abstract properties of neural networks, such as their ability to perform distributed com- putation, to tolerate noisy inputs, and to learn.

Token 14735:
Although we understand now that other kindsof systems—including Bayesian networks—have these properties, neural networks remainone of the most popular and effective forms of learning system and are worthy of study intheir own right.

Token 14736:
18.7.1 Neural network structures Neural networks are composed of nodes or units (see Figure 18.19) connected by directed UNIT links .

Token 14737:
A link from unit ito unit jserves to propagate the activation aifromitoj.8Each link LINK ACTIVATION also has a numeric weight wi,jassociated with it, which determines the strength and sign of WEIGHT the connection.

Token 14738:
Just as in linear regression models, each unit has a dummy input a0=1with an associated weight w0,j.

Token 14739:
Each unit jﬁrst computes a weighted sum of its inputs: inj=n/summationdisplay i=0wi,jai.

Token 14740:
Then it applies an activation function gto this sum to derive the output:ACTIVATION FUNCTION aj=g(inj)=g/parenleftBiggn/summationdisplay i=0wi,jai/parenrightBigg .

Token 14741:
(18.9) 8A note on notation: for this section, we are forced to suspend our usual conventions.

Token 14742:
Input attributes are still indexed by i, so that an “external” activation aiis given by input xi; but index jwill refer to internal units rather than examples.

Token 14743:
Throughout this section, the mathematical derivations concern a single generic example x, omitting the usual summations over examples to obtain results for the whole data set.

Token 14744:
Section 18.7.

Token 14745:
Artiﬁcial Neural Networks 729 The activation function gis typically either a hard threshold (Figure 18.17(a)), in which case the unit is called a perceptron , or a logistic function (Figure 18.17(b)), in which case the term PERCEPTRON sigmoid perceptron is sometimes used.

Token 14746:
Both of these nonlinear activation function ensureSIGMOID PERCEPTRON the important property that the entire network of units can represent a nonlinear function (see Exercise 18.22).

Token 14747:
As mentioned in the discussion of logistic regression (page 725), the logistic activation function has the added advantage of being differentiable.

Token 14748:
Having decided on the mathematical model for individual “neurons,” the next task is to connect them together to form a network.

Token 14749:
There are two fundamentally distinct ways todo this.

Token 14750:
A feed-forward network has connections only in one direction—that is, it forms a FEED-FORWARD NETWORK directed acyclic graph.

Token 14751:
Every node receives input from “upstream” nodes and delivers output to “downstream” nodes; there are no loops.

Token 14752:
A feed-forward network represents a function of its current input; thus, it has no internal state other than the weights themselves.

Token 14753:
A recurrent network , on the other hand, feeds its outputs back into its own inputs.

Token 14754:
This means thatRECURRENT NETWORK the activation levels of the network form a dynamical system that may reach a stable state or exhibit oscillations or even chaotic behavior.

Token 14755:
Moreover, the response of the network to a giveninput depends on its initial state, which may depend on previous inputs.

Token 14756:
Hence, recurrentnetworks (unlike feed-forward networks) can support short-term memory.

Token 14757:
This makes them more interesting as models of the brain, but also more difﬁcult to understand.

Token 14758:
This section will concentrate on feed-forward networks; some pointers for further reading on recurrentnetworks are given at the end of the chapter.

Token 14759:
Feed-forward networks are usually arranged in layers , such that each unit receives input LAYERS only from units in the immediately preceding layer.

Token 14760:
In the next two subsections, we will look at single-layer networks, in which every unit connects directly from the network’s inputs to its outputs, and multilayer networks, which have one or more layers of hidden units that are HIDDEN UNIT not connected to the outputs of the network.

Token 14761:
So far in this chapter, we have considered only learning problems with a single output variable y, but neural networks are often used in cases where multiple outputs are appropriate.

Token 14762:
For example, if we want to train a network to addtwo input bits, each a 0 or a 1, we will need one output for the sum bit and one for the carry bit.

Token 14763:
Also, when the learning problem involves classiﬁcation into more than two classes—for example, when learning to categorize images of handwritten digits—it is common to use oneoutput unit for each class.

Token 14764:
18.7.2 Single-layer feed-forward neural networks (perceptrons) A network with all the inputs connected directly to the outputs is called a single-layer neural network ,o ra perceptron network .

Token 14765:
Figure 18.20 shows a simple two-input, two-outputPERCEPTRON NETWORK perceptron network.

Token 14766:
With such a network, we might hope to learn the two-bit adder function, for example.

Token 14767:
Here are all the training data we will need: x1 x2 y3(carry) y4(sum) 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0

Token 14768:
730 Chapter 18.

Token 14769:
Learning from Examples The ﬁrst thing to notice is that a perceptron network with moutputs is really mseparate networks, because each weight affects only one of the outputs.

Token 14770:
Thus, there will be msepa- rate training processes.

Token 14771:
Furthermore, depending on the type of activation function used, thetraining processes will be either the perceptron learning rule (Equation (18.7) on page 724) or gradient descent rule for the logistic regression (Equation (18.8) on page 727).

Token 14772:
If you try either method on the two-bit-adder data, something interesting happens.

Token 14773:
Unit 3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,unit 4 is not defective!

Token 14774:
The problem is with the sum function itself.

Token 14775:
We saw in Section 18.6that linear classiﬁers (whether hard or soft) can represent linear decision boundaries in the in-put space.

Token 14776:
This works ﬁne for the carry function, which is a logical AND (see Figure 18.21(a)).

Token 14777:
The sum function, however, is an XOR (exclusive OR) of the two inputs.

Token 14778:
As Figure 18.21(c) illustrates, this function is not linearly separable so the perceptron cannot learn it.

Token 14779:
The linearly separable functions constitute just a small fraction of all Boolean func- tions; Exercise 18.20 asks you to quantify this fraction.

Token 14780:
The inability of perceptrons to learneven such simple functions as XOR was a signiﬁcant setback to the nascent neural network w3,5 3,6w 4,5w 4,6w5 6w1,3 1,4w 2,3w 2,4w1 23 4w1,3 1,4w 2,3w 2,4w1 23 4 (b) (a) Figure 18.20 (a) A perceptron network with two inputs and two output units.

Token 14781:
(b) A neural network with two inputs, one hidden layer of two units, and one output unit. Not shown are the dummy inputs and their associated weights.

Token 14782:
(a)x1andx21 0 01x1 x2 (b)x1orx2011 0x1 x2 (c)x1xorx2? 011 0x1 x2 Figure 18.21 Linear separability in threshold perceptrons.

Token 14783:
Black dots indicate a point in the input space where the value of the function is 1, and white dots indicate a point where the value is 0.

Token 14784:
The perceptron returns 1 on the region on the non-shaded side of the line. In (c), no such line exists that correctly classiﬁes the inputs.

Token 14785:
Section 18.7.

Token 14786:
Artiﬁcial Neural Networks 731 0.40.50.60.70.80.91 010 20 30 40 50 60 70 80 90100Proportion correct on test set Training set sizePerceptron Decision tree 0.40.50.60.70.80.91 010 20 30 40 50 60 70 80 90100Proportion correct on test set Training set sizePerceptron Decision tree (a) (b) Figure 18.22 Comparing the performance of perceptrons and decision trees.

Token 14787:
(a) Percep- trons are better at learning the majority function of 11 inputs.

Token 14788:
(b) Decision trees are better at learning the WillWait predicate in the restaurant example. community in the 1960s.

Token 14789:
Perceptrons are far from useless, however.

Token 14790:
Section 18.6.4 noted that logistic regression (i.e., training a sigmoid perceptron) is even today a very popular andeffective tool.

Token 14791:
Moreover, a perceptron can represent some quite “complex” Boolean func-tions very compactly.

Token 14792:
For example, the majority function , which outputs a 1 only if more than half of its ninputs are 1, can be represented by a perceptron with each w i=1and with w0=−n/2.

Token 14793:
A decision tree would need exponentially many nodes to represent this function.

Token 14794:
Figure 18.22 shows the learning curve for a perceptron on two different problems.

Token 14795:
On the left, we show the curve for learning the majority function with 11 Boolean inputs (i.e., the function outputs a 1 if 6 or more inputs are 1).

Token 14796:
As we would expect, the perceptron learns the function quite quickly, because the majority function is linearly separable.

Token 14797:
On the otherhand, the decision-tree learner makes no progress, because the majority function is very hard(although not impossible) to represent as a decision tree.

Token 14798:
On the right, we have the restaurantexample. The solution problem is easily represented as a decision tree, but is not linearlyseparable.

Token 14799:
The best plane through the data correctly classiﬁes only 65%.

Token 14800:
18.7.3 Multilayer feed-forward neural networks (McCulloch and Pitts, 1943) were well aware that a single threshold unit would not solve alltheir problems.

Token 14801:
In fact, their paper proves that such a unit can represent the basic Booleanfunctions AND,OR,a n d NOT and then goes on to argue that any desired functionality can be obtained by connecting large numbers of units into (possibly recurrent) networks of arbitrarydepth.

Token 14802:
The problem was that nobody knew how to train such networks.

Token 14803:
This turns out to be an easy problem if we think of a network the right way: as a function h w(x)parameterized by the weights w. Consider the simple network shown in Fig- ure 18.20(b), which has two input units, two hidden units, and two output unit.

Token 14804:
(In addition,each unit has a dummy input ﬁxed at 1.) Given an input vector x=(x 1,x2), the activations

Token 14805:
732 Chapter 18.

Token 14806:
Learning from Examples -4-2024 x1-4-2024 x200.20.40.60.81hW(x1,x2) -4-2024 x1-4-2024 x200.20.40.60.81hW(x1,x2) (a) (b) Figure 18.23 (a) The result of combining two opposite-facing soft threshold functions to produce a ridge.

Token 14807:
(b) The result of combining two ridges to produce a bump. of the input units are set to (a1,a2)=(x1,x2).

Token 14808:
The output at unit 5 is given by a5=g(w0,5,+w3,5a3+w4,5a4) =g(w0,5,+w3,5g(w0,3+w1,3a1+w2,3a2)+w4,5g(w04+w1,4a1+w2,4a2)) =g(w0,5,+w3,5g(w0,3+w1,3x1+w2,3x2)+w4,5g(w04+w1,4x1+w2,4x2)).

Token 14809:
Thus, we have the output expressed as a function of the inputs and the weights. A similar expression holds for unit 6.

Token 14810:
As long as we can calculate the derivatives of such expressionswith respect to the weights, we can use the gradient-descent loss-minimization method totrain the network.

Token 14811:
Section 18.7.4 shows exactly how to do this.

Token 14812:
And because the functionrepresented by a network can be highly nonlinear—composed, as it is, of nested nonlinear softthreshold functions—we can see neural networks as a tool for doing nonlinear regression .

Token 14813:
NONLINEAR REGRESSION Before delving into learning rules, let us look at the ways in which networks generate complicated functions.

Token 14814:
First, remember that each unit in a sigmoid network represents a soft threshold in its input space, as shown in Figure 18.17(c) (page 726).

Token 14815:
With one hidden layerand one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholdedlinear combination of several such functions.

Token 14816:
For example, by adding two opposite-facingsoft threshold functions and thresholding the result, we can obtain a “ridge” function as shownin Figure 18.23(a).

Token 14817:
Combining two such ridges at right angles to each other (i.e., combiningthe outputs from four hidden units), we obtain a “bump” as shown in Figure 18.23(b).

Token 14818:
With more hidden units, we can produce more bumps of different sizes in more places.

Token 14819:
In fact, with a single, sufﬁciently large hidden layer, it is possible to represent any continuousfunction of the inputs with arbitrary accuracy; with two layers, even discontinuous functionscan be represented.

Token 14820:
9Unfortunately, for any particular network structure, it is harder to char- acterize exactly which functions can be represented and which ones cannot.

Token 14821:
9The proof is complex, but the main point is that the required number of hidden units grows exponentially with the number of inputs.

Token 14822:
For example, 2n/nhidden units are needed to encode all Boolean functions of ninputs.

Token 14823:
Section 18.7.

Token 14824:
Artiﬁcial Neural Networks 733 18.7.4 Learning in multilayer networks First, let us dispense with one minor complication arising in multilayer networks: interactions among the learning problems when the network has multiple outputs.

Token 14825:
In such cases, weshould think of the network as implementing a vector function h wrather than a scalar function hw; for example, the network in Figure 18.20(b) returns a vector [a5,a6].

Token 14826:
Similarly, the target output will be a vector y.

Token 14827:
Whereas a perceptron network decomposes into mseparate learning problems for an m-output problem, this decomposition fails in a multilayer network.

Token 14828:
For example, both a5anda6in Figure 18.20(b) depend on all of the input-layer weights, so updates to those weights will depend on errors in both a5anda6.

Token 14829:
Fortunately, this dependency is very simple in the case of any loss function that is additive across the components of the error vector y−hw(x).F o rt h e L2loss, we have, for any weight w, ∂ ∂wLoss(w)=∂ ∂w|y−hw(x)|2=∂ ∂w/summationdisplay k(yk−ak)2=/summationdisplay k∂ ∂w(yk−ak)2(18.10) where the index kranges over nodes in the output layer.

Token 14830:
Each term in the ﬁnal summation is just the gradient of the loss for the kth output, computed as if the other outputs did not exist.

Token 14831:
Hence, we can decompose an m-output learning problem into mlearning problems, provided we remember to add up the gradient contributions from each of them when updatingthe weights.

Token 14832:
The major complication comes from the addition of hidden layers to the network.

Token 14833:
Whereas the error y−h wat the output layer is clear, the error at the hidden layers seems mysterious because the training data do not say what value the hidden nodes should have.Fortunately, it turns out that we can back-propagate the error from the output layer to the BACK-PROPAGATION hidden layers.

Token 14834:
The back-propagation process emerges directly from a derivation of the overall error gradient.

Token 14835:
First, we will describe the process with an intuitive justiﬁcation; then, we will show the derivation.

Token 14836:
At the output layer, the weight-update rule is identical to Equation (18.8).

Token 14837:
We have multiple output units, so let Errkbe the kth component of the error vector y−hw.

Token 14838:
We will also ﬁnd it convenient to deﬁne a modiﬁed error Δk=Errk×g/prime(ink), so that the weight- update rule becomes wj,k←wj,k+α×aj×Δk.

Token 14839:
(18.11) To update the connections between the input units and the hidden units, we need to deﬁne a quantity analogous to the error term for output nodes.

Token 14840:
Here is where we do the error back-propagation.

Token 14841:
The idea is that hidden node jis “responsible” for some fraction of the error Δ k in each of the output nodes to which it connects.

Token 14842:
Thus, the Δkvalues are divided according to the strength of the connection between the hidden node and the output node and are prop-agated back to provide the Δ jvalues for the hidden layer.

Token 14843:
The propagation rule for the Δ values is the following: Δj=g/prime(inj)/summationdisplay kwj,kΔk. (18.12)

Token 14844:
734 Chapter 18.

Token 14845:
Learning from Examples function BACK-PROP-LEARNING (examples ,network )returns a neural network inputs :examples , a set of examples, each with input vector xand output vector y network , a multilayer network with Llayers, weights wi,j, activation function g local variables :Δ, a vector of errors, indexed by network node repeat for each weight wi,jinnetwork do wi,j←a small random number for each example (x,y)inexamples do /*Propagate the inputs forward to compute the outputs */ for each nodeiin the input layer do ai←xi for/lscript=2toLdo for each nodejin layer /lscriptdo inj←/summationtext iwi,jai aj←g(inj) /*Propagate deltas backward from output layer to input layer */ for each nodejin the output layer do Δ[j]←g/prime(inj)×(yj−aj) for/lscript=L−1to1do for each nodeiin layer /lscriptdo Δ[i]←g/prime(ini)/summationtext jwi,jΔ[j] /*Update every weight in network using deltas */ for each weight wi,jinnetwork do wi,j←wi,j+α×ai×Δ[j] until some stopping criterion is satisﬁed return network Figure 18.24 The back-propagation algorithm for learning in multilayer networks.

Token 14846:
Now the weight-update rule for the weights between the inputs and the hidden layer is essen- tially identical to the update rule for the output layer: wi,j←wi,j+α×ai×Δj.

Token 14847:
The back-propagation process can be summarized as follows: •Compute the Δvalues for the output units, using the observed error.

Token 14848:
•Starting with output layer, repeat the following for each layer in the network, until the earliest hidden layer is reached: –Propagate the Δvalues back to the previous layer.

Token 14849:
–Update the weights between the two layers. The detailed algorithm is shown in Figure 18.24.

Token 14850:
For the mathematically inclined, we will now derive the back-propagation equations from ﬁrst principles.

Token 14851:
The derivation is quite similar to the gradient calculation for logistic

Token 14852:
Section 18.7.

Token 14853:
Artiﬁcial Neural Networks 735 regression (leading up to Equation (18.8) on page 727), except that we have to use the chain rule more than once.

Token 14854:
Following Equation (18.10), we compute just the gradient for Loss k=(yk−ak)2at thekth output.

Token 14855:
The gradient of this loss with respect to weights connecting the hidden layer to the output layer will be zero except for weights wj,kthat connect to the kth output unit.

Token 14856:
For those weights, we have ∂Loss k ∂wj,k=−2(yk−ak)∂ak ∂wj,k=−2(yk−ak)∂g(ink) ∂wj,k =−2(yk−ak)g/prime(ink)∂ink ∂wj,k=−2(yk−ak)g/prime(ink)∂ ∂wj,k⎛ ⎝/summationdisplay jwj,kaj⎞ ⎠ =−2(yk−ak)g/prime(ink)aj=−ajΔk, withΔkdeﬁned as before.

Token 14857:
To obtain the gradient with respect to the wi,jweights connecting the input layer to the hidden ¡layer, we have to expand out the activations ajand reapply the chain rule.

Token 14858:
We will show the derivation in gory detail because it is interesting to see how thederivative operator propagates back through the network: ∂Loss k ∂wi,j=−2(yk−ak)∂ak ∂wi,j=−2(yk−ak)∂g(ink) ∂wi,j =−2(yk−ak)g/prime(ink)∂ink ∂wi,j=−2Δk∂ ∂wi,j⎛ ⎝/summationdisplay jwj,kaj⎞ ⎠ =−2Δkwj,k∂aj ∂wi,j=−2Δkwj,k∂g(inj) ∂wi,j =−2Δkwj,kg/prime(inj)∂inj ∂wi,j =−2Δkwj,kg/prime(inj)∂ ∂wi,j/parenleftBigg/summationdisplay iwi,jai/parenrightBigg =−2Δkwj,kg/prime(inj)ai=−aiΔj, where Δjis deﬁned as before.

Token 14859:
Thus, we obtain the update rules obtained earlier from intuitive considerations.

Token 14860:
It is also clear that the process can be continued for networks with more thanone hidden layer, which justiﬁes the general algorithm given in Figure 18.24.

Token 14861:
Having made it through (or skipped over) all the mathematics, let’s see how a single- hidden-layer network performs on the restaurant problem.

Token 14862:
First, we need to determine thestructure of the network. We have 10 attributes describing each example, so we will need10 input units.

Token 14863:
Should we have one hidden layer or two? How many nodes in each layer?Should they be fully connected?

Token 14864:
There is no good theory that will tell us the answer. (See the next section.)

Token 14865:
As always, we can use cross-validation: try several different structures and see which one works best.

Token 14866:
It turns out that a network with one hidden layer containing four nodesis about right for this problem. In Figure 18.25, we show two curves.

Token 14867:
The ﬁrst is a trainingcurve showing the mean squared error on a given training set of 100 restaurant examples

Token 14868:
736 Chapter 18.

Token 14869:
Learning from Examples 02468101214 0 50 100 150 200 250 300 350 400Total error on training set Number of epochs0.40.50.60.70.80.91 010 20 30 40 50 60 70 80 90100Proportion correct on test set Training set sizeDecision tree Multilayer network (a) (b) Figure 18.25 (a) Training curve showing the gradual reduction in error as weights are modiﬁed over several epochs, for a given set of examples in the restaurant domain.

Token 14870:
(b) Comparative learning curves showing that decision-tree learning does slightly better on therestaurant problem than back-propagation in a multilayer network.

Token 14871:
during the weight-updating process. This demonstrates that the network does indeed converge to a perfect ﬁt to the training data.

Token 14872:
The second curve is the standard learning curve for therestaurant data.

Token 14873:
The neural network does learn well, although not quite as fast as decision-tree learning; this is perhaps not surprising, because the data were generated from a simpledecision tree in the ﬁrst place.

Token 14874:
Neural networks are capable of far more complex learning tasks of course, although it must be said that a certain amount of twiddling is needed to get the network structure rightand to achieve convergence to something close to the global optimum in weight space.

Token 14875:
Thereare literally tens of thousands of published applications of neural networks. Section 18.11.1looks at one such application in more depth.

Token 14876:
18.7.5 Learning neural network structures So far, we have considered the problem of learning weights, given a ﬁxed network structure;just as with Bayesian networks, we also need to understand how to ﬁnd the best networkstructure.

Token 14877:
If we choose a network that is too big, it will be able to memorize all the examplesby forming a large lookup table, but will not necessarily generalize well to inputs that havenot been seen before.

Token 14878:
10In other words, like all statistical models, neural networks are subject tooverﬁtting when there are too many parameters in the model.

Token 14879:
We saw this in Figure 18.1 (page 696), where the high-parameter models in (b) and (c) ﬁt all the data, but might notgeneralize as well as the low-parameter models in (a) and (d).

Token 14880:
If we stick to fully connected networks, the only choices to be made concern the number 10It has been observed that very large networks dogeneralize well as long as the weights are kept small .T h i s restriction keeps the activation values in the linear region of the sigmoid function g(x)where xis close to zero.

Token 14881:
This, in turn, means that the network behaves like a linear function (Exercise 18.22) with far fewer parameters.

Token 14882:
Section 18.8. Nonparametric Models 737 of hidden layers and their sizes. The usual approach is to try several and keep the best.

Token 14883:
The cross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test set.

Token 14884:
That is, we choose the network architecture that gives the highest prediction accuracy onthe validation sets.

Token 14885:
If we want to consider networks that are not fully connected, then we need to ﬁnd some effective search method through the very large space of possible connection topologies.

Token 14886:
The optimal brain damage algorithm begins with a fully connected network and removes OPTIMAL BRAIN DAMAGE connections from it.

Token 14887:
After the network is trained for the ﬁrst time, an information-theoretic approach identiﬁes an optimal selection of connections that can be dropped.

Token 14888:
The networkis then retrained, and if its performance has not decreased then the process is repeated.

Token 14889:
Inaddition to removing connections, it is also possible to remove units that are not contributingmuch to the result.

Token 14890:
Several algorithms have been proposed for growing a larger network from a smaller one. One, the tiling algorithm, resembles decision-list learning.

Token 14891:
The idea is to start with a single TILING unit that does its best to produce the correct output on as many of the training examples as possible.

Token 14892:
Subsequent units are added to take care of the examples that the ﬁrst unit got wrong.The algorithm adds only as many units as are needed to cover all the examples.

Token 14893:
18.8 N ONPARAMETRIC MODELS Linear regression and neural networks use the training data to estimate a ﬁxed set of param-eters w. That deﬁnes our hypothesis h w(x), and at that point we can throw away the training data, because they are all summarized by w. A learning model that summarizes data with a set of parameters of ﬁxed size (independent of the number of training examples) is called aparametric model .

Token 14894:
PARAMETRIC MODEL No matter how much data you throw at a parametric model, it won’t change its mind about how many parameters it needs.

Token 14895:
When data sets are small, it makes sense to have a strong restriction on the allowable hypotheses, to avoid overﬁtting.

Token 14896:
But when there are thousands ormillions or billions of examples to learn from, it seems like a better idea to let the data speakfor themselves rather than forcing them to speak through a tiny vector of parameters.

Token 14897:
If thedata say that the correct answer is a very wiggly function, we shouldn’t restrict ourselves tolinear or slightly wiggly functions.

Token 14898:
Anonparametric model is one that cannot be characterized by a bounded set of param- NONPARAMETRIC MODEL eters.

Token 14899:
For example, suppose that each hypothesis we generate simply retains within itself all of the training examples and uses all of them to predict the next example.

Token 14900:
Such a hypothesisfamily would be nonparametric because the effective number of parameters is unbounded—it grows with the number of examples.

Token 14901:
This approach is called instance-based learning or INSTANCE-BASED LEARNING memory-based learning .

Token 14902:
The simplest instance-based learning method is table lookup :t a k e TABLE LOOKUP all the training examples, put them in a lookup table, and then when asked for h(x), see if xis in the table; if it is, return the corresponding y.

Token 14903:
The problem with this method is that it does not generalize well: when xis not in the table all it can do is return some default value.

Token 14904:
738 Chapter 18.

Token 14905:
Learning from Examples 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 4.5 5 5.5 6 6.5 7x1 x2 2.5 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 4.5 5 5.5 6 6.5 7x1 x2 (k=1)( k=5) Figure 18.26 (a) Ak-nearest-neighbor model showing the extent of the explosion class for the data in Figure 18.15, with k=1.

Token 14906:
Overﬁtting is apparent. (b) With k=5, the overﬁtting problem goes away for this data set.

Token 14907:
18.8.1 Nearest neighbor models We can improve on table lookup with a slight variation: given a query xq,ﬁ n dt h e kexamples that are nearest toxq.

Token 14908:
This is called k-nearest neighbors lookup. We’ll use the notationNEAREST NEIGHBORS NN(k,xq)to denote the set of knearest neighbors.

Token 14909:
To do classiﬁcation, ﬁrst ﬁnd NN(k,xq), then take the plurality vote of the neighbors (which is the majority vote in the case of binary classiﬁcation).

Token 14910:
To avoid ties, kis always chosen to be an odd number.

Token 14911:
To do regression, we can take the mean or median of the k neighbors, or we can solve a linear regression problem on the neighbors.

Token 14912:
In Figure 18.26, we show the decision boundary of k-nearest-neighbors classiﬁcation fork=1 and 5 on the earthquake data set from Figure 18.15.

Token 14913:
Nonparametric methods are still subject to underﬁtting and overﬁtting, just like parametric methods.

Token 14914:
In this case 1-nearestneighbors is overﬁtting; it reacts too much to the black outlier in the upper right and the whiteoutlier at (5.4, 3.7).

Token 14915:
The 5-nearest-neighbors decision boundary is good; higher kwould underﬁt.

Token 14916:
As usual, cross-validation can be used to select the best value of k. The very word “nearest” implies a distance metric.

Token 14917:
How do we measure the distance from a query point x qto an example point xj?

Token 14918:
Typically, distances are measured with a Minkowski distance orLpnorm, deﬁned asMINKOWSKI DISTANCE Lp(xj,xq)=(/summationdisplay i|xj,i−xq,i|p)1/p.

Token 14919:
Withp=2this is Euclidean distance and with p=1it is Manhattan distance.

Token 14920:
With Boolean attribute values, the number of attributes on which the two points differ is called the Ham- ming distance .

Token 14921:
Often p=2is used if the dimensions are measuring similar properties, such HAMMING DISTANCE as the width, height and depth of parts on a conveyor belt, and Manhattan distance is used if they are dissimilar, such as age, weight, and gender of a patient.

Token 14922:
Note that if we use the rawnumbers from each dimension then the total distance will be affected by a change in scalein any dimension.

Token 14923:
That is, if we change dimension ifrom measurements in centimeters to

Token 14924:
Section 18.8. Nonparametric Models 739 miles while keeping the other dimensions the same, we’ll get different nearest neighbors.

Token 14925:
To avoid this, it is common to apply normalization to the measurements in each dimension.

Token 14926:
One NORMALIZATION simple approach is to compute the mean μiand standard deviation σiof the values in each dimension, and rescale them so that xj,ibecomes (xj,i−μi)/σi.

Token 14927:
A more complex metric known as the Mahalanobis distance takes into account the covariance between dimensions.MAHALANOBIS DISTANCE In low-dimensional spaces with plenty of data, nearest neighbors works very well: we are likely to have enough nearby data points to get a good answer.

Token 14928:
But as the number ofdimensions rises we encounter a problem: the nearest neighbors in high-dimensional spacesare usually not very near!

Token 14929:
Consider k-nearest-neighbors on a data set of Npoints uniformly distributed throughout the interior of an n-dimensional unit hypercube.

Token 14930:
We’ll deﬁne the k- neighborhood of a point as the smallest hypercube that contains the k-nearest neighbors.

Token 14931:
Let /lscriptbe the average side length of a neighborhood.

Token 14932:
Then the volume of the neighborhood (which contains kpoints) is /lscript nand the volume of the full cube (which contains Npoints) is 1.

Token 14933:
So, on average, /lscriptn=k/N .T a k i n g nth roots of both sides we get /lscript=(k/N)1/n. To be concrete, let k=1 0 andN=1,000,000.

Token 14934:
In two dimensions ( n=2; a unit square), the average neighborhood has /lscript=0.003, a small fraction of the unit square, and in 3 dimensions /lscriptis just 2% of the edge length of the unit cube.

Token 14935:
But by the time we get to 17 dimensions, /lscriptis half the edge length of the unit hypercube, and in 200 dimensions it is 94%.

Token 14936:
This problem has been called the curse of dimensionality .CURSE OF DIMENSIONALITY Another way to look at it: consider the points that fall within a thin shell making up the outer 1% of the unit hypercube.

Token 14937:
These are outliers; in general it will be hard to ﬁnd a goodvalue for them because we will be extrapolating rather than interpolating.

Token 14938:
In one dimension,these outliers are only 2% of the points on the unit line (those points where x<.

Token 14939:
01or x>.99), but in 200 dimensions, over 98% of the points fall within this thin shell—almost all the points are outliers.

Token 14940:
You can see an example of a poor nearest-neighbors ﬁt on outliersif you look ahead to Figure 18.28(b).

Token 14941:
TheNN(k,x q)function is conceptually trivial: given a set of Nexamples and a query xq, iterate through the examples, measure the distance to xqfrom each one, and keep the best k. If we are satisﬁed with an implementation that takes O(N)execution time, then that is the end of the story.

Token 14942:
But instance-based methods are designed for large data sets, so we wouldlike an algorithm with sublinear run time.

Token 14943:
Elementary analysis of algorithms tells us thatexact table lookup is O(N)with a sequential table, O(logN)with a binary tree, and O(1) with a hash table.

Token 14944:
We will now see that binary trees and hash tables are also applicable forﬁnding nearest neighbors.

Token 14945:
18.8.2 Finding nearest neighbors with k-d trees A balanced binary tree over data with an arbitrary number of dimensions is called a k-d tree , K-D TREE for k-dimensional tree.

Token 14946:
(In our notation, the number of dimensions is n, so they would be n-d trees.

Token 14947:
The construction of a k-d tree is similar to the construction of a one-dimensional balanced binary tree.

Token 14948:
We start with a set of examples and at the root node we split them alongtheith dimension by testing whether x i≤m.

Token 14949:
We chose the value mto be the median of the examples along the ith dimension; thus half the examples will be in the left branch of the tree

Token 14950:
740 Chapter 18.

Token 14951:
Learning from Examples 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 25 50 75 100 125 150 175 200Edge length of neighborhood Number of dimensions 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 25 50 75 100 125 150 175 200Proportion of points in exterior shell Number of dimensions (a) (b) Figure 18.27 The curse of dimensionality: (a) The l ength of the average neighborhood for 10-nearest-neighbors in a unit hypercube with 1,000,000 points, as a function of the number of dimensions.

Token 14952:
(b) The proportion of points that fall within a thin shell consisting of theouter 1% of the hypercube, as a function of the number of dimensions.

Token 14953:
Sampled from 10,000 randomly distributed points. and half in the right.

Token 14954:
We then recursively make a tree for the left and right sets of examples, stopping when there are fewer than two examples left.

Token 14955:
To choose a dimension to split on ateach node of the tree, one can simply select dimension imodnat level iof the tree.

Token 14956:
(Note that we may need to split on any given dimension several times as we proceed down the tree.

Token 14957:
)Another strategy is to split on the dimension that has the widest spread of values.

Token 14958:
Exact lookup from a k-d tree is just like lookup from a binary tree (with the slight complication that you need to pay attention to which dimension you are testing at each node).

Token 14959:
But nearest neighbor lookup is more complicated.

Token 14960:
As we go down the branches, splittingthe examples in half, in some cases we can discard the other half of the examples. But notalways.

Token 14961:
Sometimes the point we are querying for falls very close to the dividing boundary.The query point itself might be on the left hand side of the boundary, but one or more oftheknearest neighbors might actually be on the right-hand side.

Token 14962:
We have to test for this possibility by computing the distance of the query point to the dividing boundary, and then searching both sides if we can’t ﬁnd kexamples on the left that are closer than this distance.

Token 14963:
Because of this problem, k-d trees are appropriate only when there are many more examplesthan dimensions, preferably at least 2 nexamples.

Token 14964:
Thus, k-d trees work well with up to 10 dimensions with thousands of examples or up to 20 dimensions with millions of examples.

Token 14965:
If we don’t have enough examples, lookup is no faster than a linear scan of the entire data set.

Token 14966:
18.8.3 Locality-sensitive hashing Hash tables have the potential to provide even faster lookup than binary trees.

Token 14967:
But how can we ﬁnd nearest neighbors using a hash table, when hash codes rely on an exact match?

Token 14968:
Hash codes randomly distribute values among the bins, but we want to have near points groupedtogether in the same bin; we want a locality-sensitive hash (LSH).

Token 14969:
LOCALITY-SENSITIVE HASH

Token 14970:
Section 18.8.

Token 14971:
Nonparametric Models 741 We can’t use hashes to solve NN(k,xq)exactly, but with a clever use of randomized algorithms, we can ﬁnd an approximate solution.

Token 14972:
First we deﬁne the approximate near- neighbors problem: given a data set of example points and a query point xq, ﬁnd, with highAPPROXIMATE NEAR-NEIGHBORS probability, an example point (or points) that is near xq.

Token 14973:
To be more precise, we require that if there is a point xjthat is within a radius rofxq, then with high probability the algorithm will ﬁnd a point xj/primethat is within distance crofq.

Token 14974:
If there is no point within radius rthen the algorithm is allowed to report failure.

Token 14975:
The values of cand “high probability” are parameters of the algorithm.

Token 14976:
To solve approximate near neighbors, we will need a hash function g(x)that has the property that, for any two points xjandxj/prime, the probability that they have the same hash code is small if their distance is more than cr, and is high if their distance is less than r.F o r simplicity we will treat each point as a bit string.

Token 14977:
(Any features that are not Boolean can beencoded into a set of Boolean features.)

Token 14978:
The intuition we rely on is that if two points are close together in an n-dimensional space, then they will necessarily be close when projected down onto a one-dimensional space(a line).

Token 14979:
In fact, we can discretize the line into bins—hash buckets—so that, with high prob-ability, near points project down to exactly the same bin.

Token 14980:
Points that are far away from each other will tend to project down into different bins for most projections, but there will always be a few projections that coincidentally project far-apart points into the same bin.

Token 14981:
Thus, thebin for point x qcontains many (but not all) points that are near to xq, as well as some points that are far away.

Token 14982:
The trick of LSH is to create multiple random projections and combine them.

Token 14983:
A random projection is just a random subset of the bit-string representation.

Token 14984:
We choose /lscriptdifferent random projections and create /lscripthash tables, g1(x),...,g /lscript(x).

Token 14985:
We then enter all the examples into each hash table.

Token 14986:
Then when given a query point xq, we fetch the set of points in bin gk(q) for each k, and union these sets together into a set of candidate points, C. Then we compute the actual distance to xqfor each of the points in Cand return the kclosest points.

Token 14987:
With high probability, each of the points that are near to xqwill show up in at least one of the bins, and although some far-away points will show up as well, we can ignore those.

Token 14988:
With large real- world problems, such as ﬁnding the near neighbors in a data set of 13 million Web imagesusing 512 dimensions (Torralba et al.

Token 14989:
, 2008), locality-sensitive hashing needs to examine only a few thousand images out of 13 million to ﬁnd nearest neighbors; a thousand-fold speedupover exhaustive or k-d tree approaches.

Token 14990:
18.8.4 Nonparametric regression Now we’ll look at nonparametric approaches to regression rather than classiﬁcation.

Token 14991:
Fig- ure 18.28 shows an example of some different models.

Token 14992:
In (a), we have perhaps the simplestmethod of all, known informally as “connect-the-dots,” and superciliously as “piecewise- linear nonparametric regression.” This model creates a function h(x)that, when given a query x q, solves the ordinary linear regression problem with just two points: the training examples immediately to the left and right of xq.

Token 14993:
When noise is low, this trivial method is actually not too bad, which is why it is a standard feature of charting software in spreadsheets.

Token 14994:
742 Chapter 18.

Token 14995:
Learning from Examples 0 1 2 3 4 5 6 7 8 0 2 4 6 8 10 12 14 0 1 2 3 4 5 6 7 8 0 2 4 6 8 10 12 14 (a) (b) 0 1 2 3 4 5 6 7 8 0 2 4 6 8 10 12 14 0 1 2 3 4 5 6 7 8 0 2 4 6 8 10 12 14 (c) (d) Figure 18.28 Nonparametric regression models: (a) connect the dots, (b) 3-nearest neigh- bors average, (c) 3-nearest-neighbors linear regression, (d) locally weighted regression with a quadratic kernel of width k=10 .

Token 14996:
But when the data are noisy, the resulting function is spiky, and does not generalize well.

Token 14997:
k-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots.

Token 14998:
In-NEAREST- NEIGHBORS REGRESSIONstead of using just the two examples to the left and right of a query point xq,w eu s et h e knearest neighbors (here 3).

Token 14999:
A larger value of ktends to smooth out the magnitude of the spikes, although the resulting function has discontinuities.

Token 15000:
In (b), we have the k-nearest- neighbors average: h(x)is the mean value of the kpoints,/summationtextyj/k.

Token 15001:
Notice that at the outlying points, near x=0andx=1 4 , the estimates are poor because all the evidence comes from one side (the interior), and ignores the trend.

Token 15002:
In (c), we have k-nearest-neighbor linear regression, which ﬁnds the best line through the kexamples.

Token 15003:
This does a better job of capturing trends at the outliers, but is still discontinuous.

Token 15004:
In both (b) and (c), we’re left with the question of how to choose a good value for k. The answer, as usual, is cross-validation.

Token 15005:
Locally weighted regression (Figure 18.28(d)) gives us the advantages of nearest neigh-LOCALLYWEIGHTED REGRESSION bors, without the discontinuities.

Token 15006:
To avoid discontinuities in h(x), we need to avoid disconti-

Token 15007:
Section 18.8.

Token 15008:
Nonparametric Models 743 0 0.5 1 -10 -5 0 5 10 Figure 18.29 A quadratic kernel, K(d)= m ax ( 0 ,1−(2|x|/k)2), with kernel width k=1 0 , centered on the query point x=0.

Token 15009:
nuities in the set of examples we use to estimate h(x).

Token 15010:
The idea of locally weighted regression is that at each query point xq, the examples that are close to xqare weighted heavily, and the examples that are farther away are weighted less heavily or not at all.

Token 15011:
The decrease in weightover distance is always gradual, not sudden.

Token 15012:
We decide how much to weight each example with a function known as a kernel .A KERNEL kernel function looks like a bump; in Figure 18.29 we see the speciﬁc kernel used to generate Figure 18.28(d).

Token 15013:
We can see that the weight provided by this kernel is highest in the center and reaches zero at a distance of ±5.

Token 15014:
Can we choose just any function for a kernel? No.

Token 15015:
First, note that we invoke a kernel function KwithK(Distance (xj,xq)),w h e r e xqis a query point that is a given distance from xj, and we want to know how much to weight that distance.

Token 15016:
SoKshould be symmetric around 0 and have a maximum at 0. The area under the kernel must remain bounded as we go to ±∞.

Token 15017:
Other shapes, such as Gaussians, have been used for kernels, but the latest research suggests that the choice of shape doesn’t matter much.

Token 15018:
We do have to be careful about the width of the kernel. Again, this is a parameter of the modelthat is best chosen by cross-validation.

Token 15019:
Just as in choosing the kfor nearest neighbors, if the kernels are too wide we’ll get underﬁtting and if they are too narrow we’ll get overﬁtting.

Token 15020:
InFigure 18.29(d), the value of k=1 0 gives a smooth curve that looks about right—but maybe it does not pay enough attention to the outlier at x=6; a narrower kernel width would be more responsive to individual points.

Token 15021:
Doing locally weighted regression with kernels is now straightforward.

Token 15022:
For a given query point x qwe solve the following weighted regression problem using gradient descent: w∗=a r g m i n w/summationdisplay jK(Distance (xq,xj))(yj−w·xj)2, where Distance is any of the distance metrics discussed for nearest neighbors.

Token 15023:
Then the answer is h(xq)=w∗·xq. Note that we need to solve a new regression problem for every query point—that’s what it means to be local .

Token 15024:
(In ordinary linear regression, we solved the regression problem once, globally, and then used the same hwfor any query point.)

Token 15025:
Mitigating against this extra work

Token 15026:
744 Chapter 18.

Token 15027:
Learning from Examples is the fact that each regression problem will be easier to solve, because it involves only the examples with nonzero weight—the examples whose kernels overlap the query point.

Token 15028:
Whenkernel widths are small, this may be just a few points.

Token 15029:
Most nonparametric models have the advantage that it is easy to do leave-one-out cross- validation without having to recompute everything.

Token 15030:
With a k-nearest-neighbors model, for instance, when given a test example (x,y)we retrieve the knearest neighbors once, compute the per-example loss L(y,h(x))from them, and record that as the leave-one-out result for every example that is not one of the neighbors.

Token 15031:
Then we retrieve the k+1nearest neighbors and record distinct results for leaving out each of the kneighbors.

Token 15032:
With Nexamples the whole process is O(k), notO(kN).

Token 15033:
18.9 S UPPORT VECTOR MACHINES Thesupport vector machine or SVM framework is currently the most popular approach forSUPPORT VECTOR MACHINE “off-the-shelf” supervised learning: if you don’t have any specialized prior knowledge about a domain, then the SVM is an excellent method to try ﬁrst.

Token 15034:
There are three properties that make SVMs attractive: 1.

Token 15035:
SVMs construct a maximum margin separator —a decision boundary with the largest possible distance to example points. This helps them generalize well.

Token 15036:
2.

Token 15037:
SVMs create a linear separating hyperplane, but they have the ability to embed the data into a higher-dimensional space, using the so-called kernel trick .

Token 15038:
Often, data that are not linearly separable in the original input space are easily separable in the higher-dimensional space.

Token 15039:
The high-dimensional linear separator is actually nonlinear in theoriginal space.

Token 15040:
This means the hypothesis space is greatly expanded over methods thatuse strictly linear representations. 3.

Token 15041:
SVMs are a nonparametric method—they retain training examples and potentially need to store them all.

Token 15042:
On the other hand, in practice they often end up retaining only asmall fraction of the number of examples—sometimes as few as a small constant times the number of dimensions.

Token 15043:
Thus SVMs combine the advantages of nonparametric and parametric models: they have the ﬂexibility to represent complex functions, but theyare resistant to overﬁtting.

Token 15044:
You could say that SVMs are successful because of one key insight and one neat trick. We will cover each in turn.

Token 15045:
In Figure 18.30(a), we have a binary classiﬁcation problem with threecandidate decision boundaries, each a linear separator.

Token 15046:
Each of them is consistent with allthe examples, so from the point of view of 0/1 loss, each would be equally good.

Token 15047:
Logisticregression would ﬁnd some separating line; the exact location of the line depends on allthe example points.

Token 15048:
The key insight of SVMs is that some examples are more important than others, and that paying attention to them can lead to better generalization.

Token 15049:
Consider the lowest of the three separating lines in (a). It comes very close to 5 of the black examples.

Token 15050:
Although it classiﬁes all the examples correctly, and thus minimizes loss, it

Token 15051:
Section 18.9.

Token 15052:
Support Vector Machines 745 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) (b) Figure 18.30 Support vector machine classiﬁcation: (a) Two classes of points (black and white circles) and three candidate linear separators.

Token 15053:
(b) The maximum margin separator(heavy line), is at the midpoint of the margin (area between dashed lines).

Token 15054:
The support vectors (points with large circles) are the examples closest to the separator.

Token 15055:
should make you nervous that so many examples are close to the line; it may be that other black examples will turn out to fall on the other side of the line.

Token 15056:
SVMs address this issue: Instead of minimizing expected empirical loss on the training data, SVMs attempt to minimize expected generalization loss.

Token 15057:
We don’t know where the as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn from the same distribution as the previously seen examples, there are some arguments from computational learning theory (Section 18.5) suggesting that we minimize generalization lossby choosing the separator that is farthest away from the examples we have seen so far.

Token 15058:
Wecall this separator, shown in Figure 18.30(b) the maximum margin separator .T h e margin MAXIMUM MARGIN SEPARATOR MARGIN is the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the separator to the nearest example point.

Token 15059:
Now, how do we ﬁnd this separator?

Token 15060:
Before showing the equations, some notation: Traditionally SVMs use the convention that class labels are +1 and -1, instead of the +1 and0 we have been using so far.

Token 15061:
Also, where we put the intercept into the weight vector w(and a corresponding dummy 1 value into x j,0), SVMs do not do that; they keep the intercept as a separate parameter, b.

Token 15062:
With that in mind, the separator is deﬁned as the set of points {x:w·x+b=0}.

Token 15063:
We could search the space of wandbwith gradient descent to ﬁnd the parameters that maximize the margin while correctly classifying all the examples.

Token 15064:
However, it turns out there is another approach to solving this problem.

Token 15065:
We won’t show the details, but will just say that there is an alternative representation called the dual

Token 15066:
746 Chapter 18.

Token 15067:
Learning from Examples representation, in which the optimal solution is found by solving argmax α/summationdisplay jαj−1 2/summationdisplay j,kαjαkyjyk(xj·xk) (18.13) subject to the constraints αj≥0and/summationtext jαjyj=0.

Token 15068:
T h i si sa quadratic programmingQUADRATIC PROGRAMMING optimization problem, for which there are good software packages.

Token 15069:
Once we have found the vector αwe can get back to wwith the equation w=/summationtext jαjxj, or we can stay in the dual representation.

Token 15070:
There are three important properties of Equation (18.13). First, the expressionis convex; it has a single global maximum that can be found efﬁciently.

Token 15071:
Second, the data enter the expression only in the form of dot products of pairs of points.

Token 15072:
This second property is also true of the equation for the separator itself; once the optimal αjhave been calculated, it is h(x)=sign⎛ ⎝/summationdisplay jαjyj(x·xj)−b⎞ ⎠.

Token 15073:
(18.14) A ﬁnal important property is that the weights αjassociated with each data point are zero ex- cept for the support vectors —the points closest to the separator.

Token 15074:
(They are called “support” SUPPORT VECTOR vectors because they “hold up” the separating plane.)

Token 15075:
Because there are usually many fewer support vectors than examples, SVMs gain some of the advantages of parametric models.

Token 15076:
What if the examples are not linearly separable?

Token 15077:
Figure 18.31(a) shows an input space deﬁned by attributes x=(x1,x2), with positive examples ( y=+1 ) inside a circular region and negative examples ( y=−1) outside.

Token 15078:
Clearly, there is no linear separator for this problem.

Token 15079:
Now, suppose we re-express the input data—i.e., we map each input vector xto a new vector of feature values, F(x).

Token 15080:
In particular, let us use the three features f1=x2 1,f 2=x2 2,f 3=√ 2x1x2.

Token 15081:
(18.15) We will see shortly where these came from, but for now, just look at what happens.

Token 15082:
Fig- ure 18.31(b) shows the data in the new, three-dimensional space deﬁned by the three features;t h ed a t aa r e linearly separable in this space!

Token 15083:
This phenomenon is actually fairly general: if data are mapped into a space of sufﬁciently high dimension, then they will almost always be linearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to make them line up.

Token 15084:
Here, we used only three dimensions; 11Exercise 18.16 asks you to show that four dimensions sufﬁce for linearly separating a circle anywhere in the plane (not just at the origin), and ﬁve dimensions sufﬁce to linearly separate any ellipse.

Token 15085:
In general (with some special cases excepted) if we have Ndata points then they will always be separable in spaces ofN−1dimensions or more (Exercise 18.25).

Token 15086:
Now, we would not usually expect to ﬁnd a linear separator in the input space x,b u t we can ﬁnd linear separators in the high-dimensional feature space F(x)simply by replacing xj·xkin Equation (18.13) with F(xj)·F(xk).

Token 15087:
This by itself is not remarkable—replacing xby F(x)inanylearning algorithm has the required effect—but the dot product has some special properties.

Token 15088:
It turns out that F(xj)·F(xk)can often be computed without ﬁrst computing F 11The reader may notice that we could have used just f1andf2, but the 3D mapping illustrates the idea better.

Token 15089:
Section 18.9.

Token 15090:
Support Vector Machines 747 -1.5-1-0.500.511.5 -1.5 -1 -0.5 0 0.5 1 1.5x2 x10 0.5 1 1.5 2x12 0.511.522.5 x22-3-2-10123√2x1x2 (a) (b) Figure 18.31 (a) A two-dimensional training set with positive examples as black cir- cles and negative examples as white circles.

Token 15091:
The true decision boundary, x2 1+x2 2≤1, is also shown. (b) The same data after mapping into a three-dimensional input space (x2 1,x22,√ 2x1x2).

Token 15092:
The circular decision boundary in (a) becomes a linear decision boundary in three dimensions. Figure 18.30(b) gives a closeup of the separator in (b).

Token 15093:
for each point. In our three-dimensional feature space deﬁned by Equation (18.15), a little bit of algebra shows that F(xj)·F(xk)=( xj·xk)2.

Token 15094:
(That’s why the√ 2is inf3.) The expression (xj·xk)2is called a kernel function ,12and KERNEL FUNCTION is usually written as K(xj,xk).

Token 15095:
The kernel function can be applied to pairs of input data to evaluate dot products in some corresponding feature space.

Token 15096:
So, we can ﬁnd linear separators in the higher-dimensional feature space F(x)simply by replacing xj·xkin Equation (18.13) with a kernel function K(xj,xk).

Token 15097:
Thus, we can learn in the higher-dimensional space, but we compute only kernel functions rather than the full list of features for each data point.

Token 15098:
The next step is to see that there’s nothing special about the kernel K(xj,xk)=( xj·xk)2.

Token 15099:
It corresponds to a particular higher-dimensional feature space, but other kernel functions correspond to other feature spaces.

Token 15100:
A venerable result in mathematics, Mercer’s theo- rem (1909), tells us that any “reasonable”13kernel function corresponds to some feature MERCER’S THEOREM space.

Token 15101:
These feature spaces can be very large, even for innocuous-looking kernels.

Token 15102:
For ex- ample, the polynomial kernel ,K(xj,xk)=(1+ xj·xk)d, corresponds to a feature spacePOLYNOMIAL KERNEL whose dimension is exponential in d. 12This usage of “kernel function” is slightly different from the kernels in locally weighted regression.

Token 15103:
Some SVM kernels are distance metrics, but not all are. 13Here, “reasonable” means that the matrix Kjk=K(xj,xk)is positive deﬁnite.

Token 15104:
748 Chapter 18.

Token 15105:
Learning from Examples This then is the clever kernel trick : Plugging these kernels into Equation (18.13), KERNEL TRICK optimal linear separators can be found efﬁciently in feature spaces with billions of (or, in some cases, inﬁnitely many) dimensions.

Token 15106:
The resulting linear separators, when mapped back to the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-aries between the positive and negative examples.

Token 15107:
In the case of inherently noisy data, we may not want a linear separator in some high- dimensional space.

Token 15108:
Rather, we’d like a decision surface in a lower-dimensional space thatdoes not cleanly separate the classes, but reﬂects the reality of the noisy data.

Token 15109:
That is pos-sible with the soft margin classiﬁer, which allows examples to fall on the wrong side of the SOFT MARGIN decision boundary, but assigns them a penalty proportional to the distance required to move them back on the correct side.

Token 15110:
The kernel method can be applied not only with learning algorithms that ﬁnd optimal linear separators, but also with any other algorithm that can be reformulated to work onlywith dot products of pairs of data points, as in Equations 18.13 and 18.14.

Token 15111:
Once this isdone, the dot product is replaced by a kernel function and we have a kernelized version KERNELIZATION of the algorithm.

Token 15112:
This can be done easily for k-nearest-neighbors and perceptron learning (Section 18.7.2), among others.

Token 15113:
18.10 E NSEMBLE LEARNING So far we have looked at learning methods in which a single hypothesis, chosen from ahypothesis space, is used to make predictions.

Token 15114:
The idea of ensemble learning methods is ENSEMBLE LEARNING to select a collection, or ensemble , of hypotheses from the hypothesis space and combine their predictions.

Token 15115:
For example, during cross-validation we might generate twenty differentdecision trees, and have them vote on the best classiﬁcation for a new example.

Token 15116:
The motivation for ensemble learning is simple.

Token 15117:
Consider an ensemble of K=5hy- potheses and suppose that we combine their predictions using simple majority voting.

Token 15118:
For the ensemble to misclassify a new example, at least three of the ﬁve hypotheses have to misclas- sify it .

Token 15119:
The hope is that this is much less likely than a misclassiﬁcation by a single hypothesis.

Token 15120:
Suppose we assume that each hypothesis h kin the ensemble has an error of p—that is, the probability that a randomly chosen example is misclassiﬁed by hkisp.

Token 15121:
Furthermore, suppose we assume that the errors made by each hypothesis are independent .

Token 15122:
In that case, if pis small, then the probability of a large number of misclassiﬁcations occurring is minuscule.

Token 15123:
For ex- ample, a simple calculation (Exercise 18.18) shows that using an ensemble of ﬁve hypothesesreduces an error rate of 1 in 10 down to an error rate of less than 1 in 100.

Token 15124:
Now, obviouslythe assumption of independence is unreasonable, because hypotheses are likely to be misledin the same way by any misleading aspects of the training data.

Token 15125:
But if the hypotheses are at least a little bit different, thereby reducing the correlation between their errors, then ensemble learning can be very useful.

Token 15126:
Another way to think about the ensemble idea is as a generic way of enlarging the hypothesis space.

Token 15127:
That is, think of the ensemble itself as a hypothesis and the new hypothesis

Token 15128:
Section 18.10.

Token 15129:
Ensemble Learning 749 +++ + + ++++++++ +––– –– – –––– – – – –– – – –– –– ––––– – ––––– –––– –––– – Figure 18.32 Illustration of the increased expressive power obtained by ensemble learn- ing.

Token 15130:
We take three linear threshold hypotheses, each of which classiﬁes positively on the unshaded side, and classify as positive any example classiﬁed positively by all three.

Token 15131:
Theresulting triangular region is a hypothesis not expressible in the original hypothesis space.

Token 15132:
space as the set of all possible ensembles constructable from hypotheses in the original space.

Token 15133:
Figure 18.32 shows how this can result in a more expressive hypothesis space.

Token 15134:
If the originalhypothesis space allows for a simple and efﬁcient learning algorithm, then the ensemblemethod provides a way to learn a much more expressive class of hypotheses without incurringmuch additional computational or algorithmic complexity.

Token 15135:
The most widely used ensemble method is called boosting .

Token 15136:
To understand how it works, BOOSTING we need ﬁrst to explain the idea of a weighted training set .

Token 15137:
In such a training set, eachWEIGHTED TRAINING SET example has an associated weight wj≥0.

Token 15138:
The higher the weight of an example, the higher is the importance attached to it during the learning of a hypothesis.

Token 15139:
It is straightforward to modify the learning algorithms we have seen so far to operate with weighted training sets.14 Boosting starts with wj=1for all the examples (i.e., a normal training set).

Token 15140:
From this set, it generates the ﬁrst hypothesis, h1. This hypothesis will classify some of the training ex- amples correctly and some incorrectly.

Token 15141:
We would like the next hypothesis to do better on themisclassiﬁed examples, so we increase their weights while decreasing the weights of the cor-rectly classiﬁed examples.

Token 15142:
From this new weighted training set, we generate hypothesis h 2.

Token 15143:
The process continues in this way until we have generated Khypotheses, where Kis an input to the boosting algorithm.

Token 15144:
The ﬁnal ensemble hypothesis is a weighted-majority combination of all the Khypotheses, each weighted according to how well it performed on the training set.

Token 15145:
Figure 18.33 shows how the algorithm works conceptually.

Token 15146:
There are many variants of the ba-sic boosting idea, with different ways of adjusting the weights and combining the hypotheses.

Token 15147:
One speciﬁc algorithm, called A DABOOST , is shown in Figure 18.34.

Token 15148:
A DABOOST has a very important property: if the input learning algorithm Lis aweak learning algorithm—which WEAK LEARNING 14For learning algorithms in which this is not possible, one can instead create a replicated training set where thejth example appears wjtimes, using randomization to handle fractional weights.

Token 15149:
750 Chapter 18. Learning from Examples hh1 =h 2 =h 3 =h 4 = Figure 18.33 How the boosting algorithm works.

Token 15150:
Each shaded rectangle corresponds to an example; the height of the rectangle corresponds to the weight.

Token 15151:
The checks and crosses indicate whether the example was classiﬁed correctly by the current hypothesis.

Token 15152:
The size ofthe decision tree indicates the weight of that hypothesis in the ﬁnal ensemble.

Token 15153:
means that Lalways returns a hypothesis with accuracy on the training set that is slightly better than random guessing (i.e., 50% +/epsilon1for Boolean classiﬁcation)—then A DABOOST will return a hypothesis that classiﬁes the training data perfectly for large enough K. Thus, the algorithm boosts the accuracy of the original learning algorithm on the training data.

Token 15154:
This result holds no matter how inexpressive the original hypothesis space and no matter howcomplex the function being learned.

Token 15155:
Let us see how well boosting does on the restaurant data.

Token 15156:
We will choose as our original hypothesis space the class of decision stumps , which are decision trees with just one test, at DECISION STUMP the root.

Token 15157:
The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not very effective for this data set, reaching a prediction performance of only 81% on 100 trainingexamples.

Token 15158:
When boosting is applied (with K=5), the performance is better, reaching 93% after 100 examples.

Token 15159:
An interesting thing happens as the ensemble size Kincreases.

Token 15160:
Figure 18.35(b) shows the training set performance (on 100 examples) as a function of K. Notice that the error reaches zero when Kis 20; that is, a weighted-majority combination of 20 decision stumps sufﬁces to ﬁt the 100 examples exactly.

Token 15161:
As more stumps are added to the ensemble, the error remains at zero.

Token 15162:
The graph also shows that the test set performance continues to increase long after the training set error has reached zero.

Token 15163:
AtK=2 0 , the test performance is 0.95 (or 0.05 error), and the performance increases to 0.98 as late as K= 137 , before gradually dropping to 0.95.

Token 15164:
This ﬁnding, which is quite robust across data sets and hypothesis spaces, came as quite a surprise when it was ﬁrst noticed.

Token 15165:
Ockham’s razor tells us not to make hypotheses more

Token 15166:
Section 18.10.

Token 15167:
Ensemble Learning 751 function ADABOOST (examples ,L,K)returns a weighted-majority hypothesis inputs :examples ,s e to f Nlabeled examples (x1,y1),...,(xN,yN) L, a learning algorithm K, the number of hypotheses in the ensemble local variables :w, a vector of Nexample weights, initially 1/N h, a vector of Khypotheses z, a vector of Khypothesis weights fork=1toKdo h[k]←L(examples ,w) error←0 forj=1toNdo if h[k](xj)/negationslash=yjthenerror←error +w[j] forj=1toNdo if h[k](xj)=yjthen w [j]←w[j]·error/(1−error) w←NORMALIZE (w) z[k]←log (1−error)/error return WEIGHTED -MAJORITY (h,z) Figure 18.34 The A DABOOST variant of the boosting method for ensemble learning.

Token 15168:
The algorithm generates hypotheses by successively reweighting the training examples.

Token 15169:
The func- tion W EIGHTED -MAJORITY generates a hypothesis that returns the output value with the highest vote from the hypotheses in h, with votes weighted by z.

Token 15170:
0.50.550.60.650.70.750.80.850.90.951 0 20 40 60 80 100Proportion correct on test set Training set sizeBoosted decision stumps Decision stump 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0 50 100 150 200Training/test accuracy Number of hypotheses KTraining error Test error (a) (b) Figure 18.35 (a) Graph showing the performance of boosted decision stumps with K=5 versus unboosted decision stumps on the restaurant data.

Token 15171:
(b) The proportion correct on the training set and the test set as a function of K, the number of hypotheses in the ensemble.

Token 15172:
Notice that the test set accuracy improves slightly even after the training accuracy reaches 1,i.e., after the ensemble ﬁts the data exactly.

Token 15173:
752 Chapter 18.

Token 15174:
Learning from Examples complex than necessary, but the graph tells us that the predictions improve as the ensemble hypothesis gets more complex!

Token 15175:
Various explanations have been proposed for this.

Token 15176:
One viewis that boosting approximates Bayesian learning (see Chapter 20), which can be shown to be an optimal learning algorithm, and the approximation improves as more hypotheses areadded.

Token 15177:
Another possible explanation is that the addition of further hypotheses enables the ensemble to be more deﬁnite in its distinction between positive and negative examples, which helps it when it comes to classifying new examples.

Token 15178:
18.10.1 Online Learning So far, everything we have done in this chapter has relied on the assumption that the data arei.i.d.

Token 15179:
(independent and identically distributed).

Token 15180:
On the one hand, that is a sensible assumption:if the future bears no resemblance to the past, then how can we predict anything?

Token 15181:
On the otherhand, it is too strong an assumption: it is rare that our inputs have captured all the information that would make the future truly independent of the past.

Token 15182:
In this section we examine what to do when the data are not i.i.d. ; when they can change over time.

Token 15183:
In this case, it matters when we make a prediction, so we will adopt the perspective called online learning : an agent receives an input x jfrom nature, predicts the corresponding ONLINELEARNING yj, and then is told the correct answer.

Token 15184:
Then the process repeats with xj+1, and so on.

Token 15185:
One might think this task is hopeless—if nature is adversarial, all the predictions may be wrong.It turns out that there are some guarantees we can make.

Token 15186:
Let us consider the situation where our input consists of predictions from a panel of experts.

Token 15187:
For example, each day a set of Kpundits predicts whether the stock market will go up or down, and our task is to pool those predictions and make our own.

Token 15188:
One way to do this is to keep track of how well each expert performs, and choose to believe them in proportion to their past performance.

Token 15189:
This is called the randomized weighted majority algorithm .W eRANDOMIZED WEIGHTED MAJORITYALGORITHM can described it more formally: 1.

Token 15190:
Initialize a set of weights {w1,...,w K}all to 1. 2. Receive the predictions {ˆy1,...,ˆyK}from the experts. 3.

Token 15191:
Randomly choose an expert k∗, in proportion to its weight: P(k)=wk/(/summationtext k/primewk/prime). 4. Predict ˆyk∗. 5. Receive the correct answer y.

Token 15192:
6. For each expert ksuch that ˆyk/negationslash=y, update wk←βwk Hereβis a number, 0<β< 1, that tells how much to penalize an expert for each mistake.

Token 15193:
We measure the success of this algorithm in terms of regret , which is deﬁned as the REGRET number of additional mistakes we make compared to the expert who, in hindsight, had the best prediction record.

Token 15194:
Let M∗be the number of mistakes made by the best expert.

Token 15195:
Then the number of mistakes, M, made by the random weighted majority algorithm, is bounded by15 M<M∗ln(1/β)+l n K 1−β.

Token 15196:
15See (Blum, 1996) for the proof.

Token 15197:
Section 18.11. Practical Machine Learning 753 This bound holds for anysequence of examples, even ones chosen by adversaries trying to do their worst.

Token 15198:
To be speciﬁc, when there are K=1 0 experts, if we choose β=1/2then our number of mistakes is bounded by 1.39M∗+4.6,a n di f β=3/4by1.15M∗+9.2.I n general, if βis close to 1 then we are responsive to change over the long run; if the best expert changes, we will pick up on it before too long.

Token 15199:
However, we pay a penalty at the beginning, when we start with all experts trusted equally; we may accept the advice of the bad experts for too long.

Token 15200:
When βis closer to 0, these two factors are reversed.

Token 15201:
Note that we can choose β to get asymptotically close to M∗in the long run; this is called no-regret learning (becauseNO-REGRET LEARNING the average amount of regret per trial tends to 0 as the number of trials increases).

Token 15202:
Online learning is helpful when the data may be changing rapidly over time.

Token 15203:
It is also useful for applications that involve a large collection of data that is constantly growing, evenif changes are gradual.

Token 15204:
For example, with a database of millions of Web images, you wouldn’twant to train, say, a linear regression model on all the data, and then retrain from scratch everytime a new image is added.

Token 15205:
It would be more practical to have an online algorithm that allowsimages to be added incrementally.

Token 15206:
For most learning algorithms based on minimizing loss,there is an online version based on minimizing regret.

Token 15207:
It is a bonus that many of these onlinealgorithms come with guaranteed bounds on regret.

Token 15208:
To some observers, it is surprising that there are such tight bounds on how well we can do compared to a panel of experts.

Token 15209:
To others, the really surprising thing is that when panelsof human experts congregate—predicting stock market prices, sports outcomes, or politicalcontests—the viewing public is so willing to listen to them pontiﬁcate and so unwilling toquantify their error rates.

Token 15210:
18.11 P RACTICAL MACHINE LEARNING We have introduced a wide range of machine learning techniques, each illustrated with simplelearning tasks.

Token 15211:
In this section, we consider two aspects of practical machine learning.

Token 15212:
The ﬁrstinvolves ﬁnding algorithms capable of learning to recognize handwritten digits and squeezingevery last drop of predictive performance out of them.

Token 15213:
The second involves anything but—pointing out that obtaining, cleaning, and representing the data can be at least as important asalgorithm engineering.

Token 15214:
18.11.1 Case study: Handwritten digit recognition Recognizing handwritten digits is an important problem with many applications, includingautomated sorting of mail by postal code, automated reading of checks and tax returns, anddata entry for hand-held computers.

Token 15215:
It is an area where rapid progress has been made, in partbecause of better learning algorithms and in part because of the availability of better training sets.

Token 15216:
The United States National Institute of Science and Technology ( NIST ) has archived a database of 60,000 labeled digits, each 20×20 =400 pixels with 8-bit grayscale values.

Token 15217:
It has become one of the standard benchmark problems for comparing new learning algorithms. Some example digits are shown in Figure 18.36.

Token 15218:
754 Chapter 18. Learning from Examples Figure 18.36 Examples from the NIST database of handwritten digits.

Token 15219:
Top row: examples of digits 0–9 that are easy to identify. Bottom row: more difﬁcult examples of the same digits.

Token 15220:
Many different learning approaches have been tried.

Token 15221:
One of the ﬁrst, and probably the simplest, is the 3-nearest-neighbor classiﬁer, which also has the advantage of requiring no training time.

Token 15222:
As a memory-based algorithm, however, it must store all 60,000 images, andits run time performance is slow. It achieved a test error rate of 2.4%.

Token 15223:
Asingle-hidden-layer neural network was designed for this problem with 400 input units (one per pixel) and 10 output units (one per class).

Token 15224:
Using cross-validation, it was found that roughly 300 hidden units gave the best performance.

Token 15225:
With full interconnections between layers, there were a total of 123,300 weights. This network achieved a 1.6% error rate.

Token 15226:
A series of specialized neural networks called LeNet were devised to take advantage of the structure of the problem—that the input consists of pixels in a two–dimensional array,and that small changes in the position or slant of an image are unimportant.

Token 15227:
Each networkhad an input layer of 32×32units, onto which the 20×20pixels were centered so that each input unit is presented with a local neighborhood of pixels.

Token 15228:
This was followed by three layersof hidden units.

Token 15229:
Each layer consisted of several planes of n×narrays, where nis smaller than the previous layer so that the network is down-sampling the input, and where the weightsof every unit in a plane are constrained to be identical, so that the plane is acting as a featuredetector: it can pick out a feature such as a long vertical line or a short semi-circular arc.

Token 15230:
The output layer had 10 units.

Token 15231:
Many versions of this architecture were tried; a representative one had hidden layers with 768, 192, and 30 units, respectively.

Token 15232:
The training set was augmentedby applying afﬁne transformations to the actual inputs: shifting, slightly rotating, and scalingthe images.

Token 15233:
(Of course, the transformations have to be small, or else a 6 will be transformedinto a 9!) The best error rate achieved by LeNet was 0.9%.

Token 15234:
Aboosted neural network combined three copies of the LeNet architecture, with the second one trained on a mix of patterns that the ﬁrst one got 50% wrong, and the third onetrained on patterns for which the ﬁrst two disagreed.

Token 15235:
During testing, the three nets voted withthe majority ruling. The test error rate was 0.7%.

Token 15236:
Asupport vector machine (see Section 18.9) with 25,000 support vectors achieved an error rate of 1.1%.

Token 15237:
This is remarkable because the SVM technique, like the simple nearest- neighbor approach, required almost no thought or iterated experimentation on the part of the developer, yet it still came close to the performance of LeNet, which had had years of devel-opment.

Token 15238:
Indeed, the support vector machine makes no use of the structure of the problem,and would perform just as well if the pixels were presented in a permuted order.

Token 15239:
Section 18.11.

Token 15240:
Practical Machine Learning 755 Avirtual support vector machine starts with a regular SVM and then improves itVIRTUAL SUPPORT VECTOR MACHINE with a technique that is designed to take advantage of the structure of the problem.

Token 15241:
Instead of allowing products of all pixel pairs, this approach concentrates on kernels formed from pairsof nearby pixels.

Token 15242:
It also augments the training set with transformations of the examples, justas LeNet did.

Token 15243:
A virtual SVM achieved the best error rate recorded to date, 0.56%.

Token 15244:
Shape matching is a technique from computer vision used to align corresponding parts of two different images of objects (Belongie et al. , 2002).

Token 15245:
The idea is to pick out a set of points in each of the two images, and then compute, for each point in the ﬁrst image,which point in the second image it corresponds to.

Token 15246:
From this alignment, we then compute atransformation between the images. The transformation gives us a measure of the distancebetween the images.

Token 15247:
This distance measure is better motivated than just counting the numberof differing pixels, and it turns out that a 3–nearest neighbor algorithm using this distancemeasure performs very well.

Token 15248:
Training on only 20,000 of the 60,000 digits, and using 100sample points per image extracted from a Canny edge detector, a shape matching classiﬁerachieved 0.63% test error.

Token 15249:
Humans are estimated to have an error rate of about 0.2% on this problem.

Token 15250:
This ﬁgure is somewhat suspect because humans have not been tested as extensively as have machine learning algorithms.

Token 15251:
On a similar data set of digits from the United States Postal Service, human errors were at 2.5%.

Token 15252:
The following ﬁgure summarizes the error rates, run time performance, memory re- quirements, and amount of training time for the seven algorithms we have discussed.

Token 15253:
It also adds another measure, the percentage of digits that must be rejected to achieve 0.5% error.

Token 15254:
For example, if the SVM is allowed to reject 1.8% of the inputs—that is, pass them on for someone else to make the ﬁnal judgment—then its error rate on the remaining 98.2% of theinputs is reduced from 1.1% to 0.5%.

Token 15255:
The following table summarizes the error rate and some of the other characteristics of the seven techniques we have discussed.

Token 15256:
3 300 Boosted Virtual Shape NN Hidden LeNet LeNet SVM SVM Match Error rate (pct.)

Token 15257:
2.4 1.6 0.9 0.7 1.1 0.56 0.63 Run time (millisec/digit) 1000 10 30 50 2000 200 Memory requirements (Mbyte) 12 .49 .012 .21 11 Training time (days) 0 7 14 30 10 % rejected to reach 0.5% error 8.1 3.2 1.8 0.5 1.8 18.11.2 Case study: Word senses and house prices In a textbook we need to deal with simple, toy data to get the ideas across: a small data set,usually in two dimensions.

Token 15258:
But in practical applications of machine learning, the data set is usually large, multidimensional, and messy.

Token 15259:
The data are not handed to the analyst in a prepackaged set of (x,y)values; rather the analyst needs to go out and acquire the right data.

Token 15260:
There is a task to be accomplished, and most of the engineering problem is deciding whatdata are necessary to accomplish the task; a smaller part is choosing and implementing an

Token 15261:
756 Chapter 18.

Token 15262:
Learning from Examples 0.75 0.8 0.85 0.9 0.95 1 1 10 100 1000Proportion correct on test set Training set size (millions of words) Figure 18.37 Learning curves for ﬁve learning algorithms on a common task.

Token 15263:
Note that there appears to be more room for improvement in the horizontal direction (more training data) than in the vertical direction (different machine learning algorithm).

Token 15264:
Adapted from Banko and Brill (2001). appropriate machine learning method to process the data.

Token 15265:
Figure 18.37 shows a typical real- world example, comparing ﬁve learning algorithms on the task of word-sense classiﬁcation(given a sentence such as “The bank folded,” classify the word “bank” as “money-bank” or“river-bank”).

Token 15266:
The point is that machine learning researchers have focused mainly on thevertical direction: Can I invent a new learning algorithm that performs better than previouslypublished algorithms on a standard training set of 1 million words?

Token 15267:
But the graph showsthere is more room for improvement in the horizontal direction: instead of inventing a newalgorithm, all I need to do is gather 10 million words of training data; even the worst algorithm at 10 million words is performing better than the best algorithm at 1 million.

Token 15268:
As we gather even more data, the curves continue to rise, dwarﬁng the differences between algorithms.

Token 15269:
Consider another problem: the task of estimating the true value of houses that are for sale.

Token 15270:
In Figure 18.13 we showed a toy version of this problem, doing linear regression of house size to asking price.

Token 15271:
You probably noticed many limitations of this model.

Token 15272:
First, it ismeasuring the wrong thing: we want to estimate the selling price of a house, not the askingprice.

Token 15273:
To solve this task we’ll need data on actual sales.

Token 15274:
But that doesn’t mean we shouldthrow away the data about asking price—we can use it as one of the input features.

Token 15275:
Besidesthe size of the house, we’ll need more information: the number of rooms, bedrooms andbathrooms; whether the kitchen and bathrooms have been recently remodeled; the age ofthe house; we’ll also need information about the lot, and the neighborhood.

Token 15276:
But how dowe deﬁne neighborhood? By zip code?

Token 15277:
What if part of one zip code is on the “wrong”side of the highway or train tracks, and the other part is desirable? What about the schooldistrict?

Token 15278:
Should the name of the school district be a feature, or the average test scores ?I n addition to deciding what features to include, we will have to deal with missing data; different areas have different customs on what data are reported, and individual cases will always bemissing some data.

Token 15279:
If the data you want are not available, perhaps you can set up a socialnetworking site to encourage people to share and correct data.

Token 15280:
In the end, this process of

Token 15281:
Section 18.12.

Token 15282:
Summary 757 deciding what features to use, and how to use them, is just as important as choosing between linear regression, decision trees, or some other form of learning.

Token 15283:
That said, one does have to pick a method (or methods) for a problem.

Token 15284:
There is no guaranteed way to pick the best method, but there are some rough guidelines.

Token 15285:
Decisiontrees are good when there are a lot of discrete features and you believe that many of them may be irrelevant.

Token 15286:
Nonparametric methods are good when you have a lot of data and no prior knowledge, and when you don’t want to worry too much about choosing just the right features(as long as there are fewer than 20 or so).

Token 15287:
However, nonparametric methods usually give youa function hthat is more expensive to run.

Token 15288:
Support vector machines are often considered the best method to try ﬁrst, provided the data set is not too large.

Token 15289:
18.12 S UMMARY This chapter has concentrated on inductive learning of functions from examples.

Token 15290:
The mainpoints were as follows: •Learning takes many forms, depending on the nature of the agent, the component to be improved, and the available feedback.

Token 15291:
•If the available feedback provides the correct answer for example inputs, then the learn- ing problem is called supervised learning .

Token 15292:
The task is to learn a function y=h(x).

Token 15293:
Learning a discrete-valued function is called classiﬁcation ; learning a continuous func- tion is called regression .

Token 15294:
•Inductive learning involves ﬁnding a hypothesis that agrees well with the examples.

Token 15295:
Ockham’s razor suggests choosing the simplest consistent hypothesis. The difﬁculty of this task depends on the chosen representation.

Token 15296:
•Decision trees can represent all Boolean functions.

Token 15297:
The information-gain heuristic provides an efﬁcient method for ﬁnding a simple, consistent decision tree.

Token 15298:
•The performance of a learning algorithm is measured by the learning curve ,w h i c h shows the prediction accuracy on the test set as a function of the training-set size.

Token 15299:
•When there are multiple models to choose from, cross-validation can be used to select a model that will generalize well.

Token 15300:
•Sometimes not all errors are equal. A loss function tells us how bad each error is; the goal is then to minimize loss over a validation set.

Token 15301:
•Computational learning theory analyzes the sample complexity and computational complexity of inductive learning.

Token 15302:
There is a tradeoff between the expressiveness of thehypothesis language and the ease of learning. •Linear regression is a widely used model.

Token 15303:
The optimal parameters of a linear regres- sion model can be found by gradient descent search, or computed exactly.

Token 15304:
•A linear classiﬁer with a hard threshold—also known as a perceptron —can be trained by a simple weight update rule to ﬁt data that are linearly separable .

Token 15305:
In other cases, the rule fails to converge.

Token 15306:
758 Chapter 18.

Token 15307:
Learning from Examples •Logistic regression replaces the perceptron’s hard threshold with a soft threshold de- ﬁned by a logistic function.

Token 15308:
Gradient descent works well even for noisy data that arenot linearly separable.

Token 15309:
•Neural networks represent complex nonlinear functions with a network of linear- threshold units.

Token 15310:
termMultilayer feed-forward neural networks can represent any func-tion, given enough units.

Token 15311:
The back-propagation algorithm implements a gradient de- scent in parameter space to minimize the output error.

Token 15312:
•Nonparametric models use all the data to make each prediction, rather than trying to summarize the data ﬁrst with a few parameters.

Token 15313:
Examples include nearest neighbors andlocally weighted regression .

Token 15314:
•Support vector machines ﬁnd linear separators with maximum margin to improve the generalization performance of the classiﬁer.

Token 15315:
Kernel methods implicitly transform the input data into a high-dimensional space where a linear separator may exist, even ifthe original data are non-separable.

Token 15316:
•Ensemble methods such as boosting often perform better than individual methods.

Token 15317:
In online learning we can aggregate the opinions of experts to come arbitrarily close to the best expert’s performance, even when the distribution of the data is constantly shifting.

Token 15318:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Chapter 1 outlined the history of philosophical investigations into inductive learning.

Token 15319:
Williamof Ockham 16(1280–1349), the most inﬂuential philosopher of his century and a major con- tributor to medieval epistemology, logic, and metaphysics, is credited with a statement called“Ockham’s Razor”—in Latin, Entia non sunt multiplicanda praeter necessitatem ,a n di nE n - glish, “Entities are not to be multiplied beyond necessity.” Unfortunately, this laudable pieceof advice is nowhere to be found in his writings in precisely these words (although he did say “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without necessity”).

Token 15320:
A similar sentiment was expressed by Aristotle in 350 B.C.i n Physics book I, chapter VI: “For the more limited, if adequate, is always preferable.” The ﬁrst notable use of decision trees was in EPAM, the “Elementary Perceiver And Memorizer” (Feigenbaum, 1961), which was a simulation of human concept learning.

Token 15321:
ID3(Quinlan, 1979) added the crucial idea of choosing the attribute with maximum entropy; it isthe basis for the decision tree algorithm in this chapter.

Token 15322:
Information theory was developed byClaude Shannon to aid in the study of communication (Shannon and Weaver, 1949).

Token 15323:
(Shan-non also contributed one of the earliest examples of machine learning, a mechanical mousenamed Theseus that learned to navigate through a maze by trial and error.)

Token 15324:
The χ 2method of tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree package, can be found in Quinlan (1993).

Token 15325:
An independent tradition of decision tree learning exists in the statistical literature. Classiﬁcation and Regression Trees (Breiman et al.

Token 15326:
, 1984), known as the “CART book,” is the principal reference.

Token 15327:
16The name is often misspelled as “Occam,” perhaps from the French rendering, “Guillaume d’Occam.”

Token 15328:


Token 15329:
Bibliographical and Historical Notes 759 Cross-validation was ﬁrst introduced by Larson (1931), and in a form close to what we show by Stone (1974) and Golub et al.

Token 15330:
(1979). The regularization procedure is due to Tikhonov (1963).

Token 15331:
Guyon and Elisseeff (2003) introduce a journal issue devoted to the prob-lem of feature selection. Banko and Brill (2001) and Halevy et al.

Token 15332:
(2009) discuss the advan- tages of using large amounts of data.

Token 15333:
It was Robert Mercer, a speech researcher who said in 1985 “There is no data like more data.” (Lyman and Varian, 2003) estimate that about 5 exabytes ( 5×10 18bytes) of data was produced in 2002, and that the rate of production is doubling every 3 years.

Token 15334:
Theoretical analysis of learning algorithms began with the work of Gold (1967) on identiﬁcation in the limit .

Token 15335:
This approach was motivated in part by models of scientiﬁc discovery from the philosophy of science (Popper, 1962), but has been applied mainly to theproblem of learning grammars from example sentences (Osherson et al.

Token 15336:
, 1986).

Token 15337:
Whereas the identiﬁcation-in-the-limit approach concentrates on eventual convergence, the study of Kolmogorov complexity oralgorithmic complexity , developed independently KOLMOGOROV COMPLEXITY by Solomonoff (1964, 2009) and Kolmogorov (1965), attempts to provide a formal deﬁnition for the notion of simplicity used in Ockham’s razor.

Token 15338:
To escape the problem that simplicitydepends on the way in which information is represented, it is proposed that simplicity be measured by the length of the shortest program for a universal Turing machine that correctly reproduces the observed data.

Token 15339:
Although there are many possible universal Turing machines,and hence many possible “shortest” programs, these programs differ in length by at most aconstant that is independent of the amount of data.

Token 15340:
This beautiful insight, which essentiallyshows that anyinitial representation bias will eventually be overcome by the data itself, is marred only by the undecidability of computing the length of the shortest program.

Token 15341:
Approx- imate measures such as the minimum description length , or MDL (Rissanen, 1984, 2007) MINIMUM DESCRIPTIONLENGTH can be used instead and have produced excellent results in practice.

Token 15342:
The text by Li and Vi- tanyi (1993) is the best source for Kolmogorov complexity. The theory of PAC-learning was inaugurated by Leslie Valiant (1984).

Token 15343:
His work stressed the importance of computational and sample complexity.

Token 15344:
With Michael Kearns (1990), Valiant showed that several concept classes cannot be PAC-learned tractably, even though sufﬁcient information is available in the examples.

Token 15345:
Some positive results were obtained for classes suchas decision lists (Rivest, 1987).

Token 15346:
An independent tradition of sample-complexity analysis has existed in statistics, begin- ning with the work on uniform convergence theory (Vapnik and Chervonenkis, 1971).

Token 15347:
The UNIFORM CONVERGENCETHEORY so-called VC dimension provides a measure roughly analogous to, but more general than, the VC DIMENSION ln|H|measure obtained from PAC analysis.

Token 15348:
The VC dimension can be applied to continuous function classes, to which standard PAC analysis does not apply.

Token 15349:
PAC-learning theory andVC theory were ﬁrst connected by the “four Germans” (none of whom actually is German):Blumer, Ehrenfeucht, Haussler, and Warmuth (1989).

Token 15350:
Linear regression with squared error loss goes back to Legendre (1805) and Gauss (1809), who were both working on predicting orbits around the sun.

Token 15351:
The modern use of multivariate regression for machine learning is covered in texts such as Bishop (2007).

Token 15352:
Ng(2004) analyzed the differences between L 1andL2regularization.

Token 15353:
760 Chapter 18.

Token 15354:
Learning from Examples The term logistic function comes from Pierre-Franc ¸ois Verhulst (1804–1849), a statis- tician who used the curve to model population growth with limited resources, a more realis-tic model than the unconstrained geometric growth proposed by Thomas Malthus.

Token 15355:
Verhulstcalled it the courbe logistique , because of its relation to the logarithmic curve.

Token 15356:
The term re- gression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin, and initiator of the ﬁelds of meteorology, ﬁngerprint analysis, and statistical correlation, who used it in the sense of regression to the mean.

Token 15357:
The term curse of dimensionality comes from Richard Bellman (1961).

Token 15358:
Logistic regression can be solved with gradient descent, or with the Newton-Raphson method (Newton, 1671; Raphson, 1690).

Token 15359:
A variant of the Newton method called L-BFGS issometimes used for large-dimensional problems; the L stands for “limited memory,” meaningthat it avoids creating the full matrices all at once, and instead creates parts of them on theﬂy.

Token 15360:
BFGS are authors’ initials (Byrd et al. , 1995).

Token 15361:
Nearest-neighbors models date back at least to Fix and Hodges (1951) and have been a standard tool in statistics and pattern recognition ever since.

Token 15362:
Within AI, they were popularizedby Stanﬁll and Waltz (1986), who investigated methods for adapting the distance metric to thedata.

Token 15363:
Hastie and Tibshirani (1996) developed a way to localize the metric to each point in the space, depending on the distribution of data around that point.

Token 15364:
Gionis et al.

Token 15365:
(1999) introduced locality-sensitive hashing, which has revolutionized the retrieval of similar objects in high-dimensional spaces, particularly in computer vision.

Token 15366:
Andoni and Indyk (2006) provide arecent survey of LSH and related methods. The ideas behind kernel machines come from Aizerman et al.

Token 15367:
(1964) (who also in- troduced the kernel trick), but the full development of the theory is due to Vapnik and his colleagues (Boser et al. , 1992).

Token 15368:
SVMs were made practical with the introduction of the soft-margin classiﬁer for handling noisy data in a paper that won the 2008 ACM Theoryand Practice Award (Cortes and Vapnik, 1995), and of the Sequential Minimal Optimization(SMO) algorithm for efﬁciently solving SVM problems using quadratic programming (Platt,1999).

Token 15369:
SVMs have proven to be very popular and effective for tasks such as text categoriza- tion (Joachims, 2001), computational genomics (Cristianini and Hahn, 2007), and natural lan- guage processing, such as the handwritten digit recognition of DeCoste and Sch¨ olkopf (2002).

Token 15370:
As part of this process, many new kernels have been designed that work with strings, trees,and other nonnumerical data types.

Token 15371:
A related technique that also uses the kernel trick to im-plicitly represent an exponential feature space is the voted perceptron (Freund and Schapire,1999; Collins and Duffy, 2002).

Token 15372:
Textbooks on SVMs include Cristianini and Shawe-Taylor(2000) and Sch¨ olkopf and Smola (2002).

Token 15373:
A friendlier exposition appears in the AI Magazine article by Cristianini and Sch¨ olkopf (2002).

Token 15374:
Bengio and LeCun (2007) show some of the limitations of SVMs and other local, nonparametric methods for learning functions that havea global structure but do not have local smoothness.

Token 15375:
Ensemble learning is an increasingly popular technique for improving the performance of learning algorithms.

Token 15376:
Bagging (Breiman, 1996), the ﬁrst effective method, combines hy- BAGGING potheses learned from multiple bootstrap data sets, each generated by subsampling the orig- inal data set.

Token 15377:
The boosting method described in this chapter originated with theoretical work by Schapire (1990).

Token 15378:
The A DABOOST algorithm was developed by Freund and Schapire

Token 15379:
Bibliographical and Historical Notes 761 (1996) and analyzed theoretically by Schapire (2003). Friedman et al.

Token 15380:
(2000) explain boost- ing from a statistician’s viewpoint.

Token 15381:
Online learning is covered in a survey by Blum (1996)and a book by Cesa-Bianchi and Lugosi (2006). Dredze et al.

Token 15382:
(2008) introduce the idea of conﬁdence-weighted online learning for classiﬁcation: in addition to keeping a weight foreach parameter, they also maintain a measure of conﬁdence, so that a new example can have a large effect on features that were rarely seen before (and thus had low conﬁdence) and a small effect on common features that have already been well-estimated.

Token 15383:
The literature on neural networks is rather too large (approximately 150,000 papers to date) to cover in detail.

Token 15384:
Cowan and Sharp (1988b, 1988a) survey the early history, beginningwith the work of McCulloch and Pitts (1943).

Token 15385:
(As mentioned in Chapter 1, John McCarthyhas pointed to the work of Nicolas Rashevsky (1936, 1938) as the earliest mathematical modelof neural learning.)

Token 15386:
Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,1948), worked with McCulloch and Pitts and inﬂuenced a number of young researchers in-cluding Marvin Minsky, who may have been the ﬁrst to develop a working neural network inhardware in 1951 (see Minsky and Papert, 1988, pp.

Token 15387:
ix–x).

Token 15388:
Turing (1948) wrote a researchreport titled Intelligent Machinery that begins with the sentence “I propose to investigate the question as to whether it is possible for machinery to show intelligent behaviour” and goes on to describe a recurrent neural network architecture he called “B-type unorganized machines” and an approach to training them.

Token 15389:
Unfortunately, the report went unpublished until 1969, andwas all but ignored until recently.

Token 15390:
Frank Rosenblatt (1957) invented the modern “perceptron” and proved the percep- tron convergence theorem (1960), although it had been foreshadowed by purely mathemat- ical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg, 1954).

Token 15391:
Some early work was also done on multilayer networks, including Gamba percep- trons (Gamba et al. , 1961) and madalines (Widrow, 1962).

Token 15392:
Learning Machines (Nilsson, 1965) covers much of this early work and more.

Token 15393:
The subsequent demise of early perceptronresearch efforts was hastened (or, the authors later claimed, merely explained) by the bookPerceptrons (Minsky and Papert, 1969), which lamented the ﬁeld’s lack of mathematical rigor.

Token 15394:
The book pointed out that single-layer perceptrons could represent only linearly sepa- rable concepts and noted the lack of effective learning algorithms for multilayer networks.

Token 15395:
The papers in (Hinton and Anderson, 1981), based on a conference in San Diego in 1979, can be regarded as marking a renaissance of connectionism.

Token 15396:
The two-volume “PDP”(Parallel Distributed Processing) anthology (Rumelhart et al. , 1986a) and a short article in Nature (Rumelhart et al.

Token 15397:
, 1986b) attracted a great deal of attention—indeed, the number of papers on “neural networks” multiplied by a factor of 200 between 1980–84 and 1990–94.The analysis of neural networks using the physical theory of magnetic spin glasses (Amitet al.

Token 15398:
, 1985) tightened the links between statistical mechanics and neural network theory— providing not only useful mathematical insights but also respectability .

Token 15399:
The back-propagation technique had been invented quite early (Bryson and Ho, 1969) but it was rediscovered several times (Werbos, 1974; Parker, 1985).

Token 15400:
The probabilistic interpretation of neural networks has several sources, including Baum and Wilczek (1988) and Bridle (1990).

Token 15401:
The role of the sigmoid function is discussed byJordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay

Token 15402:
762 Chapter 18. Learning from Examples (1992) and is explored further by Neal (1996).

Token 15403:
The capacity of neural networks to represent functions was investigated by Cybenko (1988, 1989), who showed that two hidden layers areenough to represent any function and a single layer is enough to represent any continuous function.

Token 15404:
The “optimal brain damage” method for removing useless connections is by LeCunet al.

Token 15405:
(1989), and Sietsma and Dow (1988) show how to remove useless units.

Token 15406:
The tiling algorithm for growing larger structures is due to M´ ezard and Nadal (1989). LeCun et al.

Token 15407:
(1995) survey a number of algorithms for handwritten digit recognition. Improved error ratessince then were reported by Belongie et al.

Token 15408:
(2002) for shape matching and DeCoste and Sch¨olkopf (2002) for virtual support vectors.

Token 15409:
At the time of writing, the best test error rate reported is 0.39% by Ranzato et al. (2007) using a convolutional neural network.

Token 15410:
The complexity of neural network learning has been investigated by researchers in com- putational learning theory.

Token 15411:
Early computational results were obtained by Judd (1990), whoshowed that the general problem of ﬁnding a set of weights consistent with a set of examplesis NP-complete, even under very restrictive assumptions.

Token 15412:
Some of the ﬁrst sample complexityresults were obtained by Baum and Haussler (1989), who showed that the number of exam-ples required for effective learning grows as roughly WlogW,w h e r e Wis the number of weights.

Token 15413:
17Since then, a much more sophisticated theory has been developed (Anthony and Bartlett, 1999), including the important result that the representational capacity of a network depends on the sizeof the weights as well as on their number, a result that should not be surprising in the light of our discussion of regularization.

Token 15414:
The most popular kind of neural network that we did not cover is the radial basis function , or RBF, network.

Token 15415:
A radial basis function combines a weighted collection of kernelsRADIAL BASIS FUNCTION (usually Gaussians, of course) to do function approximation.

Token 15416:
RBF networks can be trained in two phases: ﬁrst, an unsupervised clustering approach is used to train the parameters of theGaussians—the means and variances—are trained, as in Section 20.3.1.

Token 15417:
In the second phase,the relative weights of the Gaussians are determined. This is a system of linear equations,which we know how to solve directly.

Token 15418:
Thus, both phases of RBF training have a nice beneﬁt:the ﬁrst phase is unsupervised, and thus does not require labeled training data, and the second phase, although supervised, is efﬁcient.

Token 15419:
See Bishop (1995) for more details.

Token 15420:
Recurrent networks , in which units are linked in cycles, were mentioned in the chap- ter but not explored in depth.

Token 15421:
Hopﬁeld networks (Hopﬁeld, 1982) are probably the best- HOPFIELD NETWORK understood class of recurrent networks.

Token 15422:
They use bidirectional connections with symmetric weights (i.e., wi,j=wj,i), all of the units are both input and output units, the activation function gis the sign function, and the activation levels can only be ±1.

Token 15423:
A Hopﬁeld network functions as an associative memory : after the network trains on a set of examples, a newASSOCIATIVE MEMORY stimulus will cause it to settle into an activation pattern corresponding to the example in the training set that most closely resembles the new stimulus.

Token 15424:
For example, if the training set con- sists of a set of photographs, and the new stimulus is a small piece of one of the photographs,then the network activation levels will reproduce the photograph from which the piece was taken.

Token 15425:
Notice that the original photographs are not stored separately in the network; each 17This approximately conﬁrmed “Uncle Bernie’s rule.” The rule was named after Bernie Widrow, who recom- mended using roughly ten times as many examples as weights.

Token 15426:
Exercises 763 weight is a partial encoding of all the photographs.

Token 15427:
One of the most interesting theoretical results is that Hopﬁeld networks can reliably store up to 0.138Ntraining examples, where N is the number of units in the network.

Token 15428:
Boltzmann machines (Hinton and Sejnowski, 1983, 1986) also use symmetric weights,BOLTZMANN MACHINE but include hidden units.

Token 15429:
In addition, they use a stochastic activation function, such that the probability of the output being 1 is some function of the total weighted input.

Token 15430:
Boltz- mann machines therefore undergo state transitions that resemble a simulated annealing search(see Chapter 4) for the conﬁguration that best approximates the training set.

Token 15431:
It turns out thatBoltzmann machines are very closely related to a special case of Bayesian networks evaluatedwith a stochastic simulation algorithm.

Token 15432:
(See Section 14.5.) For neural nets, Bishop (1995), Ripley (1996), and Haykin (2008) are the leading texts.

Token 15433:
The ﬁeld of computational neuroscience is covered by Dayan and Abbott (2001).

Token 15434:
The approach taken in this chapter was inﬂuenced by the excellent course notes of David Cohn, Tom Mitchell, Andrew Moore, and Andrew Ng.

Token 15435:
There are several top-notch textbooksin Machine Learning (Mitchell, 1997; Bishop, 2007) and in the closely allied and overlappingﬁelds of pattern recognition (Ripley, 1996; Duda et al.

Token 15436:
, 2001), statistics (Wasserman, 2004; Hastie et al. , 2001), data mining (Hand et al.

Token 15437:
, 2001; Witten and Frank, 2005), computational learning theory (Kearns and Vazirani, 1994; Vapnik, 1998) and information theory (Shannon and Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006).

Token 15438:
Other books concentrate onimplementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michieet al. , 1994).

Token 15439:
Current research in machine learning is published in the annual proceedings of the International Conference on Machine Learning (ICML) and the conference on Neural Information Processing Systems (NIPS), in Machine Learning and the Journal of Machine Learning Research , and in mainstream AI journals.

Token 15440:
EXERCISES 18.1 Consider the problem faced by an infant learning to speak and understand a language.

Token 15441:
Explain how this process ﬁts into the general learning model.

Token 15442:
Describe the percepts and actions of the infant, and the types of learning the infant must do.

Token 15443:
Describe the subfunctionsthe infant is trying to learn in terms of inputs and outputs, and available example data.

Token 15444:
18.2 Repeat Exercise 18.1 for the case of learning to play tennis (or some other sport with which you are familiar).

Token 15445:
Is this supervised learning or reinforcement learning?

Token 15446:
18.3 Suppose we generate a training set from a decision tree and then apply decision-tree learning to that training set.

Token 15447:
Is it the case that the learning algorithm will eventually returnthe correct tree as the training-set size goes to inﬁnity? Why or why not?

Token 15448:
18.4 In the recursive construction of decision trees, it sometimes happens that a mixed set of positive and negative examples remains at a leaf node, even after all the attributes havebeen used.

Token 15449:
Suppose that we have ppositive examples and nnegative examples.

Token 15450:
764 Chapter 18. Learning from Examples a.

Token 15451:
Show that the solution used by D ECISION -TREE-LEARNING , which picks the majority classiﬁcation, minimizes the absolute error over the set of examples at the leaf.

Token 15452:
b. Show that the class probability p/(p+n)minimizes the sum of squared errors.

Token 15453:
CLASS PROBABILITY 18.5 Suppose that an attribute splits the set of examples Einto subsets Ekand that each subset has pkpositive examples and nknegative examples.

Token 15454:
Show that the attribute has strictly positive information gain unless the ratio pk/(pk+nk)is the same for all k. 18.6 Consider the following data set comprised of three binary input attributes ( A1,A2,a n d A3) and one binary output: Example A1 A2 A3 Output y x1 1 0 0 0 x2 1 0 1 0 x3 0 1 0 0 x4 1 1 1 1 x5 1 1 0 1 Use the algorithm in Figure 18.5 (page 702) to learn a decision tree for these data.

Token 15455:
Show the computations made to determine the attribute to split at each node.

Token 15456:
18.7 A decision graph is a generalization of a decision tree that allows nodes (i.e., attributes used for splits) to have multiple parents, rather than just a single parent.

Token 15457:
The resulting graphmust still be acyclic.

Token 15458:
Now, consider the XOR function of three binary input attributes, which produces the value 1 if and only if an odd number of the three input attributes has value 1. a.

Token 15459:
Draw a minimal-sized decision treefor the three-input XOR function. b. Draw a minimal-sized decision graph for the three-input XOR function.

Token 15460:
18.8 This exercise considers χ 2pruning of decision trees (Section 18.3.5). a.

Token 15461:
Create a data set with two input attributes, such that the information gain at the root of the tree for both attributes is zero, but there is a decision tree of depth 2 that is consistent with all the data.

Token 15462:
What would χ2pruning do on this data set if applied bottom up? If applied top down? b. Modify D ECISION -TREE-LEARNING to include χ2-pruning.

Token 15463:
You might wish to con- sult Quinlan (1986) or Kearns and Mansour (1998) for details.

Token 15464:
18.9 The standard D ECISION -TREE-LEARNING algorithm described in the chapter does not handle cases in which some examples have missing attribute values.

Token 15465:
a. First, we need to ﬁnd a way to classify such examples, given a decision tree that includes tests on the attributes for which values can be missing.

Token 15466:
Suppose that an example xhas a missing value for attribute Aand that the decision tree tests for Aat a node that x reaches.

Token 15467:
One way to handle this case is to pretend that the example has allpossible values for the attribute, but to weight each value according to its frequency among allof the examples that reach that node in the decision tree.

Token 15468:
The classiﬁcation algorithmshould follow all branches at any node for which a value is missing and should multiply

Token 15469:
Exercises 765 the weights along each path. Write a modiﬁed classiﬁcation algorithm for decision trees that has this behavior. b.

Token 15470:
Now modify the information-gain calculation so that in any given collection of exam- plesCat a given node in the tree during the construction process, the examples with missing values for any of the remaining attributes are given “as-if” values according tothe frequencies of those values in the set C. 18.10 In Section 18.3.6, we noted that attributes with many different possible values can cause problems with the gain measure.

Token 15471:
Such attributes tend to split the examples into numer-ous small classes or even singleton classes, thereby appearing to be highly relevant accordingto the gain measure.

Token 15472:
The gain-ratio criterion selects attributes according to the ratio between their gain and their intrinsic information content—that is, the amount of information con-tained in the answer to the question, “What is the value of this attribute?” The gain-ratio crite-rion therefore tries to measure how efﬁciently an attribute provides information on the correctclassiﬁcation of an example.

Token 15473:
Write a mathematical expression for the information content ofan attribute, and implement the gain ratio criterion in D ECISION -TREE-LEARNING .

Token 15474:
18.11 Suppose you are running a learning experiment on a new algorithm for Boolean clas- siﬁcation.

Token 15475:
You have a data set consisting of 100 positive and 100 negative examples.

Token 15476:
You plan to use leave-one-out cross-validation and compare your algorithm to a baseline function, a simple majority classiﬁer.

Token 15477:
(A majority classiﬁer is given a set of training data and thenalways outputs the class that is in the majority in the training set, regardless of the input.

Token 15478:
)You expect the majority classiﬁer to score about 50% on leave-one-out cross-validation, butto your surprise, it scores zero every time.

Token 15479:
Can you explain why? 18.12 Construct a decision list to classify the data below.

Token 15480:
Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classiﬁes the greatest number of examples correctly.

Token 15481:
If multiple tests have the same number of attributes and classify the same number of examples, then break thetie using attributes with lower index numbers (e.g., select A 1overA2).

Token 15482:
Example A1 A2 A3 A4 y x1 1 0 0 0 1 x2 1 0 1 1 1 x3 0 1 0 0 1 x4 0 1 1 0 0 x5 1 1 0 1 1 x6 0 1 0 1 0 x7 0 0 1 1 1 x8 0 0 1 0 0 18.13 Prove that a decision list can represent the same function as a decision tree while using at most as many rules as there are leaves in the decision tree for that function.

Token 15483:
Give anexample of a function represented by a decision list using strictly fewer rules than the numberof leaves in a minimal-sized decision tree for that same function.

Token 15484:
766 Chapter 18. Learning from Examples 18.14 This exercise concerns the expressiveness of decision lists (Section 18.5). a.

Token 15485:
Show that decision lists can represent any Boolean function, if the size of the tests is not limited. b.

Token 15486:
Show that if the tests can contain at most kliterals each, then decision lists can represent any function that can be represented by a decision tree of depth k. 18.15 Suppose a 7-nearest-neighbors regression search returns {7,6,8,4,7,11,100}as the 7 nearest yvalues for a given xvalue.

Token 15487:
What is the value of ˆythat minimizes the L1loss function on this data?

Token 15488:
There is a common name in statistics for this value as a function of theyvalues; what is it? Answer the same two questions for the L 2loss function.

Token 15489:
18.16 Figure 18.31 showed how a circle at the origin can be linearly separated by mapping from the features (x1,x2)to the two dimensions (x2 1,x22).

Token 15490:
But what if the circle is not located at the origin? What if it is an ellipse, not a circle?

Token 15491:
The general equation for a circle (andhence the decision boundary) is (x 1−a)2+(x2−b)2−r2=0, and the general equation for an ellipse is c(x1−a)2+d(x2−b)2−1=0 .

Token 15492:
a.

Token 15493:
Expand out the equation for the circle and show what the weights wiwould be for the decision boundary in the four-dimensional feature space (x1,x2,x2 1,x22).

Token 15494:
Explain why this means that any circle is linearly separable in this space. b.

Token 15495:
Do the same for ellipses in the ﬁve-dimensional feature space (x1,x2,x2 1,x22,x1x2).

Token 15496:
18.17 Construct a support vector machine that computes the XOR function.

Token 15497:
Use values of +1 and –1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like ([−1,1],1)or([−1,−1],−1).

Token 15498:
Map the input [x1,x2]into a space consisting of x1andx1x2. Draw the four input points in this space, and the maximal margin separator.

Token 15499:
What is themargin? Now draw the separating line back in the original Euclidean input space.

Token 15500:
18.18 Consider an ensemble learning algorithm that uses simple majority voting among Klearned hypotheses.

Token 15501:
Suppose that each hypothesis has error /epsilon1and that the errors made by each hypothesis are independent of the others’.

Token 15502:
Calculate a formula for the error of the ensemble algorithm in terms of Kand/epsilon1, and evaluate it for the cases where K=5, 10, and 20 and /epsilon1=0.1, 0.2, and 0.4.

Token 15503:
If the independence assumption is removed, is it possible for the ensemble error to be worse than/epsilon1?

Token 15504:
18.19 Construct by hand a neural network that computes the XOR function of two inputs. Make sure to specify what sort of units you are using.

Token 15505:
18.20 Recall from Chapter 18 that there are 22ndistinct Boolean functions of ninputs. How many of these are representable by a threshold perceptron?

Token 15506:
18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be in- terpreted as a probability passigned by the model to the proposition that f(x)=1 ; the prob- ability that f(x)=0 is therefore 1−p.

Token 15507:
Write down the probability pas a function of x and calculate the derivative of logpwith respect to each weight wi. Repeat the process for log(1−p).

Token 15508:
These calculations give a learning rule for minimizing the negative-log-likelihood

Token 15509:
Exercises 767 loss function for a probabilistic hypothesis. Comment on any resemblance to other learning rules in the chapter.

Token 15510:
18.22 Suppose you had a neural network with linear activation functions.

Token 15511:
That is, for each unit the output is some constant ctimes the weighted sum of the inputs. a. Assume that the network has one hidden layer.

Token 15512:
For a given assignment to the weights w, write down equations for the value of the units in the output layer as a function of wand the input layer x, without any explicit mention of the output of the hidden layer.

Token 15513:
Show that there is a network with no hidden units that computes the same function.

Token 15514:
b. Repeat the calculation in part (a), but this time do it for a network with any number of hidden layers.

Token 15515:
c. Suppose a network with one hidden layer and linear activation functions has ninput and output nodes and hhidden nodes.

Token 15516:
What effect does the transformation in part (a) to a network with no hidden layers have on the total number of weights?

Token 15517:
Discuss inparticular the case h/lessmuchn. 18.23 Suppose that a training set contains only a single example, repeated 100 times.

Token 15518:
In 80 of the 100 cases, the single output value is 1; in the other 20, it is 0.

Token 15519:
What will a back-propagation network predict for this example, assuming that it has been trained and reachesa global optimum?

Token 15520:
( Hint: to ﬁnd the global optimum, differentiate the error function and set it to zero.)

Token 15521:
18.24 The neural network whose learning performance is measured in Figure 18.25 has four hidden nodes. This number was chosen somewhat arbitrarily.

Token 15522:
Use a cross-validation method to ﬁnd the best number of hidden nodes.

Token 15523:
18.25 Consider the problem of separating Ndata points into positive and negative examples using a linear separator.

Token 15524:
Clearly, this can always be done for N=2points on a line of dimension d=1, regardless of how the points are labeled or where they are located (unless the points are in the same place).

Token 15525:
a. Show that it can always be done for N=3points on a plane of dimension d=2, unless they are collinear. b.

Token 15526:
Show that it cannot always be done for N=4points on a plane of dimension d=2.

Token 15527:
c. Show that it can always be done for N=4points in a space of dimension d=3, unless they are coplanar.

Token 15528:
d. Show that it cannot always be done for N=5points in a space of dimension d=3.

Token 15529:
e. The ambitious student may wish to prove that Npoints in general position (but not N+1) are linearly separable in a space of dimension N−1.

Token 15530:
19KNOWLEDGE IN LEARNING In which we examine the problem of learning when you know something already.

Token 15531:
In all of the approaches to learning described in the previous chapter, the idea is to construct a function that has the input–output behavior observed in the data.

Token 15532:
In each case, the learningmethods can be understood as searching a hypothesis space to ﬁnd a suitable function, startingfrom only a very basic assumption about the form of the function, such as “second-degreepolynomial” or “decision tree” and perhaps a preference for simpler hypotheses.

Token 15533:
Doing thisamounts to saying that before you can learn something new, you must ﬁrst forget (almost)everything you know.

Token 15534:
In this chapter, we study learning methods that can take advantageofprior knowledge about the world.

Token 15535:
In most cases, the prior knowledge is represented PRIOR KNOWLEDGE as general ﬁrst-order logical theories; thus for the ﬁrst time we bring together the work on knowledge representation and learning.

Token 15536:
19.1 A L OGICAL FORMULATION OF LEARNING Chapter 18 deﬁned pure inductive learning as a process of ﬁnding a hypothesis that agrees with the observed examples.

Token 15537:
Here, we specialize this deﬁnition to the case where the hypoth- esis is represented by a set of logical sentences.

Token 15538:
Example descriptions and classiﬁcations willalso be logical sentences, and a new example can be classiﬁed by inferring a classiﬁcationsentence from the hypothesis and the example description.

Token 15539:
This approach allows for incre-mental construction of hypotheses, one sentence at a time.

Token 15540:
It also allows for prior knowledge,because sentences that are already known can assist in the classiﬁcation of new examples.The logical formulation of learning may seem like a lot of extra work at ﬁrst, but it turns outto clarify many of the issues in learning.

Token 15541:
It enables us to go well beyond the simple learningmethods of Chapter 18 by using the full power of logical inference in the service of learning.

Token 15542:
19.1.1 Examples and hypotheses Recall from Chapter 18 the restaurant learning problem: learning a rule for deciding whetherto wait for a table.

Token 15543:
Examples were described by attributes such as Alternate ,Bar,Fri/Sat, 768

Token 15544:
Section 19.1. A Logical Formulation of Learning 769 and so on.

Token 15545:
In a logical setting, an example is described by a logical sentence; the attributes become unary predicates.

Token 15546:
Let us generically call the ith example Xi.

Token 15547:
For instance, the ﬁrst example from Figure 18.3 (page 700) is described by the sentences Alternate (X1)∧¬Bar(X1)∧¬Fri/Sat(X1)∧Hungry (X1)∧... We will use the notation Di(Xi)to refer to the description of Xi,w h e r e Dican be any logical expression taking a single argument.

Token 15548:
The classiﬁcation of the example is given by a literal using the goal predicate, in this case WillWait (X1) or¬WillWait (X1).

Token 15549:
The complete training set can thus be expressed as the conjunction of all the example descrip- tions and goal literals.

Token 15550:
The aim of inductive learning in general is to ﬁnd a hypothesis that classiﬁes the ex- amples well and generalizes well to new examples.

Token 15551:
Here we are concerned with hypothesesexpressed in logic; each hypothesis h jwill have the form ∀xGoal(x)⇔Cj(x), where Cj(x)is a candidate deﬁnition—some expression involving the attribute predicates.

Token 15552:
For example, a decision tree can be interpreted as a logical expression of this form.

Token 15553:
Thus, the tree in Figure 18.6 (page 702) expresses the following logical deﬁnition (which we will callh rfor future reference): ∀rWillWait (r)⇔Patrons (r,Some) ∨Patrons (r,Full)∧Hungry (r)∧Type(r,French ) ∨Patrons (r,Full)∧Hungry (r)∧Type(r,Thai) ∧Fri/Sat(r) ∨Patrons (r,Full)∧Hungry (r)∧Type(r,Burger ).

Token 15554:
(19.1) Each hypothesis predicts that a certain set of examples—namely, those that satisfy its candi- date deﬁnition—will be examples of the goal predicate.

Token 15555:
This set is called the extension of EXTENSION the predicate.

Token 15556:
Two hypotheses with different extensions are therefore logically inconsistent with each other, because they disagree on their predictions for at least one example.

Token 15557:
If theyhave the same extension, they are logically equivalent.

Token 15558:
The hypothesis space His the set of all hypotheses {h 1,...,h n}that the learning algo- rithm is designed to entertain.

Token 15559:
For example, the D ECISION -TREE-LEARNING algorithm can entertain any decision tree hypothesis deﬁned in terms of the attributes provided; its hypoth- esis space therefore consists of all these decision trees.

Token 15560:
Presumably, the learning algorithm believes that one of the hypotheses is correct; that is, it believes the sentence h1∨h2∨h3∨...∨hn.

Token 15561:
(19.2) As the examples arrive, hypotheses that are not consistent with the examples can be ruled out.

Token 15562:
Let us examine this notion of consistency more carefully.

Token 15563:
Obviously, if hypothesis hjis consistent with the entire training set, it has to be consistent with each example in the training set.

Token 15564:
What would it mean for it to be inconsistent with an example? There are two possible ways that this can happen:

Token 15565:
770 Chapter 19.

Token 15566:
Knowledge in Learning •An example can be a false negative for the hypothesis, if the hypothesis says it should FALSE NEGATIVE be negative but in fact it is positive.

Token 15567:
For instance, the new example X13described by Patrons (X13,Full)∧¬Hungry (X13)∧...∧WillWait (X13) would be a false negative for the hypothesis hrgiven earlier.

Token 15568:
From hrand the example description, we can deduce both WillWait (X13), which is what the example says, and¬WillWait (X13), which is what the hypothesis predicts.

Token 15569:
The hypothesis and the example are therefore logically inconsistent.

Token 15570:
•An example can be a false positive for the hypothesis, if the hypothesis says it should FALSE POSITIVE be positive but in fact it is negative.1 If an example is a false positive or false negative for a hypothesis, then the example and the hypothesis are logically inconsistent with each other.

Token 15571:
Assuming that the example is a correctobservation of fact, then the hypothesis can be ruled out.

Token 15572:
Logically, this is exactly analogousto the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses cor-responds to a clause and the example corresponds to a literal that resolves against one of theliterals in the clause.

Token 15573:
An ordinary logical inference system therefore could, in principle, learnfrom the example by eliminating one or more hypotheses.

Token 15574:
Suppose, for example, that theexample is denoted by the sentence I 1, and the hypothesis space is h1∨h2∨h3∨h4.T h e ni f I1is inconsistent with h2andh3, the logical inference system can deduce the new hypothesis spaceh1∨h4.

Token 15575:
We therefore can characterize inductive learning in a logical setting as a process of gradually eliminating hypotheses that are inconsistent with the examples, narrowing downthe possibilities.

Token 15576:
Because the hypothesis space is usually vast (or even inﬁnite in the case ofﬁrst-order logic), we do not recommend trying to build a learning system using resolution-based theorem proving and a complete enumeration of the hypothesis space.

Token 15577:
Instead, we willdescribe two approaches that ﬁnd logically consistent hypotheses with much less effort.

Token 15578:
19.1.2 Current-best-hypothesis search The idea behind current-best-hypothesis search is to maintain a single hypothesis, and toCURRENT- BEST- HYPOTHESIS adjust it as new examples arrive in order to maintain consistency.

Token 15579:
The basic algorithm was described by John Stuart Mill (1843), and may well have appeared even earlier.

Token 15580:
Suppose we have some hypothesis such as hr, of which we have grown quite fond. As long as each new example is consistent, we need do nothing.

Token 15581:
Then along comes a false negative example, X13. What do we do?

Token 15582:
Figure 19.1(a) shows hrschematically as a region: everything inside the rectangle is part of the extension of hr.

Token 15583:
The examples that have actually been seen so far are shown as “+” or “–”, and we see that hrcorrectly categorizes all the examples as positive or negative examples of WillWait .

Token 15584:
In Figure 19.1(b), a new example (circled) is a false negative: the hypothesis says it should be negative but it is actually positive.The extension of the hypothesis must be increased to include it.

Token 15585:
This is called generalization ; GENERALIZATION one possible generalization is shown in Figure 19.1(c).

Token 15586:
Then in Figure 19.1(d), we see a false positive: the hypothesis says the new example (circled) should be positive, but it actually is 1The terms “false positive” and “false negative” are used in medicine to describe erroneous results from lab tests.

Token 15587:
A result is a false positive if it indicates that the patient has the disease when in fact no disease is present.

Token 15588:
Section 19.1.

Token 15589:
A Logical Formulation of Learning 771 (a) (b) (c) (d) (e)+++ ++ ++–– – – – –––– – +++ ++ ++–– – – – –––– – ++++ ++ ++–– – – – –––– – ++++ ++ ++–– – – – ––– – + –+++ ++ ++–– – – – ––– – +– – Figure 19.1 (a) A consistent hypothesis.

Token 15590:
(b) A false negative. (c) The hypothesis is gen- eralized. (d) A false positive. (e) The hypothesis is specialized.

Token 15591:
function CURRENT -BEST-LEARNING (examples ,h)returns a hypothesis or fail ifexamples is empty then return h e←FIRST (examples ) ifeis consistent with hthen return CURRENT -BEST-LEARNING (REST(examples ),h) else ifeis a false positive for hthen for each h/primeinspecializations of hconsistent with examples seen so far do h/prime/prime←CURRENT -BEST-LEARNING (REST(examples ),h/prime) ifh/prime/prime/negationslash=failthen return h/prime/prime else ifeis a false negative for hthen for each h/primeingeneralizations of hconsistent with examples seen so far do h/prime/prime←CURRENT -BEST-LEARNING (REST(examples ),h/prime) ifh/prime/prime/negationslash=failthen return h/prime/prime return fail Figure 19.2 The current-best-hypothesis learning algorithm.

Token 15592:
It searches for a consis- tent hypothesis that ﬁts all the examples and backtracks when no consistent specializa- tion/generalization can be found.

Token 15593:
To start the algorithm, any hypothesis can be passed in; it will be specialized or gneralized as needed. negative.

Token 15594:
The extension of the hypothesis must be decreased to exclude the example.

Token 15595:
This is called specialization ; in Figure 19.1(e) we see one possible specialization of the hypothesis.

Token 15596:
SPECIALIZATION The “more general than” and “more speciﬁc than” relations between hypotheses provide the logical structure on the hypothesis space that makes efﬁcient search possible.

Token 15597:
We can now specify the C URRENT -BEST-LEARNING algorithm, shown in Figure 19.2.

Token 15598:
Notice that each time we consider generalizing or specializing the hypothesis, we must checkfor consistency with the other examples, because an arbitrary increase/decrease in the exten-sion might include/exclude previously seen negative/positive examples.

Token 15599:
772 Chapter 19. Knowledge in Learning We have deﬁned generalization and specialization as operations that change the exten- sion of a hypothesis.

Token 15600:
Now we need to determine exactly how they can be implemented as syntactic operations that change the candidate deﬁnition associated with the hypothesis, sothat a program can carry them out.

Token 15601:
This is done by ﬁrst noting that generalization and special-ization are also logical relationships between hypotheses.

Token 15602:
If hypothesis h 1, with deﬁnition C1, is a generalization of hypothesis h2with deﬁnition C2, then we must have ∀xC2(x)⇒C1(x).

Token 15603:
Therefore in order to construct a generalization of h2, we simply need to ﬁnd a deﬁni- tionC1that is logically implied by C2. This is easily done.

Token 15604:
For example, if C2(x)is Alternate (x)∧Patrons (x,Some), then one possible generalization is given by C1(x)≡ Patrons (x,Some).

Token 15605:
This is called dropping conditions .

Token 15606:
Intuitively, it generates a weakerDROPPING CONDITIONS deﬁnition and therefore allows a larger set of positive examples.

Token 15607:
There are a number of other generalization operations, depending on the language being operated on.

Token 15608:
Similarly, we can specialize a hypothesis by adding extra conditions to its candidate deﬁnition or by removingdisjuncts from a disjunctive deﬁnition.

Token 15609:
Let us see how this works on the restaurant example,using the data in Figure 18.3. •The ﬁrst example, X 1, is positive.

Token 15610:
The attribute Alternate (X1)is true, so let the initial hypothesis be h1:∀xWillWait (x)⇔Alternate (x). •The second example, X2, is negative.

Token 15611:
h1predicts it to be positive, so it is a false positive. Therefore, we need to specialize h1.

Token 15612:
This can be done by adding an extra condition that will rule out X2, while continuing to classify X1as positive.

Token 15613:
One possibility is h2:∀xWillWait (x)⇔Alternate (x)∧Patrons (x,Some). •The third example, X3, is positive.

Token 15614:
h2predicts it to be negative, so it is a false negative.

Token 15615:
Therefore, we need to generalize h2.W ed r o pt h e Alternate condition, yielding h3:∀xWillWait (x)⇔Patrons (x,Some).

Token 15616:
•The fourth example, X4, is positive. h3predicts it to be negative, so it is a false negative. We therefore need to generalize h3.

Token 15617:
We cannot drop the Patrons condition, because that would yield an all-inclusive hypothesis that would be inconsistent with X2.O n e possibility is to add a disjunct: h4:∀xWillWait (x)⇔Patrons (x,Some) ∨(Patrons (x,Full)∧Fri/Sat(x)).

Token 15618:
Already, the hypothesis is starting to look reasonable.

Token 15619:
Obviously, there are other possibilities consistent with the ﬁrst four examples; here are two of them: h/prime 4:∀xWillWait (x)⇔¬WaitEstimate (x,30-60).

Token 15620:
h/prime/prime 4:∀xWillWait (x)⇔Patrons (x,Some) ∨(Patrons (x,Full)∧WaitEstimate (x,10-30)).

Token 15621:
The C URRENT -BEST-LEARNING algorithm is described nondeterministically, because at any point, there may be several possible specializations or generalizations that can be applied.

Token 15622:
The

Token 15623:
Section 19.1.

Token 15624:
A Logical Formulation of Learning 773 function VERSION -SPACE -LEARNING (examples )returns a version space local variables :V, the version space: the set of all hypotheses V←the set of all hypotheses for each example einexamples do ifVis not empty thenV←VERSION -SPACE -UPDATE (V,e) return V function VERSION -SPACE -UPDATE (V,e)returns an updated version space V←{h∈V:his consistent with e} Figure 19.3 The version space learning algorithm.

Token 15625:
It ﬁnds a subset of Vthat is consistent with all the examples .

Token 15626:
choices that are made will not necessarily lead to the simplest hypothesis, and may lead to an unrecoverable situation where no simple modiﬁcation of the hypothesis is consistent with allof the data.

Token 15627:
In such cases, the program must backtrack to a previous choice point.

Token 15628:
The C URRENT -BEST-LEARNING algorithm and its variants have been used in many machine learning systems, starting with Patrick Winston’s (1970) “arch-learning” program.With a large number of examples and a large space, however, some difﬁculties arise: 1.

Token 15629:
Checking all the previous examples over again for each modiﬁcation is very expensive. 2. The search process may involve a great deal of backtracking.

Token 15630:
As we saw in Chapter 18, hypothesis space can be a doubly exponentially large place.

Token 15631:
19.1.3 Least-commitment search Backtracking arises because the current-best-hypothesis approach has to choose a particular hypothesis as its best guess even though it does not have enough data yet to be sure of thechoice.

Token 15632:
What we can do instead is to keep around all and only those hypotheses that areconsistent with all the data so far.

Token 15633:
Each new example will either have no effect or will getrid of some of the hypotheses.

Token 15634:
Recall that the original hypothesis space can be viewed as adisjunctive sentence h 1∨h2∨h3...∨hn.

Token 15635:
As various hypotheses are found to be inconsistent with the examples, this disjunction shrinks, retaining only those hypotheses not ruled out.

Token 15636:
Assuming that the original hypothesis spacedoes in fact contain the right answer, the reduced disjunction must still contain the right an-swer because only incorrect hypotheses have been removed.

Token 15637:
The set of hypotheses remainingis called the version space , and the learning algorithm (sketched in Figure 19.3) is called the VERSION SPACE version space learning algorithm (also the candidate elimination algorithm).CANDIDATE ELIMINATION One important property of this approach is that it is incremental : one never has to go back and reexamine the old examples.

Token 15638:
All remaining hypotheses are guaranteed to be consistent with them already. But there is an obvious problem. We already said that the

Token 15639:
774 Chapter 19. Knowledge in Learning This region all inconsistentThis region all inconsistent More general More specific S1G1 S2G2G3 . . . Gm . .

Token 15640:
.Sn Figure 19.4 The version space contains all hypotheses consistent with the examples.

Token 15641:
hypothesis space is enormous, so how can we possibly write down this enormous disjunction? The following simple analogy is very helpful.

Token 15642:
How do you represent all the real num- bers between 1 and 2? After all, there are an inﬁnite number of them!

Token 15643:
The answer is to use an interval representation that just speciﬁes the boundaries of the set: [1,2].

Token 15644:
It works becausewe have an ordering on the real numbers. We also have an ordering on the hypothesis space, namely, generalization/specialization.

Token 15645:
This is a partial ordering, which means that each boundary will not be a point but rather aset of hypotheses called a boundary set .

Token 15646:
The great thing is that we can represent the entire BOUNDARY SET version space using just two boundary sets: a most general boundary (the G-set ) and a most G-SET speciﬁc boundary (the S-set ).Everything in between is guaranteed to be consistent with the S-SET examples .

Token 15647:
Before we prove this, let us recap: •The current version space is the set of hypotheses consistent with all the examples so far.

Token 15648:
It is represented by the S-set and G-set, each of which is a set of hypotheses.

Token 15649:
•Every member of the S-set is consistent with all observations so far, and there are no consistent hypotheses that are more speciﬁc.

Token 15650:
•Every member of the G-set is consistent with all observations so far, and there are no consistent hypotheses that are more general.

Token 15651:
We want the initial version space (before any examples have been seen) to represent all possi- ble hypotheses.

Token 15652:
We do this by setting the G-set to contain True (the hypothesis that contains everything), and the S-set to contain False (the hypothesis whose extension is empty).

Token 15653:
Figure 19.4 shows the general structure of the boundary-set representation of the version space.

Token 15654:
To show that the representation is sufﬁcient, we need the following two properties:

Token 15655:
Section 19.1. A Logical Formulation of Learning 775 1.

Token 15656:
Every consistent hypothesis (other than those in the boundary sets) is more speciﬁc than some member of the G-set, and more general than some member of the S-set.

Token 15657:
(That is,there are no “stragglers” left outside.) This follows directly from the deﬁnitions of S andG.

Token 15658:
If there were a straggler h, then it would have to be no more speciﬁc than any member of G, in which case it belongs in G; or no more general than any member of S, in which case it belongs in S. 2.

Token 15659:
Every hypothesis more speciﬁc than some member of the G-set and more general than some member of the S-set is a consistent hypothesis.

Token 15660:
(That is, there are no “holes” be-tween the boundaries.)

Token 15661:
Any hbetween SandGmust reject all the negative examples rejected by each member of G(because it is more speciﬁc), and must accept all the pos- itive examples accepted by any member of S(because it is more general).

Token 15662:
Thus, hmust agree with all the examples, and therefore cannot be inconsistent.

Token 15663:
Figure 19.5 shows the situation: there are no known examples outside Sbut inside G, so any hypothesis in the gap must be consistent.

Token 15664:
We have therefore shown that ifSandGare maintained according to their deﬁnitions, then they provide a satisfactory representation of the version space.

Token 15665:
The only remaining problemis how to update SandGfor a new example (the job of the V ERSION -SPACE -UPDATE function).

Token 15666:
This may appear rather complicated at ﬁrst, but from the deﬁnitions and with the help of Figure 19.4, it is not too hard to reconstruct the algorithm.

Token 15667:
+ + + ++ ++++ +– – ––– – – – – ––– – – S1G1 G2 Figure 19.5 The extensions of the members of GandS.

Token 15668:
No known examples lie in between the two sets of boundaries. We need to worry about the members SiandGiof the S- and G-sets.

Token 15669:
For each one, the new example may be a false positive or a false negative. 1.

Token 15670:
False positive for Si: This means Siis too general, but there are no consistent special- izations of Si(by deﬁnition), so we throw it out of the S-set.

Token 15671:
2.

Token 15672:
False negative for Si: This means Siis too speciﬁc, so we replace it by all its immediate generalizations, provided they are more speciﬁc than some member of G. 3.

Token 15673:
False positive for Gi: This means Giis too general, so we replace it by all its immediate specializations, provided they are more general than some member of S.

Token 15674:
776 Chapter 19. Knowledge in Learning 4.

Token 15675:
False negative for Gi: This means Giis too speciﬁc, but there are no consistent gener- alizations of Gi(by deﬁnition) so we throw it out of the G-set.

Token 15676:
We continue these operations for each new example until one of three things happens: 1.

Token 15677:
We have exactly one hypothesis left in the version space, in which case we return it as the unique hypothesis. 2.

Token 15678:
The version space collapses —either S or G becomes empty, indicating that there are no consistent hypotheses for the training set.

Token 15679:
This is the same case as the failure of thesimple version of the decision tree algorithm. 3.

Token 15680:
We run out of examples and have several hypotheses remaining in the version space.

Token 15681:
This means the version space represents a disjunction of hypotheses.

Token 15682:
For any newexample, if all the disjuncts agree, then we can return their classiﬁcation of the example.If they disagree, one possibility is to take the majority vote.

Token 15683:
We leave as an exercise the application of the V ERSION -SPACE -LEARNING algorithm to the restaurant data.

Token 15684:
There are two principal drawbacks to the version-space approach: •If the domain contains noise or insufﬁcient attributes for exact classiﬁcation, the version space will always collapse.

Token 15685:
•If we allow unlimited disjunction in the hypothesis space, the S-set will always contain a single most-speciﬁc hypothesis, namely, the disjunction of the descriptions of the positive examples seen to date.

Token 15686:
Similarly, the G-set will contain just the negation of the disjunction of the descriptions of the negative examples.

Token 15687:
•For some hypothesis spaces, the number of elements in the S-set or G-set may grow exponentially in the number of attributes, even though efﬁcient learning algorithms exist for those hypothesis spaces.

Token 15688:
To date, no completely successful solution has been found for the problem of noise.

Token 15689:
The problem of disjunction can be addressed by allowing only limited forms of disjunction or by including a generalization hierarchy of more general predicates.

Token 15690:
For example, instead ofGENERALIZATION HIERARCHY using the disjunction WaitEstimate (x,30-60)∨WaitEstimate (x,>60), we might use the single literal LongWait (x).

Token 15691:
The set of generalization and specialization operations can be easily extended to handle this.

Token 15692:
The pure version space algorithm was ﬁrst applied in the Meta-D ENDRAL system, which was designed to learn rules for predicting how molecules would break into pieces ina mass spectrometer (Buchanan and Mitchell, 1978).

Token 15693:
Meta-D ENDRAL was able to generate rules that were sufﬁciently novel to warrant publication in a journal of analytical chemistry—the ﬁrst real scientiﬁc knowledge generated by a computer program.

Token 15694:
It was also used in the elegant L EXsystem (Mitchell et al.

Token 15695:
, 1983), which was able to learn to solve symbolic integra- tion problems by studying its own successes and failures.

Token 15696:
Although version space methodsare probably not practical in most real-world learning problems, mainly because of noise,they provide a good deal of insight into the logical structure of hypothesis space.

Token 15697:
Section 19.2.

Token 15698:
Knowledge in Learning 777 Observations Predictions HypothesesPrior knowledge Knowledge-based inductive learning Figure 19.6 A cumulative learning process uses, and adds to, its stock of background knowledge over time.

Token 15699:
19.2 K NOWLEDGE IN LEARNING The preceding section described the simplest setting for inductive learning.

Token 15700:
To understand the role of prior knowledge, we need to talk about the logical relationships among hypotheses, example descriptions, and classiﬁcations.

Token 15701:
Let Descriptions denote the conjunction of all the example descriptions in the training set, and let Classiﬁcations denote the conjunction of all the example classiﬁcations.

Token 15702:
Then a Hypothesis that “explains the observations” must satisfy the following property (recall that |=means “logically entails”): Hypothesis∧Descriptions|=Classiﬁcations .

Token 15703:
(19.3) We call this kind of relationship an entailment constraint ,i nw h i c h Hypothesis is the “un-ENTAILMENT CONSTRAINT known.” Pure inductive learning means solving this constraint, where Hypothesis is drawn from some predeﬁned hypothesis space.

Token 15704:
For example, if we consider a decision tree as alogical formula (see Equation (19.1) on page 769), then a decision tree that is consistent withall the examples will satisfy Equation (19.3).

Token 15705:
If we place norestrictions on the logical form of the hypothesis, of course, then Hypothesis =Classiﬁcations also satisﬁes the constraint.

Token 15706:
Ockham’s razor tells us to prefer small , consistent hypotheses, so we try to do better than simply memorizing the examples.

Token 15707:
This simple knowledge-free picture of inductive learning persisted until the early 1980s.

Token 15708:
The modern approach is to design agents that already know something and are trying to learn some more.

Token 15709:
This may not sound like a terriﬁcally deep insight, but it makes quite a difference to the way we design agents.

Token 15710:
It might also have some relevance to our theories about howscience itself works. The general idea is shown schematically in Figure 19.6.

Token 15711:
An autonomous learning agent that uses background knowledge must somehow obtain the background knowledge in the ﬁrst place, in order for it to be used in the new learningepisodes.

Token 15712:
This method must itself be a learning process. The agent’s life history will there-fore be characterized by cumulative ,o r incremental , development.

Token 15713:
Presumably, the agent could start out with nothing, performing inductions in vacuo like a good little pure induc- tion program.

Token 15714:
But once it has eaten from the Tree of Knowledge, it can no longer pursuesuch naive speculations and should use its background knowledge to learn more and moreeffectively.

Token 15715:
The question is then how to actually do this.

Token 15716:
778 Chapter 19. Knowledge in Learning 19.2.1 Some simple examples Let us consider some commonsense examples of learning with background knowledge.

Token 15717:
Many apparently rational cases of inferential behavior in the face of observations clearly do notfollow the simple principles of pure induction.

Token 15718:
•Sometimes one leaps to general conclusions after only one observation.

Token 15719:
Gary Larson once drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard onthe end of a pointed stick.

Token 15720:
He is watched by an amazed crowd of his less intellectualcontemporaries, who have been using their bare hands to hold their victuals over the ﬁre.This enlightening experience is enough to convince the watchers of a general principleof painless cooking.

Token 15721:
•Or consider the case of the traveler to Brazil meeting her ﬁrst Brazilian.

Token 15722:
On hearing him speak Portuguese, she immediately concludes that Brazilians speak Portuguese, yet ondiscovering that his name is Fernando, she does not conclude that all Brazilians arecalled Fernando.

Token 15723:
Similar examples appear in science.

Token 15724:
For example, when a freshmanphysics student measures the density and conductance of a sample of copper at a par-ticular temperature, she is quite conﬁdent in generalizing those values to all pieces of copper.

Token 15725:
Yet when she measures its mass, she does not even consider the hypothesis that all pieces of copper have that mass.

Token 15726:
On the other hand, it would be quite reasonable tomake such a generalization over all pennies.

Token 15727:
•Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti- cated medical student observing a consulting session between a patient and an expert internist.

Token 15728:
After a series of questions and answers, the expert tells the patient to take a course of a particular antibiotic.

Token 15729:
The medical student infers the general rule that thatparticular antibiotic is effective for a particular type of infection.

Token 15730:
These are all cases in which the use of background knowledge allows much faster learning than one might expect from a pure induction program.

Token 15731:
19.2.2 Some general schemes In each of the preceding examples, one can appeal to prior knowledge to try to justify the generalizations chosen.

Token 15732:
We will now look at what kinds of entailment constraints are operat-ing in each case.

Token 15733:
The constraints will involve the Background knowledge, in addition to the Hypothesis and the observed Descriptions andClassiﬁcations .

Token 15734:
In the case of lizard toasting, the cavemen generalize by explaining the success of the pointed stick: it supports the lizard while keeping the hand away from the ﬁre.

Token 15735:
From thisexplanation, they can infer a general rule: that any long, rigid, sharp object can be used to toastsmall, soft-bodied edibles.

Token 15736:
This kind of generalization process has been called explanation- based learning ,o rEBL .

Token 15737:
Notice that the general rule follows logically from the background EXPLANATION- BASED LEARNINGknowledge possessed by the cavemen.

Token 15738:
Hence, the entailment constraints satisﬁed by EBL are the following: Hypothesis∧Descriptions|=Classiﬁcations Background|=Hypothesis .

Token 15739:
Section 19.2. Knowledge in Learning 779 Because EBL uses Equation (19.3), it was initially thought to be a way to learn from ex- amples.

Token 15740:
But because it requires that the background knowledge be sufﬁcient to explain theHypothesis , which in turn explains the observations, the agent does not actually learn any- thing factually new from the example.

Token 15741:
The agent could have derived the example from what it already knew, although that might have required an unreasonable amount of computation.

Token 15742:
EBL is now viewed as a method for converting ﬁrst-principles theories into useful, special- purpose knowledge.

Token 15743:
We describe algorithms for EBL in Section 19.3.

Token 15744:
The situation of our traveler in Brazil is quite different, for she cannot necessarily ex- plain why Fernando speaks the way he does, unless she knows her papal bulls.

Token 15745:
Moreover,the same generalization would be forthcoming from a traveler entirely ignorant of colonialhistory.

Token 15746:
The relevant prior knowledge in this case is that, within any given country, mostpeople tend to speak the same language; on the other hand, Fernando is not assumed to bethe name of all Brazilians because this kind of regularity does not hold for names.

Token 15747:
Similarly,the freshman physics student also would be hard put to explain the particular values that shediscovers for the conductance and density of copper.

Token 15748:
She does know, however, that the mate-rial of which an object is composed and its temperature together determine its conductance.In each case, the prior knowledge Background concerns the relevance of a set of features to RELEVANCE the goal predicate.

Token 15749:
This knowledge, together with the observations , allows the agent to infer a new, general rule that explains the observations: Hypothesis∧Descriptions|=Classiﬁcations , Background∧Descriptions∧Classiﬁcations |=Hypothesis .

Token 15750:
(19.4) We call this kind of generalization relevance-based learning ,o rRBL (although the name isRELEVANCE-BASED LEARNING not standard).

Token 15751:
Notice that whereas RBL does make use of the content of the observations, it does not produce hypotheses that go beyond the logical content of the background knowledgeand the observations.

Token 15752:
It is a deductive form of learning and cannot by itself account for the creation of new knowledge starting from scratch.

Token 15753:
In the case of the medical student watching the expert, we assume that the student’s prior knowledge is sufﬁcient to infer the patient’s disease Dfrom the symptoms.

Token 15754:
This is not, however, enough to explain the fact that the doctor prescribes a particular medicine M. The student needs to propose another rule, namely, that Mgenerally is effective against D. Given this rule and the student’s prior knowledge, the student can now explain why the expertprescribes Min this particular case.

Token 15755:
We can generalize this example to come up with the entailment constraint Background∧Hypothesis∧Descriptions|=Classiﬁcations .

Token 15756:
(19.5) That is, the background knowledge and the new hypothesis combine to explain the examples.

Token 15757:
As with pure inductive learning, the learning algorithm should propose hypotheses that are as simple as possible, consistent with this constraint.

Token 15758:
Algorithms that satisfy constraint (19.5) are called knowledge-based inductive learning ,o rKBIL , algorithms.KNOWLEDGE-BASED INDUCTIVELEARNING KBIL algorithms, which are described in detail in Section 19.5, have been studied mainly in the ﬁeld of inductive logic programming ,o rILP.

Token 15759:
In ILP systems, prior knowl-INDUCTIVELOGIC PROGRAMMING edge plays two key roles in reducing the complexity of learning:

Token 15760:
780 Chapter 19. Knowledge in Learning 1.

Token 15761:
Because any hypothesis generated must be consistent with the prior knowledge as well as with the new observations, the effective hypothesis space size is reduced to includeonly those theories that are consistent with what is already known.

Token 15762:
2.

Token 15763:
For any given set of observations, the size of the hypothesis required to construct an explanation for the observations can be much reduced, because the prior knowledgewill be available to help out the new rules in explaining the observations.

Token 15764:
The smallerthe hypothesis, the easier it is to ﬁnd.

Token 15765:
In addition to allowing the use of prior knowledge in induction, ILP systems can formulate hypotheses in general ﬁrst-order logic, rather than in the restricted attribute-based languageof Chapter 18.

Token 15766:
This means that they can learn in environments that cannot be understood bysimpler systems.

Token 15767:
19.3 E XPLANATION -BASED LEARNING Explanation-based learning is a method for extracting general rules from individual obser-vations.

Token 15768:
As an example, consider the problem of differentiating and simplifying algebraic expressions (Exercise 9.17).

Token 15769:
If we differentiate an expression such as X 2with respect to X, we obtain 2X.

Token 15770:
(We use a capital letter for the arithmetic unknown X, to distinguish it from the logical variable x.)

Token 15771:
In a logical reasoning system, the goal might be expressed as ASK(Derivative (X2,X)=d,KB), with solution d=2X.

Token 15772:
Anyone who knows differential calculus can see this solution “by inspection” as a result of practice in solving such problems.

Token 15773:
A student encountering such problems for the ﬁrst time,or a program with no experience, will have a much more difﬁcult job.

Token 15774:
Application of thestandard rules of differentiation eventually yields the expression 1×(2×(X (2−1))),a n d eventually this simpliﬁes to 2X.

Token 15775:
In the authors’ logic programming implementation, this takes 136 proof steps, of which 99 are on dead-end branches in the proof.

Token 15776:
After such anexperience, we would like the program to solve the same problem much more quickly the next time it arises.

Token 15777:
The technique of memoization has long been used in computer science to speed up MEMOIZATION programs by saving the results of computation.

Token 15778:
The basic idea of memo functions is to accumulate a database of input–output pairs; when the function is called, it ﬁrst checks the database to see whether it can avoid solving the problem from scratch.

Token 15779:
Explanation-basedlearning takes this a good deal further, by creating general rules that cover an entire class of cases.

Token 15780:
In the case of differentiation, memoization would remember that the derivative ofX 2with respect to Xis2X, but would leave the agent to calculate the derivative of Z2with respect to Zfrom scratch.

Token 15781:
We would like to be able to extract the general rule that for any arithmetic unknown u, the derivative of u2with respect to uis2u.

Token 15782:
(An even more general rule for uncan also be produced, but the current example sufﬁces to make the point.)

Token 15783:
In logical terms, this is expressed by the rule ArithmeticUnknown (u)⇒Derivative (u2,u)=2u.

Token 15784:
Section 19.3.

Token 15785:
Explanation-Based Learning 781 If the knowledge base contains such a rule, then any new case that is an instance of this rule can be solved immediately.

Token 15786:
This is, of course, merely a trivial example of a very general phenomenon.

Token 15787:
Once some- thing is understood, it can be generalized and reused in other circumstances.

Token 15788:
It becomes an“obvious” step and can then be used as a building block in solving problems still more com- plex.

Token 15789:
Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathe- matica ,w r o t e “Civilization advances by extending the number of important operations that we can do without thinking about them, ” perhaps himself applying EBL to his understanding of events such as Zog’s discovery.

Token 15790:
If you have understood the basic idea of the differenti-ation example, then your brain is already busily trying to extract the general principles ofexplanation-based learning from it.

Token 15791:
Notice that you hadn’t already invented EBL before you saw the example.

Token 15792:
Like the cavemen watching Zog, you (and we) needed an example beforewe could generate the basic principles.

Token 15793:
This is because explaining why something is a good idea is much easier than coming up with the idea in the ﬁrst place.

Token 15794:
19.3.1 Extracting general rules from examples The basic idea behind EBL is ﬁrst to construct an explanation of the observation using priorknowledge, and then to establish a deﬁnition of the class of cases for which the same expla-nation structure can be used.

Token 15795:
This deﬁnition provides the basis for a rule covering all of thecases in the class.

Token 15796:
The “explanation” can be a logical proof, but more generally it can be anyreasoning or problem-solving process whose steps are well deﬁned.

Token 15797:
The key is to be able toidentify the necessary conditions for those same steps to apply to another case.

Token 15798:
We will use for our reasoning system the simple backward-chaining theorem prover described in Chapter 9.

Token 15799:
The proof tree for Derivative (X 2,X)=2Xis too large to use as an example, so we will use a simpler problem to illustrate the generalization method.

Token 15800:
Suppose our problem is to simplify 1×(0 +X). The knowledge base includes the following rules: Rewrite (u,v)∧Simplify (v,w)⇒Simplify (u,w).

Token 15801:
Primitive (u)⇒Simplify (u,u). ArithmeticUnknown (u)⇒Primitive (u). Number (u)⇒Primitive (u). Rewrite (1×u,u). Rewrite (0 +u,u). ...

Token 15802:
The proof that the answer is Xis shown in the top half of Figure 19.7. The EBL method actually constructs two proof trees simultaneously.

Token 15803:
The second proof tree uses a variabilized goal in which the constants from the original goal are replaced by variables.

Token 15804:
As the originalproof proceeds, the variabilized proof proceeds in step, using exactly the same rule applica- tions .

Token 15805:
This could cause some of the variables to become instantiated.

Token 15806:
For example, in order to use the rule Rewrite (1×u,u),t h ev a r i a b l e xin the subgoal Rewrite (x×(y+z),v)must be bound to 1.

Token 15807:
Similarly, ymust be bound to 0 in the subgoal Rewrite (y+z,v/prime)in order to use the rule Rewrite (0 +u,u).

Token 15808:
Once we have the generalized proof tree, we take the leaves

Token 15809:
782 Chapter 19.

Token 15810:
Knowledge in Learning Primitive (X) ArithmeticUnknown (X) Primitive (z) ArithmeticUnknown (z)Simplify (X,w) Yes, { }Yes, {x / 1, v / y+z}Simplify (y+z,w ) Rewrite (y+z,v' ) Yes, {y / 0, v'/ z}{w / X} Yes, { }Yes, {v / 0+X} Yes, {v' / X} Simplify (z,w) {w / z}Simplify (1 × (0+X),w) Rewrite (x ×(y+z),v)Simplify (x ×(y+z),w)Rewrite ( 1×( 0+X),v) Simplify (0+X,w ) Rewrite (0+X,v' ) Figure 19.7 Proof trees for the simpliﬁcation problem.

Token 15811:
The ﬁrst tree shows the proof for the original problem instance, from which we can derive ArithmeticUnknown (z)⇒Simplify (1×(0 +z),z).

Token 15812:
The second tree shows the proof for a problem instance with all constants replaced by vari- ables, from which we can derive a variety of other rules.

Token 15813:
(with the necessary bindings) and form a general rule for the goal predicate: Rewrite (1×(0 +z),0+z)∧Rewrite (0 +z,z)∧ArithmeticUnknown (z) ⇒Simplify (1×(0 +z),z).

Token 15814:
Notice that the ﬁrst two conditions on the left-hand side are true regardless of the value of z.

Token 15815:
We can therefore drop them from the rule, yielding ArithmeticUnknown (z)⇒Simplify (1×(0 +z),z).

Token 15816:
In general, conditions can be dropped from the ﬁnal rule if they impose no constraints on the variables on the right-hand side of the rule, because the resulting rule will still be true andwill be more efﬁcient.

Token 15817:
Notice that we cannot drop the condition ArithmeticUnknown (z), because not all possible values of zare arithmetic unknowns.

Token 15818:
Values other than arithmetic unknowns might require different forms of simpliﬁcation: for example, if zwere2×3,t h e n the correct simpliﬁcation of 1×(0 + (2×3))would be 6and not 2×3.

Token 15819:
To recap, the basic EBL process works as follows: 1.

Token 15820:
Given an example, construct a proof that the goal predicate applies to the example using the available background knowledge.

Token 15821:
Section 19.3. Explanation-Based Learning 783 2.

Token 15822:
In parallel, construct a generalized proof tree for the variabilized goal using the same inference steps as in the original proof. 3.

Token 15823:
Construct a new rule whose left-hand side consists of the leaves of the proof tree and whose right-hand side is the variabilized goal (after applying the necessary bindingsfrom the generalized proof).

Token 15824:
4. Drop any conditions from the left-hand side that are true regardless of the values of the variables in the goal.

Token 15825:
19.3.2 Improving efﬁciency The generalized proof tree in Figure 19.7 actually yields more than one generalized rule.

Token 15826:
Forexample, if we terminate, or prune , the growth of the right-hand branch in the proof tree when it reaches the Primitive step, we get the rule Primitive (z)⇒Simplify (1×(0 +z),z).

Token 15827:
This rule is as valid as, but more general than, the rule using ArithmeticUnknown , because it covers cases where zis a number.

Token 15828:
We can extract a still more general rule by pruning after the step Simplify (y+z,w), yielding the rule Simplify (y+z,w)⇒Simplify (1×(y+z),w).

Token 15829:
In general, a rule can be extracted from any partial subtree of the generalized proof tree. Now we have a problem: which of these rules do we choose?

Token 15830:
The choice of which rule to generate comes down to the question of efﬁciency.

Token 15831:
There are three factors involved in the analysis of efﬁciency gains from EBL: 1.

Token 15832:
Adding large numbers of rules can slow down the reasoning process, because the in- ference mechanism must still check those rules even in cases where they do not yield asolution.

Token 15833:
In other words, it increases the branching factor in the search space. 2.

Token 15834:
To compensate for the slowdown in reasoning, the derived rules must offer signiﬁcant increases in speed for the cases that they do cover.

Token 15835:
These increases come about mainlybecause the derived rules avoid dead ends that would otherwise be taken, but also be- cause they shorten the proof itself.

Token 15836:
3. Derived rules should be as general as possible, so that they apply to the largest possible set of cases.

Token 15837:
A common approach to ensuring that derived rules are efﬁcient is to insist on the operational- ityof each subgoal in the rule.

Token 15838:
A subgoal is operational if it is “easy” to solve.

Token 15839:
For example, OPERATIONALITY the subgoal Primitive (z)is easy to solve, requiring at most two steps, whereas the subgoal Simplify (y+z,w)could lead to an arbitrary amount of inference, depending on the values ofyandz.

Token 15840:
If a test for operationality is carried out at each step in the construction of the generalized proof, then we can prune the rest of a branch as soon as an operational subgoal is found, keeping just the operational subgoal as a conjunct of the new rule.

Token 15841:
Unfortunately, there is usually a tradeoff between operationality and generality.

Token 15842:
More speciﬁc subgoals are generally easier to solve but cover fewer cases.

Token 15843:
Also, operationalityis a matter of degree: one or two steps is deﬁnitely operational, but what about 10 or 100?

Token 15844:
784 Chapter 19. Knowledge in Learning Finally, the cost of solving a given subgoal depends on what other rules are available in the knowledge base.

Token 15845:
It can go up or down as more rules are added.

Token 15846:
Thus, EBL systems reallyface a very complex optimization problem in trying to maximize the efﬁciency of a giveninitial knowledge base.

Token 15847:
It is sometimes possible to derive a mathematical model of the effecton overall efﬁciency of adding a given rule and to use this model to select the best rule to add.

Token 15848:
The analysis can become very complicated, however, especially when recursive rules are involved.

Token 15849:
One promising approach is to address the problem of efﬁciency empirically,simply by adding several rules and seeing which ones are useful and actually speed things up.

Token 15850:
Empirical analysis of efﬁciency is actually at the heart of EBL.

Token 15851:
What we have been calling loosely the “efﬁciency of a given knowledge base” is actually the average-case com-plexity on a distribution of problems.

Token 15852:
By generalizing from past example problems, EBL makes the knowledge base more efﬁcient for the kind of problems that it is reasonable to expect.

Token 15853:
This works as long as the distribution of past examples is roughly the same as for future examples—the same assumption used for PAC-learning in Section 18.5.

Token 15854:
If the EBLsystem is carefully engineered, it is possible to obtain signiﬁcant speedups.

Token 15855:
For example, avery large Prolog-based natural language system designed for speech-to-speech translationbetween Swedish and English was able to achieve real-time performance only by the appli- cation of EBL to the parsing process (Samuelsson and Rayner, 1991).

Token 15856:
19.4 L EARNING USING RELEV ANCE INFORMATION Our traveler in Brazil seems to be able to make a conﬁdent generalization concerning the lan- guage spoken by other Brazilians.

Token 15857:
The inference is sanctioned by her background knowledge,namely, that people in a given country (usually) speak the same language.

Token 15858:
We can express this in ﬁrst-order logic as follows: 2 Nationality (x,n)∧Nationality (y,n)∧Language (x,l)⇒Language (y,l).

Token 15859:
(19.6) (Literal translation: “If xandyhave the same nationality nandxspeaks language l,t h e n y also speaks it.”) It is not difﬁcult to show that, from this sentence and the observation that Nationality (Fernando ,Brazil )∧Language (Fernando ,Portuguese ), the following conclusion is entailed (see Exercise 19.1): Nationality (x,Brazil )⇒Language (x,Portuguese ).

Token 15860:
Sentences such as (19.6) express a strict form of relevance: given nationality, language is fully determined.

Token 15861:
(Put another way: language is a function of nationality.) These sentencesare called functional dependencies ordeterminations .

Token 15862:
They occur so commonly in certain FUNCTIONAL DEPENDENCY DETERMINATION kinds of applications (e.g., deﬁning database designs) that a special syntax is used to write them.

Token 15863:
We adopt the notation of Davies (1985): Nationality (x,n)/followsLanguage (x,l).

Token 15864:
2We assume for the sake of simplicity that a person speaks only one language.

Token 15865:
Clearly, the rule would have to be amended for countries such as Switzerland and India.

Token 15866:
Section 19.4.

Token 15867:
Learning Using Relevance Information 785 As usual, this is simply a syntactic sugaring, but it makes it clear that the determination is really a relationship between the predicates: nationality determines language.

Token 15868:
The relevantproperties determining conductance and density can be expressed similarly: Material (x,m)∧Temperature (x,t)/followsConductance (x,ρ); Material (x,m)∧Temperature (x,t)/followsDensity (x,d).

Token 15869:
The corresponding generalizations follow logically from the determinations and observations.

Token 15870:
19.4.1 Determining the hypothesis space Although the determinations sanction general conclusions concerning all Brazilians, or all pieces of copper at a given temperature, they cannot, of course, yield a general predictivetheory for allnationalities, or for alltemperatures and materials, from a single example.

Token 15871:
Their main effect can be seen as limiting the space of hypotheses that the learning agent needconsider.

Token 15872:
In predicting conductance, for example, one need consider only material and tem-perature and can ignore mass, ownership, day of the week, the current president, and so on.Hypotheses can certainly include terms that are in turn determined by material and temper- ature, such as molecular structure, thermal energy, or free-electron density.

Token 15873:
Determinations specify a sufﬁcient basis vocabulary from which to construct hypotheses concerning the target predicate.

Token 15874:
This statement can be proven by showing that a given determination is logically equivalent to a statement that the correct deﬁnition of the target predicate is one of the set ofall deﬁnitions expressible using the predicates on the left-hand side of the determination.

Token 15875:
Intuitively, it is clear that a reduction in the hypothesis space size should make it eas- ier to learn the target predicate.

Token 15876:
Using the basic results of computational learning theory(Section 18.5), we can quantify the possible gains.

Token 15877:
First, recall that for Boolean functions,log(|H|)examples are required to converge to a reasonable hypothesis, where |H|is the size of the hypothesis space.

Token 15878:
If the learner has nBoolean features with which to construct hypotheses, then, in the absence of further restrictions, |H|=O(2 2n), so the number of ex- amples is O(2n).

Token 15879:
If the determination contains dpredicates in the left-hand side, the learner will require only O(2d)examples, a reduction of O(2n−d).

Token 15880:
19.4.2 Learning and using relevance information As we stated in the introduction to this chapter, prior knowledge is useful in learning; but it too has to be learned.

Token 15881:
In order to provide a complete story of relevance-based learning, we must therefore provide a learning algorithm for determinations.

Token 15882:
The learning algorithm we now present is based on a straightforward attempt to ﬁnd the simplest determination con- sistent with the observations.

Token 15883:
A determination P/followsQsays that if any examples match on P, then they must also match on Q.

Token 15884:
A determination is therefore consistent with a set of examples if every pair that matches on the predicates on the left-hand side also matches on the goal predicate.

Token 15885:
For example, suppose we have the following examples of conductance measurements on material samples:

Token 15886:
786 Chapter 19.

Token 15887:
Knowledge in Learning function MINIMAL -CONSISTENT -DET(E,A)returns a set of attributes inputs :E,as e to fe x a m p l e s A, a set of attributes, of size n fori=0tondo for each subset AiofAof size ido ifCONSISTENT -DET?

Token 15888:
(Ai,E)then return Ai function CONSISTENT -DET?

Token 15889:
(A,E)returns a truth value inputs :A, a set of attributes E,as e to fe x a m p l e s local variables :H, a hash table for each example einEdo ifsome example in Hhas the same values as efor the attributes A but a different classiﬁcation then return false store the class of einH, indexed by the values for attributes Aof the example e return true Figure 19.8 An algorithm for ﬁnding a minimal consistent determination.

Token 15890:
Sample Mass Temperature Material Size Conductance S1 12 26 Copper 3 0.59 S1 12 100 Copper 3 0.57 S2 24 26 Copper 6 0.59 S3 12 26 Lead 2 0.05 S3 12 100 Lead 2 0.04 S4 24 26 Lead 4 0.05 The minimal consistent determination is Material∧Temperature/followsConductance .T h e r e is a nonminimal but consistent determination, namely, Mass∧Size∧Temperature /follows Conductance .

Token 15891:
This is consistent with the examples because mass and size determine density and, in our data set, we do not have two different materials with the same density.

Token 15892:
As usual,we would need a larger sample set in order to eliminate a nearly correct hypothesis.

Token 15893:
There are several possible algorithms for ﬁnding minimal consistent determinations.

Token 15894:
The most obvious approach is to conduct a search through the space of determinations, check-ing all determinations with one predicate, two predicates, and so on, until a consistent deter-mination is found.

Token 15895:
We will assume a simple attribute-based representation, like that used fordecision tree learning in Chapter 18.

Token 15896:
A determination dwill be represented by the set of attributes on the left-hand side, because the target predicate is assumed to be ﬁxed.

Token 15897:
The basic algorithm is outlined in Figure 19.8. The time complexity of this algorithm depends on the size of the smallest consistent determination.

Token 15898:
Suppose this determination has pattributes out of the ntotal attributes.

Token 15899:
Then the algorithm will not ﬁnd it until searching the subsets of Aof size p.T h e r ea r e/parenleftbig n p/parenrightbig =O(np)

Token 15900:
Section 19.4.

Token 15901:
Learning Using Relevance Information 787 0.40.50.60.70.80.91 0 20 40 60 80 100 120 140Proportion correct on test set Training set sizeRBDTL DTL Figure 19.9 A performance comparison between D ECISION -TREE-LEARNING and RBDTL on randomly generated data for a target function that depends on only 5 of 16 attributes.

Token 15902:
such subsets; hence the algorithm is exponential in the size of the minimal determination.

Token 15903:
It turns out that the problem is NP-complete, so we cannot expect to do better in the generalcase.

Token 15904:
In most domains, however, there will be sufﬁcient local structure (see Chapter 14 for adeﬁnition of locally structured domains) that pwill be small.

Token 15905:
Given an algorithm for learning determinations, a learning agent has a way to construct a minimal hypothesis within which to learn the target predicate.

Token 15906:
For example, we can combine M INIMAL -CONSISTENT -DETwith the D ECISION -TREE-LEARNING algorithm.

Token 15907:
This yields a relevance-based decision-tree learning algorithm RBDTL that ﬁrst identiﬁes a minimal set of relevant attributes and then passes this set to the decision tree algorithm for learning.Unlike D ECISION -TREE-LEARNING , RBDTL simultaneously learns and uses relevance in- formation in order to minimize its hypothesis space.

Token 15908:
We expect that RBDTL will learn fasterthan D ECISION -TREE-LEARNING , and this is in fact the case.

Token 15909:
Figure 19.9 shows the learning performance for the two algorithms on randomly generated data for a function that depends on only 5 of 16 attributes.

Token 15910:
Obviously, in cases where all the available attributes are relevant, RBDTL will show no advantage.

Token 15911:
This section has only scratched the surface of the ﬁeld of declarative bias , which aims DECLARATIVE BIAS to understand how prior knowledge can be used to identify the appropriate hypothesis space within which to search for the correct target deﬁnition.

Token 15912:
There are many unanswered questions: •How can the algorithms be extended to handle noise? •Can we handle continuous-valued variables?

Token 15913:
•How can other kinds of prior knowledge be used, besides determinations?

Token 15914:
•How can the algorithms be generalized to cover any ﬁrst-order theory, rather than just an attribute-based representation?

Token 15915:
Some of these questions are addressed in the next section.

Token 15916:
788 Chapter 19.

Token 15917:
Knowledge in Learning 19.5 I NDUCTIVE LOGIC PROGRAMMING Inductive logic programming (ILP) combines inductive methods with the power of ﬁrst-order representations, concentrating in particular on the representation of hypotheses as logic pro-grams.

Token 15918:
3It has gained popularity for three reasons. First, ILP offers a rigorous approach to the general knowledge-based inductive learning problem.

Token 15919:
Second, it offers complete algo- rithms for inducing general, ﬁrst-order theories from examples, which can therefore learnsuccessfully in domains where attribute-based algorithms are hard to apply.

Token 15920:
An example isin learning how protein structures fold (Figure 19.10).

Token 15921:
The three-dimensional conﬁgurationof a protein molecule cannot be represented reasonably by a set of attributes, because theconﬁguration inherently refers to relationships between objects, not to attributes of a single object.

Token 15922:
First-order logic is an appropriate language for describing the relationships.

Token 15923:
Third,inductive logic programming produces hypotheses that are (relatively) easy for humans toread.

Token 15924:
For example, the English translation in Figure 19.10 can be scrutinized and criticizedby working biologists.

Token 15925:
This means that inductive logic programming systems can participate in the scientiﬁc cycle of experimentation, hypothesis generation, debate, and refutation.

Token 15926:
Such participation would not be possible for systems that generate “black-box” classiﬁers, such as neural networks.

Token 15927:
19.5.1 An example Recall from Equation (19.5) that the general knowledge-based induction problem is to “solve” the entailment constraint Background∧Hypothesis∧Descriptions|=Classiﬁcations for the unknown Hypothesis ,g i v e nt h e Background knowledge and examples described by Descriptions andClassiﬁcations .

Token 15928:
To illustrate this, we will use the problem of learning family relationships from examples.

Token 15929:
The descriptions will consist of an extended family tree, described in terms of Mother ,Father ,a n dMarried relations and Male andFemale properties.

Token 15930:
As an example, we will use the family tree from Exercise 8.14, shown here in Figure 19.11.

Token 15931:
The corresponding descriptions are as follows: Father (Philip ,Charles )Father (Philip ,Anne) ...

Token 15932:
Mother (Mum,Margaret )Mother (Mum,Elizabeth )... Married (Diana ,Charles )Married (Elizabeth ,Philip )... Male(Philip ) Male(Charles ) ...

Token 15933:
Female (Beatrice ) Female (Margaret ) ... The sentences in Classiﬁcations depend on the target concept being learned.

Token 15934:
We might want to learn Grandparent ,BrotherInLaw ,o rAncestor , for example.

Token 15935:
For Grandparent ,t h e 3It might be appropriate at this point for the reader to refer to Chapter 7 for some of the underlying concepts, including Horn clauses, conjunctive normal form, uniﬁcation, and resolution.

Token 15936:
Section 19.5.

Token 15937:
Inductive Logic Programming 789 complete set of Classiﬁcations contains 20×20 =400 conjuncts of the form Grandparent (Mum,Charles )Grandparent (Elizabeth ,Beatrice )... ¬Grandparent (Mum,Harry )¬Grandparent (Spencer ,Peter)... We could of course learn from a subset of this complete set.

Token 15938:
The object of an inductive learning program is to come up with a set of sentences for theHypothesis such that the entailment constraint is satisﬁed.

Token 15939:
Suppose, for the moment, that the agent has no background knowledge: Background is empty.

Token 15940:
Then one possible solution 2mhr - Four-helical up-and-down bundleH:1[19-37] H:2[41-64]H:3[71-84] H:4[93-108]H:5[111-113] H:1[8-17] H:2[26-33] H:3[40-50]H:4[61-64] H:5[66-70]H:6[79-88] H:7[99-106]E:1[57-59]E:2[96-98] 1omd - EF-Hand (a) (b) Figure 19.10 (a) and (b) show positive and negative examples, respectively, of the “four-helical up-and-down bundle” concept in the domain of protein folding.

Token 15941:
Each example structure is coded into a logica l expression of about 100 conjuncts such as TotalLength (D2mhr,118)∧NumberHelices (D2mhr,6)∧.... From these descriptions and from classiﬁcations such as Fold(FOUR-HELICAL -UP-AND-DOWN -BUNDLE ,D2mhr), the ILP system P ROGOL (Muggleton, 1995) learned the following rule: Fold(FOUR-HELICAL -UP-AND-DOWN -BUNDLE ,p)⇐ Helix(p,h1)∧Length (h1,HIGH)∧Position (p,h1,n) ∧(1≤n≤3)∧Adjacent (p,h1,h2)∧Helix(p,h2).

Token 15942:
This kind of rule could not be learned, or even represented, by an attribute-based mechanism such as we saw in previous chapters.

Token 15943:
The rule can be translated into English as “ Protein p has fold class “Four-helical up-and-down-bundle” if it contains a long helix h1at a secondary structure position between 1 and 3 and h1is next to a second helix.”

Token 15944:
790 Chapter 19.

Token 15945:
Knowledge in Learning forHypothesis is the following: Grandparent (x,y)⇔[∃zMother (x,z)∧Mother (z,y)] ∨ [∃zMother (x,z)∧Father (z,y)] ∨ [∃zFather (x,z)∧Mother (z,y)] ∨ [∃zFather (x,z)∧Father (z,y)].

Token 15946:
Notice that an attribute-based learning algorithm, such as D ECISION -TREE-LEARNING , will get nowhere in solving this problem.

Token 15947:
In order to express Grandparent as an attribute (i.e., a unary predicate), we would need to make pairs of people into objects: Grandparent (/angbracketleftMum,Charles/angbracketright)... Then we get stuck in trying to represent the example descriptions.

Token 15948:
The only possible attributes are horrible things such as FirstElementIsMotherOfElizabeth (/angbracketleftMum,Charles/angbracketright).

Token 15949:
The deﬁnition of Grandparent in terms of these attributes simply becomes a large disjunc- tion of speciﬁc cases that does not generalize to new examples at all.

Token 15950:
Attribute-based learning algorithms are incapable of learning relational predicates.

Token 15951:
Thus, one of the principal advan- tages of ILP algorithms is their applicability to a much wider range of problems, includingrelational problems.

Token 15952:
The reader will certainly have noticed that a little bit of background knowledge would help in the representation of the Grandparent deﬁnition.

Token 15953:
For example, if Background in- cluded the sentence Parent (x,y)⇔[Mother (x,y)∨Father (x,y)], then the deﬁnition of Grandparent would be reduced to Grandparent (x,y)⇔[∃zParent (x,z)∧Parent (z,y)].

Token 15954:
This shows how background knowledge can dramatically reduce the size of hypotheses re- quired to explain the observations.

Token 15955:
It is also possible for ILP algorithms to create new predicates in order to facilitate the expression of explanatory hypotheses.

Token 15956:
Given the example data shown earlier, it is entirelyreasonable for the ILP program to propose an additional predicate, which we would call BeatriceAndrew Eugenie William HarryCharles DianaMum George Philip Elizabeth Margaret Kydd Spencer PeterMark ZaraAnne Sarah Edward Sophie Louise James Figure 19.11 A typical family tree.

Token 15957:
Section 19.5. Inductive Logic Programming 791 “Parent ,” in order to simplify the deﬁnitions of the target predicates.

Token 15958:
Algorithms that can generate new predicates are called constructive induction algorithms.

Token 15959:
Clearly, constructiveCONSTRUCTIVE INDUCTION induction is a necessary part of the picture of cumulative learning.

Token 15960:
It has been one of the hardest problems in machine learning, but some ILP techniques provide effective mechanismsfor achieving it.

Token 15961:
In the rest of this chapter, we will study the two principal approaches to ILP.

Token 15962:
The ﬁrst uses a generalization of decision tree methods, and the second uses techniques based oninverting a resolution proof.

Token 15963:
19.5.2 Top-down inductive learning methods The ﬁrst approach to ILP works by starting with a very general rule and gradually specializingit so that it ﬁts the data.

Token 15964:
This is essentially what happens in decision-tree learning, where adecision tree is gradually grown until it is consistent with the observations.

Token 15965:
To do ILP weuse ﬁrst-order literals instead of attributes, and the hypothesis is a set of clauses instead of adecision tree.

Token 15966:
This section describes F OIL(Quinlan, 1990), one of the ﬁrst ILP programs.

Token 15967:
Suppose we are trying to learn a deﬁnition of the Grandfather (x,y)predicate, using the same family data as before.

Token 15968:
As with decision-tree learning, we can divide the examples into positive and negative examples.

Token 15969:
Positive examples are /angbracketleftGeorge ,Anne/angbracketright,/angbracketleftPhilip ,Peter/angbracketright,/angbracketleftSpencer ,Harry/angbracketright, ... and negative examples are /angbracketleftGeorge ,Elizabeth/angbracketright,/angbracketleftHarry ,Zara/angbracketright,/angbracketleftCharles ,Philip/angbracketright, ... Notice that each example is a pair of objects, because Grandfather is a binary predicate.

Token 15970:
In all, there are 12 positive examples in the family tree and 388 negative examples (all the other pairs of people).

Token 15971:
FOILconstructs a set of clauses, each with Grandfather (x,y)as the head.

Token 15972:
The clauses must classify the 12 positive examples as instances of the Grandfather (x,y)relationship, while ruling out the 388 negative examples.

Token 15973:
The clauses are Horn clauses, with the extension that negated literals are allowed in the body of a clause and are interpreted using negation asfailure, as in Prolog.

Token 15974:
The initial clause has an empty body: ⇒Grandfather (x,y). This clause classiﬁes every example as positive, so it needs to be specialized.

Token 15975:
We do this by adding literals one at a time to the left-hand side. Here are three potential additions: Father (x,y)⇒Grandfather (x,y).

Token 15976:
Parent (x,z)⇒Grandfather (x,y). Father (x,z)⇒Grandfather (x,y).

Token 15977:
(Notice that we are assuming that a clause deﬁning Parent is already part of the background knowledge.)

Token 15978:
The ﬁrst of these three clauses incorrectly classiﬁes all of the 12 positive exam- ples as negative and can thus be ignored.

Token 15979:
The second and third agree with all of the positiveexamples, but the second is incorrect on a larger fraction of the negative examples—twice asmany, because it allows mothers as well as fathers.

Token 15980:
Hence, we prefer the third clause.

Token 15981:
792 Chapter 19.

Token 15982:
Knowledge in Learning Now we need to specialize this clause further, to rule out the cases in which xis the father of some z,b u tzis not a parent of y.

Token 15983:
Adding the single literal Parent (z,y)gives Father (x,z)∧Parent (z,y)⇒Grandfather (x,y), which correctly classiﬁes all the examples.

Token 15984:
F OILwill ﬁnd and choose this literal, thereby solving the learning task.

Token 15985:
In general, the solution is a set of Horn clauses, each of whichimplies the target predicate.

Token 15986:
For example, if we didn’t have the Parent predicate in our vocabulary, then the solution might be Father (x,z)∧Father (z,y)⇒Grandfather (x,y) Father (x,z)∧Mother (z,y)⇒Grandfather (x,y).

Token 15987:
Note that each of these clauses covers some of the positive examples, that together they cover all the positive examples, and that N EW-CLAUSE is designed in such a way that no clause will incorrectly cover a negative example.

Token 15988:
In general F OILwill have to search through many unsuccessful clauses before ﬁnding a correct solution.

Token 15989:
This example is a very simple illustration of how F OILoperates. A sketch of the com- plete algorithm is shown in Figure 19.12.

Token 15990:
Essentially, the algorithm repeatedly constructs aclause, literal by literal, until it agrees with some subset of the positive examples and none ofthe negative examples.

Token 15991:
Then the positive examples covered by the clause are removed fromthe training set, and the process continues until no positive examples remain.

Token 15992:
The two mainsubroutines to be explained are N EW-LITERALS , which constructs all possible new literals to add to the clause, and C HOOSE -LITERAL , which selects a literal to add.

Token 15993:
NEW-LITERALS takes a clause and constructs all possible “useful” literals that could be added to the clause.

Token 15994:
Let us use as an example the clause Father (x,z)⇒Grandfather (x,y).

Token 15995:
There are three kinds of literals that can be added: 1.Literals using predicates : the literal can be negated or unnegated, any existing predicate (including the goal predicate) can be used, and the arguments must all be variables.

Token 15996:
Any variable can be used for any argument of the predicate, with one restriction: each literal must include at least one variable from an earlier literal or from the head of the clause.

Token 15997:
Literals such as Mother (z,u),Married (z,z),¬Male(y),a n dGrandfather (v,x)are allowed, whereas Married (u,v)is not.

Token 15998:
Notice that the use of the predicate from the head of the clause allows F OILto learn recursive deﬁnitions.

Token 15999:
2.Equality and inequality literals : these relate variables already appearing in the clause. For example, we might add z/negationslash=x.

Token 16000:
These literals can also include user-speciﬁed con- stants.

Token 16001:
For learning arithmetic we might use 0 and 1, and for learning list functions wemight use the empty list [].

Token 16002:
3.Arithmetic comparisons : when dealing with functions of continuous variables, literals such as x>y andy≤zcan be added.

Token 16003:
As in decision-tree learning, a constant threshold value can be chosen to maximize the discriminatory power of the test.

Token 16004:
The resulting branching factor in this search space is very large (see Exercise 19.6), but F OIL can also use type information to reduce it.

Token 16005:
For example, if the domain included numbers as

Token 16006:
Section 19.5.

Token 16007:
Inductive Logic Programming 793 function FOIL(examples ,target )returns a set of Horn clauses inputs :examples , set of examples target , a literal for the goal predicate local variables :clauses , set of clauses, initially empty whileexamples contains positive examples do clause←NEW-CLAUSE (examples ,target ) remove positive examples covered by clause fromexamples addclause toclauses return clauses function NEW-CLAUSE (examples ,target )returns a Horn clause local variables :clause , a clause with target as head and an empty body l, a literal to be added to the clause extended examples , a set of examples with values for new variables extended examples←examples whileextended examples contains negative examples do l←CHOOSE -LITERAL (NEW-LITERALS (clause ),extended examples ) append lto the body of clause extended examples←set of examples created by applying E XTEND -EXAMPLE to each example in extended examples return clause function EXTEND -EXAMPLE (example ,literal )returns a set of examples ifexample satisﬁes literal then return the set of examples created by extending example with each possible constant value for each new variable in literal else return the empty set Figure 19.12 Sketch of the F OILalgorithm for learning sets of ﬁrst-order Horn clauses from examples.

Token 16008:
N EW-LITERALS and C HOOSE -LITERAL are explained in the text.

Token 16009:
well as people, type restrictions would prevent N EW-LITERALS from generating literals such asParent (x,n),w h e r e xis a person and nis a number.

Token 16010:
CHOOSE -LITERAL uses a heuristic somewhat similar to information gain (see page 704) to decide which literal to add.

Token 16011:
The exact details are not important here, and a number ofdifferent variations have been tried.

Token 16012:
One interesting additional feature of F OILis the use of Ockham’s razor to eliminate some hypotheses.

Token 16013:
If a clause becomes longer (according to somemetric) than the total length of the positive examples that the clause explains, that clause isnot considered as a potential hypothesis.

Token 16014:
This technique provides a way to avoid overcomplex clauses that ﬁt noise in the data.

Token 16015:
F OILand its relatives have been used to learn a wide variety of deﬁnitions.

Token 16016:
One of the most impressive demonstrations (Quinlan and Cameron-Jones, 1993) involved solving a longsequence of exercises on list-processing functions from Bratko’s (1986) Prolog textbook.

Token 16017:
In

Token 16018:
794 Chapter 19.

Token 16019:
Knowledge in Learning each case, the program was able to learn a correct deﬁnition of the function from a small set of examples, using the previously learned functions as background knowledge.

Token 16020:
19.5.3 Inductive learning with inverse deduction The second major approach to ILP involves inverting the normal deductive proof process.Inverse resolution is based on the observation that if the example Classiﬁcations follow INVERSE RESOLUTION fromBackground∧Hypothesis∧Descriptions , then one must be able to prove this fact by resolution (because resolution is complete).

Token 16021:
If we can “run the proof backward,” then we canﬁnd aHypothesis such that the proof goes through.

Token 16022:
The key, then, is to ﬁnd a way to invert the resolution process.

Token 16023:
We will show a backward proof process for inverse resolution that consists of individual backward steps.

Token 16024:
Recall that an ordinary resolution step takes two clauses C 1andC2and resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C and produces two clauses C1andC2, such that Cis the result of resolving C1andC2.

Token 16025:
Alternatively, it may take a resolvent Cand clause C1and produce a clause C2such that C is the result of resolving C1andC2.

Token 16026:
The early steps in an inverse resolution process are shown in Figure 19.13, where we focus on the positive example Grandparent (George ,Anne).

Token 16027:
The process begins at the end of the proof (shown at the bottom of the ﬁgure). We take the resolvent Cto be empty clause (i.e.

Token 16028:
a contradiction) and C2to be¬Grandparent (George ,Anne), which is the nega- tion of the goal example.

Token 16029:
The ﬁrst inverse step takes CandC2and generates the clause Grandparent (George ,Anne)forC1.

Token 16030:
The next step takes this clause as Cand the clause Parent (Elizabeth ,Anne)asC2, and generates the clause ¬Parent (Elizabeth ,y)∨Grandparent (George ,y) asC1.

Token 16031:
The ﬁnal step treats this clause as the resolvent.

Token 16032:
With Parent (George ,Elizabeth )as C2, one possible clause C1is the hypothesis Parent (x,z)∧Parent (z,y)⇒Grandparent (x,y).

Token 16033:
Now we have a resolution proof that the hypothesis, descriptions, and background knowledge entail the classiﬁcation Grandparent (George ,Anne).

Token 16034:
Clearly, inverse resolution involves a search.

Token 16035:
Each inverse resolution step is nonde- terministic, because for any C, there can be many or even an inﬁnite number of clauses C1andC2that resolve to C. For example, instead of choosing ¬Parent (Elizabeth ,y)∨ Grandparent (George ,y)forC1in the last step of Figure 19.13, the inverse resolution step might have chosen any of the following sentences: ¬Parent (Elizabeth ,Anne)∨Grandparent (George ,Anne).

Token 16036:
¬Parent (z,Anne)∨Grandparent (George ,Anne). ¬Parent (z,y)∨Grandparent (George ,y). ... (See Exercises 19.4 and 19.5.)

Token 16037:
Furthermore, the clauses that participate in each step can be chosen from the Background knowledge, from the example Descriptions , from the negated

Token 16038:
Section 19.5.

Token 16039:
Inductive Logic Programming 795 Classiﬁcations , or from hypothesized clauses that have already been generated in the inverse resolution tree.

Token 16040:
The large number of possibilities means a large branching factor (and there-fore an inefﬁcient search) without additional controls.

Token 16041:
A number of approaches to taming thesearch have been tried in implemented ILP systems: 1.

Token 16042:
Redundant choices can be eliminated—for example, by generating only the most spe- ciﬁc hypotheses possible and by requiring that all the hypothesized clauses be consistent with each other, and with the observations.

Token 16043:
This last criterion would rule out the clause ¬Parent (z,y)∨Grandparent (George ,y), listed before. 2. The proof strategy can be restricted.

Token 16044:
For example, we saw in Chapter 9 that linear resolution is a complete, restricted strategy.

Token 16045:
Linear resolution produces proof trees that have a linear branching structure—the whole tree follows one line, with only singleclauses branching off that line (as in Figure 19.13).

Token 16046:
3. The representation language can be restricted, for example by eliminating function sym- bols or by allowing only Horn clauses.

Token 16047:
For instance, P ROGOL operates with Horn clauses using inverse entailment .

Token 16048:
The idea is to change the entailment constraintINVERSE ENTAILMENT Background∧Hypothesis∧Descriptions|=Classiﬁcations to the logically equivalent form Background∧Descriptions∧¬Classiﬁcations |=¬Hypothesis .

Token 16049:
From this, one can use a process similar to the normal Prolog Horn-clause deduction, with negation-as-failure to derive Hypothesis .

Token 16050:
Because it is restricted to Horn clauses, this is an incomplete method, but it can be more efﬁcient than full resolution.

Token 16051:
It is also possible to apply complete inference with inverse entailment (Inoue, 2001). 4.

Token 16052:
Inference can be done with model checking rather than theorem proving.

Token 16053:
The P ROGOL system (Muggleton, 1995) uses a form of model checking to limit the search.

Token 16054:
That {y/Anne }Parent (Elizabeth,Anne ) Grandparent (George,Anne ) Grandparent (George,Anne )Grandparent (George,y ) Parent (Elizabeth,y )>{x/George, z/Elizabeth }Parent (George,Elizabeth ) >Parent (z,y)Grandparent (x,y) >Parent (x,z) ¬ ¬ ¬ ¬ Figure 19.13 Early steps in an inverse resolution process.

Token 16055:
The shaded clauses are generated by inverse resolution steps from the clause to the right and the clause below.The unshaded clauses are from the Descriptions andClassiﬁcations (including negated Classiﬁcations ).

Token 16056:
796 Chapter 19. Knowledge in Learning is, like answer set programming, it generates possible values for logical variables, and checks for consistency.

Token 16057:
5. Inference can be done with ground propositional clauses rather than in ﬁrst-order logic.

Token 16058:
The L INUS system (Lavra uca n dD uzeroski, 1994) works by translating ﬁrst-order the- ories into propositional logic, solving them with a propositional learning system, andthen translating back.

Token 16059:
Working with propositional formulas can be more efﬁcient onsome problems, as we saw with SATP LAN in Chapter 10.

Token 16060:
19.5.4 Making discoveries with inductive logic programming An inverse resolution procedure that inverts a complete resolution strategy is, in principle, acomplete algorithm for learning ﬁrst-order theories.

Token 16061:
That is, if some unknown Hypothesis generates a set of examples, then an inverse resolution procedure can generate Hypothesis from the examples.

Token 16062:
This observation suggests an interesting possibility: Suppose that theavailable examples include a variety of trajectories of falling bodies.

Token 16063:
Would an inverse reso-lution program be theoretically capable of inferring the law of gravity?

Token 16064:
The answer is clearlyyes, because the law of gravity allows one to explain the examples, given suitable background mathematics.

Token 16065:
Similarly, one can imagine that electromagnetism, quantum mechanics, and the theory of relativity are also within the scope of ILP programs.

Token 16066:
Of course, they are also withinthe scope of a monkey with a typewriter; we still need better heuristics and new ways tostructure the search space.

Token 16067:
One thing that inverse resolution systems willdo for you is invent new predicates.

Token 16068:
This ability is often seen as somewhat magical, because computers are often thought of as “merely working with what they are given.” In fact, new predicates fall directly out of the inverseresolution step.

Token 16069:
The simplest case arises in hypothesizing two new clauses C 1andC2,g i v e n ac l a u s e C. The resolution of C1andC2eliminates a literal that the two clauses share; hence, it is quite possible that the eliminated literal contained a predicate that does not appear in C. Thus, when working backward, one possibility is to generate a new predicate from which to reconstruct the missing literal.

Token 16070:
Figure 19.14 shows an example in which the new predicate Pis generated in the process of learning a deﬁnition for Ancestor .

Token 16071:
Once generated, Pcan be used in later inverse resolu- tion steps. For example, a later step might hypothesize that Mother (x,y)⇒P(x,y).

Token 16072:
Thus, the new predicate Phas its meaning constrained by the generation of hypotheses that involve it.

Token 16073:
Another example might lead to the constraint Father (x,y)⇒P(x,y).

Token 16074:
In other words, the predicate Pis what we usually think of as the Parent relationship.

Token 16075:
As we mentioned earlier, the invention of new predicates can signiﬁcantly reduce the size of the deﬁnition ofthe goal predicate.

Token 16076:
Hence, by including the ability to invent new predicates, inverse resolutionsystems can often solve learning problems that are infeasible with other techniques.

Token 16077:
Some of the deepest revolutions in science come from the invention of new predicates and functions—for example, Galileo’s invention of acceleration or Joule’s invention of ther- mal energy.

Token 16078:
Once these terms are available, the discovery of new laws becomes (relatively)easy.

Token 16079:
The difﬁcult part lies in realizing that some new entity, with a speciﬁc relationshipto existing entities, will allow an entire body of observations to be explained with a much

Token 16080:
Section 19.6.

Token 16081:
Summary 797 {x/George }Father (x,y)P(x,y) > Father (George,y )Ancestor (George,y ) >P(George,y )Ancestor (George,y ) >¬ ¬ Figure 19.14 An inverse resolution step that generates a new predicate P. simpler and more elegant theory than previously existed.

Token 16082:
As yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their discoveries have been deemed publishable in the scientiﬁc literature.

Token 16083:
For example, in theJournal of Molecular Biology , Turcotte et al.

Token 16084:
(2001) describe the automated discovery of rules for protein folding by the ILP program P ROGOL .

Token 16085:
Many of the rules discovered by P ROGOL could have been derived from known principles, but most had not been previously published as part of a standard biological database.

Token 16086:
(See Figure 19.10 for an example.). In related work, Srinivasan et al.

Token 16087:
(1994) dealt with the problem of discovering molecular-structure- based rules for the mutagenicity of nitroaromatic compounds.

Token 16088:
These compounds are found inautomobile exhaust fumes.

Token 16089:
For 80% of the compounds in a standard database, it is possible toidentify four important features, and linear regression on these features outperforms ILP.

Token 16090:
Forthe remaining 20%, the features alone are not predictive, and ILP identiﬁes relationships thatallow it to outperform linear regression, neural nets, and decision trees.

Token 16091:
Most impressively,King et al.

Token 16092:
(2009) endowed a robot with the ability to perform molecular biology experiments and extended ILP techniques to include experiment design, thereby creating an autonomousscientist that actually discovered new knowledge about the functional genomics of yeast.

Token 16093:
Forall these examples it appears that the ability both to represent relations and to use background knowledge contribute to ILP’s high performance.

Token 16094:
The fact that the rules found by ILP can be interpreted by humans contributes to the acceptance of these techniques in biology journalsrather than just computer science journals.

Token 16095:
ILP has made contributions to other sciences besides biology.

Token 16096:
One of the most impor- tant is natural language processing, where ILP has been used to extract complex relationalinformation from text.

Token 16097:
These results are summarized in Chapter 23.

Token 16098:
19.6 S UMMARY This chapter has investigated various ways in which prior knowledge can help an agent tolearn from new experiences.

Token 16099:
Because much prior knowledge is expressed in terms of rela-tional models rather than attribute-based models, we have also covered systems that allowlearning of relational models.

Token 16100:
The important points are: •The use of prior knowledge in learning leads to a picture of cumulative learning ,i n which learning agents improve their learning ability as they acquire more knowledge.

Token 16101:
•Prior knowledge helps learning by eliminating otherwise consistent hypotheses and by

Token 16102:
798 Chapter 19. Knowledge in Learning “ﬁlling in” the explanation of examples, thereby allowing for shorter hypotheses.

Token 16103:
These contributions often result in faster learning from fewer examples.

Token 16104:
•Understanding the different logical roles played by prior knowledge, as expressed by entailment constraints , helps to deﬁne a variety of learning techniques.

Token 16105:
•Explanation-based learning (EBL) extracts general rules from single examples by ex- plaining the examples and generalizing the explanation.

Token 16106:
It provides a deductive method for turning ﬁrst-principles knowledge into useful, efﬁcient, special-purpose expertise.

Token 16107:
•Relevance-based learning (RBL) uses prior knowledge in the form of determinations to identify the relevant attributes, thereby generating a reduced hypothesis space and speeding up learning.

Token 16108:
RBL also allows deductive generalizations from single examples.

Token 16109:
•Knowledge-based inductive learning (KBIL) ﬁnds inductive hypotheses that explain sets of observations with the help of background knowledge.

Token 16110:
•Inductive logic programming (ILP) techniques perform KBIL on knowledge that is expressed in ﬁrst-order logic.

Token 16111:
ILP methods can learn relational knowledge that is notexpressible in attribute-based systems.

Token 16112:
•ILP can be done with a top-down approach of reﬁning a very general rule or through a bottom-up approach of inverting the deductive process.

Token 16113:
•ILP methods naturally generate new predicates with which concise new theories can be expressed and show promise as general-purpose scientiﬁc theory formation systems.

Token 16114:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Although the use of prior knowledge in learning would seem to be a natural topic for philoso-phers of science, little formal work was done until quite recently.

Token 16115:
Fact, Fiction, and Forecast , by the philosopher Nelson Goodman (1954), refuted the earlier supposition that inductionwas simply a matter of seeing enough examples of some universally quantiﬁed proposition and then adopting it as a hypothesis.

Token 16116:
Consider, for example, the hypothesis “All emeralds are grue,” where grue means “green if observed before time t, but blue if observed thereafter.” At any time up to t, we might have observed millions of instances conﬁrming the rule that emeralds are grue, and no disconﬁrming instances, and yet we are unwilling to adopt the rule.This can be explained only by appeal to the role of relevant prior knowledge in the inductionprocess.

Token 16117:
Goodman proposes a variety of different kinds of prior knowledge that might be use-ful, including a version of determinations called overhypotheses .

Token 16118:
Unfortunately, Goodman’s ideas were never pursued in machine learning. Thecurrent-best-hypothesis approach is an old idea in philosophy (Mill, 1843).

Token 16119:
Early work in cognitive psychology also suggested that it is a natural form of concept learning in humans (Bruner et al. , 1957).

Token 16120:
In AI, the approach is most closely associated with the work of Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning descriptions of complex objects.

Token 16121:
The version space method (Mitchell, 1977, 1982) takes a different approach, maintaining the set of allconsistent hypotheses and eliminating those found to be inconsistent with new examples.

Token 16122:
The approach was used in the Meta-D ENDRAL

Token 16123:


Token 16124:
Bibliographical and Historical Notes 799 expert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell’s (1983) LEXsystem, which learns to solve calculus problems.

Token 16125:
A third inﬂuential thread was formed by the work of Michalski and colleagues on the AQ series of algorithms, which learned setsof logical rules (Michalski, 1969; Michalski et al.

Token 16126:
, 1986). EBL had its roots in the techniques used by the S TRIPS planner (Fikes et al. , 1972).

Token 16127:
When a plan was constructed, a generalized version of it was saved in a plan library and used in later planning as a macro-operator .

Token 16128:
Similar ideas appeared in Anderson’s ACT* architecture, under the heading of knowledge compilation (Anderson, 1983), and in the SOAR architecture, as chunking (Laird et al.

Token 16129:
, 1986).

Token 16130:
Schema acquisition (DeJong, 1981), analytical generalization (Mitchell, 1982), and constraint-based generalization (Minton, 1984) were immediate precursors of the rapid growth of interest in EBL stimulated by thepapers of Mitchell et al.

Token 16131:
(1986) and DeJong and Mooney (1986).

Token 16132:
Hirsh (1987) introduced the EBL algorithm described in the text, showing how it could be incorporated directly into alogic programming system.

Token 16133:
Van Harmelen and Bundy (1988) explain EBL as a variant of thepartial evaluation method used in program analysis systems (Jones et al. , 1993).

Token 16134:
Initial enthusiasm for EBL was tempered by Minton’s ﬁnding (1988) that, without ex- tensive extra work, EBL could easily slow down a program signiﬁcantly.

Token 16135:
Formal probabilistic analysis of the expected payoff of EBL can be found in Greiner (1989) and Subramanian and Feldman (1990).

Token 16136:
An excellent survey of early work on EBL appears in Dietterich (1990).

Token 16137:
Instead of using examples as foci for generalization, one can use them directly to solve new problems, in a process known as analogical reasoning .

Token 16138:
This form of reasoning ranges ANALOGICAL REASONING from a form of plausible reasoning based on degree of similarity (Gentner, 1983), through a form of deductive inference based on determinations but requiring the participation of the example (Davies and Russell, 1987), to a form of “lazy” EBL that tailors the direction ofgeneralization of the old example to ﬁt the needs of the new problem.

Token 16139:
This latter form ofanalogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993) andderivational analogy (Veloso and Carbonell, 1993).

Token 16140:
Relevance information in the form of functional dependencies was ﬁrst developed in the database community, where it is used to structure large sets of attributes into manage- able subsets.

Token 16141:
Functional dependencies were used for analogical reasoning by Carbonelland Collins (1973) and rediscovered and given a full logical analysis by Davies and Rus-sell (Davies, 1985; Davies and Russell, 1987).

Token 16142:
Their role as prior knowledge in inductivelearning was explored by Russell and Grosof (1987).

Token 16143:
The equivalence of determinations toa restricted-vocabulary hypothesis space was proved in Russell (1988).

Token 16144:
Learning algorithmsfor determinations and the improved performance obtained by RBDTL were ﬁrst shown inthe F OCUS algorithm, due to Almuallim and Dietterich (1991).

Token 16145:
Tadepalli (1993) describes a very ingenious algorithm for learning with determinations that shows large improvements inlearning speed.

Token 16146:
The idea that inductive learning can be performed by inverse deduction can be traced to W. S. Jevons (1874), who wrote, “The study both of Formal Logic and of the Theory of Probabilities has led me to adopt the opinion that there is no such thing as a distinct methodof induction as contrasted with deduction, but that induction is simply an inverse employ-ment of deduction.” Computational investigations began with the remarkable Ph.D. thesis by

Token 16147:
800 Chapter 19. Knowledge in Learning Gordon Plotkin (1971) at Edinburgh.

Token 16148:
Although Plotkin developed many of the theorems and methods that are in current use in ILP, he was discouraged by some undecidability results forcertain subproblems in induction.

Token 16149:
MIS (Shapiro, 1981) reintroduced the problem of learninglogic programs, but was seen mainly as a contribution to the theory of automated debug-ging.

Token 16150:
Work on rule induction, such as the ID3 (Quinlan, 1986) and CN2 (Clark and Niblett, 1989) systems, led to F OIL(Quinlan, 1990), which for the ﬁrst time allowed practical induc- tion of relational rules.

Token 16151:
The ﬁeld of relational learning was reinvigorated by Muggleton andBuntine (1988), whose C IGOL program incorporated a slightly incomplete version of inverse resolution and was capable of generating new predicates.

Token 16152:
The inverse resolution method alsoappears in (Russell, 1986), with a simple algorithm given in a footnote.

Token 16153:
The next major sys-tem was G OLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on Plotkin’s concept of relative least general generalization.

Token 16154:
I TOU (Rouveirol and Puget, 1989) and C LINT (De Raedt, 1992) were other systems of that era.

Token 16155:
More recently, P ROGOL (Mug- gleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailmentand has been applied to a number of practical problems, particularly in biology and naturallanguage processing.

Token 16156:
Muggleton (2000) describes an extension of P ROGOL to handle uncer- tainty in the form of stochastic logic programs.

Token 16157:
A formal analysis of ILP methods appears in Muggleton (1991), a large collection of papers in Muggleton (1992), and a collection of techniques and applications in the bookby Lavra uca n dD uzeroski (1994).

Token 16158:
Page and Srinivasan (2002) give a more recent overview of the ﬁeld’s history and challenges for the future.

Token 16159:
Early complexity results by Haussler (1989) suggested that learning ﬁrst-order sentences was intractible.

Token 16160:
However, with better understand- ing of the importance of syntactic restrictions on clauses, positive results have been obtained even for clauses with recursion (D uzeroski et al.

Token 16161:
, 1992). Learnability results for ILP are surveyed by Kietz and D uzeroski (1994) and Cohen and Page (1995).

Token 16162:
Although ILP now seems to be the dominant approach to constructive induction, it has not been the only approach taken.

Token 16163:
So-called discovery systems aim to model the process DISCOVERY SYSTEM of scientiﬁc discovery of new concepts, usually by a direct search in the space of concept deﬁnitions.

Token 16164:
Doug Lenat’s Automated Mathematician, or AM (Davis and Lenat, 1982), used discovery heuristics expressed as expert system rules to guide its search for concepts andconjectures in elementary number theory.

Token 16165:
Unlike most systems designed for mathematicalreasoning, AM lacked a concept of proof and could only make conjectures.

Token 16166:
It rediscoveredGoldbach’s conjecture and the Unique Prime Factorization theorem.

Token 16167:
AM’s architecture wasgeneralized in the E URISKO system (Lenat, 1983) by adding a mechanism capable of rewrit- ing the system’s own discovery heuristics.

Token 16168:
E URISKO was applied in a number of areas other than mathematical discovery, although with less success than AM.

Token 16169:
The methodology of AMand E URISKO has been controversial (Ritchie and Hanna, 1984; Lenat and Brown, 1984).

Token 16170:
Another class of discovery systems aims to operate with real scientiﬁc data to ﬁnd new laws. The systems D ALTON ,GLAUBER ,a n dS TAHL (Langley et al.

Token 16171:
, 1987) are rule-based systems that look for quantitative relationships in experimental data from physical systems; in each case, the system has been able to recapitulate a well-known discovery from the his-tory of science.

Token 16172:
Discovery systems based on probabilistic techniques—especially clusteringalgorithms that discover new categories—are discussed in Chapter 20.

Token 16173:


Token 16174:
Exercises 801 EXERCISES 19.1 Show, by translating into conjunctive normal form and applying resolution, that the conclusion drawn on page 784 concerning Brazilians is sound.

Token 16175:
19.2 For each of the following determinations, write down the logical representation and explain why the determination is true (if it is): a.

Token 16176:
Design and denomination determine the mass of a coin. b. For a given program, input determines output.

Token 16177:
c. Climate, food intake, exercise, and metabolism determine weight gain and loss.

Token 16178:
d. Baldness is determined by the baldness (or lack thereof) of one’s maternal grandfather.

Token 16179:
19.3 Would a probabilistic version of determinations be useful? Suggest a deﬁnition.

Token 16180:
19.4 Fill in the missing values for the clauses C1orC2(or both) in the following sets of clauses, given that Cis the resolvent of C1andC2: a.C=True⇒P(A,B),C1=P(x,y)⇒Q(x,y),C2=??.

Token 16181:
b.C=True⇒P(A,B),C1=??,C2=??. c.C=P(x,y)⇒P(x,f(y)),C1=??,C2=??.

Token 16182:
If there is more than one possible solution, provide one example of each different kind.

Token 16183:
19.5 Suppose one writes a logic program that carries out a resolution inference step.

Token 16184:
That is, letResolve (c1,c2,c)succeed if cis the result of resolving c1andc2.

Token 16185:
Normally, Resolve would be used as part of a theorem prover by calling it with c1andc2instantiated to par- ticular clauses, thereby generating the resolvent c. Now suppose instead that we call it with cinstantiated and c1andc2uninstantiated.

Token 16186:
Will this succeed in generating the appropriate results of an inverse resolution step?

Token 16187:
Would you need any special modiﬁcations to the logicprogramming system for this to work?

Token 16188:
19.6 Suppose that F OILis considering adding a literal to a clause using a binary predicate Pand that previous literals (including the head of the clause) contain ﬁve different variables.

Token 16189:
a. How many functionally different literals can be generated?

Token 16190:
Two literals are functionally identical if they differ only in the names of the newvariables that they contain. b.

Token 16191:
Can you ﬁnd a general formula for the number of different literals with a predicate of arityrwhen there are nvariables previously used?

Token 16192:
c. Why does F OILnot allow literals that contain no previously used variables?

Token 16193:
19.7 Using the data from the family tree in Figure 19.11, or a subset thereof, apply the F OIL algorithm to learn a deﬁnition for the Ancestor predicate.

Token 16194:
20LEARNING PROBABILISTIC MODELS In which we view learning as a form of uncertain reasoning from observations.

Token 16195:
Chapter 13 pointed out the prevalence of uncertainty in real environments.

Token 16196:
Agents can handle uncertainty by using the methods of probability and decision theory, but ﬁrst they must learntheir probabilistic theories of the world from experience.

Token 16197:
This chapter explains how theycan do that, by formulating the learning task itself as a process of probabilistic inference(Section 20.1).

Token 16198:
We will see that a Bayesian view of learning is extremely powerful, providinggeneral solutions to the problems of noise, overﬁtting, and optimal prediction.

Token 16199:
It also takesinto account the fact that a less-than-omniscient agent can never be certain about which theoryof the world is correct, yet must still make decisions by using some theory of the world.

Token 16200:
We describe methods for learning probability models—primarily Bayesian networks— in Sections 20.2 and 20.3.

Token 16201:
Some of the material in this chapter is fairly mathematical, al-though the general lessons can be understood without plunging into the details.

Token 16202:
It may beneﬁtthe reader to review Chapters 13 and 14 and peek at Appendix A.

Token 16203:
20.1 S TATISTICAL LEARNING The key concepts in this chapter, just as in Chapter 18, are data andhypotheses .

Token 16204:
Here, the data are evidence —that is, instantiations of some or all of the random variables describing the domain.

Token 16205:
The hypotheses in this chapter are probabilistic theories of how the domain works,including logical theories as a special case.

Token 16206:
Consider a simple example. Our favorite Surprise candy comes in two ﬂavors: cherry (yum) and lime (ugh).

Token 16207:
The manufacturer has a peculiar sense of humor and wraps each pieceof candy in the same opaque wrapper, regardless of ﬂavor.

Token 16208:
The candy is sold in very large bags, of which there are known to be ﬁve kinds—again, indistinguishable from the outside: h 1: 100% cherry, h2: 75% cherry + 25% lime, h3: 50% cherry + 50% lime, h4: 25% cherry + 75% lime, h5: 100% lime .

Token 16209:
802

Token 16210:
Section 20.1.

Token 16211:
Statistical Learning 803 Given a new bag of candy, the random variable H(for hypothesis ) denotes the type of the bag, with possible values h1through h5.His not directly observable, of course.

Token 16212:
As the pieces of candy are opened and inspected, data are revealed— D1,D2,...,DN, where each Diis a random variable with possible values cherry andlime .

Token 16213:
The basic task faced by the agent is to predict the ﬂavor of the next piece of candy.1Despite its apparent triviality, this scenario serves to introduce many of the major issues.

Token 16214:
The agent really does need to infer a theory of its world, albeit a very simple one.

Token 16215:
Bayesian learning simply calculates the probability of each hypothesis, given the data, BAYESIAN LEARNING and makes predictions on that basis.

Token 16216:
That is, the predictions are made by using allthe hy- potheses, weighted by their probabilities, rather than by using just a single “best” hypothesis.In this way, learning is reduced to probabilistic inference.

Token 16217:
Let Drepresent all the data, with observed value d; then the probability of each hypothesis is obtained by Bayes’ rule: P(h i|d)=αP(d|hi)P(hi).

Token 16218:
(20.1) Now, suppose we want to make a prediction about an unknown quantity X.T h e nw eh a v e P(X|d)=/summationdisplay iP(X|d,hi)P(hi|d)=/summationdisplay iP(X|hi)P(hi|d), (20.2) where we have assumed that each hypothesis determines a probability distribution over X.

Token 16219:
This equation shows that predictions are weighted averages over the predictions of the indi-vidual hypotheses.

Token 16220:
The hypotheses themselves are essentially “intermediaries” between theraw data and the predictions.

Token 16221:
The key quantities in the Bayesian approach are the hypothesis prior ,P(h i),a n dt h e likelihood of the data under each hypothesis, P(d|hi).

Token 16222:
HYPOTHESIS PRIOR LIKELIHOOD For our candy example, we will assume for the time being that the prior distribution overh1,...,h 5is given by/angbracketleft0.1,0.2,0.4,0.2,0.1/angbracketright, as advertised by the manufacturer.

Token 16223:
The likelihood of the data is calculated under the assumption that the observations are i.i.d.

Token 16224:
(see page 708), so that P(d|hi)=/productdisplay jP(dj|hi).

Token 16225:
(20.3) For example, suppose the bag is really an all-lime bag ( h5) and the ﬁrst 10 candies are all lime; then P(d|h3)is0.510, because half the candies in an h3bag are lime.2Figure 20.1(a) shows how the posterior probabilities of the ﬁve hypotheses change as the sequence of 10lime candies is observed.

Token 16226:
Notice that the probabilities start out at their prior values, so h 3 is initially the most likely choice and remains so after 1 lime candy is unwrapped.

Token 16227:
After 2 lime candies are unwrapped, h4is most likely; after 3 or more, h5(the dreaded all-lime bag) is the most likely.

Token 16228:
After 10 in a row, we are fairly certain of our fate.

Token 16229:
Figure 20.1(b) showsthe predicted probability that the next candy is lime, based on Equation (20.2).

Token 16230:
As we would expect, it increases monotonically toward 1.

Token 16231:
1Statistically sophisticated readers will r ecognize this scenario as a variant of the urn-and-ball setup.

Token 16232:
We ﬁnd urns and balls less compelling than candy; furthermore, candy lends itself to other tasks, such as deciding whether to trade the bag with a friend—see Exercise 20.2.

Token 16233:
2We stated earlier that the bags of candy are very large; otherwise, the i.i.d. assumption fails to hold.

Token 16234:
Technically, it is more correct (but less hygienic) to rewrap each candy after inspection and return it to the bag.

Token 16235:
804 Chapter 20.

Token 16236:
Learning Probabilistic Models 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10Posterior probability of hypothesis Number of observations in dP(h1 | d) P(h2 | d) P(h3 | d) P(h4 | d) P(h5 | d) 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10Probability that next candy is lime Number of observations in d (a) (b) Figure 20.1 (a) Posterior probabilities P(hi|d1,...,d N)from Equation (20.1).

Token 16237:
The number of observations Nranges from 1 to 10, and each observation is of a lime candy.

Token 16238:
(b) Bayesian prediction P(dN+1=lime|d1,...,d N)from Equation (20.2).

Token 16239:
The example shows that the Bayesian prediction eventually agrees with the true hy- pothesis. This is characteristic of Bayesian learning.

Token 16240:
For any ﬁxed prior that does not rule out the true hypothesis, the posterior probability of any false hypothesis will, under certaintechnical conditions, eventually vanish.

Token 16241:
This happens simply because the probability of gen-erating “uncharacteristic” data indeﬁnitely is vanishingly small.

Token 16242:
(This point is analogous toone made in the discussion of PAC learning in Chapter 18.)

Token 16243:
More important, the Bayesianprediction is optimal , whether the data set be small or large.

Token 16244:
Given the hypothesis prior, any other prediction is expected to be correct less often.

Token 16245:
The optimality of Bayesian learning comes at a price, of course.

Token 16246:
For real learning problems, the hypothesis space is usually very large or inﬁnite, as we saw in Chapter 18.

Token 16247:
In some cases, the summation in Equation (20.2) (or integration, in the continuous case) can becarried out tractably, but in most cases we must resort to approximate or simpliﬁed methods.

Token 16248:
A very common approximation—one that is usually adopted in science—is to make pre- dictions based on a single most probable hypothesis—that is, an h ithat maximizes P(hi|d).

Token 16249:
This is often called a maximum a posteriori or MAP (pronounced “em-ay-pee”) hypothesis.MAXIMUM A POSTERIORI Predictions made according to an MAP hypothesis hMAP are approximately Bayesian to the extent that P(X|d)≈P(X|hMAP).

Token 16250:
In our candy example, hMAP=h5after three lime can- dies in a row, so the MAP learner then predicts that the fourth candy is lime with probability1.0—a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Fig-ure 20.1(b).

Token 16251:
As more data arrive, the MAP and Bayesian predictions become closer, because the competitors to the MAP hypothesis become less and less probable.

Token 16252:
Although our example doesn’t show it, ﬁnding MAP hypotheses is often much easier than Bayesian learning, because it requires solving an optimization problem instead of a largesummation (or integration) problem.

Token 16253:
We will see examples of this later in the chapter.

Token 16254:
Section 20.1. Statistical Learning 805 In both Bayesian learning and MAP learning, the hypothesis prior P(hi)plays an im- portant role.

Token 16255:
We saw in Chapter 18 that overﬁtting can occur when the hypothesis space is too expressive, so that it contains many hypotheses that ﬁt the data set well.

Token 16256:
Rather thanplacing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learningmethods use the prior to penalize complexity .

Token 16257:
Typically, more complex hypotheses have a lower prior probability—in part because there are usually many more complex hypotheses than simple hypotheses.

Token 16258:
On the other hand, more complex hypotheses have a greater capac-ity to ﬁt the data.

Token 16259:
(In the extreme case, a lookup table can reproduce the data exactly withprobability 1.)

Token 16260:
Hence, the hypothesis prior embodies a tradeoff between the complexity of ahypothesis and its degree of ﬁt to the data.

Token 16261:
We can see the effect of this tradeoff most clearly in the logical case, where Hcontains only deterministic hypotheses.

Token 16262:
In that case, P(d|h i)is 1 if hiis consistent and 0 otherwise.

Token 16263:
Looking at Equation (20.1), we see that hMAP will then be the simplest logical theory that is consistent with the data.

Token 16264:
Therefore, maximum a posteriori learning provides a natural embodiment of Ockham’s razor.

Token 16265:
Another insight into the tradeoff between complexity and degree of ﬁt is obtained by taking the logarithm of Equation (20.1).

Token 16266:
Choosing hMAP to maximize P(d|hi)P(hi)is equivalent to minimizing −log2P(d|hi)−log2P(hi).

Token 16267:
Using the connection between information encoding and probability that we introduced in Chapter 18.3.4, we see that the −log2P(hi)term equals the number of bits required to spec- ify the hypothesis hi.F u r t h e r m o r e , −log2P(d|hi)is the additional number of bits required to specify the data, given the hypothesis.

Token 16268:
(To see this, consider that no bits are requiredif the hypothesis predicts the data exactly—as with h 5and the string of lime candies—and log21=0 .)

Token 16269:
Hence, MAP learning is choosing the hypothesis that provides maximum com- pression of the data.

Token 16270:
The same task is addressed more directly by the minimum description length , or MDL, learning method.

Token 16271:
Whereas MAP learning expresses simplicity by assigning higher probabilities to simpler hypotheses, MDL expresses it directly by counting the bits in a binary encoding of the hypotheses and data.

Token 16272:
A ﬁnal simpliﬁcation is provided by assuming a uniform prior over the space of hy- potheses.

Token 16273:
In that case, MAP learning reduces to choosing an hithat maximizes P(d|hi). This is called a maximum-likelihood (ML) hypothesis, hML.

Token 16274:
Maximum-likelihood learningMAXIMUM- LIKELIHOOD is very common in statistics, a discipline in which many researchers distrust the subjective nature of hypothesis priors.

Token 16275:
It is a reasonable approach when there is no reason to prefer onehypothesis over another ap r i o r i —for example, when all hypotheses are equally complex.

Token 16276:
It provides a good approximation to Bayesian and MAP learning when the data set is large, because the data swamps the prior distribution over hypotheses, but it has problems (as we shall see) with small data sets.

Token 16277:
806 Chapter 20.

Token 16278:
Learning Probabilistic Models 20.2 L EARNING WITH COMPLETE DATA The general task of learning a probability model, given data that are assumed to be generated from that model, is called density estimation .

Token 16279:
(The term applied originally to probability DENSITYESTIMATION density functions for continuous variables, but is used now for discrete distributions too.)

Token 16280:
This section covers the simplest case, where we have complete data .

Token 16281:
Data are com- COMPLETE DATA plete when each data point contains values for every variable in the probability model being learned.

Token 16282:
We focus on parameter learning —ﬁnding the numerical parameters for a proba-PARAMETER LEARNING bility model whose structure is ﬁxed.

Token 16283:
For example, we might be interested in learning the conditional probabilities in a Bayesian network with a given structure.

Token 16284:
We will also look brieﬂy at the problem of learning structure and at nonparametric density estimation.

Token 16285:
20.2.1 Maximum-likelihood parameter learning: Discrete models Suppose we buy a bag of lime and cherry candy from a new manufacturer whose lime–cherry proportions are completely unknown; the fraction could be anywhere between 0 and 1.

Token 16286:
Inthat case, we have a continuum of hypotheses.

Token 16287:
The parameter in this case, which we call θ, is the proportion of cherry candies, and the hypothesis is h θ. (The proportion of limes is just1−θ.)

Token 16288:
If we assume that all proportions are equally likely ap r i o r i , then a maximum- likelihood approach is reasonable.

Token 16289:
If we model the situation with a Bayesian network, weneed just one random variable, Flavor (the ﬂavor of a randomly chosen candy from the bag).

Token 16290:
It has values cherry andlime , where the probability of cherry isθ(see Figure 20.2(a)).

Token 16291:
Now suppose we unwrap Ncandies, of which care cherries and /lscript=N−care limes.

Token 16292:
According to Equation (20.3), the likelihood of this particular data set is P(d|h θ)=N/productdisplay j=1P(dj|hθ)=θc·(1−θ)/lscript.

Token 16293:
The maximum-likelihood hypothesis is given by the value of θthat maximizes this expres- sion.

Token 16294:
The same value is obtained by maximizing the log likelihood , LOG LIKELIHOOD L(d|hθ)=l o g P(d|hθ)=N/summationdisplay j=1logP(dj|hθ)=clogθ+/lscriptlog(1−θ).

Token 16295:
(By taking logarithms, we reduce the product to a sum over the data, which is usually easier to maximize.)

Token 16296:
To ﬁnd the maximum-likelihood value of θ, we differentiate Lwith respect to θand set the resulting expression to zero: dL(d|hθ) dθ=c θ−/lscript 1−θ=0⇒θ=c c+/lscript=c N. In English, then, the maximum-likelihood hypothesis hMLasserts that the actual proportion of cherries in the bag is equal to the observed proportion in the candies unwrapped so far!

Token 16297:
It appears that we have done a lot of work to discover the obvious.

Token 16298:
In fact, though, we have laid out one standard method for maximum-likelihood parameter learning, a methodwith broad applicability:

Token 16299:
Section 20.2.

Token 16300:
Learning with Complete Data 807 FlavorP(F=cherry ) (a)θP(F=cherry ) Flavor Wrapper (b)θ F cherry limeP(W=red | F) θ1 θ2 Figure 20.2 (a) Bayesian network model for the case of candies with an unknown propor- tion of cherries and limes.

Token 16301:
(b) Model for the c ase where the wrapper color depends (proba- bilistically) on the candy ﬂavor. 1.

Token 16302:
Write down an expression for the likelihood of the data as a function of the parameter(s). 2.

Token 16303:
Write down the derivative of the log likelihood with respect to each parameter.3. Find the parameter values such that the derivatives are zero.

Token 16304:
The trickiest step is usually the last.

Token 16305:
In our example, it was trivial, but we will see that in many cases we need to resort to iterative solution algorithms or other numerical optimizationtechniques, as described in Chapter 4.

Token 16306:
The example also illustrates a signiﬁcant problem with maximum-likelihood learning in general: when the data set is small enough that some events have not yet been observed—for instance, no cherry candies—the maximum-likelihood hypothesis assigns zero probability to those events.

Token 16307:
Various tricks are used to avoid this problem, such as initializing the counts for each event to 1 instead of 0. Let us look at another example.

Token 16308:
Suppose this new candy manufacturer wants to give a little hint to the consumer and uses candy wrappers colored red and green.

Token 16309:
The Wrapper for each candy is selected probabilistically , according to some unknown conditional distribution, depending on the ﬂavor.

Token 16310:
The corresponding probability model is shown in Figure 20.2(b).Notice that it has three parameters: θ,θ 1,a n dθ2.

Token 16311:
With these parameters, the likelihood of seeing, say, a cherry candy in a green wrapper can be obtained from the standard semanticsfor Bayesian networks (page 513): P(Flavor =cherry ,Wrapper =green|h θ,θ1,θ2) =P(Flavor =cherry|hθ,θ1,θ2)P(Wrapper =green|Flavor =cherry ,hθ,θ1,θ2) =θ·(1−θ1).

Token 16312:
Now we unwrap Ncandies, of which care cherries and /lscriptare limes.

Token 16313:
The wrapper counts are as follows: rcof the cherries have red wrappers and gchave green, while r/lscriptof the limes have red and g/lscripthave green.

Token 16314:
The likelihood of the data is given by P(d|hθ,θ1,θ2)=θc(1−θ)/lscript·θrc 1(1−θ1)gc·θr/lscript 2(1−θ2)g/lscript.

Token 16315:
808 Chapter 20.

Token 16316:
Learning Probabilistic Models This looks pretty horrible, but taking logarithms helps: L=[clogθ+/lscriptlog(1−θ)] + [rclogθ1+gclog(1−θ1)] + [r/lscriptlogθ2+g/lscriptlog(1−θ2)].

Token 16317:
The beneﬁt of taking logs is clear: the log likelihood is the sum of three terms, each of which contains a single parameter.

Token 16318:
When we take derivatives with respect to each parameter and setthem to zero, we get three independent equations, each containing just one parameter: ∂L ∂θ=c θ−/lscript 1−θ=0⇒θ=c c+/lscript∂L ∂θ1=rc θ1−gc 1−θ1=0⇒θ1=rc rc+gc∂L ∂θ2=r/lscript θ2−g/lscript 1−θ2=0⇒θ2=r/lscript r/lscript+g/lscript.

Token 16319:
The solution for θis the same as before.

Token 16320:
The solution for θ1, the probability that a cherry candy has a red wrapper, is the observed fraction of cherry candies with red wrappers, andsimilarly for θ 2.

Token 16321:
These results are very comforting, and it is easy to see that they can be extended to any Bayesian network whose conditional probabilities are represented as tables.

Token 16322:
The most impor-tant point is that, with complete data, the maximum-likelihood parameter learning problem for a Bayesian network decomposes into separate learning problems, one for each parameter.

Token 16323:
(See Exercise 20.6 for the nontabulated case, where each parameter affects several conditional probabilities.)

Token 16324:
The second point is that the parameter values for a variable, given its parents,are just the observed frequencies of the variable values for each setting of the parent values.As before, we must be careful to avoid zeroes when the data set is small.

Token 16325:
20.2.2 Naive Bayes models Probably the most common Bayesian network model used in machine learning is the naive Bayes model ﬁrst introduced on page 499.

Token 16326:
In this model, the “class” variable C(which is to be predicted) is the root and the “attribute” variables Xiare the leaves.

Token 16327:
The model is “naive” because it assumes that the attributes are conditionally independent of each other, given the class.

Token 16328:
(The model in Figure 20.2(b) is a naive Bayes model with class Flavor and just one attribute, Wrapper .)

Token 16329:
Assuming Boolean variables, the parameters are θ=P(C=true),θi1=P(Xi=true|C=true),θi2=P(Xi=true|C=false).

Token 16330:
The maximum-likelihood parameter values are found in exactly the same way as for Fig- ure 20.2(b).

Token 16331:
Once the model has been trained in this way, it can be used to classify new exam-ples for which the class variable Cis unobserved.

Token 16332:
With observed attribute values x 1,...,x n, the probability of each class is given by P(C|x1,...,x n)=αP(C)/productdisplay iP(xi|C).

Token 16333:
A deterministic prediction can be obtained by choosing the most likely class.

Token 16334:
Figure 20.3 shows the learning curve for this method when it is applied to the restaurant problem from Chapter 18.

Token 16335:
The method learns fairly well but not as well as decision-tree learning; this is presumably because the true hypothesis—which is a decision tree—is not representable ex-actly using a naive Bayes model.

Token 16336:
Naive Bayes learning turns out to do surprisingly well in awide range of applications; the boosted version (Exercise 20.4) is one of the most effective

Token 16337:
Section 20.2.

Token 16338:
Learning with Complete Data 809 0.40.50.60.70.80.91 0 20 40 60 80 100Proportion correct on test set Training set sizeDecision tree Naive Bayes Figure 20.3 The learning curve for naive Bayes learning applied to the restaurant problem from Chapter 18; the learning curve for decision-tree learning is shown for comparison.

Token 16339:
general-purpose learning algorithms.

Token 16340:
Naive Bayes learning scales well to very large prob- lems: with nBoolean attributes, there are just 2n+1parameters, and no search is required to ﬁnd hML, the maximum-likelihood naive Bayes hypothesis.

Token 16341:
Finally, naive Bayes learning systems have no difﬁculty with noisy or missing data and can give probabilistic predictions when appropriate.

Token 16342:
20.2.3 Maximum-likelihood parameter learning: Continuous models Continuous probability models such as the linear Gaussian model were introduced in Sec- tion 14.3.

Token 16343:
Because continuous variables are ubiquitous in real-world applications, it is impor-tant to know how to learn the parameters of continuous models from data.

Token 16344:
The principles formaximum-likelihood learning are identical in the continuous and discrete cases.

Token 16345:
Let us begin with a very simple case: learning the parameters of a Gaussian density function on a single variable.

Token 16346:
That is, the data are generated as follows: P(x)=1 √ 2πσe−(x−μ)2 2σ2. The parameters of this model are the mean μand the standard deviation σ.

Token 16347:
(Notice that the normalizing “constant” depends on σ, so we cannot ignore it.)

Token 16348:
Let the observed values be x1,...,x N. Then the log likelihood is L=N/summationdisplay j=1log1 √ 2πσe−(xj−μ)2 2σ2=N(−log√ 2π−logσ)−N/summationdisplay j=1(xj−μ)2 2σ2.

Token 16349:
Setting the derivatives to zero as usual, we obtain ∂L ∂μ=−1 σ2/summationtextN j=1(xj−μ)=0 ⇒μ=P jxj N ∂L ∂σ=−N σ+1 σ3/summationtextN j=1(xj−μ)2=0⇒σ=/radicalBig P j(xj−μ)2 N.(20.4) That is, the maximum-likelihood value of the mean is the sample average and the maximum- likelihood value of the standard deviation is the square root of the sample variance.

Token 16350:
Again,these are comforting results that conﬁrm “commonsense” practice.

Token 16351:
810 Chapter 20.

Token 16352:
Learning Probabilistic Models 00.20.40.60.81 x00.20.40.60.81 y00.511.522.533.54P(y |x) 00.20.40.60.81 00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1y x (a) (b) Figure 20.4 (a) A linear Gaussian model described as y=θ1x+θ2plus Gaussian noise with ﬁxed variance.

Token 16353:
(b) A set of 50 data points generated from this model. Now consider a linear Gaussian model with one continuous parent Xand a continuous childY.

Token 16354:
As explained on page 520, Yhas a Gaussian distribution whose mean depends linearly on the value of Xand whose standard deviation is ﬁxed.

Token 16355:
To learn the conditional distribution P(Y|X), we can maximize the conditional likelihood P(y|x)=1 √ 2πσe−(y−(θ1x+θ2))2 2σ2 .

Token 16356:
(20.5) Here, the parameters are θ1,θ2,a n dσ. The data are a collection of (xj,yj)pairs, as illustrated in Figure 20.4.

Token 16357:
Using the usual methods (Exercise 20.5), we can ﬁnd the maximum-likelihood values of the parameters. The point here is different.

Token 16358:
If we consider just the parameters θ1 andθ2that deﬁne the linear relationship between xandy, it becomes clear that maximizing the log likelihood with respect to these parameters is the same as minimizing the numerator (y−(θ1x+θ2))2in the exponent of Equation (20.5).

Token 16359:
This is the L2loss, the squared er- ror between the actual value yand the prediction θ1x+θ2.

Token 16360:
This is the quantity minimized by the standard linear regression procedure described in Section 18.6.

Token 16361:
Now we can under- stand why: minimizing the sum of squared errors gives the maximum-likelihood straight-linemodel, provided that the data are generated with Gaussian noise of ﬁxed variance .

Token 16362:
20.2.4 Bayesian parameter learning Maximum-likelihood learning gives rise to some very simple procedures, but it has some serious deﬁciencies with small data sets.

Token 16363:
For example, after seeing one cherry candy, the maximum-likelihood hypothesis is that the bag is 100% cherry (i.e., θ=1.0).

Token 16364:
Unless one’s hypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable conclusion.

Token 16365:
It is more likely that the bag is a mixture of lime and cherry.

Token 16366:
The Bayesian approach to parameter learning starts by deﬁning a prior probability distribution over the possible hypotheses.

Token 16367:
We call this the hypothesis prior . Then, as data arrives, the posterior HYPOTHESIS PRIOR probability distribution is updated.

Token 16368:
Section 20.2.

Token 16369:
Learning with Complete Data 811 00.511.522.5 0 0.2 0.4 0.6 0.8 1P(Θ = θ) Parameter θ[1,1][2,2][5,5] 0123456 0 0.2 0.4 0.6 0.8 1P(Θ = θ) Parameter θ[3,1][6,2][30,10] (a) (b) Figure 20.5 Examples of the beta[a,b]distribution for different values of [a,b].

Token 16370:
The candy example in Figure 20.2(a) has one parameter, θ: the probability that a ran- domly selected piece of candy is cherry-ﬂavored.

Token 16371:
In the Bayesian view, θis the (unknown) value of a random variable Θthat deﬁnes the hypothesis space; the hypothesis prior is just the prior distribution P(Θ).

Token 16372:
Thus, P(Θ =θ)is the prior probability that the bag has a fraction θof cherry candies.

Token 16373:
If the parameter θcan be any value between 0 and 1, then P(Θ)must be a continuous distribution that is nonzero only between 0 and 1 and that integrates to 1.

Token 16374:
The uniform densityP(θ)=Uniform [0,1](θ)is one candidate. (See Chapter 13.)

Token 16375:
It turns out that the uniform density is a member of the family of beta distributions .

Token 16376:
Each beta distribution is deﬁned by BETA DISTRIBUTION twohyperparameters3aandbsuch that HYPERPARAMETER beta[a,b](θ)=αθa−1(1−θ)b−1, (20.6) forθin the range [0,1].

Token 16377:
The normalization constant α, which makes the distribution integrate to 1, depends on aandb. (See Exercise 20.7.)

Token 16378:
Figure 20.5 shows what the distribution looks like for various values of aandb.

Token 16379:
The mean value of the distribution is a/(a+b),s ol a r g e r values of asuggest a belief that Θis closer to 1 than to 0.

Token 16380:
Larger values of a+bmake the distribution more peaked, suggesting greater certainty about the value of Θ.

Token 16381:
Thus, the beta family provides a useful range of possibilities for the hypothesis prior.

Token 16382:
Besides its ﬂexibility, the beta family has another wonderful property: if Θhas a prior beta[a,b], then, after a data point is observed, the posterior distribution for Θis also a beta distribution.

Token 16383:
In other words, beta is closed under update.

Token 16384:
The beta family is called the conjugate prior for the family of distributions for a Boolean variable.4Let’s see how this CONJUGATE PRIOR works.

Token 16385:
Suppose we observe a cherry candy; then we have 3They are called hyperparameters because they parameterize a distribution over θ, which is itself a parameter.

Token 16386:
4Other conjugate priors include the Dirichlet family for the parameters of a discrete multivalued distribution and the Normal–Wishart family for the parameters of a Gaussian distribution.

Token 16387:
See Bernardo and Smith (1994).

Token 16388:
812 Chapter 20.

Token 16389:
Learning Probabilistic Models Flavor 1 Wrapper 1Flavor 2 Wrapper 2Flavor 3 Wrapper 3Θ Θ1 Θ2 Figure 20.6 A Bayesian network that corresponds to a Bayesian learning process.

Token 16390:
Poste- rior distributions for the parameter variables Θ,Θ1,a n dΘ2can be inferred from their prior distributions and the evidence in the Flavor iandWrapper ivariables.

Token 16391:
P(θ|D1=cherry )=αP(D1=cherry|θ)P(θ) =α/primeθ·beta[a,b](θ)=α/primeθ·θa−1(1−θ)b−1 =α/primeθa(1−θ)b−1=b e t a [ a+1,b](θ).

Token 16392:
Thus, after seeing a cherry candy, we simply increment the aparameter to get the posterior; similarly, after seeing a lime candy, we increment the bparameter.

Token 16393:
Thus, we can view the a andbhyperparameters as virtual counts , in the sense that a prior beta[a,b]behaves exactly VIRTUAL COUNTS as if we had started out with a uniform prior beta[1 ,1]and seen a−1actual cherry candies andb−1actual lime candies.

Token 16394:
By examining a sequence of beta distributions for increasing values of aandb, keeping the proportions ﬁxed, we can see vividly how the posterior distribution over the parameterΘchanges as data arrive.

Token 16395:
For example, suppose the actual bag of candy is 75% cherry. Fig- ure 20.5(b) shows the sequence beta[3 ,1],beta[6 ,2],beta[30 ,10].

Token 16396:
Clearly, the distribution is converging to a narrow peak around the true value of Θ.

Token 16397:
For large data sets, then, Bayesian learning (at least in this case) converges to the same answer as maximum-likelihood learning.

Token 16398:
Now let us consider a more complicated case.

Token 16399:
The network in Figure 20.2(b) has three parameters, θ,θ 1,a n dθ2,w h e r e θ1is the probability of a red wrapper on a cherry candy and θ2is the probability of a red wrapper on a lime candy.

Token 16400:
The Bayesian hypothesis prior must cover all three parameters—that is, we need to specify P(Θ,Θ1,Θ2).

Token 16401:
Usually, we assume parameter independence :PARAMETER INDEPENDENCE P(Θ,Θ1,Θ2)=P(Θ)P(Θ1)P(Θ2).

Token 16402:
Section 20.2.

Token 16403:
Learning with Complete Data 813 With this assumption, each parameter can have its own beta distribution that is updated sep- arately as data arrive.

Token 16404:
Figure 20.6 shows how we can incorporate the hypothesis prior andany data into one Bayesian network. The nodes Θ,Θ 1,Θ2have no parents.

Token 16405:
But each time we make an observation of a wrapper and corresponding ﬂavor of a piece of candy, we add anodeFlavor i, which is dependent on the ﬂavor parameter Θ: P(Flavor i=cherry|Θ=θ)=θ.

Token 16406:
We also add a node Wrapperi, which is dependent on Θ1andΘ2: P(Wrapper i=red|Flavor i=cherry ,Θ1=θ1)=θ1 P(Wrapper i=red|Flavor i=lime,Θ2=θ2)=θ2.

Token 16407:
Now, the entire Bayesian learning process can be formulated as an inference problem.

Token 16408:
We add new evidence nodes, then query the unknown nodes (in this case, Θ,Θ1,Θ2).

Token 16409:
This for- mulation of learning and prediction makes it clear that Bayesian learning requires no extra“principles of learning.” Furthermore, there is, in essence, just one learning algorithm —the inference algorithm for Bayesian networks.

Token 16410:
Of course, the nature of these networks is some- what different from those of Chapter 14 because of the potentially huge number of evidencevariables representing the training set and the prevalence of continuous-valued parametervariables.

Token 16411:
20.2.5 Learning Bayes net structures So far, we have assumed that the structure of the Bayes net is given and we are just trying tolearn the parameters.

Token 16412:
The structure of the network represents basic causal knowledge aboutthe domain that is often easy for an expert, or even a naive user, to supply.

Token 16413:
In some cases,however, the causal model may be unavailable or subject to dispute—for example, certaincorporations have long claimed that smoking does not cause cancer—so it is important tounderstand how the structure of a Bayes net can be learned from data.

Token 16414:
This section gives abrief sketch of the main ideas. The most obvious approach is to search for a good model.

Token 16415:
We can start with a model containing no links and begin adding parents for each node, ﬁtting the parameters with the methods we have just covered and measuring the accuracy of the resulting model.

Token 16416:
Alterna-tively, we can start with an initial guess at the structure and use hill-climbing or simulated annealing search to make modiﬁcations, retuning the parameters after each change in the structure.

Token 16417:
Modiﬁcations can include reversing, adding, or deleting links.

Token 16418:
We must not in-troduce cycles in the process, so many algorithms assume that an ordering is given for thevariables, and that a node can have parents only among those nodes that come earlier in theordering (just as in the construction process in Chapter 14).

Token 16419:
For full generality, we also needto search over possible orderings.

Token 16420:
There are two alternative methods for deciding when a good structure has been found.

Token 16421:
The ﬁrst is to test whether the conditional independence assertions implicit in the structure areactually satisﬁed in the data.

Token 16422:
For example, the use of a naive Bayes model for the restaurantproblem assumes that P(Fri/Sat,Bar|WillWait )=P(Fri/Sat|WillWait )P(Bar|WillWait )

Token 16423:
814 Chapter 20.

Token 16424:
Learning Probabilistic Models and we can check in the data that the same equation holds between the corresponding condi- tional frequencies.

Token 16425:
But even if the structure describes the true causal nature of the domain,statistical ﬂuctuations in the data set mean that the equation will never be satisﬁed exactly , so we need to perform a suitable statistical test to see if there is sufﬁcient evidence that theindependence hypothesis is violated.

Token 16426:
The complexity of the resulting network will depend on the threshold used for this test—the stricter the independence test, the more links will be added and the greater the danger of overﬁtting.

Token 16427:
An approach more consistent with the ideas in this chapter is to assess the degree to which the proposed model explains the data (in a probabilistic sense).

Token 16428:
We must be carefulhow we measure this, however.

Token 16429:
If we just try to ﬁnd the maximum-likelihood hypothesis,we will end up with a fully connected network, because adding more parents to a node can-not decrease the likelihood (Exercise 20.8).

Token 16430:
We are forced to penalize model complexity insome way.

Token 16431:
The MAP (or MDL) approach simply subtracts a penalty from the likelihood ofeach structure (after parameter tuning) before comparing different structures.

Token 16432:
The Bayesianapproach places a joint prior over structures and parameters.

Token 16433:
There are usually far too manystructures to sum over (superexponential in the number of variables), so most practitionersuse MCMC to sample over structures.

Token 16434:
Penalizing complexity (whether by MAP or Bayesian methods) introduces an important connection between the optimal structure and the nature of the representation for the condi-tional distributions in the network.

Token 16435:
With tabular distributions, the complexity penalty for anode’s distribution grows exponentially with the number of parents, but with, say, noisy-ORdistributions, it grows only linearly.

Token 16436:
This means that learning with noisy-OR (or other com-pactly parameterized) models tends to produce learned structures with more parents than doeslearning with tabular distributions.

Token 16437:
20.2.6 Density estimation with nonparametric models It is possible to learn a probability model without making any assumptions about its structure and parameterization by adopting the nonparametric methods of Section 18.8.

Token 16438:
The task of nonparametric density estimation is typically done in continuous domains, such as thatNONPARAMETRIC DENSITYESTIMATION shown in Figure 20.7(a).

Token 16439:
The ﬁgure shows a probability density function on a space deﬁned by two continuous variables.

Token 16440:
In Figure 20.7(b) we see a sample of data points from thisdensity function. The question is, can we recover the model from the samples?

Token 16441:
First we will consider k-nearest-neighbors models.

Token 16442:
(In Chapter 18 we saw nearest- neighbor models for classiﬁcation and regression; here we see them for density estimation.

Token 16443:
)Given a sample of data points, to estimate the unknown probability density at a query point x we can simply measure the density of the data points in the neighborhood of x.

Token 16444:
Figure 20.7(b) shows two query points (small squares).

Token 16445:
For each query point we have drawn the smallestcircle that encloses 10 neighbors—the 10-nearest-neighborhood.

Token 16446:
We can see that the central circle is large, meaning there is a low density there, and the circle on the right is small, meaning there is a high density there.

Token 16447:
In Figure 20.8 we show three plots of density estimationusing k-nearest-neighbors, for different values of k. It seems clear that (b) is about right, while (a) is too spiky ( kis too small) and (c) is too smooth ( kis too big).

Token 16448:
Section 20.2.

Token 16449:
Learning with Complete Data 815 00.20.40.60.8100.20.40.60.81 024681012141618Density 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 (a) (b) Figure 20.7 (a) A 3D plot of the mixture of Gaussians from Figure 20.11(a).

Token 16450:
(b) A 128- point sample of points from the mixture, together with two query points (small squares) and their10-nearest-neighborhoods (medium and large circles).

Token 16451:
00.20.40.60.8 00.20.40.60.81Density 00.20.40.60.8 00.20.40.60.81Density 00.20.40.60.8 00.20.40.60.81Density (a) (b) (c) Figure 20.8 Density estimation using k-nearest-neighbors, applied to the data in Fig- ure 20.7(b), for k=3,10,a n d40respectively.

Token 16452:
k=3 is too spiky, 40 is too smooth, and 10 is just about right. The best value for kcan be chosen by cross-validation.

Token 16453:
00.20.40.60.8 00.20.40.60.81Density 00.20.40.60.8 00.20.40.60.81Density 00.20.40.60.8 00.20.40.60.81Density (a) (b) (c) Figure 20.9 Kernel density estimation for the data in Figure 20.7(b), using Gaussian ker- nels with w=0.02,0.07,a n d0.20respectively.

Token 16454:
w=0.07is about right.

Token 16455:
816 Chapter 20. Learning Probabilistic Models Another possibility is to use kernel functions , as we did for locally weighted regres- sion.

Token 16456:
To apply a kernel model to density estimation, assume that each data point generates itsown little density function, using a Gaussian kernel.

Token 16457:
The estimated density at a query point x is then the average density as given by each kernel function: P(x)=1 NN/summationdisplay j=1K(x,xj).

Token 16458:
We will assume spherical Gaussians with standard deviation walong each axis: K(x,xj)=1 (w2√ 2π)de−D(x,xj)2 2w2, where dis the number of dimensions in xandDis the Euclidean distance function.

Token 16459:
We still have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows values that are too small, just right, and too large.

Token 16460:
A good value of wcan be chosen by using cross-validation.

Token 16461:
20.3 L EARNING WITH HIDDEN VARIABLES :THEEM A LGORITHM The preceding section dealt with the fully observable case.

Token 16462:
Many real-world problems havehidden variables (sometimes called latent variables ), which are not observable in the data LATENT VARIABLE that are available for learning.

Token 16463:
For example, medical records often include the observed symptoms, the physician’s diagnosis, the treatment applied, and perhaps the outcome of thetreatment, but they seldom contain a direct observation of the disease itself!

Token 16464:
(Note that thediagnosis is not the disease ; it is a causal consequence of the observed symptoms, which are in turn caused by the disease.)

Token 16465:
One might ask, “If the disease is not observed, why not constructa model without it?” The answer appears in Figure 20.10, which shows a small, ﬁctitious diagnostic model for heart disease.

Token 16466:
There are three observable predisposing factors and three observable symptoms (which are too depressing to name).

Token 16467:
Assume that each variable hasthree possible values (e.g., none ,moderate ,a n dsevere ).

Token 16468:
Removing the hidden variable from the network in (a) yields the network in (b); the total number of parameters increases from 78 to 708.

Token 16469:
Thus, latent variables can dramatically reduce the number of parameters required to specify a Bayesian network.

Token 16470:
This, in turn, can dramatically reduce the amount of data needed to learn the parameters.

Token 16471:
Hidden variables are important, but they do complicate the learning problem.

Token 16472:
In Fig- ure 20.10(a), for example, it is not obvious how to learn the conditional distribution forHeartDisease , given its parents, because we do not know the value of HeartDisease in each case; the same problem arises in learning the distributions for the symptoms.

Token 16473:
This section describes an algorithm called expectation–maximization , or EM, that solves this problem EXPECTATION– MAXIMIZATION in a very general way.

Token 16474:
We will show three examples and then provide a general description.

Token 16475:
The algorithm seems like magic at ﬁrst, but once the intuition has been developed, one canﬁnd applications for EM in a huge range of learning problems.

Token 16476:
Section 20.3.

Token 16477:
Learning with Hidden Variables: The EM Algorithm 817 Smoking Diet Exercise Symptom1Symptom2Symptom3 (a) (b)HeartDiseaseSmoking Diet Exercise Symptom1Symptom2Symptom3222 54 666222 54 162 486 Figure 20.10 (a) A simple diagnostic network for heart disease, which is assumed to be a hidden variable.

Token 16478:
Each variable has three possible values and is labeled with the numberof independent parameters in its conditional distribution; the total number is 78.

Token 16479:
(b) The equivalent network with HeartDisease removed. Note that the symptom variables are no longer conditionally indepe ndent given their parents.

Token 16480:
This ne twork requires 708 parameters.

Token 16481:
20.3.1 Unsupervised clustering: Learning mixtures of Gaussians Unsupervised clustering is the problem of discerning multiple categories in a collection ofUNSUPERVISED CLUSTERING objects.

Token 16482:
The problem is unsupervised because the category labels are not given.

Token 16483:
For example, suppose we record the spectra of a hundred thousand stars; are there different types of stars revealed by the spectra, and, if so, how many types and what are their characteristics?

Token 16484:
We are all familiar with terms such as “red giant” and “white dwarf,” but the stars do not carry these labels on their hats—astronomers had to perform unsupervised clustering to identifythese categories.

Token 16485:
Other examples include the identiﬁcation of species, genera, orders, andso on in the Linnæan taxonomy and the creation of natural kinds for ordinary objects (seeChapter 12).

Token 16486:
Unsupervised clustering begins with data. Figure 20.11(b) shows 500 data points, each of which speciﬁes the values of two continuous attributes.

Token 16487:
The data points might correspond to stars, and the attributes might correspond to spectral intensities at two particular frequen- cies.

Token 16488:
Next, we need to understand what kind of probability distribution might have generated the data.

Token 16489:
Clustering presumes that the data are generated from a mixture distribution ,P. MIXTURE DISTRIBUTION Such a distribution has kcomponents , each of which is a distribution in its own right.

Token 16490:
A COMPONENT data point is generated by ﬁrst choosing a component and then generating a sample from that component.

Token 16491:
Let the random variable Cdenote the component, with values 1,...,k ; then the mixture distribution is given by P(x)=k/summationdisplay i=1P(C=i)P(x|C=i), where xrefers to the values of the attributes for a data point.

Token 16492:
For continuous data, a natural choice for the component distributions is the multivariate Gaussian, which gives the so-calledmixture of Gaussians family of distributions.

Token 16493:
The parameters of a mixture of Gaussians are MIXTURE OF GAUSSIANS

Token 16494:
818 Chapter 20.

Token 16495:
Learning Probabilistic Models 00.20.40.60.81 0 0.2 0.4 0.6 0.8 100.20.40.60.81 0 0.2 0.4 0.6 0.8 100.20.40.60.81 0 0.2 0.4 0.6 0.8 1 (a) (b) (c) Figure 20.11 (a) A Gaussian mixture model with three components; the weights (left-to- right) are 0.2, 0.3, and 0.5.

Token 16496:
(b) 500 data points sampled from the model in (a). (c) The modelreconstructed by EM from the data in (b).

Token 16497:
wi=P(C=i)(the weight of each component), μi(the mean of each component), and Σi (the covariance of each component).

Token 16498:
Figure 20.11(a) shows a mixture of three Gaussians; this mixture is in fact the source of the data in (b) as well as being the model shown in Figure 20.7(a) on page 815.

Token 16499:
The unsupervised clustering problem, then, is to recover a mixture model like the one in Figure 20.11(a) from raw data like that in Figure 20.11(b).

Token 16500:
Clearly, if we knew which com- ponent generated each data point, then it would be easy to recover the component Gaussians:we could just select all the data points from a given component and then apply (a multivariateversion of) Equation (20.4) (page 809) for ﬁtting the parameters of a Gaussian to a set of data.On the other hand, if we knew the parameters of each component, then we could, at least in a probabilistic sense, assign each data point to a component.

Token 16501:
The problem is that we knowneither the assignments nor the parameters.

Token 16502:
The basic idea of EM in this context is to pretend that we know the parameters of the model and then to infer the probability that each data point belongs to each component.

Token 16503:
After that, we reﬁt the components to the data, where each component is ﬁtted to the entire data setwith each point weighted by the probability that it belongs to that component.

Token 16504:
The processiterates until convergence.

Token 16505:
Essentially, we are “completing” the data by inferring probabilitydistributions over the hidden variables—which component each data point belongs to—basedon the current model.

Token 16506:
For the mixture of Gaussians, we initialize the mixture-model parame-ters arbitrarily and then iterate the following two steps: 1.E-step : Compute the probabilities p ij=P(C=i|xj), the probability that datum xj was generated by component i.

Token 16507:
By Bayes’ rule, we have pij=αP(xj|C=i)P(C=i).

Token 16508:
The term P(xj|C=i)is just the probability at xjof the ith Gaussian, and the term P(C=i)is just the weight parameter for the ith Gaussian.

Token 16509:
Deﬁne ni=/summationtext jpij,t h e effective number of data points currently assigned to component i.

Token 16510:
2.M-step : Compute the new mean, covariance, and component weights using the follow- ing steps in sequence:

Token 16511:
Section 20.3.

Token 16512:
Learning with Hidden Variables: The EM Algorithm 819 μi←/summationdisplay jpijxj/ni Σi←/summationdisplay jpij(xj−μi)(xj−μi)/latticetop/ni wi←ni/N where Nis the total number of data points.

Token 16513:
The E-step, or expectation step, can be viewed as computing the expected values pijof the hidden indicator variables Zij,w h e r e Zijis 1 if INDICATOR VARIABLE datum xjwas generated by the ith component and 0 otherwise.

Token 16514:
The M-step, or maximization step, ﬁnds the new values of the parameters that maximize the log likelihood of the data,given the expected values of the hidden indicator variables.

Token 16515:
The ﬁnal model that EM learns when it is applied to the data in Figure 20.11(a) is shown in Figure 20.11(c); it is virtually indistinguishable from the original model from which the data were generated.

Token 16516:
Figure 20.12(a) plots the log likelihood of the data according to thecurrent model as EM progresses. There are two points to notice.

Token 16517:
First, the log likelihood for the ﬁnal learned model slightly exceeds that of the original model, from which the data were generated.

Token 16518:
This might seem surprising, but it simply reﬂects the fact that the data were generated randomly and might not provide an exact reﬂection of the underlying model.

Token 16519:
The second point is that EM increases the log likelihood of the data at every iteration. This fact can be proved in general.

Token 16520:
Furthermore, under certain conditions (that hold in ost cases), EM can be proven to reacha local maximum in likelihood.

Token 16521:
(In rare cases, it could reach a saddle point or even a localminimum.)

Token 16522:
In this sense, EM resembles a gradient-based hill-climbing algorithm, but notice that it has no “step size” parameter.

Token 16523:
-200-1000100200300400500600700 0 5 10 15 20Log-likelihood L Iteration number-2025-2020-2015-2010-2005-2000-1995-1990-1985-1980-1975 0 20 40 60 80 100 120Log-likelihood L Iteration number (a) (b) Figure 20.12 Graphs showing the log likelihood of the data, L, as a function of the EM iteration.

Token 16524:
The horizontal line shows the log likelihood according to the true model. (a) Graph for the Gaussian mixture model in Figure 20.11.

Token 16525:
(b) Graph for the Bayesian network in Figure 20.13(a).

Token 16526:
820 Chapter 20.

Token 16527:
Learning Probabilistic Models (a) (b)C X HoleBagP(Bag= 1) θ Wrapper FlavorBag 1 2P(F=cherry | B) θF2θF1 Figure 20.13 (a) A mixture model for candy.

Token 16528:
The proportions of different ﬂavors, wrap- pers, presence of holes depend on the bag, which is not observed.

Token 16529:
(b) Bayesian network for a Gaussian mixture.

Token 16530:
The mean and covariance of the observable variables Xdepend on the component C. Things do not always go as well as Figure 20.12(a) might suggest.

Token 16531:
It can happen, for example, that one Gaussian component shrinks so that it covers just a single data point.

Token 16532:
Thenits variance will go to zero and its likelihood will go to inﬁnity!

Token 16533:
Another problem is thattwo components can “merge,” acquiring identical means and variances and sharing their datapoints.

Token 16534:
These kinds of degenerate local maxima are serious problems, especially in high dimensions.

Token 16535:
One solution is to place priors on the model parameters and to apply the MAP version of EM.

Token 16536:
Another is to restart a component with new random parameters if it gets toosmall or too close to another component.

Token 16537:
Sensible initialization also helps.

Token 16538:
20.3.2 Learning Bayesian networks with hidden variables To learn a Bayesian network with hidden variables, we apply the same insights that workedfor mixtures of Gaussians.

Token 16539:
Figure 20.13 represents a situation in which there are two bags ofcandies that have been mixed together.

Token 16540:
Candies are described by three features: in additionto theFlavor and the Wrapper , some candies have a Hole in the middle and some do not.

Token 16541:
The distribution of candies in each bag is described by a naive Bayes model: the features are independent, given the bag, but the conditional probability distribution for each feature depends on the bag.

Token 16542:
The parameters are as follows: θis the prior probability that a candy comes from Bag 1; θ F1andθF2are the probabilities that the ﬂavor is cherry, given that the candy comes from Bag 1 or Bag 2 respectively; θW1andθW2give the probabilities that the wrapper is red; and θH1andθH2give the probabilities that the candy has a hole.

Token 16543:
Notice that the overall model is a mixture model.

Token 16544:
(In fact, we can also model the mixture of Gaussians as a Bayesian network, as shown in Figure 20.13(b).)

Token 16545:
In the ﬁgure, the bag is a hiddenvariable because, once the candies have been mixed together, we no longer know which bageach candy came from.

Token 16546:
In such a case, can we recover the descriptions of the two bags by

Token 16547:
Section 20.3. Learning with Hidden Variables: The EM Algorithm 821 observing candies from the mixture?

Token 16548:
Let us work through an iteration of EM for this problem. First, let’s look at the data.

Token 16549:
We generated 1000 samples from a model whose true parametersare as follows: θ=0.5,θ F1=θW1=θH1=0.8,θF2=θW2=θH2=0.3.

Token 16550:
(20.7) That is, the candies are equally likely to come from either bag; the ﬁrst is mostly cherries with red wrappers and holes; the second is mostly limes with green wrappers and no holes.

Token 16551:
The counts for the eight possible kinds of candy are as follows: W=red W=green H=1 H=0 H=1 H=0 F=cherry 273 93 104 90 F=lime 79 100 94 167 We start by initializing the parameters.

Token 16552:
For numerical simplicity, we arbitrarily choose5 θ(0)=0.6,θ(0) F1=θ(0) W1=θ(0) H1=0.6,θ(0) F2=θ(0) W2=θ(0) H2=0.4.

Token 16553:
(20.8) First, let us work on the θparameter.

Token 16554:
In the fully observable case, we would estimate this directly from the observed counts of candies from bags 1 and 2.

Token 16555:
Because the bag is a hidden variable, we calculate the expected counts instead.

Token 16556:
The expected count ˆN(Bag=1) is the sum, over all candies, of the probability that the candy came from bag 1: θ(1)=ˆN(Bag=1 )/N=N/summationdisplay j=1P(Bag=1|ﬂavor j,wrapper j,holes j)/N .

Token 16557:
These probabilities can be computed by any inference algorithm for Bayesian networks.

Token 16558:
For a naive Bayes model such as the one in our example, we can do the inference “by hand,”using Bayes’ rule and applying conditional independence: θ (1)=1 NN/summationdisplay j=1P(ﬂavor j|Bag=1)P(wrapper j|Bag=1)P(holes j|Bag=1)P(Bag=1) /summationtext iP(ﬂavor j|Bag=i)P(wrapper j|Bag=i)P(holes j|Bag=i)P(Bag=i).

Token 16559:
Applying this formula to, say, the 273 red-wrapped cherry candies with holes, we get a con- tribution of 273 1000·θ(0) F1θ(0) W1θ(0) H1θ(0) θ(0) F1θ(0) W1θ(0) H1θ(0)+θ(0) F2θ(0) W2θ(0) H2(1−θ(0))≈0.22797 .

Token 16560:
Continuing with the other seven kinds of candy in the table of counts, we obtain θ(1)=0.6124 . Now let us consider the other parameters, such as θF1.

Token 16561:
In the fully observable case, we would estimate this directly from the observed counts of cherry and lime candies from bag 1.

Token 16562:
The expected count of cherry candies from bag 1 is given by /summationdisplay j:Flavor j=cherryP(Bag=1|Flavor j=cherry ,wrapper j,holes j).

Token 16563:
5It is better in practice to choose them randomly, to avoid local maxima due to symmetry.

Token 16564:
822 Chapter 20. Learning Probabilistic Models Again, these probabilities can be calculated by any Bayes net algorithm.

Token 16565:
Completing this process, we obtain the new values of all the parameters: θ(1)=0.6124,θ(1) F1=0.6684,θ(1) W1=0.6483,θ(1) H1=0.6558, θ(1) F2=0.3887,θ(1) W2=0.3817,θ(1) H2=0.3827.

Token 16566:
(20.9) The log likelihood of the data increases from about −2044 initially to about −2021 after the ﬁrst iteration, as shown in Figure 20.12(b).

Token 16567:
That is, the update improves the likelihood itself by a factor of about e23≈1010.

Token 16568:
By the tenth iteration, the learned model is a better ﬁt than the original model ( L=−1982.214). Thereafter, progress becomes very slow.

Token 16569:
This is not uncommon with EM, and many practical systems combine EM with a gradient-basedalgorithm such as Newton–Raphson (see Chapter 4) for the last phase of learning.

Token 16570:
The general lesson from this example is that the parameter updates for Bayesian net- work learning with hidden variables are directly available from the results of inference on each example.

Token 16571:
Moreover, only local posterior probabilities are needed for each parame- ter.Here, “local” means that the CPT for each variable Xican be learned from posterior probabilities involving just Xiand its parents Ui.

Token 16572:
Deﬁning θijkto be the CPT parameter P(Xi=xij|Ui=uik), the update is given by the normalized expected counts as follows: θijk←ˆN(Xi=xij,Ui=uik)/ˆN(Ui=uik).

Token 16573:
The expected counts are obtained by summing over the examples, computing the probabilities P(Xi=xij,Ui=uik)for each by using any Bayes net inference algorithm.

Token 16574:
For the exact algorithms—including variable elimination—all these probabilities are obtainable directly asa by-product of standard inference, with no need for extra computations speciﬁc to learning.Moreover, the information needed for learning is available locally for each parameter.

Token 16575:
20.3.3 Learning hidden Markov models Our ﬁnal application of EM involves learning the transition probabilities in hidden Markov models (HMMs).

Token 16576:
Recall from Section 15.3 that a hidden Markov model can be represented by a dynamic Bayes net with a single discrete state variable, as illustrated in Figure 20.14.

Token 16577:
Each data point consists of an observation sequence of ﬁnite length, so the problem is to learn the transition probabilities from a set of observation sequences (or from just one long sequence).

Token 16578:
We have already worked out how to learn Bayes nets, but there is one complication: in Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, theindividual transition probabilities from state ito state jat time t,θ ijt=P(Xt+1=j|Xt=i), arerepeated across time—that is, θijt=θijfor all t. To estimate the transition probability from state ito state j, we simply calculate the expected proportion of times that the system undergoes a transition to state jwhen in state i: θij←/summationdisplay tˆN(Xt+1=j, Xt=i)//summationdisplay tˆN(Xt=i).

Token 16579:
The expected counts are computed by an HMM inference algorithm.

Token 16580:
The forward–backward algorithm shown in Figure 15.4 can be modiﬁed very easily to compute the necessary prob- abilities.

Token 16581:
One important point is that the probabilities required are obtained by smoothing

Token 16582:
Section 20.3.

Token 16583:
Learning with Hidden Variables: The EM Algorithm 823 0.3f0.7tP(R)1R0 0.7P(R0) 0.2f0.9tP(U)1R1Umbrella 1Rain0Rain10.7P(R0) 4 0.2f0.9tP(U) R4ft 0.30.7P(R )4R3 Umbrella 4Rain4 0.2f0.9tP(U)3R3ftR 0.30.7P(R )3 2 Umbrella 3Rain3 0.2f0.9tP(U)2R2ftR 0.30.7P(R)2 1 Umbrella 2Rain2 0.2f0.9tP(U)1R1ftR 0.30.7P(R)1 0 Umbrella 1Rain0Rain1 Figure 20.14 An unrolled dynamic Bayesian network that represents a hidden Markov model (repeat of Figure 15.16).

Token 16584:
rather than ﬁltering ; that is, we need to pay attention to subsequent evidence in estimating the probability that a particular transition occurred.

Token 16585:
The evidence in a murder case is usuallyobtained after the crime (i.e., the transition from state ito state j) has taken place.

Token 16586:
20.3.4 The general form of the EM algorithm We have seen several instances of the EM algorithm.

Token 16587:
Each involves computing expected values of hidden variables for each example and then recomputing the parameters, using theexpected values as if they were observed values.

Token 16588:
Let xbe all the observed values in all the examples, let Zdenote all the hidden variables for all the examples, and let θbe all the parameters for the probability model.

Token 16589:
Then the EM algorithm is θ (i+1)=a r g m a x θ/summationdisplay zP(Z=z|x,θ(i))L(x,Z=z|θ). This equation is the EM algorithm in a nutshell.

Token 16590:
The E-step is the computation of the summa- tion, which is the expectation of the log likelihood of the “completed” data with respect to thedistribution P(Z=z|x,θ (i)), which is the posterior over the hidden variables, given the data.

Token 16591:
The M-step is the maximization of this expected log likelihood with respect to the parame- ters.

Token 16592:
For mixtures of Gaussians, the hidden variables are the Zijs, where Zijis 1 if example j was generated by component i.

Token 16593:
For Bayes nets, Zijis the value of unobserved variable Xiin example j.

Token 16594:
For HMMs, Zjtis the state of the sequence in example jat time t. Starting from the general form, it is possible to derive an EM algorithm for a speciﬁc application once the appropriate hidden variables have been identiﬁed.

Token 16595:
As soon as we understand the general idea of EM, it becomes easy to derive all sorts of variants and improvements.

Token 16596:
For example, in many cases the E-step—the computation ofposteriors over the hidden variables—is intractable, as in large Bayes nets.

Token 16597:
It turns out thatone can use an approximate E-step and still obtain an effective learning algorithm.

Token 16598:
With a sampling algorithm such as MCMC (see Section 14.5), the learning process is very intuitive: each state (conﬁguration of hidden and observed variables) visited by MCMC is treated ex- actly as if it were a complete observation.

Token 16599:
Thus, the parameters can be updated directly aftereach MCMC transition.

Token 16600:
Other forms of approximate inference, such as variational and loopymethods, have also proved effective for learning very large networks.

Token 16601:
824 Chapter 20.

Token 16602:
Learning Probabilistic Models 20.3.5 Learning Bayes net structures with hidden variables In Section 20.2.5, we discussed the problem of learning Bayes net structures with complete data.

Token 16603:
When unobserved variables may be inﬂuencing the data that are observed, things getmore difﬁcult.

Token 16604:
In the simplest case, a human expert might tell the learning algorithm that cer-tain hidden variables exist, leaving it to the algorithm to ﬁnd a place for them in the network structure.

Token 16605:
For example, an algorithm might try to learn the structure shown in Figure 20.10(a) on page 817, given the information that HeartDisease (a three-valued variable) should be in- cluded in the model.

Token 16606:
As in the complete-data case, the overall algorithm has an outer loop thatsearches over structures and an inner loop that ﬁts the network parameters given the structure.

Token 16607:
If the learning algorithm is not told which hidden variables exist, then there are two choices: either pretend that the data is really complete—which may force the algorithm tolearn a parameter-intensive model such as the one in Figure 20.10(b)—or invent new hidden variables in order to simplify the model.

Token 16608:
The latter approach can be implemented by includingnew modiﬁcation choices in the structure search: in addition to modifying links, the algorithmcan add or delete a hidden variable or change its arity.

Token 16609:
Of course, the algorithm will not knowthat the new variable it has invented is called HeartDisease ; nor will it have meaningful names for the values.

Token 16610:
Fortunately, newly invented hidden variables will usually be connected to preexisting variables, so a human expert can often inspect the local conditional distributionsinvolving the new variable and ascertain its meaning.

Token 16611:
As in the complete-data case, pure maximum-likelihood structure learning will result in a completely connected network (moreover, one with no hidden variables), so some form ofcomplexity penalty is required.

Token 16612:
We can also apply MCMC to sample many possible networkstructures, thereby approximating Bayesian learning.

Token 16613:
For example, we can learn mixtures ofGaussians with an unknown number of components by sampling over the number; the approx-imate posterior distribution for the number of Gaussians is given by the sampling frequenciesof the MCMC process.

Token 16614:
For the complete-data case, the inner loop to learn the parameters is very fast—just a matter of extracting conditional frequencies from the data set.

Token 16615:
When there are hidden vari- ables, the inner loop may involve many iterations of EM or a gradient-based algorithm, andeach iteration involves the calculation of posteriors in a Bayes net, which is itself an NP-hardproblem.

Token 16616:
To date, this approach has proved impractical for learning complex models.

Token 16617:
Onepossible improvement is the so-called structural EM algorithm, which operates in much the STRUCTURAL EM same way as ordinary (parametric) EM except that the algorithm can update the structure as well as the parameters.

Token 16618:
Just as ordinary EM uses the current parameters to compute theexpected counts in the E-step and then applies those counts in the M-step to choose newparameters, structural EM uses the current structure to compute expected counts and then ap-plies those counts in the M-step to evaluate the likelihood for potential new structures.

Token 16619:
(Thiscontrasts with the outer-loop/inner-loop method, which computes new expected counts for each potential structure.)

Token 16620:
In this way, structural EM may make several structural alterations to the network without once recomputing the expected counts, and is capable of learning non-trivial Bayes net structures.

Token 16621:
Nonetheless, much work remains to be done before we can saythat the structure-learning problem is solved.

Token 16622:
Section 20.4.

Token 16623:
Summary 825 20.4 S UMMARY Statistical learning methods range from simple calculation of averages to the construction of complex models such as Bayesian networks.

Token 16624:
They have applications throughout computerscience, engineering, computational biology, neuroscience, psychology, and physics.

Token 16625:
Thischapter has presented some of the basic ideas and given a ﬂavor of the mathematical under-pinnings.

Token 16626:
The main points are as follows: •Bayesian learning methods formulate learning as a form of probabilistic inference, using the observations to update a prior distribution over hypotheses.

Token 16627:
This approachprovides a good way to implement Ockham’s razor, but quickly becomes intractable forcomplex hypothesis spaces.

Token 16628:
•Maximum a posteriori (MAP) learning selects a single most likely hypothesis given the data.

Token 16629:
The hypothesis prior is still used and the method is often more tractable than full Bayesian learning.

Token 16630:
•Maximum-likelihood learning simply selects the hypothesis that maximizes the likeli- hood of the data; it is equivalent to MAP learning with a uniform prior.

Token 16631:
In simple casessuch as linear regression and fully observable Bayesian networks, maximum-likelihoodsolutions can be found easily in closed form.

Token 16632:
Naive Bayes learning is a particularly effective technique that scales well.

Token 16633:
•When some variables are hidden, local maximum likelihood solutions can be found using the EM algorithm.

Token 16634:
Applications include clustering using mixtures of Gaussians,learning Bayesian networks, and learning hidden Markov models.

Token 16635:
•Learning the structure of Bayesian networks is an example of model selection .T h i s usually involves a discrete search in the space of structures.

Token 16636:
Some method is required for trading off model complexity against degree of ﬁt.

Token 16637:
•Nonparametric models represent a distribution using the collection of data points. Thus, the number of parameters grows with the training set.

Token 16638:
Nearest-neighbors methodslook at the examples nearest to the point in question, whereas kernel methods form a distance-weighted combination of all the examples.

Token 16639:
Statistical learning continues to be a very active area of research.

Token 16640:
Enormous strides have been made in both theory and practice, to the point where it is possible to learn almost any modelfor which exact or approximate inference is feasible.

Token 16641:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The application of statistical learning techniques in AI was an active area of research in theearly years (see Duda and Hart, 1973) but became separated from mainstream AI as thelatter ﬁeld concentrated on symbolic methods.

Token 16642:
A resurgence of interest occurred shortly afterthe introduction of Bayesian network models in the late 1980s; at roughly the same time,

Token 16643:
826 Chapter 20. Learning Probabilistic Models a statistical view of neural network learning began to emerge.

Token 16644:
In the late 1990s, there was a noticeable convergence of interests in machine learning, statistics, and neural networks,centered on methods for creating large probabilistic models from data.

Token 16645:
The naive Bayes model is one of the oldest and simplest forms of Bayesian network, dating back to the 1950s. Its origins were mentioned in Chapter 13.

Token 16646:
Its surprising success is partially explained by Domingos and Pazzani (1997).

Token 16647:
A boosted form of naive Bayes learn- ing won the ﬁrst KDD Cup data mining competition (Elkan, 1997).

Token 16648:
Heckerman (1998) givesan excellent introduction to the general problem of Bayes net learning.

Token 16649:
Bayesian parame-ter learning with Dirichlet priors for Bayesian networks was discussed by Spiegelhalter et al. (1993).

Token 16650:
The B UGS software package (Gilks et al.

Token 16651:
, 1994) incorporates many of these ideas and provides a very powerful tool for formulating and learning complex probability models.

Token 16652:
Theﬁrst algorithms for learning Bayes net structures used conditional independence tests (Pearl,1988; Pearl and Verma, 1991). Spirtes et al.

Token 16653:
(1993) developed a comprehensive approach embodied in the T ETRAD package for Bayes net learning.

Token 16654:
Algorithmic improvements since then led to a clear victory in the 2001 KDD Cup data mining competition for a Bayes netlearning method (Cheng et al.

Token 16655:
, 2002). (The speciﬁc task here was a bioinformatics prob- lem with 139,351 features!)

Token 16656:
A structure-learning approach based on maximizing likelihood was developed by Cooper and Herskovits (1992) and improved by Heckerman et al. (1994).

Token 16657:
Several algorithmic advances since that time have led to quite respectable performance inthe complete-data case (Moore and Wong, 2003; Teyssier and Koller, 2005).

Token 16658:
One importantcomponent is an efﬁcient data structure, the AD-tree, for caching counts over all possiblecombinations of variables and values (Moore and Lee, 1997).

Token 16659:
Friedman and Goldszmidt(1996) pointed out the inﬂuence of the representation of local conditional distributions on thelearned structure.

Token 16660:
The general problem of learning probability models with hidden variables and miss- ing data was addressed by Hartley (1958), who described the general idea of what was latercalled EM and gave several examples.

Token 16661:
Further impetus came from the Baum–Welch algo-rithm for HMM learning (Baum and Petrie, 1966), which is a special case of EM.

Token 16662:
The paper by Dempster, Laird, and Rubin (1977), which presented the EM algorithm in general form and analyzed its convergence, is one of the most cited papers in both computer science andstatistics.

Token 16663:
(Dempster himself views EM as a schema rather than an algorithm, since a gooddeal of mathematical work may be required before it can be applied to a new family of dis-tributions.)

Token 16664:
McLachlan and Krishnan (1997) devote an entire book to the algorithm and itsproperties.

Token 16665:
The speciﬁc problem of learning mixture models, including mixtures of Gaus-sians, is covered by Titterington et al. (1985).

Token 16666:
Within AI, the ﬁrst successful system that used EM for mixture modeling was A UTOCLASS (Cheeseman et al. , 1988; Cheeseman and Stutz, 1996).

Token 16667:
A UTOCLASS has been applied to a number of real-world scientiﬁc classiﬁcation tasks, including the discovery of new types of stars from spectral data (Goebel et al.

Token 16668:
, 1989) and new classes of proteins and introns in DNA/protein sequence databases (Hunter and States, 1992).

Token 16669:
For maximum-likelihood parameter learning in Bayes nets with hidden variables, EM and gradient-based methods were introduced around the same time by Lauritzen (1995), Rus-sellet al.

Token 16670:
(1995), and Binder et al. (1997a).

Token 16671:
The structural EM algorithm was developed by Friedman (1998) and applied to maximum-likelihood learning of Bayes net structures with

Token 16672:
Exercises 827 latent variables. Friedman and Koller (2003). describe Bayesian structure learning.

Token 16673:
The ability to learn the structure of Bayesian networks is closely connected to the issue of recovering causal information from data.

Token 16674:
That is, is it possible to learn Bayes nets in such a way that the recovered network structure indicates real causal inﬂuences?

Token 16675:
For manyyears, statisticians avoided this question, believing that observational data (as opposed to data generated from experimental trials) could yield only correlational information—after all, any two variables that appear related might in fact be inﬂuenced by a third, unknown causalfactor rather than inﬂuencing each other directly.

Token 16676:
Pearl (2000) has presented convincingarguments to the contrary, showing that there are in fact many cases where causality can beascertained and developing the causal network formalism to express causes and the effects CAUSAL NETWORK of intervention as well as ordinary conditional probabilities.

Token 16677:
Nonparametric density estimation, also called Parzen window density estimation, was investigated initially by Rosenblatt (1956) and Parzen (1962).

Token 16678:
Since that time, a huge litera-ture has developed investigating the properties of various estimators. Devroye (1987) gives athorough introduction.

Token 16679:
There is also a rapidly growing literature on nonparametric Bayesianmethods, originating with the seminal work of Ferguson (1973) on the Dirichlet process , DIRICHLETPROCESS which can be thought of as a distribution over Dirichlet distributions.

Token 16680:
These methods are par- ticularly useful for mixtures with unknown numbers of components.

Token 16681:
Ghahramani (2005) and Jordan (2005) provide useful tutorials on the many applications of these ideas to statisticallearning.

Token 16682:
The text by Rasmussen and Williams (2006) covers the Gaussian process ,w h i c h GAUSSIAN PROCESS gives a way of deﬁning prior distributions over the space of continuous functions.

Token 16683:
The material in this chapter brings together work from the ﬁelds of statistics and pattern recognition, so the story has been told many times in many ways.

Token 16684:
Good texts on Bayesianstatistics include those by DeGroot (1970), Berger (1985), and Gelman et al. (1995). Bishop (2007) and Hastie et al.

Token 16685:
(2009) provide an excellent introduction to statistical machine learn- ing.

Token 16686:
For pattern classiﬁcation, the classic text for many years has been Duda and Hart (1973),now updated (Duda et al. , 2001).

Token 16687:
The annual NIPS (Neural Information Processing Confer- ence) conference, whose proceedings are published as the series Advances in Neural Informa- tion Processing Systems , is now dominated by Bayesian papers.

Token 16688:
Papers on learning Bayesian networks also appear in the Uncertainty in AI andMachine Learning conferences and in sev- eral statistics conferences.

Token 16689:
Journals speciﬁc to neural networks include Neural Computation , Neural Networks ,a n dt h e IEEE Transactions on Neural Networks .

Token 16690:
Speciﬁcally Bayesian venues include the Valencia International Meetings on Bayesian Statistics and the journal Bayesian Analysis .

Token 16691:
EXERCISES 20.1 The data used for Figure 20.1 on page 804 can be viewed as being generated by h5.

Token 16692:
For each of the other four hypotheses, generate a data set of length 100 and plot the cor-responding graphs for P(h i|d1,...,d N)andP(DN+1=lime|d1,...,d N).

Token 16693:
Comment on your results.

Token 16694:
828 Chapter 20.

Token 16695:
Learning Probabilistic Models 20.2 Suppose that Ann’s utilities for cherry and lime candies are cAand/lscriptA, whereas Bob’s utilities are cBand/lscriptB.

Token 16696:
(But once Ann has unwrapped a piece of candy, Bob won’t buy it.)

Token 16697:
Presumably, if Bob likes lime candies much more than Ann, it would be wise for Annto sell her bag of candies once she is sufﬁciently sure of its lime content.

Token 16698:
On the other hand,if Ann unwraps too many candies in the process, the bag will be worth less.

Token 16699:
Discuss the problem of determining the optimal point at which to sell the bag.

Token 16700:
Determine the expected utility of the optimal procedure, given the prior distribution from Section 20.1.

Token 16701:
20.3 Two statisticians go to the doctor and are both given the same prognosis: A 40% chance that the problem is the deadly disease A, and a 60% chance of the fatal disease B. Fortunately, there are anti- Aand anti- Bdrugs that are inexpensive, 100% effective, and free of side-effects.

Token 16702:
The statisticians have the choice of taking one drug, both, or neither. Whatwill the ﬁrst statistician (an avid Bayesian) do?

Token 16703:
How about the second statistician, who alwaysuses the maximum likelihood hypothesis?

Token 16704:
The doctor does some research and discovers that disease Bactually comes in two versions, dextro- Band levo- B, which are equally likely and equally treatable by the anti- B drug.

Token 16705:
Now that there are three hypotheses, what will the two statisticians do?

Token 16706:
20.4 Explain how to apply the boosting method of Chapter 18 to naive Bayes learning.

Token 16707:
Test the performance of the resulting algorithm on the restaurant learning problem.

Token 16708:
20.5 Consider Ndata points (x j,yj),w h e r et h e yjs are generated from the xjs according to the linear Gaussian model in Equation (20.5).

Token 16709:
Find the values of θ1,θ2,a n dσthat maximize the conditional log likelihood of the data.

Token 16710:
20.6 Consider the noisy-OR model for fever described in Section 14.3.

Token 16711:
Explain how to apply maximum-likelihood learning to ﬁt the parameters of such a model to a set of completedata.

Token 16712:
( Hint: use the chain rule for partial derivatives.) 20.7 This exercise investigates properties of the Beta distribution deﬁned in Equation (20.6). a.

Token 16713:
By integrating over the range [0,1], show that the normalization constant for the dis- tribution beta[a,b]is given by α=Γ (a+b)/Γ(a)Γ(b)where Γ(x)is the Gamma function ,d e ﬁ n e db y Γ(x+1 )= x·Γ(x)andΓ(1)= 1 .

Token 16714:
( F o ri n t e g e r x,Γ(x+1 )= x!.) GAMMA FUNCTION b. Show that the mean is a/(a+b). c. Find the mode(s) (the most likely value(s) of θ).

Token 16715:
d. Describe the distribution beta[/epsilon1,/epsilon1]for very small /epsilon1. What happens as such a distribution is updated?

Token 16716:
20.8 Consider an arbitrary Bayesian network, a complete data set for that network, and the likelihood for the data set according to the network.

Token 16717:
Give a simple proof that the likelihoodof the data cannot decrease if we add a new link to the network and recompute the maximum-likelihood parameter values.

Token 16718:
20.9 Consider a single Boolean random variable Y(the “classiﬁcation”).

Token 16719:
Let the prior probability P(Y=true)beπ.L e t ’ st r yt oﬁ n d π, given a training set D=(y 1,...,y N)with Nindependent samples of Y.

Token 16720:
Furthermore, suppose pof the Nare positive and nof the N are negative.

Token 16721:
Exercises 829 a.

Token 16722:
Write down an expression for the likelihood of D(i.e., the probability of seeing this particular sequence of examples, given a ﬁxed value of π)i nt e r m so f π,p,a n dn.

Token 16723:
b. By differentiating the log likelihood L, ﬁnd the value of πthat maximizes the likelihood.

Token 16724:
c. Now suppose we add in kBoolean random variables X1,X2,...,X k(the “attributes”) that describe each sample, and suppose we assume that the attributes are conditionallyindependent of each other given the goal Y.

Token 16725:
Draw the Bayes net corresponding to this assumption.

Token 16726:
d. Write down the likelihood for the data including the attributes, using the following additional notation: •α iisP(Xi=true|Y=true).

Token 16727:
•βiisP(Xi=true|Y=false). •p+ iis the count of samples for which Xi=true andY=true . •n+ iis the count of samples for which Xi=false andY=true .

Token 16728:
•p− iis the count of samples for which Xi=true andY=false . •n− iis the count of samples for which Xi=false andY=false .

Token 16729:
[Hint: consider ﬁrst the probability of seeing a single example with speciﬁed values for X1,X2,...,X kandY.]

Token 16730:
e. By differentiating the log likelihood L, ﬁnd the values of αiandβi(in terms of the var- ious counts) that maximize the likelihood and say in words what these values represent.

Token 16731:
f.L e tk=2, and consider a data set with 4 all four possible examples of the XOR function.

Token 16732:
Compute the maximum likelihood estimates of π,α1,α2,β1,a n dβ2.

Token 16733:
g. Given these estimates of π,α1,α2,β1,a n dβ2, what are the posterior probabilities P(Y=true|x1,x2)for each example?

Token 16734:
20.10 Consider the application of EM to learn the parameters for the network in Fig- ure 20.13(a), given the true parameters in Equation (20.7). a.

Token 16735:
Explain why the EM algorithm would not work if there were just two attributes in the model rather than three. b.

Token 16736:
Show the calculations for the ﬁrst iteration of EM starting from Equation (20.8).

Token 16737:
c. What happens if we start with all the parameters set to the same value p?

Token 16738:
(Hint: you may ﬁnd it helpful to investigate this empirically before deriving the general result.)

Token 16739:
d. Write out an expression for the log likelihood of the tabulated candy data on page 821 in terms of the parameters, calculate the partial derivatives with respect to each parameter, and investigate the nature of the ﬁxed point reached in part (c).

Token 16740:
21REINFORCEMENT LEARNING In which we examine how an agent can learn from success and failure, from re- ward and punishment.

Token 16741:
21.1 I NTRODUCTION Chapters 18, 19, and 20 covered methods that learn functions, logical theories, and probabilitymodels from examples.

Token 16742:
In this chapter, we will study how agents can learn what to do in the absence of labeled examples of what to do.

Token 16743:
Consider, for example, the problem of learning to play chess.

Token 16744:
A supervised learning agent needs to be told the correct move for each position it encounters, but such feedback isseldom available.

Token 16745:
In the absence of feedback from a teacher, an agent can learn a transitionmodel for its own moves and can perhaps learn to predict the opponent’s moves, but without some feedback about what is good and what is bad, the agent will have no grounds for decid- ing which move to make.

Token 16746:
The agent needs to know that something good has happened when it (accidentally) checkmates the opponent, and that something bad has happened when it is checkmated—or vice versa, if the game is suicide chess.

Token 16747:
This kind of feedback is called areward ,o rreinforcement .

Token 16748:
In games like chess, the reinforcement is received only at the end REINFORCEMENT of the game. In other environments, the rewards come more frequently.

Token 16749:
In ping-pong, each point scored can be considered a reward; when learning to crawl, any forward motion is anachievement.

Token 16750:
Our framework for agents regards the reward as part of the input percept, but the agent must be “hardwired” to recognize that part as a reward rather than as just anothersensory input.

Token 16751:
Thus, animals seem to be hardwired to recognize pain and hunger as negativerewards and pleasure and food intake as positive rewards.

Token 16752:
Reinforcement has been carefullystudied by animal psychologists for over 60 years.

Token 16753:
Rewards were introduced in Chapter 17, where they served to deﬁne optimal policies inMarkov decision processes (MDPs).

Token 16754:
An optimal policy is a policy that maximizes the expected total reward.

Token 16755:
The task of reinforcement learning is to use observed rewards to learn an optimal (or nearly optimal) policy for the environment.

Token 16756:
Whereas in Chapter 17 the agenthas a complete model of the environment and knows the reward function, here we assume no 830

Token 16757:
Section 21.1. Introduction 831 prior knowledge of either.

Token 16758:
Imagine playing a new game whose rules you don’t know; after a hundred or so moves, your opponent announces, “You lose.” This is reinforcement learningin a nutshell.

Token 16759:
In many complex domains, reinforcement learning is the only feasible way to train a program to perform at high levels.

Token 16760:
For example, in game playing, it is very hard for a human to provide accurate and consistent evaluations of large numbers of positions, which would be needed to train an evaluation function directly from examples.

Token 16761:
Instead, the program can betold when it has won or lost, and it can use this information to learn an evaluation functionthat gives reasonably accurate estimates of the probability of winning from any given position.Similarly, it is extremely difﬁcult to program an agent to ﬂy a helicopter; yet given appropriatenegative rewards for crashing, wobbling, or deviating from a set course, an agent can learn toﬂy by itself.

Token 16762:
Reinforcement learning might be considered to encompass all of AI: an agent is placed in an environment and must learn to behave successfully therein.

Token 16763:
To keep the chapter man-ageable, we will concentrate on simple environments and simple agent designs.

Token 16764:
For the mostpart, we will assume a fully observable environment, so that the current state is supplied byeach percept.

Token 16765:
On the other hand, we will assume that the agent does not know how the en- vironment works or what its actions do, and we will allow for probabilistic action outcomes.

Token 16766:
Thus, the agent faces an unknown Markov decision process.

Token 16767:
We will consider three of theagent designs ﬁrst introduced in Chapter 2: •Autility-based agent learns a utility function on states and uses it to select actions that maximize the expected outcome utility.

Token 16768:
•AQ-learning agent learns an action-utility function ,o rQ-function ,g i v i n gt h ee x - Q-LEARNING Q-FUNCTION pected utility of taking a given action in a given state.

Token 16769:
•Areﬂex agent learns a policy that maps directly from states to actions.

Token 16770:
A utility-based agent must also have a model of the environment in order to make decisions, because it must know the states to which its actions will lead.

Token 16771:
For example, in order to make use of a backgammon evaluation function, a backgammon program must know what its legal moves are and how they affect the board position .

Token 16772:
Only in this way can it apply the utility function to the outcome states.

Token 16773:
A Q-learning agent, on the other hand, can compare theexpected utilities for its available choices without needing to know their outcomes, so it doesnot need a model of the environment.

Token 16774:
On the other hand, because they do not know wheretheir actions lead, Q-learning agents cannot look ahead; this can seriously restrict their abilityto learn, as we shall see.

Token 16775:
We begin in Section 21.2 with passive learning , where the agent’s policy is ﬁxed and PASSIVE LEARNING the task is to learn the utilities of states (or state–action pairs); this could also involve learning a model of the environment.

Token 16776:
Section 21.3 covers active learning , where the agent must also ACTIVE LEARNING learn what to do.

Token 16777:
The principal issue is exploration : an agent must experience as much as EXPLORATION possible of its environment in order to learn how to behave in it.

Token 16778:
Section 21.4 discusses how an agent can use inductive learning to learn much faster from its experiences.

Token 16779:
Section 21.5covers methods for learning direct policy representations in reﬂex agents.

Token 16780:
An understandingof Markov decision processes (Chapter 17) is essential for this chapter.

Token 16781:
832 Chapter 21.

Token 16782:
Reinforcement Learning 21.2 P ASSIVE REINFORCEMENT LEARNING To keep things simple, we start with the case of a passive learning agent using a state-based representation in a fully observable environment.

Token 16783:
In passive learning, the agent’s policy π is ﬁxed: in state s, it always executes the action π(s).

Token 16784:
Its goal is simply to learn how good the policy is—that is, to learn the utility function Uπ(s).

Token 16785:
We will use as our example the 4×3world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the corresponding utilities.

Token 16786:
Clearly, the passive learning task is similar to the policy evaluation task, part of the policy iteration algorithm described in Section 17.3.

Token 16787:
The main difference is that the passive learning agent does not know the transition model P(s/prime|s,a),w h i c h speciﬁes the probability of reaching state s/primefrom state safter doing action a; nor does it know the reward function R(s), which speciﬁes the reward for each state.

Token 16788:
–1+1 123 1234 123123 –1+ 1 40.6110.812 0.6550.7620.918 0.7050.6600.868 0.388 (a) (b) Figure 21.1 (a) A policy πfor the 4×3world; this policy happens to be optimal with rewards of R(s)=−0.04in the nonterminal states and no discounting.

Token 16789:
(b) The utilities of the states in the 4×3world, given policy π. The agent executes a set of trials in the environment using its policy π.

Token 16790:
In each trial, the TRIAL agent starts in state (1,1) and experiences a sequence of state transitions until it reaches one of the terminal states, (4,2) or (4,3).

Token 16791:
Its percepts supply both the current state and the reward received in that state.

Token 16792:
Typical trials might look like this: (1,1)-.04/squiggleright(1,2)-.04/squiggleright(1,3)-.04/squiggleright(1,2)-.04/squiggleright(1,3)-.04/squiggleright(2,3)-.04/squiggleright(3,3)-.04/squiggleright(4,3)+1 (1,1)-.04/squiggleright(1,2)-.04/squiggleright(1,3)-.04/squiggleright(2,3)-.04/squiggleright(3,3)-.04/squiggleright(3,2)-.04/squiggleright(3,3)-.04/squiggleright(4,3)+1 (1,1)-.04/squiggleright(2,1)-.04/squiggleright(3,1)-.04/squiggleright(3,2)-.04/squiggleright(4,2)-1.

Token 16793:
Note that each state percept is subscripted with the reward received.

Token 16794:
The object is to use the information about rewards to learn the expected utility Uπ(s)associated with each nontermi- nal state s. The utility is deﬁned to be the expected sum of (discounted) rewards obtained if

Token 16795:
Section 21.2. Passive Reinforcement Learning 833 policy πis followed.

Token 16796:
As in Equation (17.2) on page 650, we write Uπ(s)=E/bracketleftBigg∞/summationdisplay t=0γtR(St)/bracketrightBigg (21.1) where R(s)is the reward for a state, St(a random variable) is the state reached at time twhen executing policy π,a n dS0=s.

Token 16797:
We will include a discount factor γin all of our equations, but for the 4×3world we will set γ=1.

Token 16798:
21.2.1 Direct utility estimation A simple method for direct utility estimation was invented in the late 1950s in the area ofDIRECT UTILITY ESTIMATION adaptive control theory by Widrow and Hoff (1960).

Token 16799:
The idea is that the utility of a stateADAPTIVE CONTROL THEORY is the expected total reward from that state onward (called the expected reward-to-go ), and REWARD-TO-GO each trial provides a sample of this quantity for each state visited.

Token 16800:
For example, the ﬁrst trial in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), twosamples of 0.76 and 0.84 for (1,2), two samples of 0.80 and 0.88 for (1,3), and so on.

Token 16801:
Thus,at the end of each sequence, the algorithm calculates the observed reward-to-go for each stateand updates the estimated utility for that state accordingly, just by keeping a running averagefor each state in a table.

Token 16802:
In the limit of inﬁnitely many trials, the sample average will convergeto the true expectation in Equation (21.1).

Token 16803:
It is clear that direct utility estimation is just an instance of supervised learning where each example has the state as input and the observed reward-to-go as output.

Token 16804:
This means that we have reduced reinforcement learning to a standard inductive learning problem, as discussed in Chapter 18.

Token 16805:
Section 21.4 discusses the use of more powerful kinds of represen- tations for the utility function.

Token 16806:
Learning techniques for those representations can be applied directly to the observed data.

Token 16807:
Direct utility estimation succeeds in reducing the reinforcement learning problem to an inductive learning problem, about which much is known.

Token 16808:
Unfortunately, it misses a veryimportant source of information, namely, the fact that the utilities of states are not indepen-dent!

Token 16809:
The utility of each state equals its own reward plus the expected utility of its successor states.

Token 16810:
That is, the utility values obey the Bellman equations for a ﬁxed policy (see also Equation (17.10)): Uπ(s)=R(s)+γ/summationdisplay s/primeP(s/prime|s,π(s))Uπ(s/prime).

Token 16811:
(21.2) By ignoring the connections between states, direct utility estimation misses opportunities for learning.

Token 16812:
For example, the second of the three trials given earlier reaches the state (3,2),which has not previously been visited.

Token 16813:
The next transition reaches (3,3), which is knownfrom the ﬁrst trial to have a high utility.

Token 16814:
The Bellman equation suggests immediately that(3,2) is also likely to have a high utility, because it leads to (3,3), but direct utility estimation learns nothing until the end of the trial.

Token 16815:
More broadly, we can view direct utility estimation as searching for Uin a hypothesis space that is much larger than it needs to be, in that it includes many functions that violate the Bellman equations.

Token 16816:
For this reason, the algorithmoften converges very slowly.

Token 16817:
834 Chapter 21.

Token 16818:
Reinforcement Learning function PASSIVE -ADP-A GENT (percept )returns an action inputs :percept , a percept indicating the current state s/primeand reward signal r/prime persistent :π, a ﬁxed policy mdp , an MDP with model P,r e w a r d s R, discount γ U, a table of utilities, initially empty Nsa, a table of frequencies for state–action pairs, initially zero Ns/prime|sa, a table of outcome frequencies given state–action pairs, initially zero s,a, the previous state and action, initially null ifs/primeis new thenU[s/prime]←r/prime;R[s/prime]←r/prime ifsis not null then increment Nsa[s,a]a n dNs/prime|sa[s/prime,s,a] for each tsuch that Ns/prime|sa[t,s,a] is nonzero do P(t|s,a)←Ns/prime|sa[t,s,a]/Nsa[s,a] U←POLICY -EVA L UAT I O N (π,U,mdp ) ifs/prime.TERMINAL ?thens,a←nullelses,a←s/prime,π[s/prime] return a Figure 21.2 A passive reinforcement learning agent based on adaptive dynamic program- ming.

Token 16819:
The P OLICY -EVA L UAT I O N function solves the ﬁxed-policy Bellman equations, as described on page 657.

Token 16820:
21.2.2 Adaptive dynamic programming Anadaptive dynamic programming (or ADP) agent takes advantage of the constraintsADAPTIVE DYNAMIC PROGRAMMING among the utilities of states by learning the transition model that connects them and solv- ing the corresponding Markov decision process using a dynamic programming method.

Token 16821:
Fora passive learning agent, this means plugging the learned transition model P(s /prime|s,π(s))and the observed rewards R(s)into the Bellman equations (21.2) to calculate the utilities of the states.

Token 16822:
As we remarked in our discussion of policy iteration in Chapter 17, these equations are linear (no maximization involved) so they can be solved using any linear algebra pack-age.

Token 16823:
Alternatively, we can adopt the approach of modiﬁed policy iteration (see page 657), using a simpliﬁed value iteration process to update the utility estimates after each change to the learned model.

Token 16824:
Because the model usually changes only slightly with each observation, the value iteration process can use the previous utility estimates as initial values and should converge quite quickly.

Token 16825:
The process of learning the model itself is easy, because the environment is fully ob- servable.

Token 16826:
This means that we have a supervised learning task where the input is a state–actionpair and the output is the resulting state.

Token 16827:
In the simplest case, we can represent the tran-sition model as a table of probabilities.

Token 16828:
We keep track of how often each action outcome occurs and estimate the transition probability P(s /prime|s,a)from the frequency with which s/prime is reached when executing ains.

Token 16829:
For example, in the three trials given on page 832, Right is executed three times in (1,3) and two out of three times the resulting state is (2,3), so P((2,3)|(1,3),Right)is estimated to be 2/3.

Token 16830:
Section 21.2.

Token 16831:
Passive Reinforcement Learning 835 00.20.40.60.81 0 20 40 60 80 100Utility estimates Number of trials(1,1)(1,3) (3,2)(3,3)(4,3) 00.10.20.30.40.50.6 0 20 40 60 80 100RMS error in utility Number of trials (a) (b) Figure 21.3 The passive ADP learning curves for the 4×3world, given the optimal policy shown in Figure 21.1.

Token 16832:
(a) The utility estimates for a selected subset of states, as a function of the number of trials.

Token 16833:
Notice the large changes occurring around the 78th trial—this is theﬁrst time that the agent falls into the −1terminal state at (4,2).

Token 16834:
(b) The root-mean-square error (see Appendix A) in the estimate for U(1,1), averaged over 20 runs of 100 trials each.

Token 16835:
The full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor- mance on the 4×3world is shown in Figure 21.3.

Token 16836:
In terms of how quickly its value es- timates improve, the ADP agent is limited only by its ability to learn the transition model.In this sense, it provides a standard against which to measure other reinforcement learning algorithms.

Token 16837:
It is, however, intractable for large state spaces. In backgammon, for example, it would involve solving roughly 10 50equations in 1050unknowns.

Token 16838:
A reader familiar with the Bayesian learning ideas of Chapter 20 will have noticed that the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transitionmodel; moreover, by choosing a policy based solely on the estimated model it is acting as ifthe model were correct.

Token 16839:
This is not necessarily a good idea!

Token 16840:
For example, a taxi agent that didn’t know about how trafﬁc lights might ignore a red light once or twice without noill effects and then formulate a policy to ignore red lights from then on.

Token 16841:
Instead, it mightbe a good idea to choose a policy that, while not optimal for the model estimated by maxi-mum likelihood, works reasonably well for the whole range of models that have a reasonablechance of being the true model.

Token 16842:
There are two mathematical approaches that have this ﬂavor.

Token 16843:
The ﬁrst approach, Bayesian reinforcement learning , assumes a prior probability BAYESIAN REINFORCEMENTLEARNING P(h)for each hypothesis habout what the true model is; the posterior probability P(h|e)is obtained in the usual way by Bayes’ rule given the observations to date.

Token 16844:
Then, if the agent hasdecided to stop learning, the optimal policy is the one that gives the highest expected utility.Letu π hbe the expected utility, averaged over all possible start states, obtained by executing policy πin model h.T h e nw eh a v e π∗=a r g m a x π/summationdisplay hP(h|e)uπ h.

Token 16845:
836 Chapter 21. Reinforcement Learning In some special cases, this policy can even be computed!

Token 16846:
If the agent will continue learning in the future, however, then ﬁnding an optimal policy becomes considerably more difﬁcult,because the agent must consider the effects of future observations on its beliefs about thetransition model.

Token 16847:
The problem becomes a POMDP whose belief states are distributions overmodels.

Token 16848:
This concept provides an analytical foundation for understanding the exploration problem described in Section 21.3.

Token 16849:
The second approach, derived from robust control theory , allows for a setof possible ROBUST CONTROL THEORY modelsHand deﬁnes an optimal robust policy as one that gives the best outcome in the worst case overH: π∗=a r g m a x πmin huπ h. Often, the setHwill be the set of models that exceed some likelihood threshold on P(h|e), so the robust and Bayesian approaches are related.

Token 16850:
Sometimes, the robust solution can becomputed efﬁciently.

Token 16851:
There are, moreover, reinforcement learning algorithms that tend toproduce robust solutions, although we do not cover them here.

Token 16852:
21.2.3 Temporal-difference learning Solving the underlying MDP as in the preceding section is not the only way to bring theBellman equations to bear on the learning problem.

Token 16853:
Another way is to use the observedtransitions to adjust the utilities of the observed states so that they agree with the constraintequations.

Token 16854:
Consider, for example, the transition from (1,3) to (2,3) in the second trial onpage 832.

Token 16855:
Suppose that, as a result of the ﬁrst trial, the utility estimates are U π(1,3)= 0 .84 andUπ(2,3)= 0 .92.

Token 16856:
Now, if this transition occurred all the time, we would expect the utili- ties to obey the equation Uπ(1,3) =−0.04 +Uπ(2,3), soUπ(1,3)would be 0.88.

Token 16857:
Thus, its current estimate of 0.84 might be a little low and should be increased.

Token 16858:
More generally, when a transition occurs from state sto state s/prime, we apply the following update to Uπ(s): Uπ(s)←Uπ(s)+α(R(s)+γUπ(s/prime)−Uπ(s)).

Token 16859:
(21.3) Here,αis the learning rate parameter.

Token 16860:
Because this update rule uses the difference in utilities between successive states, it is often called the temporal-difference , or TD, equation.TEMPORAL- DIFFERENCE All temporal-difference methods work by adjusting the utility estimates towards the ideal equilibrium that holds locally when the utility estimates are correct.

Token 16861:
In the case of pas- sive learning, the equilibrium is given by Equation (21.2).

Token 16862:
Now Equation (21.3) does in factcause the agent to reach the equilibrium given by Equation (21.2), but there is some subtletyinvolved.

Token 16863:
First, notice that the update involves only the observed successor s /prime, whereas the actual equilibrium conditions involve all possible next states.

Token 16864:
One might think that this causes an improperly large change in Uπ(s)when a very rare transition occurs; but, in fact, because rare transitions occur only rarely, the average value ofUπ(s)will converge to the correct value.

Token 16865:
Furthermore, if we change αfrom a ﬁxed parameter to a function that decreases as the number of times a state has been visited increases, then Uπ(s)itself will converge to the

Token 16866:
Section 21.2.

Token 16867:
Passive Reinforcement Learning 837 function PASSIVE -TD-A GENT (percept )returns an action inputs :percept , a percept indicating the current state s/primeand reward signal r/prime persistent :π, a ﬁxed policy U, a table of utilities, initially empty Ns, a table of frequencies for states, initially zero s,a,r, the previous state, action, and reward, initially null ifs/primeis new thenU[s/prime]←r/prime ifsis not null then increment Ns[s] U[s]←U[s]+α(Ns[s])(r+γU[s/prime]−U[s]) ifs/prime.TERMINAL ?thens,a,r←nullelses,a,r←s/prime,π[s/prime],r/prime return a Figure 21.4 A passive reinforcement learning agent that learns utility estimates using tem- poral differences.

Token 16868:
The step-size function α(n)is chosen to ensure convergence, as described in the text.

Token 16869:
correct value.1This gives us the agent program shown in Figure 21.4. Figure 21.5 illustrates the performance of the passive TD agent on the 4×3world.

Token 16870:
It does not learn quite as fast as the ADP agent and shows much higher variability, but it is much simpler and requires much less computation per observation.

Token 16871:
Notice that TD does not need a transition model to perform its updates.

Token 16872:
The environment supplies the connection between neighboring states in the form of observed transitions.

Token 16873:
The ADP approach and the TD approach are actually closely related.

Token 16874:
Both try to make local adjustments to the utility estimates in order to make each state “agree” with its succes-sors.

Token 16875:
One difference is that TD adjusts a state to agree with its observed successor (Equa- tion (21.3)), whereas ADP adjusts the state to agree with allof the successors that might occur, weighted by their probabilities (Equation (21.2)).

Token 16876:
This difference disappears whenthe effects of TD adjustments are averaged over a large number of transitions, because thefrequency of each successor in the set of transitions is approximately proportional to its prob- ability.

Token 16877:
A more important difference is that whereas TD makes a single adjustment per ob- served transition, ADP makes as many as it needs to restore consistency between the utilityestimates Uand the environment model P. Although the observed transition makes only a local change in P, its effects might need to be propagated throughout U.

Token 16878:
Thus, TD can be viewed as a crude but efﬁcient ﬁrst approximation to ADP.

Token 16879:
Each adjustment made by ADP could be seen, from the TD point of view, as a re- sult of a “pseudoexperience” generated by simulating the current environment model.

Token 16880:
Itis possible to extend the TD approach to use an environment model to generate severalpseudoexperiences—transitions that the TD agent can imagine might happen, given its current model.

Token 16881:
For each observed transition, the TD agent can generate a large number of imaginary 1The technical conditions are given on page 725.

Token 16882:
In Figure 21.5 we have used α(n)=6 0 /(59 + n),w h i c h satisﬁes the conditions.

Token 16883:
838 Chapter 21.

Token 16884:
Reinforcement Learning 00.20.40.60.81 0 100 200 300 400 500Utility estimates Number of trials(1,1)(1,3) (2,1)(3,3)(4,3) 00.10.20.30.40.50.6 0 20 40 60 80 100RMS error in utility Number of trials (a) (b) Figure 21.5 The TD learning curves for the 4×3world.

Token 16885:
(a) The utility estimates for a selected subset of states, as a function of the number of trials.

Token 16886:
(b) The root-mean-square error in the estimate for U(1,1), averaged over 20 runs of 500 trials each.

Token 16887:
Only the ﬁrst 100 trials are shown to enable comparison with Figure 21.3. transitions.

Token 16888:
In this way, the resulting utility estimates will approximate more and more closely those of ADP—of course, at the expense of increased computation time.

Token 16889:
In a similar vein, we can generate more efﬁcient versions of ADP by directly approxi- mating the algorithms for value iteration or policy iteration.

Token 16890:
Even though the value iterationalgorithm is efﬁcient, it is intractable if we have, say, 10 100states.

Token 16891:
However, many of the necessary adjustments to the state values on each iteration will be extremely tiny.

Token 16892:
One pos-sible approach to generating reasonably good answers quickly is to bound the number ofadjustments made after each observed transition.

Token 16893:
One can also use a heuristic to rank the pos-sible adjustments so as to carry out only the most signiﬁcant ones.

Token 16894:
The prioritized sweeping PRIORITIZED SWEEPING heuristic prefers to make adjustments to states whose likely successors have just undergone a large adjustment in their own utility estimates.

Token 16895:
Using heuristics like this, approximate ADP algorithms usually can learn roughly as fast as full ADP, in terms of the number of training se- quences, but can be several orders of magnitude more efﬁcient in terms of computation.

Token 16896:
(See Exercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.

Token 16897:
Approximate ADP algorithms have an additional advantage: in the early stages of learning a new environment, the environment model Poften will be far from correct, so there is little point in calculating an exact utility function to match it.

Token 16898:
An approximation algorithm can use a minimum adjustment size that decreases as the environment model becomes more accurate.This eliminates the very long value iterations that can occur early in learning due to largechanges in the model.

Token 16899:
Section 21.3.

Token 16900:
Active Reinforcement Learning 839 21.3 A CTIVE REINFORCEMENT LEARNING A passive learning agent has a ﬁxed policy that determines its behavior.

Token 16901:
An active agent must decide what actions to take.

Token 16902:
Let us begin with the adaptive dynamic programming agent andconsider how it must be modiﬁed to handle this new freedom.

Token 16903:
First, the agent will need to learn a complete model with outcome probabilities for all actions, rather than just the model for the ﬁxed policy.

Token 16904:
The simple learning mechanism usedby P ASSIVE -ADP-A GENT will do just ﬁne for this.

Token 16905:
Next, we need to take into account the fact that the agent has a choice of actions.

Token 16906:
The utilities it needs to learn are those deﬁned by theoptimal policy; they obey the Bellman equations given on page 652, which we repeat here for convenience: U(s)=R(s)+γmax a/summationdisplay s/primeP(s/prime|s,a)U(s/prime).

Token 16907:
(21.4) These equations can be solved to obtain the utility function Uusing the value iteration or policy iteration algorithms from Chapter 17.

Token 16908:
The ﬁnal issue is what to do at each step.

Token 16909:
Having obtained a utility function Uthat is optimal for the learned model, the agent can extract an optimal action by one-step look-ahead to maximize the expected utility; alternatively, if ituses policy iteration, the optimal policy is already available, so it should simply execute theaction the optimal policy recommends.

Token 16910:
Or should it?

Token 16911:
21.3.1 Exploration Figure 21.6 shows the results of one sequence of trials for an ADP agent that follows therecommendation of the optimal policy for the learned model at each step.

Token 16912:
The agent does notlearn the true utilities or the true optimal policy!

Token 16913:
What happens instead is that, in the 39th trial, it ﬁnds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),(3,2), and (3,3).

Token 16914:
(See Figure 21.6(b).)

Token 16915:
After experimenting with minor variations, from the 276th trial onward it sticks to that policy, never learning the utilities of the other states and never ﬁnding the optimal route via (1,2), (1,3), and (2,3).

Token 16916:
We call this agent the greedy agent .

Token 16917:
GREEDY AGENT Repeated experiments show that the greedy agent very seldom converges to the optimal policy for this environment and sometimes converges to really horrendous policies.

Token 16918:
How can it be that choosing the optimal action leads to suboptimal results?

Token 16919:
The answer is that the learned model is not the same as the true environment; what is optimal in thelearned model can therefore be suboptimal in the true environment.

Token 16920:
Unfortunately, the agentdoes not know what the true environment is, so it cannot compute the optimal action for thetrue environment.

Token 16921:
What, then, is to be done?

Token 16922:
What the greedy agent has overlooked is that actions do more than provide rewards according to the current learned model; they also contribute to learning the true model by af- fecting the percepts that are received.

Token 16923:
By improving the model, the agent will receive greater rewards in the future.

Token 16924:
2An agent therefore must make a tradeoff between exploitation to EXPLOITATION maximize its reward—as reﬂected in its current utility estimates—and exploration to maxi- EXPLORATION 2Notice the direct analogy to the theory of information value in Chapter 16.

Token 16925:
840 Chapter 21.

Token 16926:
Reinforcement Learning 00.511.52 050100 150 200 250 300 350 400 450 500RMS error, policy loss Number of trialsRMS error Policy loss 123123 –1+1 4 (a) (b) Figure 21.6 Performance of a greedy ADP agent that executes the action recommended by the optimal policy for the learned model.

Token 16927:
(a) RMS error in the utility estimates averaged over the nine nonterminal squares.

Token 16928:
(b) The suboptimal policy to which the greedy agent converges in this particular sequence of trials. mize its long-term well-being.

Token 16929:
Pure exploitation risks getting stuck in a rut.

Token 16930:
Pure exploration to improve one’s knowledge is of no use if one never puts that knowledge into practice.

Token 16931:
In thereal world, one constantly has to decide between continuing in a comfortable existence andstriking out into the unknown in the hopes of discovering a new and better life.

Token 16932:
With greaterunderstanding, less exploration is necessary. Can we be a little more precise than this? Is there an optimal exploration policy?

Token 16933:
This question has been studied in depth in the subﬁeld of statistical decision theory that deals withso-called bandit problems . (See sidebar.)

Token 16934:
BANDIT PROBLEM Although bandit problems are extremely difﬁcult to solve exactly to obtain an optimal exploration method, it is nonetheless possible to come up with a reasonable scheme that will eventually lead to optimal behavior by the agent.

Token 16935:
Technically, any such scheme needsto be greedy in the limit of inﬁnite exploration, or GLIE .

Token 16936:
A GLIE scheme must try each GLIE action in each state an unbounded number of times to avoid having a ﬁnite probability that an optimal action is missed because of an unusually bad series of outcomes.

Token 16937:
An ADP agent using such a scheme will eventually learn the true environment model.

Token 16938:
A GLIE scheme must also eventually become greedy, so that the agent’s actions become optimal with respect to thelearned (and hence the true) model.

Token 16939:
There are several GLIE schemes; one of the simplest is to have the agent choose a ran- dom action a fraction 1/tof the time and to follow the greedy policy otherwise.

Token 16940:
While this does eventually converge to an optimal policy, it can be extremely slow.

Token 16941:
A more sensible approach would give some weight to actions that the agent has not tried very often, whiletending to avoid actions that are believed to be of low utility.

Token 16942:
This can be implemented byaltering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively

Token 16943:
Section 21.3. Active Reinforcement Learning 841 EXPLORATION AND BANDITS In Las Vegas, a one-armed bandit is a slot machine.

Token 16944:
A gambler can insert a coin, pull the lever, and collect the winnings (if any). An n-armed bandit hasnlevers.

Token 16945:
The gambler must choose which lever to play on each successive coin—the onethat has paid off best, or maybe one that has not been tried?

Token 16946:
Then-armed bandit problem is a formal model for real problems in many vi- tally important areas, such as deciding on the annual budget for AI research anddevelopment.

Token 16947:
Each arm corresponds to an action (such as allocating $20 millionfor the development of new AI textbooks), and the payoff from pulling the arm cor-responds to the beneﬁts obtained from taking the action (immense).

Token 16948:
Exploration,whether it is exploration of a new research ﬁeld or exploration of a new shopping mall, is risky, is expensive, and has uncertain payoffs; on the other hand, failure to explore at all means that one never discovers anyactions that are worthwhile.

Token 16949:
To formulate a bandit problem properly, one must deﬁne exactly what is meant by optimal behavior.

Token 16950:
Most deﬁnitions in the literature assume that the aim is tomaximize the expected total reward obtained over the agent’s lifetime.

Token 16951:
These deﬁ-nitions require that the expectation be taken over the possible worlds that the agentcould be in, as well as over the possible results of each action sequence in any givenworld.

Token 16952:
Here, a “world” is deﬁned by the transition model P(s /prime|s,a).

Token 16953:
Thus, in or- der to act optimally, the agent needs a prior distribution over the possible models.

Token 16954:
The resulting optimization problems are usually wildly intractable.

Token 16955:
In some cases—for example, when the payoff of each machine is independent and discounted rewards are used—it is possible to calculate a Gittins index for each slot machine (Gittins, 1989).

Token 16956:
The index is a function only of the number oftimes the slot machine has been played and how much it has paid off.

Token 16957:
The index foreach machine indicates how worthwhile it is to invest more; generally speaking, thehigher the expected return and the higher the uncertainty in the utility of a givenchoice, the better.

Token 16958:
Choosing the machine with the highest index value gives anoptimal exploration policy.

Token 16959:
Unfortunately, no way has been found to extend Gittinsindices to sequential decision problems.

Token 16960:
One can use the theory of n-armed bandits to argue for the reasonableness of the selection strategy in genetic algorithms. (See Chapter 4.)

Token 16961:
If you considereach arm in an n-armed bandit problem to be a possible string of genes, and the investment of a coin in one arm to be the reproduction of those genes, then it can be proven that genetic algorithms allocate coins optimally, given an appropriate setof independence assumptions.

Token 16962:
842 Chapter 21. Reinforcement Learning unexplored state–action pairs.

Token 16963:
Essentially, this amounts to an optimistic prior over the possi- ble environments and causes the agent to behave initially as if there were wonderful rewardsscattered all over the place.

Token 16964:
Let us use U +(s)to denote the optimistic estimate of the utility (i.e., the expected reward-to-go) of the state s,a n dl e t N(s,a)be the number of times action ahas been tried in state s. Suppose we are using value iteration in an ADP learning agent; then we need to rewrite the update equation (Equation (17.6) on page 652) to incorporate the optimistic estimate.

Token 16965:
The following equation does this: U+(s)←R(s)+γmax af/parenleftbigg/summationtext s/primeP(s/prime|s,a)U+(s/prime),N(s,a)/parenrightbigg .

Token 16966:
(21.5) Here, f(u,n)is called the exploration function .

Token 16967:
It determines how greed (preference forEXPLORATION FUNCTION high values of u) is traded off against curiosity (preference for actions that have not been tried often and have low n).

Token 16968:
The function f(u,n)should be increasing in uand decreasing inn. Obviously, there are many possible functions that ﬁt these conditions.

Token 16969:
One particularly simple deﬁnition is f(u,n)=/braceleftbiggR+ifn<N e uotherwise where R+is an optimistic estimate of the best possible reward obtainable in any state and Ne is a ﬁxed parameter.

Token 16970:
This will have the effect of making the agent try each action–state pair at least Netimes.

Token 16971:
The fact that U+rather than Uappears on the right-hand side of Equation (21.5) is very important.

Token 16972:
As exploration proceeds, the states and actions near the start state might wellbe tried a large number of times.

Token 16973:
If we used U, the more pessimistic utility estimate, then the agent would soon become disinclined to explore further aﬁeld.

Token 16974:
The use of U +means that the beneﬁts of exploration are propagated back from the edges of unexplored regions,so that actions that lead toward unexplored regions are weighted more highly, rather than just actions that are themselves unfamiliar.

Token 16975:
The effect of this exploration policy can be seenclearly in Figure 21.7, which shows a rapid convergence toward optimal performance, unlikethat of the greedy approach.

Token 16976:
A very nearly optimal policy is found after just 18 trials. Noticethat the utility estimates themselves do not converge as quickly.

Token 16977:
This is because the agentstops exploring the unrewarding parts of the state space fairly soon, visiting them only “byaccident” thereafter.

Token 16978:
However, it makes perfect sense for the agent not to care about the exactutilities of states that it knows are undesirable and can be avoided.

Token 16979:
21.3.2 Learning an action-utility function Now that we have an active ADP agent, let us consider how to construct an active temporal-difference learning agent.

Token 16980:
The most obvious change from the passive case is that the agentis no longer equipped with a ﬁxed policy, so, if it learns a utility function U, it will need to learn a model in order to be able to choose an action based on Uvia one-step look-ahead.

Token 16981:
The model acquisition problem for the TD agent is identical to that for the ADP agent. Whatof the TD update rule itself?

Token 16982:
Perhaps surprisingly, the update rule (21.3) remains unchanged.This might seem odd, for the following reason: Suppose the agent takes a step that normally

Token 16983:
Section 21.3.

Token 16984:
Active Reinforcement Learning 843 0.60.811.21.41.61.822.2 0 20 40 60 80 100Utility estimates Number of trials(1,1) (1,2) (1,3) (2,3) (3,2) (3,3) (4,3) 00.20.40.60.811.21.4 0 20 40 60 80 100RMS error, policy loss Number of trialsRMS error Policy loss (a) (b) Figure 21.7 Performance of the exploratory ADP agent.

Token 16985:
using R+=2andNe=5. ( a ) Utility estimates for selected states over time. (b) The RMS error in utility values and the associated policy loss.

Token 16986:
leads to a good destination, but because of nondeterminism in the environment the agent ends up in a catastrophic state.

Token 16987:
The TD update rule will take this as seriously as if the outcome hadbeen the normal result of the action, whereas one might suppose that, because the outcomewas a ﬂuke, the agent should not worry about it too much.

Token 16988:
In fact, of course, the unlikely outcome will occur only infrequently in a large set of training sequences; hence in the long run its effects will be weighted proportionally to its probability, as we would hope.

Token 16989:
Onceagain, it can be shown that the TD algorithm will converge to the same values as ADP as thenumber of training sequences tends to inﬁnity.

Token 16990:
There is an alternative TD method, called Q-learning , which learns an action-utility representation instead of learning utilities.

Token 16991:
We will use the notation Q(s,a)to denote the value of doing action ain state s. Q-values are directly related to utility values as follows: U(s)=m a x aQ(s,a).

Token 16992:
(21.6) Q-functions may seem like just another way of storing utility information, but they have a very important property: a TD agent that learns a Q-function does not need a model of the formP(s/prime|s,a), either for learning or for action selection.

Token 16993:
For this reason, Q-learning is called a model-free method.

Token 16994:
As with utilities, we can write a constraint equation that must MODEL-FREE hold at equilibrium when the Q-values are correct: Q(s,a)=R(s)+γ/summationdisplay s/primeP(s/prime|s,a)max a/primeQ(s/prime,a/prime).

Token 16995:
(21.7) As in the ADP learning agent, we can use this equation directly as an update equation for an iteration process that calculates exact Q-values, given an estimated model.

Token 16996:
This does,however, require that a model also be learned, because the equation uses P(s /prime|s,a).T h e temporal-difference approach, on the other hand, requires no model of state transitions—all

Token 16997:
844 Chapter 21.

Token 16998:
Reinforcement Learning function Q-L EARNING -AGENT (percept )returns an action inputs :percept , a percept indicating the current state s/primeand reward signal r/prime persistent :Q, a table of action values indexed by state and action, initially zero Nsa, a table of frequencies for state–action pairs, initially zero s,a,r, the previous state, action, and reward, initially null ifTERMINAL ?

Token 16999:
(s)thenQ[s,None ]←r/prime ifsis not null then increment Nsa[s,a] Q[s,a]←Q[s,a]+α(Nsa[s,a])(r+γmax a/primeQ[s/prime,a/prime]−Q[s,a]) s,a,r←s/prime,argmaxa/primef(Q[s/prime,a/prime],Nsa[s/prime,a/prime]),r/prime return a Figure 21.8 An exploratory Q-learning agent.

Token 17000:
It is an active learner that learns the value Q(s, a)of each action in each situation.

Token 17001:
It uses the same exploration function fas the ex- ploratory ADP agent, but avoids having to learn the transition model because the Q-value of a state can be related directly to those of its neighbors.

Token 17002:
it needs are the Qvalues.

Token 17003:
The update equation for TD Q-learning is Q(s,a)←Q(s,a)+α(R(s)+γmax a/primeQ(s/prime,a/prime)−Q(s,a)), (21.8) which is calculated whenever action ais executed in state sleading to state s/prime.

Token 17004:
The complete agent design for an exploratory Q-learning agent using TD is shown in Figure 21.8.

Token 17005:
Notice that it uses exactly the same exploration function fas that used by the exploratory ADP agent—hence the need to keep statistics on actions taken (the table N).

Token 17006:
If a simpler exploration policy is used—say, acting randomly on some fraction of steps, wherethe fraction decreases over time—then we can dispense with the statistics.

Token 17007:
Q-learning has a close relative called SARSA (for State-Action-Reward-State-Action).

Token 17008:
SARSA The update rule for SARSA is very similar to Equation (21.8): Q(s,a)←Q(s,a)+α(R(s)+γQ(s/prime,a/prime)−Q(s,a)), (21.9) where a/primeis the action actually taken in state s/prime.

Token 17009:
The rule is applied at the end of each s, a, r, s/prime,a/primequintuplet—hence the name.

Token 17010:
The difference from Q-learning is quite subtle: whereas Q-learning backs up the best Q-value from the state reached in the observed transi- tion, SARSA waits until an action is actually taken and backs up the Q-value for that action.Now, for a greedy agent that always takes the action with best Q-value, the two algorithmsare identical.

Token 17011:
When exploration is happening, however, they differ signiﬁcantly.

Token 17012:
BecauseQ-learning uses the best Q-value, it pays no attention to the actual policy being followed—itis an off-policy learning algorithm, whereas SARSA is an on-policy algorithm.

Token 17013:
Q-learning is OFF-POLICY ON-POLICY more ﬂexible than SARSA, in the sense that a Q-learning agent can learn how to behave well even when guided by a random or adversarial exploration policy.

Token 17014:
On the other hand, SARSA is more realistic: for example, if the overall policy is even partly controlled by other agents, it is better to learn a Q-function for what will actually happen rather than what the agent would like to happen.

Token 17015:
Section 21.4.

Token 17016:
Generalization in Reinforcement Learning 845 Both Q-learning and SARSA learn the optimal policy for the 4×3world, but do so at a much slower rate than the ADP agent.

Token 17017:
This is because the local updates do not enforceconsistency among all the Q-values via the model.

Token 17018:
The comparison raises a general question:is it better to learn a model and a utility function or to learn an action-utility function withno model?

Token 17019:
In other words, what is the best way to represent the agent function? This is an issue at the foundations of artiﬁcial intelligence.

Token 17020:
As we stated in Chapter 1, one of the key historical characteristics of much of AI research is its (often unstated) adherence to theknowledge-based approach.

Token 17021:
This amounts to an assumption that the best way to represent the agent function is to build a representation of some aspects of the environment in whichthe agent is situated.

Token 17022:
Some researchers, both inside and outside AI, have claimed that the availability of model-free methods such as Q-learning means that the knowledge-based approach is unnec-essary.

Token 17023:
There is, however, little to go on but intuition.

Token 17024:
Our intuition, for what it’s worth, is thatas the environment becomes more complex, the advantages of a knowledge-based approachbecome more apparent.

Token 17025:
This is borne out even in games such as chess, checkers (draughts),and backgammon (see next section), where efforts to learn an evaluation function by meansof a model have met with more success than Q-learning methods.

Token 17026:
21.4 G ENERALIZATION IN REINFORCEMENT LEARNING So far, we have assumed that the utility functions and Q-functions learned by the agents are represented in tabular form with one output value for each input tuple.

Token 17027:
Such an approach works reasonably well for small state spaces, but the time to convergence and (for ADP) thetime per iteration increase rapidly as the space gets larger.

Token 17028:
With carefully controlled, approx-imate ADP methods, it might be possible to handle 10,000 states or more.

Token 17029:
This sufﬁces fortwo-dimensional maze-like environments, but more realistic worlds are out of the question.Backgammon and chess are tiny subsets of the real world, yet their state spaces contain onthe order of 10 20and1040states, respectively.

Token 17030:
It would be absurd to suppose that one must visit all these states many times in order to learn how to play the game!

Token 17031:
One way to handle such problems is to use function approximation , which simplyFUNCTION APPROXIMATION means using any sort of representation for the Q-function other than a lookup table.

Token 17032:
The representation is viewed as approximate because it might not be the case that the true utility function or Q-function can be represented in the chosen form.

Token 17033:
For example, in Chapter 5 we described an evaluation function for chess that is represented as a weighted linear function of a set of features (orbasis functions )f1,...,f n: BASIS F UNCTION ˆUθ(s)=θ1f1(s)+θ2f2(s)+···+θnfn(s).

Token 17034:
A reinforcement learning algorithm can learn values for the parameters θ=θ1,...,θ nsuch that the evaluation function ˆUθapproximates the true utility function.

Token 17035:
Instead of, say, 1040 values in a table, this function approximator is characterized by, say, n=2 0 parameters— anenormous compression.

Token 17036:
Although no one knows the true utility function for chess, no one believes that it can be represented exactly in 20 numbers.

Token 17037:
If the approximation is good

Token 17038:
846 Chapter 21.

Token 17039:
Reinforcement Learning enough, however, the agent might still play excellent chess.3Function approximation makes it practical to represent utility functions for very large state spaces, but that is not its principalbeneﬁt.

Token 17040:
The compression achieved by a function approximator allows the learning agent to generalize from states it has visited to states it has not visited.

Token 17041:
That is, the most important aspect of function approximation is not that it requires less space, but that it allows for induc- tive generalization over input states.

Token 17042:
To give you some idea of the power of this effect: by examining only one in every 1012of the possible backgammon states, it is possible to learn a utility function that allows a program to play as well as any human (Tesauro, 1992).

Token 17043:
On the ﬂip side, of course, there is the problem that there could fail to be any function in the chosen hypothesis space that approximates the true utility function sufﬁciently well.As in all inductive learning, there is a tradeoff between the size of the hypothesis space andthe time it takes to learn the function.

Token 17044:
A larger hypothesis space increases the likelihood thata good approximation can be found, but also means that convergence is likely to be delayed.

Token 17045:
Let us begin with the simplest case, which is direct utility estimation. (See Section 21.2.)

Token 17046:
With function approximation, this is an instance of supervised learning .

Token 17047:
For example, sup- pose we represent the utilities for the 4×3world using a simple linear function.

Token 17048:
The features of the squares are just their xandycoordinates, so we have ˆU θ(x,y)=θ0+θ1x+θ2y.

Token 17049:
(21.10) Thus, if (θ0,θ1,θ2)=(0.5,0.2,0.1),t h e n ˆUθ(1,1)= 0 .8.

Token 17050:
Given a collection of trials, we ob- tain a set of sample values of ˆUθ(x,y), and we can ﬁnd the best ﬁt, in the sense of minimizing the squared error, using standard linear regression.

Token 17051:
(See Chapter 18.) For reinforcement learning, it makes more sense to use an online learning algorithm that updates the parameters after each trial.

Token 17052:
Suppose we run a trial and the total rewardobtained starting at (1,1) is 0.4.

Token 17053:
This suggests that ˆU θ(1,1), currently 0.8, is too large and must be reduced. How should the parameters be adjusted to achieve this?

Token 17054:
As with neural- network learning, we write an error function and compute its gradient with respect to the parameters.

Token 17055:
If uj(s)is the observed total reward from state sonward in the jth trial, then the error is deﬁned as (half) the squared difference of the predicted total and the actual total:E j(s)=(ˆUθ(s)−uj(s))2/2.

Token 17056:
The rate of change of the error with respect to each parameter θiis∂Ej/∂θi, so to move the parameter in the direction of decreasing the error, we want θi←θi−α∂Ej(s) ∂θi=θi+α(uj(s)−ˆUθ(s))∂ˆUθ(s) ∂θi.

Token 17057:
(21.11) This is called the Widrow–Hoff rule ,o rt h e delta rule , for online least-squares.

Token 17058:
For the WIDROW–HOFF RULE DELTA RULE linear function approximator ˆUθ(s)in Equation (21.10), we get three simple update rules: θ0←θ0+α(uj(s)−ˆUθ(s)), θ1←θ1+α(uj(s)−ˆUθ(s))x, θ2←θ2+α(uj(s)−ˆUθ(s))y.

Token 17059:
3We do know that the exact utility function can be represented in a page or two of Lisp, Java, or C++.

Token 17060:
That is, it can be represented by a program that solves the game exactly every time it is called.

Token 17061:
We are interested only in function approximators that use a reasonable amount of computation.

Token 17062:
It might in fact be better to learn a very simple function approximator and combine it with a certa in amount of look-ahead search.

Token 17063:
The tradeoffs involved are currently not well understood.

Token 17064:
Section 21.4.

Token 17065:
Generalization in Reinforcement Learning 847 We can apply these rules to the example where ˆUθ(1,1)is 0.8 and uj(1,1)is 0.4. θ0,θ1, andθ2are all decreased by 0.4α, which reduces the error for (1,1).

Token 17066:
Notice that changing the parameters θin response to an observed transition between two states also changes the values ofˆUθfor every other state!

Token 17067:
This is what we mean by saying that function approximation allows a reinforcement learner to generalize from its experiences.

Token 17068:
We expect that the agent will learn faster if it uses a function approximator, provided that the hypothesis space is not too large, but includes some functions that are a reasonablygood ﬁt to the true utility function.

Token 17069:
Exercise 21.5 asks you to evaluate the performance ofdirect utility estimation, both with and without function approximation.

Token 17070:
The improvement inthe4×3world is noticeable but not dramatic, because this is a very small state space to begin with.

Token 17071:
The improvement is much greater in a 10×10world with a +1 reward at (10,10).

Token 17072:
This world is well suited for a linear utility function because the true utility function is smoothand nearly linear. (See Exercise 21.8.)

Token 17073:
If we put the +1 reward at (5,5), the true utility ismore like a pyramid and the function approximator in Equation (21.10) will fail miserably.All is not lost, however!

Token 17074:
Remember that what matters for linear function approximationis that the function be linear in the parameters —the features themselves can be arbitrary nonlinear functions of the state variables.

Token 17075:
Hence, we can include a term such as θ 3f3(x,y)= θ3/radicalbig (x−xg)2+(y−yg)2that measures the distance to the goal.

Token 17076:
We can apply these ideas equally well to temporal-difference learners.

Token 17077:
All we need do is adjust the parameters to try to reduce the temporal difference between successive states.

Token 17078:
Thenew versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)are given by θ i←θi+α[R(s)+γˆUθ(s/prime)−ˆUθ(s)]∂ˆUθ(s) ∂θi(21.12) for utilities and θi←θi+α[R(s)+γmax a/primeˆQθ(s/prime,a/prime)−ˆQθ(s,a)]∂ˆQθ(s,a) ∂θi(21.13) for Q-values.

Token 17079:
For passive TD learning, the update rule can be shown to converge to the closest possible4approximation to the true function when the function approximator is linear in the parameters.

Token 17080:
With active learning and nonlinear functions such as neural networks, all bets are off: There are some very simple cases in which the parameters can go off to inﬁnityeven though there are good solutions in the hypothesis space.

Token 17081:
There are more sophisticatedalgorithms that can avoid these problems, but at present reinforcement learning with generalfunction approximators remains a delicate art.

Token 17082:
Function approximation can also be very helpful for learning a model of the environ- ment.

Token 17083:
Remember that learning a model for an observable environment is a supervised learn- ing problem, because the next percept gives the outcome state.

Token 17084:
Any of the supervised learningmethods in Chapter 18 can be used, with suitable adjustments for the fact that we need to pre- dict a complete state description rather than just a Boolean classiﬁcation or a single real value.

Token 17085:
For a partially observable environment, the learning problem is much more difﬁcult.

Token 17086:
If we know what the hidden variables are and how they are causally related to each other and to the 4The deﬁnition of distance between utility functions is rather technical; see Tsitsiklis and Van Roy (1997).

Token 17087:
848 Chapter 21.

Token 17088:
Reinforcement Learning observable variables, then we can ﬁx the structure of a dynamic Bayesian network and use the EM algorithm to learn the parameters, as was described in Chapter 20.

Token 17089:
Inventing the hiddenvariables and learning the model structure are still open problems. Some practical examplesare described in Section 21.6.

Token 17090:
21.5 P OLICY SEARCH The ﬁnal approach we will consider for reinforcement learning problems is called policy search .

Token 17091:
In some ways, policy search is the simplest of all the methods in this chapter: the POLICY SEARCH idea is to keep twiddling the policy as long as its performance improves, then stop.

Token 17092:
Let us begin with the policies themselves. Remember that a policy πis a function that maps states to actions.

Token 17093:
We are interested primarily in parameterized representations of πthat have far fewer parameters than there are states in the state space (just as in the precedingsection).

Token 17094:
For example, we could represent πby a collection of parameterized Q-functions, one for each action, and take the action with the highest predicted value: π(s)=m a x aˆQθ(s,a).

Token 17095:
(21.14) Each Q-function could be a linear function of the parameters θ, as in Equation (21.10), or it could be a nonlinear function such as a neural network.

Token 17096:
Policy search will then ad-just the parameters θto improve the policy.

Token 17097:
Notice that if the policy is represented by Q- functions, then policy search results in a process that learns Q-functions.

Token 17098:
This process is not the same as Q-learning!

Token 17099:
In Q-learning with function approximation, the algorithm ﬁnds a value of θsuch that ˆQθis “close” to Q∗, the optimal Q-function.

Token 17100:
Policy search, on the other hand, ﬁnds a value of θthat results in good performance; the values found by the two methods may differ very substantially.

Token 17101:
(For example, the approximate Q-function deﬁnedbyˆQ θ(s,a)=Q∗(s,a)/10gives optimal performance, even though it is not at all close to Q∗.)

Token 17102:
Another clear instance of the difference is the case where π(s)is calculated using, say, depth-10 look-ahead search with an approximate utility function ˆUθ.Av a l u eo f θthat gives good results may be a long way from making ˆUθresemble the true utility function.

Token 17103:
One problem with policy representations of the kind given in Equation (21.14) is that the policy is a discontinuous function of the parameters when the actions are discrete.

Token 17104:
(For a continuous action space, the policy can be a smooth function of the parameters.)

Token 17105:
That is, therewill be values of θsuch that an inﬁnitesimal change in θcauses the policy to switch from one action to another.

Token 17106:
This means that the value of the policy may also change discontinuously, which makes gradient-based search difﬁcult.

Token 17107:
For this reason, policy search methods often use astochastic policy representation π θ(s,a), which speciﬁes the probability of selecting action STOCHASTIC POLICY ain state s. One popular representation is the softmax function : SOFTMAX FUNCTION πθ(s,a)=eˆQθ(s,a)//summationdisplay a/primeeˆQθ(s,a/prime).

Token 17108:
Softmax becomes nearly deterministic if one action is much better than the others, but it always gives a differentiable function of θ; hence, the value of the policy (which depends in

Token 17109:
Section 21.5.

Token 17110:
Policy Search 849 a continuous fashion on the action selection probabilities) is a differentiable function of θ. Softmax is a generalization of the logistic function (page 725) to multiple variables.

Token 17111:
Now let us look at methods for improving the policy. We start with the simplest case: a deterministic policy and a deterministic environment.

Token 17112:
Let ρ(θ)be the policy value , i.e., the POLICY VALUE expected reward-to-go when πθis executed.

Token 17113:
If we can derive an expression for ρ(θ)in closed form, then we have a standard optimization problem, as described in Chapter 4.

Token 17114:
We can follow thepolicy gradient vector∇θρ(θ)provided ρ(θ)is differentiable.

Token 17115:
Alternatively, if ρ(θ)is POLICY GRADIENT not available in closed form, we can evaluate πθsimply by executing it and observing the accumulated reward.

Token 17116:
We can follow the empirical gradient by hill climbing—i.e., evaluating the change in policy value for small increments in each parameter.

Token 17117:
With the usual caveats,this process will converge to a local optimum in policy space.

Token 17118:
When the environment (or the policy) is stochastic, things get more difﬁcult.

Token 17119:
Suppose we are trying to do hill climbing, which requires comparing ρ(θ)andρ(θ+Δθ)for some smallΔθ.

Token 17120:
The problem is that the total reward on each trial may vary widely, so estimates of the policy value from a small number of trials will be quite unreliable; trying to comparetwo such estimates will be even more unreliable.

Token 17121:
One solution is simply to run lots of trials,measuring the sample variance and using it to determine that enough trials have been run to get a reliable indication of the direction of improvement for ρ(θ).

Token 17122:
Unfortunately, this is impractical for many real problems where each trial may be expensive, time-consuming, andperhaps even dangerous.

Token 17123:
For the case of a stochastic policy π θ(s,a), it is possible to obtain an unbiased estimate of the gradient at θ,∇θρ(θ), directly from the results of trials executed at θ.

Token 17124:
For simplicity, we will derive this estimate for the simple case of a nonsequential environment in which thereward R(a)is obtained immediately after doing action ain the start state s 0.

Token 17125:
In this case, the policy value is just the expected value of the reward, and we have ∇θρ(θ)=∇θ/summationdisplay aπθ(s0,a)R(a)=/summationdisplay a(∇θπθ(s0,a))R(a).

Token 17126:
Now we perform a simple trick so that this summation can be approximated by samples generated from the probability distribution deﬁned by πθ(s0,a).

Token 17127:
Suppose that we have N trials in all and the action taken on the jth trial is aj.T h e n ∇θρ(θ)=/summationdisplay aπθ(s0,a)·(∇θπθ(s0,a))R(a) πθ(s0,a)≈1 NN/summationdisplay j=1(∇θπθ(s0,aj))R(aj) πθ(s0,aj).

Token 17128:
Thus, the true gradient of the policy value is approximated by a sum of terms involving the gradient of the action-selection probability in each trial.

Token 17129:
For the sequential case, thisgeneralizes to ∇ θρ(θ)≈1 NN/summationdisplay j=1(∇θπθ(s,aj))Rj(s) πθ(s,aj) for each state svisited, where ajis executed in son the jth trial and Rj(s)is the total reward received from state sonwards in the jth trial.

Token 17130:
The resulting algorithm is called REINFORCE (Williams, 1992); it is usually much more effective than hill climbing using lots of trials at each value of θ.

Token 17131:
It is still much slower than necessary, however.

Token 17132:
850 Chapter 21. Reinforcement Learning Consider the following task: given two blackjack5programs, determine which is best.

Token 17133:
One way to do this is to have each play against a standard “dealer” for a certain number ofhands and then to measure their respective winnings.

Token 17134:
The problem with this, as we have seen,is that the winnings of each program ﬂuctuate widely depending on whether it receives goodor bad cards.

Token 17135:
An obvious solution is to generate a certain number of hands in advance and have each program play the same set of hands.

Token 17136:
In this way, we eliminate the measurement error due to differences in the cards received.

Token 17137:
This idea, called correlated sampling , un-CORRELATED SAMPLING derlies a policy-search algorithm called P EGASUS (Ng and Jordan, 2000).

Token 17138:
The algorithm is applicable to domains for which a simulator is available so that the “random” outcomes ofactions can be repeated.

Token 17139:
The algorithm works by generating in advance Nsequences of ran- dom numbers, each of which can be used to run a trial of any policy.

Token 17140:
Policy search is carriedout by evaluating each candidate policy using the same set of random sequences to determine the action outcomes.

Token 17141:
It can be shown that the number of random sequences required to ensurethat the value of every policy is well estimated depends only on the complexity of the policy space, and not at all on the complexity of the underlying domain.

Token 17142:
21.6 A PPLICATIONS OF REINFORCEMENT LEARNING We now turn to examples of large-scale applications of reinforcement learning.

Token 17143:
We considerapplications in game playing, where the transition model is known and the goal is to learn theutility function, and in robotics, where the model is usually unknown.

Token 17144:
21.6.1 Applications to game playing The ﬁrst signiﬁcant application of reinforcement learning was also the ﬁrst signiﬁcant learn-ing program of any kind—the checkers program written by Arthur Samuel (1959, 1967).Samuel ﬁrst used a weighted linear function for the evaluation of positions, using up to 16 terms at any one time.

Token 17145:
He applied a version of Equation (21.12) to update the weights.

Token 17146:
There were some signiﬁcant differences, however, between his program and current methods.

Token 17147:
First,he updated the weights using the difference between the current state and the backed-up valuegenerated by full look-ahead in the search tree.

Token 17148:
This works ﬁne, because it amounts to view-ing the state space at a different granularity.

Token 17149:
A second difference was that the program didnotuse any observed rewards! That is, the values of terminal states reached in self-play were ignored.

Token 17150:
This means that it is theoretically possible for Samuel’s program not to converge, or to converge on a strategy designed to lose rather than to win.

Token 17151:
He managed to avoid this fate by insisting that the weight for material advantage should always be positive.

Token 17152:
Remarkably, this was sufﬁcient to direct the program into areas of weight space corresponding to good checkers play.

Token 17153:
Gerry Tesauro’s backgammon program TD-G AMMON (1992) forcefully illustrates the potential of reinforcement learning techniques.

Token 17154:
In earlier work (Tesauro and Sejnowski,1989), Tesauro tried learning a neural network representation of Q(s,a)directly from ex- 5Also known as twenty-one or pontoon.

Token 17155:
Section 21.6. Applications of Reinforcement Learning 851 xθ Figure 21.9 Setup for the problem of balancing a long pole on top of a moving cart.

Token 17156:
The cart can be jerked left or right by a controller that observes x,θ,˙x,a n d˙θ. amples of moves labeled with relative values by a human expert.

Token 17157:
This approach proved extremely tedious for the expert.

Token 17158:
It resulted in a program, called N EUROGAMMON ,t h a tw a s strong by computer standards, but not competitive with human experts.

Token 17159:
The TD-G AMMON project was an attempt to learn from self-play alone. The only reward signal was given atthe end of each game.

Token 17160:
The evaluation function was represented by a fully connected neuralnetwork with a single hidden layer containing 40 nodes.

Token 17161:
Simply by repeated application ofEquation (21.12), TD-G AMMON learned to play considerably better than N EUROGAMMON , even though the input representation contained just the raw board position with no computedfeatures.

Token 17162:
This took about 200,000 training games and two weeks of computer time.

Token 17163:
Although that may seem like a lot of games, it is only a vanishingly small fraction of the state space.

Token 17164:
When precomputed features were added to the input representation, a network with 80 hiddennodes was able, after 300,000 training games, to reach a standard of play comparable to thatof the top three human players worldwide.

Token 17165:
Kit Woolsey, a top player and analyst, said that“There is no question in my mind that its positional judgment is far better than mine.” 21.6.2 Application to robot control The setup for the famous cart–pole balancing problem, also known as the inverted pendu- CART–POLE lum, is shown in Figure 21.9.

Token 17166:
The problem is to control the position xof the cart so thatINVERTED PENDULUM the pole stays roughly upright ( θ≈π/2), while staying within the limits of the cart track as shown.

Token 17167:
Several thousand papers in reinforcement learning and control theory have been published on this seemingly simple problem.

Token 17168:
The cart–pole problem differs from the prob- lems described earlier in that the state variables x,θ,˙x,a n d˙θare continuous.

Token 17169:
The actions are usually discrete: jerk left or jerk right, the so-called bang-bang control regime.BANG-BANG CONTROL The earliest work on learning for this problem was carried out by Michie and Cham- bers (1968).

Token 17170:
Their B OXES algorithm was able to balance the pole for over an hour after only about 30 trials.

Token 17171:
Moreover, unlike many subsequent systems, B OXES was implemented with a

Token 17172:
852 Chapter 21. Reinforcement Learning real cart and pole, not a simulation.

Token 17173:
The algorithm ﬁrst discretized the four-dimensional state space into boxes—hence the name.

Token 17174:
It then ran trials until the pole fell over or the cart hit theend of the track.

Token 17175:
Negative reinforcement was associated with the ﬁnal action in the ﬁnal boxand then propagated back through the sequence.

Token 17176:
It was found that the discretization causedsome problems when the apparatus was initialized in a position different from those used in training, suggesting that generalization was not perfect.

Token 17177:
Improved generalization and faster learning can be obtained using an algorithm that adaptively partitions the state space accord- ing to the observed variation in the reward, or by using a continuous-state, nonlinear functionapproximator such as a neural network.

Token 17178:
Nowadays, balancing a triple inverted pendulum is a common exercise—a feat far beyond the capabilities of most humans.

Token 17179:
Still more impressive is the application of reinforcement learning to helicopter ﬂight (Figure 21.10).

Token 17180:
This work has generally used policy search (Bagnell and Schneider, 2001)as well as the P EGASUS algorithm with simulation based on a learned transition model (Ng et al.

Token 17181:
, 2004). Further details are given in Chapter 25.

Token 17182:
Figure 21.10 Superimposed time-lapse images of an autonomous helicopter performing a very difﬁcult “nose-in circle” maneuver.

Token 17183:
The helicopter is under the control of a policy developed by the P EGASUS policy-search algorithm.

Token 17184:
A simulator model was developed by observing the effects of various control manipulations on the real helicopter; then the algo- rithm was run on the simulator model overnight.

Token 17185:
A variety of controllers were developed fordifferent maneuvers.

Token 17186:
In all cases, performance far exceeded that of an expert human pilot using remote control. (Image courtesy of Andrew Ng.)

Token 17187:
Section 21.7.

Token 17188:
Summary 853 21.7 S UMMARY This chapter has examined the reinforcement learning problem: how an agent can become proﬁcient in an unknown environment, given only its percepts and occasional rewards.

Token 17189:
Rein-forcement learning can be viewed as a microcosm for the entire AI problem, but it is studiedin a number of simpliﬁed settings to facilitate progress.

Token 17190:
The major points are: •The overall agent design dictates the kind of information that must be learned.

Token 17191:
The three main designs we covered were the model-based design, using a model Pand a utility function U; the model-free design, using an action-utility function Q;a n dt h e reﬂex design, using a policy π.

Token 17192:
•Utilities can be learned using three approaches: 1.Direct utility estimation uses the total observed reward-to-go for a given state as direct evidence for learning its utility.

Token 17193:
2.Adaptive dynamic programming (ADP) learns a model and a reward function from observations and then uses value or policy iteration to obtain the utilities or an optimal policy.

Token 17194:
ADP makes optimal use of the local constraints on utilities ofstates imposed through the neighborhood structure of the environment.

Token 17195:
3.Temporal-difference (TD) methods update utility estimates to match those of suc- cessor states.

Token 17196:
They can be viewed as simple approximations to the ADP approachthat can learn without requiring a transition model.

Token 17197:
Using a learned model to gen-erate pseudoexperiences can, however, result in faster learning.

Token 17198:
•Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD approach.

Token 17199:
With TD, Q-learning requires no model in either the learning or action-selection phase.

Token 17200:
This simpliﬁes the learning problem but potentially restricts the abilityto learn in complex environments, because the agent cannot simulate the results ofpossible courses of action.

Token 17201:
•When the learning agent is responsible for selecting actions while it learns, it must trade off the estimated value of those actions against the potential for learning usefulnew information.

Token 17202:
An exact solution of the exploration problem is infeasible, but somesimple heuristics do a reasonable job.

Token 17203:
•In large state spaces, reinforcement learning algorithms must use an approximate func- tional representation in order to generalize over states.

Token 17204:
The temporal-difference signalcan be used directly to update parameters in representations such as neural networks.

Token 17205:
•Policy-search methods operate directly on a representation of the policy, attempting to improve it based on observed performance.

Token 17206:
The variation in the performance in astochastic domain is a serious problem; for simulated domains this can be overcome by ﬁxing the randomness in advance.

Token 17207:
Because of its potential for eliminating hand coding of control strategies, reinforcement learn- ing continues to be one of the most active areas of machine learning research.

Token 17208:
Applicationsin robotics promise to be particularly valuable; these will require methods for handling con-

Token 17209:
854 Chapter 21.

Token 17210:
Reinforcement Learning tinuous, high-dimensional, partially observable environments in which successful behaviors may consist of thousands or even millions of primitive actions.

Token 17211:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Turing (1948, 1950) proposed the reinforcement-learning approach, although he was not con-vinced of its effectiveness, writing, “the use of punishments and rewards can at best be a partof the teaching process.” Arthur Samuel’s work (1959) was probably the earliest successfulmachine learning research.

Token 17212:
Although this work was informal and had a number of ﬂaws,it contained most of the modern ideas in reinforcement learning, including temporal differ-encing and function approximation.

Token 17213:
Around the same time, researchers in adaptive controltheory (Widrow and Hoff, 1960), building on work by Hebb (1949), were training simple net-works using the delta rule.

Token 17214:
(This early connection between neural networks and reinforcementlearning may have led to the persistent misperception that the latter is a subﬁeld of the for-mer.)

Token 17215:
The cart–pole work of Michie and Chambers (1968) can also be seen as a reinforcementlearning method with a function approximator.

Token 17216:
The psychological literature on reinforcement learning is much older; Hilgard and Bower (1975) provide a good survey.

Token 17217:
Direct evidence for the operation of reinforcement learning in animals has been provided by investigations intothe foraging behavior of bees; there is a clear neural correlate of the reward signal in the formof a large neuron mapping from the nectar intake sensors directly to the motor cortex (Mon-tague et al.

Token 17218:
, 1995).

Token 17219:
Research using single-cell recording suggests that the dopamine system in primate brains implements something resembling value function learning (Schultz et al.

Token 17220:
, 1997).

Token 17221:
The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-mentations of temporal-difference learning, while Dayan and Niv (2008) survey the latestevidence from neuroscientiﬁc and behavioral experiments.

Token 17222:
The connection between reinforcement learning and Markov decision processes was ﬁrst made by Werbos (1977), but the development of reinforcement learning in AI stems from work at the University of Massachusetts in the early 1980s (Barto et al.

Token 17223:
, 1981). The paper by Sutton (1988) provides a good historical overview.

Token 17224:
Equation (21.3) in this chapter is a special case for λ=0of Sutton’s general TD (λ)algorithm.

Token 17225:
TD (λ)updates the utility values of all states in a sequence leading up to each transition by an amount that drops off asλ tfor states tsteps in the past.

Token 17226:
TD (1)is identical to the Widrow–Hoff or delta rule.

Token 17227:
Boyan (2002), building on work by Bradtke and Barto (1996), argues that TD (λ)and related algo- rithms make inefﬁcient use of experiences; essentially, they are online regression algorithmsthat converge much more slowly than ofﬂine regression.

Token 17228:
His LSTD (least-squares temporaldifferencing) algorithm is an online algorithm for passive reinforcement learning that givesthe same results as ofﬂine regression.

Token 17229:
Least-squares policy iteration, or LSPI (Lagoudakisand Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust, statistically efﬁcient, model-free algorithm for learning policies.

Token 17230:
The combination of temporal-difference learning with the model-based generation of simulated experiences was proposed in Sutton’s D YNA architecture (Sutton, 1990).

Token 17231:
The idea of prioritized sweeping was introduced independently by Moore and Atkeson (1993) and

Token 17232:
Bibliographical and Historical Notes 855 Peng and Williams (1993).

Token 17233:
Q-learning was developed in Watkins’s Ph.D. thesis (1989), while SARSA appeared in a technical report by Rummery and Niranjan (1994).

Token 17234:
Bandit problems, which model the problem of exploration for nonsequential decisions, are analyzed in depth by Berry and Fristedt (1985).

Token 17235:
Optimal exploration strategies for severalsettings are obtainable using the technique called Gittins indices (Gittins, 1989).

Token 17236:
A vari- ety of exploration methods for sequential decision problems are discussed by Barto et al. (1995).

Token 17237:
Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithmsthat explore unknown environments and are guaranteed to converge on near-optimal policiesin polynomial time.

Token 17238:
Bayesian reinforcement learning (Dearden et al. , 1998, 1999) provides another angle on both model uncertainty and exploration.

Token 17239:
Function approximation in reinforcement learning goes back to the work of Samuel, who used both linear and nonlinear evaluation functions and also used feature-selection meth-ods to reduce the feature space.

Token 17240:
Later methods include the CMAC (Cerebellar Model Artic- CMAC ulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel functions, and the associative neural networks of Barto et al.

Token 17241:
(1983). Neural networks are currently the most popular form of function approximator.

Token 17242:
The best-known application isTD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter.

Token 17243:
One signiﬁcant problem exhibited by neural-network-based TD learners is that they tend to forget earlier ex- periences, especially those in parts of the state space that are avoided once competence isachieved.

Token 17244:
This can result in catastrophic failure if such circumstances reappear.

Token 17245:
Function ap-proximation based on instance-based learning can avoid this problem (Ormoneit and Sen, 2002; Forbes, 2002).

Token 17246:
The convergence of reinforcement learning algorithms using function approximation is an extremely technical subject.

Token 17247:
Results for TD learning have been progressively strength-ened for the case of linear function approximators (Sutton, 1988; Dayan, 1992; Tsitsiklis andVan Roy, 1997), but several examples of divergence have been presented for nonlinear func-tions (see Tsitsiklis and Van Roy, 1997, for a discussion).

Token 17248:
Papavassiliou and Russell (1999)describe a new type of reinforcement learning that converges with any form of function ap- proximator, provided that a best-ﬁt approximation can be found for the observed data.

Token 17249:
Policy search methods were brought to the fore by Williams (1992), who developed the R EINFORCE family of algorithms.

Token 17250:
Later work by Marbach and Tsitsiklis (1998), Sutton et al.

Token 17251:
(2000), and Baxter and Bartlett (2000) strengthened and generalized the convergence resultsfor policy search.

Token 17252:
The method of correlated sampling for comparing different conﬁgurationsof a system was described formally by Kahn and Marshall (1953), but seems to have beenknown long before that.

Token 17253:
Its use in reinforcement learning is due to Van Roy (1998) and Ngand Jordan (2000); the latter paper also introduced the P EGASUS algorithm and proved its formal properties.

Token 17254:
As we mentioned in the chapter, the performance of a stochastic policy is a continu- ous function of its parameters, which helps with gradient-based search methods.

Token 17255:
This is not the only beneﬁt: Jaakkola et al.

Token 17256:
(1995) argue that stochastic policies actually work better than deterministic policies in partially observable environments, if both are limited to act-ing based on the current percept.

Token 17257:
(One reason is that the stochastic policy is less likely toget “stuck” because of some unseen hindrance.) Now, in Chapter 17 we pointed out that

Token 17258:
856 Chapter 21.

Token 17259:
Reinforcement Learning optimal policies in partially observable MDPs are deterministic functions of the belief state rather than the current percept, so we would expect still better results by keeping track of thebelief state using the ﬁltering methods of Chapter 15.

Token 17260:
Unfortunately, belief-state space is high-dimensional and continuous, and effective algorithms have not yet been developed forreinforcement learning with belief states.

Token 17261:
Real-world environments also exhibit enormous complexity in terms of the number of primitive actions required to achieve signiﬁcant reward.

Token 17262:
For example, a robot playingsoccer might make a hundred thousand individual leg motions before scoring a goal.

Token 17263:
Onecommon method, used originally in animal training, is called reward shaping .

Token 17264:
This involves REWARD SHAPING supplying the agent with additional rewards, called pseudorewards , for “making progress.” PSEUDOREWARD For example, in soccer the real reward is for scoring a goal, but pseudorewards might be given for making contact with the ball or for kicking it toward the goal.

Token 17265:
Such rewards canspeed up learning enormously and are simple to provide, but there is a risk that the agentwill learn to maximize the pseudorewards rather than the true rewards; for example, standingnext to the ball and “vibrating” causes many contacts with the ball.

Token 17266:
Ng et al.

Token 17267:
(1999) show that the agent will still learn the optimal policy provided that the pseudoreward F(s,a,s /prime) satisﬁes F(s,a,s/prime)=γΦ(s/prime)−Φ(s),w h e r e Φis an arbitrary function of the state.

Token 17268:
Φcan be constructed to reﬂect any desirable aspects of the state, such as achievement of subgoals or distance to a goal state.

Token 17269:
The generation of complex behaviors can also be facilitated by hierarchical reinforce- ment learning methods, which attempt to solve problems at multiple levels of abstraction—HIERARCHICAL REINFORCEMENT LEARNINGmuch like the HTN planning methods of Chapter 11.

Token 17270:
For example, “scoring a goal” can be broken down into “obtain possession,” “dribble towards the goal,” and “shoot;” and each ofthese can be broken down further into lower-level motor behaviors.

Token 17271:
The fundamental resultin this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviorsof arbitrary complexity can be treated just like primitive actions (albeit ones that can takevarying amounts of time) from the point of view of the higher-level behavior that invokesthem.

Token 17272:
Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al.

Token 17273:
, 2000; Andre and Russell, 2002) build on this result to develop methods for supplying an agent with a partial program that constrains the agent’s behavior to have a particular hierarchical PARTIAL PROGRAM structure.

Token 17274:
The partial-programming language for agent programs extends an ordinary pro- gramming language by adding primitives for unspeciﬁed choices that must be ﬁlled in bylearning.

Token 17275:
Reinforcement learning is then applied to learn the best behavior consistent withthe partial program.

Token 17276:
The combination of function approximation, shaping, and hierarchicalreinforcement learning has been shown to solve large-scale problems—for example, policiesthat execute for 10 4steps in state spaces of 10100states with branching factors of 1030(Marthi et al.

Token 17277:
, 2005).

Token 17278:
One key result (Dietterich, 2000) is that the hierarchical structure provides a natural additive decomposition of the overall utility function into terms that depend on small subsets of the variables deﬁning the state space.

Token 17279:
This is somewhat analogous to the represen- tation theorems underlying the conciseness of Bayes nets (Chapter 14).

Token 17280:
The topic of distributed and multiagent reinforcement learning was not touched upon in the chapter but is of great current interest.

Token 17281:
In distributed RL, the aim is to devise methods by which multiple, coordinated agents learn to optimize a common utility function. For example,

Token 17282:


Token 17283:
Bibliographical and Historical Notes 857 can we devise methods whereby separate subagents for robot navigation and robot obstacle SUBAGENT avoidance could cooperatively achieve a combined control system that is globally optimal?

Token 17284:
Some basic results in this direction have been obtained (Guestrin et al. , 2002; Russell and Zimdars, 2003).

Token 17285:
The basic idea is that each subagent learns its own Q-function from itsown stream of rewards.

Token 17286:
For example, a robot-navigation component can receive rewards for making progress towards the goal, while the obstacle-avoidance component receives negative rewards for every collision.

Token 17287:
Each global decision maximizes the sum of Q-functions and thewhole process converges to globally optimal solutions.

Token 17288:
Multiagent RL is distinguished from distributed RL by the presence of agents who cannot coordinate their actions (except by explicit communicative acts) and who may notshare the same utility function.

Token 17289:
Thus, multiagent RL deals with sequential game-theoreticproblems or Markov games , as deﬁned in Chapter 17.

Token 17290:
The consequent requirement for ran- domized policies is not a signiﬁcant complication, as we saw on page 848.

Token 17291:
What does cause problems is the fact that, while an agent is learning to defeat its opponent’s policy, the op-ponent is changing its policy to defeat the agent.

Token 17292:
Thus, the environment is nonstationary (see page 568).

Token 17293:
Littman (1994) noted this difﬁculty when introducing the ﬁrst RL algorithmsfor zero-sum Markov games.

Token 17294:
Hu and Wellman (2003) present a Q-learning algorithm for general-sum games that converges when the Nash equilibrium is unique; when there are mul- tiple equilibria, the notion of convergence is not so easy to deﬁne (Shoham et al.

Token 17295:
, 2004). Sometimes the reward function is not easy to deﬁne. Consider the task of driving a car.

Token 17296:
There are extreme states (such as crashing the car) that clearly should have a large penalty.But beyond that, it is difﬁcult to be precise about the reward function.

Token 17297:
However, it is easyenough for a human to drive for a while and then tell a robot “do it like that.” The robot thenhas the task of apprenticeship learning ; learning from an example of the task done right, APPRENTICESHIP LEARNING without explicit rewards.

Token 17298:
Ng et al. (2004) and Coates et al.

Token 17299:
(2009) show how this technique works for learning to ﬂy a helicopter; see Figure 25.25 on page 1002 for an example of the acrobatics the resulting policy is capable of.

Token 17300:
Russell (1998) describes the task of inverse reinforcement learning —ﬁguring out what the reward function must be from an exampleINVERSE REINFORCEMENTLEARNING path through that state space.

Token 17301:
This is useful as a part of apprenticeship learning, or as a part of doing science—we can understand an animal or robot by working backwards from what itdoes to what its reward function must be.

Token 17302:
This chapter has dealt only with atomic states—all the agent knows about a state is the set of available actions and the utilities of the resulting states (or of state-action pairs).

Token 17303:
Butit is also possible to apply reinforcement learning to structured representations rather thanatomic ones; this is called relational reinforcement learning (Tadepalli et al.

Token 17304:
, 2004). RELATIONAL REINFORCEMENT LEARNINGThe survey by Kaelbling et al. (1996) provides a good entry point to the literature.

Token 17305:
The text by Sutton and Barto (1998), two of the ﬁeld’s pioneers, focuses on architectures and algo-rithms, showing how reinforcement learning weaves together the ideas of learning, planning,and acting.

Token 17306:
The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a rigorous grounding in the theory of dynamic programming and stochastic convergence.

Token 17307:
Re- inforcement learning papers are published frequently in Machine Learning ,i nt h e Journal of Machine Learning Research , and in the International Conferences on Machine Learning and the Neural Information Processing Systems meetings.

Token 17308:
858 Chapter 21. Reinforcement Learning EXERCISES 21.1 Implement a passive learning agent in a simple environment, such as the 4×3world.

Token 17309:
For the case of an initially unknown environment model, compare the learning performance of the direct utility estimation, TD, and ADP algorithms.

Token 17310:
Do the comparison for the optimalpolicy and for several random policies.

Token 17311:
For which do the utility estimates converge faster?What happens when the size of the environment is increased?

Token 17312:
(Try environments with andwithout obstacles.) 21.2 Chapter 17 deﬁned a proper policy for an MDP as one that is guaranteed to reach a terminal state.

Token 17313:
Show that it is possible for a passive ADP agent to learn a transition modelfor which its policy πis improper even if πis proper for the true MDP; with such models, the P OLICY -EVA LUATI O N step may fail if γ=1.

Token 17314:
Show that this problem cannot arise if POLICY -EVA LUATI O N is applied to the learned model only at the end of a trial.

Token 17315:
21.3 Starting with the passive ADP agent, modify it to use an approximate ADP algorithm as discussed in the text. Do this in two steps: a.

Token 17316:
Implement a priority queue for adjustments to the utility estimates.

Token 17317:
Whenever a state is adjusted, all of its predecessors also become candidates for adjustment and should beadded to the queue.

Token 17318:
The queue is initialized with the state from which the most recenttransition took place. Allow only a ﬁxed number of adjustments. b.

Token 17319:
Experiment with various heuristics for ordering the priority queue, examining their ef- fect on learning rates and computation time.

Token 17320:
21.4 Write out the parameter update equations for TD learning with ˆU(x,y)=θ 0+θ1x+θ2y+θ3/radicalBig (x−xg)2+(y−yg)2.

Token 17321:
21.5 Implement an exploring reinforcement learning agent that uses direct utility estima- tion.

Token 17322:
Make two versions—one with a tabular representation and one using the function ap- proximator in Equation (21.10).

Token 17323:
Compare their performance in three environments: a.T h e 4×3world described in the chapter.

Token 17324:
b.A10×10world with no obstacles and a +1 reward at (10,10). c.A10×10world with no obstacles and a +1 reward at (5,5).

Token 17325:
21.6 Devise suitable features for reinforcement learning in stochastic grid worlds (general- izations of the 4×3world) that contain multiple obstacles and multiple terminal states with rewards of +1or−1.

Token 17326:
21.7 Extend the standard game-playing environment (Chapter 5) to incorporate a reward signal.

Token 17327:
Put two reinforcement learning agents into the environment (they may, of course, share the agent program) and have them play against each other.

Token 17328:
Apply the generalized TDupdate rule (Equation (21.12)) to update the evaluation function.

Token 17329:
You might wish to start witha simple linear weighted evaluation function and a simple game, such as tic-tac-toe.

Token 17330:


Token 17331:
Exercises 859 21.8 Compute the true utility function and the best linear approximation in xandy(as in Equation (21.10)) for the following environments: a.A10×10world with a single +1terminal state at (10,10).

Token 17332:
b. As in (a), but add a −1terminal state at (10,1). c. As in (b), but add obstacles in 10 randomly selected squares.

Token 17333:
d. As in (b), but place a wall stretching from (5,2) to (5,9). e. As in (a), but with the terminal state at (5,5).

Token 17334:
The actions are deterministic moves in the four directions. In each case, compare the results using three-dimensional plots.

Token 17335:
For each environment, propose additional features (besides x andy) that would improve the approximation and show the results.

Token 17336:
21.9 Implement the R EINFORCE and P EGASUS algorithms and apply them to the 4×3 world, using a policy family of your own choosing.

Token 17337:
Comment on the results. 21.10 Is reinforcement learning an appropriate abstract model for evolution?

Token 17338:
What connec- tion exists, if any, between hardwired reward signals and evolutionary ﬁtness?

Token 17339:
22NATURAL LANGUAGE PROCESSING In which we see how to make use of the copious knowledge that is expressed in natural language.

Token 17340:
Homo sapiens is set apart from other species by the capacity for language.

Token 17341:
Somewhere around 100,000 years ago, humans learned how to speak, and about 7,000 years ago learned to write.Although chimpanzees, dolphins, and other animals have shown vocabularies of hundreds ofsigns, only humans can reliably communicate an unbounded number of qualitatively differentmessages on any topic using discrete signs.

Token 17342:
Of course, there are other attributes that are uniquely human: no other species wears clothes, creates representational art, or watches three hours of television a day.

Token 17343:
But when Alan Turing proposed his Test (see Section 1.1.1), he based it on language, not art or TV.

Token 17344:
There are two main reasons why we want our computer agents to be able to process naturallanguages: ﬁrst, to communicate with humans, a topic we take up in Chapter 23, and second,to acquire information from written language, the focus of this chapter.

Token 17345:
There are over a trillion pages of information on the Web, almost all of it in natural language.

Token 17346:
An agent that wants to do knowledge acquisition needs to understand (at least KNOWLEDGE ACQUISITION partially) the ambiguous, messy languages that humans use.

Token 17347:
We examine the problem from the point of view of speciﬁc information-seeking tasks: text classiﬁcation, information re-trieval, and information extraction.

Token 17348:
One common factor in addressing these tasks is the use oflanguage models : models that predict the probability distribution of language expressions.

Token 17349:
LANGUAGE MODEL 22.1 L ANGUAGE MODELS Formal languages, such as the programming languages Java or Python, have precisely deﬁned language models.

Token 17350:
A language can be deﬁned as a set of strings; “ print(2 + 2) ”i sa LANGUAGE legal program in the language Python, whereas “ 2)+(2 print ” is not.

Token 17351:
Since there are an inﬁnite number of legal programs, they cannot be enumerated; instead they are speciﬁed by a set of rules called a grammar .

Token 17352:
Formal languages also have rules that deﬁne the meaning or GRAMMAR semantics of a program; for example, the rules say that the “meaning” of “ 2+2 ”i s4 ,a n d SEMANTICS the meaning of “ 1/0” is that an error is signaled.

Token 17353:
860

Token 17354:
Section 22.1. Language Models 861 Natural languages, such as English or Spanish, cannot be characterized as a deﬁnitive set of sentences.

Token 17355:
Everyone agrees that “Not to be invited is sad” is a sentence of English,but people disagree on the grammaticality of “To be not invited is sad.” Therefore, it is morefruitful to deﬁne a natural language model as a probability distribution over sentences ratherthan a deﬁnitive set.

Token 17356:
That is, rather than asking if a string of words is or is not a member of the set deﬁning the language, we instead ask for P(S=words )—what is the probability that a random sentence would be words .

Token 17357:
Natural languages are also ambiguous .

Token 17358:
“He saw her duck” can mean either that he saw AMBIGUITY a waterfowl belonging to her, or that he saw her move to evade something.

Token 17359:
Thus, again, we cannot speak of a single meaning for a sentence, but rather of a probability distribution overpossible meanings.

Token 17360:
Finally, natural languages are difﬁcult to deal with because they are very large, and constantly changing.

Token 17361:
Thus, our language models are, at best, an approximation. We startwith the simplest possible approximations and move up from there.

Token 17362:
22.1.1 N-gram character models Ultimately, a written text is composed of characters —letters, digits, punctuation, and spaces CHARACTERS in English (and more exotic characters in some other languages).

Token 17363:
Thus, one of the simplest language models is a probability distribution over sequences of characters.

Token 17364:
As in Chapter 15,we write P(c 1:N)for the probability of a sequence of Ncharacters, c1through cN.

Token 17365:
In one Web collection, P(“the”)=0.027andP(“zgq”)=0.000000002 .

Token 17366:
A sequence of written sym- bols of length nis called an n-gram (from the Greek root for writing or letters), with special case “unigram” for 1-gram, “bigram” for 2-gram, and “trigram” for 3-gram.

Token 17367:
A model of the probability distribution of n-letter sequences is thus called an n-gram model .

Token 17368:
(But be care- N-GRAM MODEL ful: we can have n-gram models over sequences of words, syllables, or other units; not just over characters.)

Token 17369:
Ann-gram model is deﬁned as a Markov chain of order n−1.

Token 17370:
Recall from page 568 that in a Markov chain the probability of character cidepends only on the immediately pre- ceding characters, not on any other characters.

Token 17371:
So in a trigram model (Markov chain of order 2) we have P(ci|c1:i−1)=P(ci|ci−2:i−1).

Token 17372:
We can deﬁne the probability of a sequence of characters P(c1:N)under the trigram model by ﬁrst factoring with the chain rule and then using the Markov assumption: P(c1:N)=N/productdisplay i=1P(ci|c1:i−1)=N/productdisplay i=1P(ci|ci−2:i−1).

Token 17373:
For a trigram character model in a language with 100 characters, P(Ci|Ci−2:i−1)has a million entries, and can be accurately estimated by counting character sequences in a body of text of 10 million characters or more.

Token 17374:
We call a body of text a corpus (plural corpora ), from the CORPUS Latin word for body .

Token 17375:
862 Chapter 22. Natural Language Processing What can we do with n-gram character models?

Token 17376:
One task for which they are well suited islanguage identiﬁcation : given a text, determine what natural language it is written in.

Token 17377:
ThisLANGUAGE IDENTIFICATION is a relatively easy task; even with short texts such as “Hello, world” or “Wie geht es dir,” it is easy to identify the ﬁrst as English and the second as German.

Token 17378:
Computer systems identifylanguages with greater than 99% accuracy; occasionally, closely related languages, such as Swedish and Norwegian, are confused.

Token 17379:
One approach to language identiﬁcation is to ﬁrst build a trigram character model of each candidate language, P(c i|ci−2:i−1,/lscript), where the variable /lscriptranges over languages.

Token 17380:
For each/lscriptthe model is built by counting trigrams in a corpus of that language. (About 100,000 characters of each language are needed.)

Token 17381:
That gives us a model of P(Text|Language ),b u t we want to select the most probable language given the text, so we apply Bayes’ rule followedby the Markov assumption to get the most probable language: /lscript ∗=a r g m a x /lscriptP(/lscript|c1:N) =a r g m a x /lscriptP(/lscript)P(c1:N|/lscript) =a r g m a x /lscriptP(/lscript)N/productdisplay i=1P(ci|ci−2:i−1,/lscript) The trigram model can be learned from a corpus, but what about the prior probability P(/lscript)?

Token 17382:
We may have some estimate of these values; for example, if we are selecting a random Webpage we know that English is the most likely language and that the probability of Macedonianwill be less than 1%.

Token 17383:
The exact number we select for these priors is not critical because the trigram model usually selects one language that is several orders of magnitude more probable than any other.

Token 17384:
Other tasks for character models include spelling correction, genre classiﬁcation, and named-entity recognition.

Token 17385:
Genre classiﬁcation means deciding if a text is a news story, a legal document, a scientiﬁc article, etc.

Token 17386:
While many features help make this classiﬁcation, counts of punctuation and other character n-gram features go a long way (Kessler et al. , 1997).

Token 17387:
Named-entity recognition is the task of ﬁnding names of things in a document and deciding what class they belong to. For example, in the text “Mr.

Token 17388:
Sopersteen was prescribedaciphex,” we should recognize that “Mr. Sopersteen” is the name of a person and “aciphex” isthe name of a drug.

Token 17389:
Character-level models are good for this task because they can associatethe character sequence “ex ” (“ex” followed by a space) with a drug name and “steen ” with a person name, and thereby identify words that they have never seen before.

Token 17390:
22.1.2 Smoothing n-gram models The major complication of n-gram models is that the training corpus provides only an esti- mate of the true probability distribution.

Token 17391:
For common character sequences such as “ th” any English corpus will give a good estimate: about 1.5% of all trigrams.

Token 17392:
On the other hand, “ ht” is very uncommon—no dictionary words start with ht.

Token 17393:
It is likely that the sequence wouldhave a count of zero in a training corpus of standard English. Does that mean we should as-signP(“ th”)=0 ?

Token 17394:
If we did, then the text “The program issues an http request” would have

Token 17395:
Section 22.1. Language Models 863 an English probability of zero, which seems wrong.

Token 17396:
We have a problem in generalization: we want our language models to generalize well to texts they haven’t seen yet.

Token 17397:
Just because wehave never seen “ http” before does not mean that our model should claim that it is impossi- ble.

Token 17398:
Thus, we will adjust our language model so that sequences that have a count of zero inthe training corpus will be assigned a small nonzero probability (and the other counts will be adjusted downward slightly so that the probability still sums to 1).

Token 17399:
The process od adjusting the probability of low-frequency counts is called smoothing .

Token 17400:
SMOOTHING The simplest type of smoothing was suggested by Pierre-Simon Laplace in the 18th cen- tury: he said that, in the lack of further information, if a random Boolean variable Xhas been false in all nobservations so far then the estimate for P(X=true)should be 1/(n+2).T h a t is, he assumes that with two more trials, one might be true and one false.

Token 17401:
Laplace smoothing(also called add-one smoothing) is a step in the right direction, but performs relatively poorly.A better approach is a backoff model , in which we start by estimating n-gram counts, but for BACKOFF MODEL any particular sequence that has a low (or zero) count, we back off to (n−1)-grams.

Token 17402:
Linear interpolation smoothing is a backoff model that combines trigram, bigram, and unigramLINEAR INTERPOLATIONSMOOTHING models by linear interpolation.

Token 17403:
It deﬁnes the probability estimate as /hatwideP(ci|ci−2:i−1)=λ3P(ci|ci−2:i−1)+λ2P(ci|ci−1)+λ1P(ci), where λ3+λ2+λ1=1.

Token 17404:
The parameter values λican be ﬁxed, or they can be trained with an expectation–maximization algorithm.

Token 17405:
It is also possible to have the values of λidepend on the counts: if we have a high count of trigrams, then we weigh them relatively more; ifonly a low count, then we put more weight on the bigram and unigram models.

Token 17406:
One camp of researchers has developed ever more sophisticated smoothing models, while the other camp suggests gathering a larger corpus so that even simple smoothing models work well.

Token 17407:
Both aregetting at the same goal: reducing the variance in the language model.

Token 17408:
One complication: note that the expression P(c i|ci−2:i−1)asks for P(c1|c-1:0)when i=1, but there are no characters before c1.

Token 17409:
We can introduce artiﬁcial characters, for example, deﬁning c0to be a space character or a special “begin text” character.

Token 17410:
Or we can fall back on lower-order Markov models, in effect deﬁning c-1:0to be the empty sequence and thus P(c1|c-1:0)=P(c1).

Token 17411:
22.1.3 Model evaluation With so many possible n-gram models—unigram, bigram, trigram, interpolated smoothing with different values of λ, etc.—how do we know what model to choose?

Token 17412:
We can evaluate a model with cross-validation.

Token 17413:
Split the corpus into a training corpus and a validation corpus.Determine the parameters of the model from the training data.

Token 17414:
Then evaluate the model onthe validation corpus. The evaluation can be a task-speciﬁc metric, such as measuring accuracy on language identiﬁcation.

Token 17415:
Alternatively we can have a task-independent model of language quality: cal- culate the probability assigned to the validation corpus by the model; the higher the proba- bility the better.

Token 17416:
This metric is inconvenient because the probability of a large corpus willbe a very small number, and ﬂoating-point underﬂow becomes an issue.

Token 17417:
A different way ofdescribing the probability of a sequence is with a measure called perplexity ,d e ﬁ n e da s PERPLEXITY

Token 17418:
864 Chapter 22.

Token 17419:
Natural Language Processing Perplexity (c1:N)=P(c1:N)−1 N. Perplexity can be thought of as the reciprocal of probability, normalized by sequence length.

Token 17420:
It can also be thought of as the weighted average branching factor of a model.

Token 17421:
Suppose thereare 100 characters in our language, and our model says they are all equally likely.

Token 17422:
Then fora sequence of any length, the perplexity will be 100.

Token 17423:
If some characters are more likely thanothers, and the model reﬂects that, then the model will have a perplexity less than 100.

Token 17424:
22.1.4 N-gram word models Now we turn to n-gram models over words rather than characters.

Token 17425:
All the same mechanism applies equally to word and character models.

Token 17426:
The main difference is that the vocabulary — VOCABULARY the set of symbols that make up the corpus and the model—is larger.

Token 17427:
There are only about 100 characters in most languages, and sometimes we build character models that are evenmore restrictive, for example by treating “A” and “a” as the same symbol or by treating allpunctuation as the same symbol.

Token 17428:
But with word models we have at least tens of thousands ofsymbols, and sometimes millions.

Token 17429:
The wide range is because it is not clear what constitutes aword.

Token 17430:
In English a sequence of letters surrounded by spaces is a word, but in some languages,like Chinese, words are not separated by spaces, and even in English many decisions must bemade to have a clear policy on word boundaries: how many words are in “ne’er-do-well”?

Token 17431:
Orin “(Tel:1-800-960-5660x123)”? Wordn-gram models need to deal with out of vocabulary words.

Token 17432:
With character mod- OUT OF VOCABULARY els, we didn’t have to worry about someone inventing a new letter of the alphabet.1But with word models there is always the chance of a new word that was not seen in the training corpus, so we need to model that explicitly in our language model.

Token 17433:
This can be done by adding just one new word to the vocabulary: <UNK> , standing for the unknown word.

Token 17434:
We can estimate n-gram counts for <UNK> by this trick: go through the training corpus, and the ﬁrst time any individual word appears it is previously unknown, so replace it with thesymbol <UNK> .

Token 17435:
All subsequent appearances of the word remain unchanged. Then compute n-gram counts for the corpus as usual, treating <UNK> just like any other word.

Token 17436:
Then when an unknown word appears in a test set, we look up its probability under <UNK> .

Token 17437:
Sometimes multiple unknown-word symbols are used, for different classes.

Token 17438:
For example, any string ofdigits might be replaced with <NUM> , or any email address with <EMAIL> .

Token 17439:
To get a feeling for what word models can do, we built unigram, bigram, and trigram models over the words in this book and then randomly sampled sequences of words from themodels.

Token 17440:
The results are Unigram: logical are as are confusion a may right tries agent goal the was ... Bigram: systems are very similar computational approach would be represented ... Trigram: planning and scheduling are integrated the success of naive bayes model is ...

Token 17441:
Even with this small sample, it should be clear that the unigram model is a poor approximation of either English or the content of an AI textbook, and that the bigram and trigram models are 1With the possible exception of the groundbreaking work of T. Geisel (1955).

Token 17442:
Section 22.2. Text Classiﬁcation 865 much better.

Token 17443:
The models agree with this assessment: the perplexity was 891 for the unigram model, 142 for the bigram model and 91 for the trigram model.

Token 17444:
With the basics of n-gram models—both character- and word-based—established, we can turn now to some language tasks.

Token 17445:
22.2 T EXT CLASSIFICATION We now consider in depth the task of text classiﬁcation , also known as categorization :g i v e nTEXT CLASSIFICATION a text of some kind, decide which of a predeﬁned set of classes it belongs to.

Token 17446:
Language iden- tiﬁcation and genre classiﬁcation are examples of text classiﬁcation, as is sentiment analysis(classifying a movie or product review as positive or negative) and spam detection (classify- SPAM DETECTION ing an email message as spam or not-spam).

Token 17447:
Since “not-spam” is awkward, researchers have coined the term ham for not-spam. We can treat spam detection as a problem in supervised learning.

Token 17448:
A training set is readily available: the positive (spam) examples are in my spam folder, the negative (ham) examples are in my inbox.

Token 17449:
Here is an excerpt: Spam: Wholesale Fashion Watches -57% today.

Token 17450:
Designer watches for cheap ... Spam: You can buy ViagraFr$1.85 All Medications at unbeatable prices!

Token 17451:
... Spam: WE CAN TREAT ANYTHING YOU SUFFER FROM JUST TRUST US ...Spam: Sta.rt earn*ing the salary yo,u d-eserve by o’btaining the prope,r crede’ntials!

Token 17452:
Ham: The practical signiﬁcance of hypertree width in identifying more ... Ham: Abstract: We will motivate the problem of social identity clustering: ... Ham: Good to see you my friend.

Token 17453:
Hey Peter, It was good to hear from you.

Token 17454:
...Ham: PDS implies convexity of the resulting optimization problem (Kernel Ridge ... From this excerpt we can start to get an idea of what might be good features to include in the supervised learning model.

Token 17455:
Word n-grams such as “for cheap” and “You can buy” seem to be indicators of spam (although they would have a nonzero probability in ham as well).Character-level features also seem important: spam is more likely to be all uppercase and tohave punctuation embedded in words.

Token 17456:
Apparently the spammers thought that the word bigram“you deserve” would be too indicative of spam, and thus wrote “yo,u d-eserve” instead.

Token 17457:
Acharacter model should detect this.

Token 17458:
We could either create a full character n-gram model of spam and ham, or we could handcraft features such as “number of punctuation marks embedded in words.” Note that we have two complementary ways of talking about classiﬁcation.

Token 17459:
In the language-modeling approach, we deﬁne one n-gram language model for P(Message|spam) by training on the spam folder, and one model for P(Message|ham)by training on the inbox.

Token 17460:
Then we can classify a new message with an application of Bayes’ rule: argmax c∈{spam,ham}P(c|message ) = argmax c∈{spam,ham}P(message|c)P(c).

Token 17461:
where P(c)is estimated just by counting the total number of spam and ham messages.

Token 17462:
This approach works well for spam detection, just as it did for language identiﬁcation.

Token 17463:
866 Chapter 22.

Token 17464:
Natural Language Processing In the machine-learning approach we represent the message as a set of feature/value pairs and apply a classiﬁcation algorithm hto the feature vector X.

Token 17465:
We can make the language-modeling and machine-learning approaches compatible by thinking of the n-grams as features.

Token 17466:
This is easiest to see with a unigram model.

Token 17467:
The features are the words in thevocabulary: “a,” “aardvark,” ..., and the values are the number of times each word appears in the message.

Token 17468:
That makes the feature vector large and sparse.

Token 17469:
If there are 100,000 words in the language model, then the feature vector has length 100,000, but for a short email messagealmost all the features will have count zero.

Token 17470:
This unigram representation has been called thebag of words model.

Token 17471:
You can think of the model as putting the words of the training corpus BAG OF WORDS in a bag and then selecting words one at a time.

Token 17472:
The notion of order of the words is lost; a unigram model gives the same probability to any permutation of a text.

Token 17473:
Higher-order n-gram models maintain some local notion of word order.

Token 17474:
With bigrams and trigrams the number of features is squared or cubed, and we can add in other, non- n-gram features: the time the message was sent, whether a URL or an image is part of the message, an ID number for the sender of the message, the sender’s number ofprevious spam and ham messages, and so on.

Token 17475:
The choice of features is the most important partof creating a good spam detector—more important than the choice of algorithm for processing the features.

Token 17476:
In part this is because there is a lot of training data, so if we can propose a feature, the data can accurately determine if it is good or not.

Token 17477:
It is necessary to constantlyupdate features, because spam detection is an adversarial task ; the spammers modify their spam in response to the spam detector’s changes.

Token 17478:
It can be expensive to run algorithms on a very large feature vector, so often a process offeature selection is used to keep only the features that best discriminate between spam and FEATURE SELECTION ham.

Token 17479:
For example, the bigram “of the” is frequent in English, and may be equally frequent in spam and ham, so there is no sense in counting it.

Token 17480:
Often the top hundred or so features do a good job of discriminating between classes.

Token 17481:
Once we have chosen a set of features, we can apply any of the supervised learning techniques we have seen; popular ones for text categorization include k-nearest-neighbors, support vector machines, decision trees, naive Bayes, and logistic regression.

Token 17482:
All of these have been applied to spam detection, usually with accuracy in the 98%–99% range.

Token 17483:
With acarefully designed feature set, accuracy can exceed 99.9%.

Token 17484:
22.2.1 Classiﬁcation by data compression Another way to think about classiﬁcation is as a problem in data compression .

Token 17485:
A lossless DATA COMPRESSION compression algorithm takes a sequence of symbols, detects repeated patterns in it, and writes a description of the sequence that is more compact than the original.

Token 17486:
For example, the text“0.142857142857142857” might be compressed to “0.

Token 17487:
[142857]*3.” Compression algorithmswork by building dictionaries of subsequences of the text, and then referring to entries in the dictionary.

Token 17488:
The example here had only one dictionary entry, “142857.” In effect, compression algorithms are creating a language model.

Token 17489:
The LZW algorithm in particular directly models a maximum-entropy probability distribution.

Token 17490:
To do classiﬁcationby compression, we ﬁrst lump together all the spam training messages and compress them as

Token 17491:
Section 22.3. Information Retrieval 867 a unit. We do the same for the ham.

Token 17492:
Then when given a new message to classify, we append it to the spam messages and compress the result. We also append it to the ham and compressthat.

Token 17493:
Whichever class compresses better—adds the fewer number of additional bytes for thenew message—is the predicted class.

Token 17494:
The idea is that a spam message will tend to sharedictionary entries with other spam messages and thus will compress better when appended to a collection that already contains the spam dictionary.

Token 17495:
Experiments with compression-based classiﬁcation on some of the standard corpora for text classiﬁcation—the 20-Newsgroups data set, the Reuters-10 Corpora, the Industry Sectorcorpora—indicate that whereas running off-the-shelf compression algorithms like gzip, RAR,and LZW can be quite slow, their accuracy is comparable to traditional classiﬁcation algo-rithms.

Token 17496:
This is interesting in its own right, and also serves to point out that there is promisefor algorithms that use character n-grams directly with no preprocessing of the text or feature selection: they seem to be captiring some real patterns.

Token 17497:
22.3 I NFORMATION RETRIEV AL Information retrieval is the task of ﬁnding documents that are relevant to a user’s need forINFORMATION RETRIEVAL information.

Token 17498:
The best-known examples of information retrieval systems are search engines on the World Wide Web.

Token 17499:
A Web user can type a query such as [AI book]2into a search engine and see a list of relevant pages.

Token 17500:
In this section, we will see how such systems are built.

Token 17501:
Aninformation retrieval (henceforth IR) system can be characterized by IR 1.A corpus of documents.

Token 17502:
Each system must decide what it wants to treat as a document: a paragraph, a page, or a multipage text. 2.Queries posed in a query language .

Token 17503:
A query speciﬁes what the user wants to know.

Token 17504:
QUERY LANGUAGE The query language can be just a list of words, such as [AI book]; or it can specify a phrase of words that must be adjacent, as in [“AI book”]; it can contain Booleanoperators as in [AI AND book]; it can include non-Boolean operators such as [AI NEARbook] or [AI book site:www.aaai.org].

Token 17505:
3.A result set. This is the subset of documents that the IR system judges to be relevant to RESULT SET RELEVANT the query.

Token 17506:
By relevant , we mean likely to be of use to the person who posed the query, for the particular information need expressed in the query.

Token 17507:
4.A presentation of the result set.

Token 17508:
This can be as simple as a ranked list of document PRESENTATION titles or as complex as a rotating color map of the result set projected onto a three- dimensional space, rendered as a two-dimensional display.

Token 17509:
The earliest IR systems worked on a Boolean keyword model .

Token 17510:
Each word in the documentBOOLEAN KEYWORD MODEL collection is treated as a Boolean feature that is true of a document if the word occurs in the document and false if it does not.

Token 17511:
So the feature “retrieval” is true for the current chapter but false for Chapter 15.

Token 17512:
The query language is the language of Boolean expressions over 2We denote a search query as [ query ].

Token 17513:
Square brackets are used rather than quotation marks so that we can distinguish the query [“two words”] from [two words].

Token 17514:
868 Chapter 22. Natural Language Processing features. A document is relevant only if the expression evaluates to true.

Token 17515:
For example, the query [information AND retrieval] is true for the current chapter and false for Chapter 15.

Token 17516:
This model has the advantage of being simple to explain and implement. However, it has some disadvantages.

Token 17517:
First, the degree of relevance of a document is a single bit, sothere is no guidance as to how to order the relevant documents for presentation.

Token 17518:
Second, Boolean expressions are unfamiliar to users who are not programmers or logicians.

Token 17519:
Users ﬁnd it unintuitive that when they want to know about farming in the states of Kansas and Nebraska they need to issue the query [farming (Kansas OR Nebraska)].

Token 17520:
Third, it can behard to formulate an appropriate query, even for a skilled user.

Token 17521:
Suppose we try [informationAND retrieval AND models AND optimization] and get an empty result set.

Token 17522:
We could try[information OR retrieval OR models OR optimization], but if that returns too many results,it is difﬁcult to know what to try next.

Token 17523:
22.3.1 IR scoring functions Most IR systems have abandoned the Boolean model and use models based on the statistics ofword counts.

Token 17524:
We describe the BM25 scoring function , which comes from the Okapi project BM25 SCORING FUNCTION of Stephen Robertson and Karen Sparck Jones at London’s City College, and has been used in search engines such as the open-source Lucene project.

Token 17525:
A scoring function takes a document and a query and returns a numeric score; the most relevant documents have the highest scores.

Token 17526:
In the BM25 function, the score is a linear weighted combination of scores for each of the words that make up the query.

Token 17527:
Three factors affect the weight of a query term: First, the frequency with which a query term appears in a document (also known as TFfor term frequency).

Token 17528:
For the query [farming in Kansas], documents that mention “farming” frequently will have higher scores.

Token 17529:
Second, the inverse document frequency of the term, or IDF .

Token 17530:
The word “in” appears in almost every document, so it has a high document frequency, and thus a low inverse document frequency, and thus itis not as important to the query as “farming” or “Kansas.” Third, the length of the document.A million-word document will probably mention all the query words, but may not actually beabout the query.

Token 17531:
A short document that mentions all the words is a much better candidate. The BM25 function takes all three of these into account.

Token 17532:
We assume we have created an index of the Ndocuments in the corpus so that we can look up TF(q i,dj), the count of the number of times word qiappears in document dj.

Token 17533:
We also assume a table of document frequency counts, DF(qi), that gives the number of documents that contain the word qi.

Token 17534:
Then, given a document djand a query consisting of the words q1:N,w eh a v e BM25(dj,q1:N)=N/summationdisplay i=1IDF(qi)·TF(qi,dj)·(k+1 ) TF(qi,dj)+k·(1−b+b·|dj| L), where|dj|is the length of document djin words, and Lis the average document length in the corpus: L=/summationtext i|di|/N.

Token 17535:
We have two parameters, kandb, that can be tuned by cross-validation; typical values are k=2.0andb=0.75.IDF(qi)is the inverse document

Token 17536:
Section 22.3. Information Retrieval 869 frequency of word qi,g i v e nb y IDF(qi)=l o gN−DF(qi)+0.5 DF(qi)+0.5.

Token 17537:
Of course, it would be impractical to apply the BM25 scoring function to every document in the corpus.

Token 17538:
Instead, systems create an index ahead of time that lists, for each vocabulary INDEX word, the documents that contain the word.

Token 17539:
This is called the hit list for the word.

Token 17540:
Then when HIT LIST given a query, we intersect the hit lists of the query words and only score the documents in the intersection.

Token 17541:
22.3.2 IR system evaluation How do we know whether an IR system is performing well?

Token 17542:
We undertake an experiment inwhich the system is given a set of queries and the result sets are scored with respect to humanrelevance judgments.

Token 17543:
Traditionally, there have been two measures used in the scoring: recalland precision. We explain them with the help of an example.

Token 17544:
Imagine that an IR system hasreturned a result set for a single query, for which we know which documents are and are notrelevant, out of a corpus of 100 documents.

Token 17545:
The document counts in each category are given in the following table: In result set Not in result set Relevant 30 20 Not relevant 10 40 Precision measures the proportion of documents in the result set that are actually relevant.

Token 17546:
PRECISION In our example, the precision is 30/(30 + 10)= .75. The false positive rate is 1−.75 =.25.

Token 17547:
Recall measures the proportion of all the relevant documents in the collection that are in RECALL the result set.

Token 17548:
In our example, recall is 30/(30 + 20)= .60. The false negative rate is 1− .60 =.40.

Token 17549:
In a very large document collection, such as the World Wide Web, recall is difﬁcult to compute, because there is no easy way to examine every page on the Web for relevance.All we can do is either estimate recall by sampling or ignore recall completely and just judgeprecision.

Token 17550:
In the case of a Web search engine, there may be thousands of documents in theresult set, so it makes more sense to measure precision for several different sizes, such as“P@10” (precision in the top 10 results) or “P@50,” rather than to estimate precision in theentire result set.

Token 17551:
It is possible to trade off precision against recall by varying the size of the result set returned.

Token 17552:
In the extreme, a system that returns every document in the document collection is guaranteed a recall of 100%, but will have low precision.

Token 17553:
Alternately, a system could return a single document and have low recall, but a decent chance at 100% precision.

Token 17554:
A summary of both measures is the F 1score, a single number that is the harmonic mean of precision and recall, 2PR/(P+R).

Token 17555:
22.3.3 IR reﬁnements There are many possible reﬁnements to the system described here, and indeed Web search engines are continually updating their algorithms as they discover new approaches and as the Web grows and changes.

Token 17556:
870 Chapter 22. Natural Language Processing One common reﬁnement is a better model of the effect of document length on relevance. Singhal et al.

Token 17557:
(1996) observed that simple document length normalization schemes tend to favor short documents too much and long documents not enough.

Token 17558:
They propose a pivoted document length normalization scheme; the idea is that the pivot is the document length atwhich the old-style normalization is correct; documents shorter than that get a boost and longer ones get a penalty.

Token 17559:
The BM25 scoring function uses a word model that treats all words as completely in- dependent, but we know that some words are correlated: “couch” is closely related to both“couches” and “sofa.” Many IR systems attempt to account for these correlations.

Token 17560:
For example, if the query is [couch], it would be a shame to exclude from the result set those documents that mention “COUCH” or “couches” but not “couch.” Most IR systemsdocase folding of “COUCH” to “couch,” and some use a stemming algorithm to reduce CASE FOLDING STEMMING “couches” to the stem form “couch,” both in the query and the documents.

Token 17561:
This typically yields a small increase in recall (on the order of 2% for English). However, it can harmprecision.

Token 17562:
For example, stemming “stocking” to “stock” will tend to decrease precision forqueries about either foot coverings or ﬁnancial instruments, although it could improve recallfor queries about warehousing.

Token 17563:
Stemming algorithms based on rules (e.g., remove “-ing”) cannot avoid this problem, but algorithms based on dictionaries (don’t remove “-ing” if the word is already listed in the dictionary) can.

Token 17564:
While stemming has a small effect in English,it is more important in other languages.

Token 17565:
In German, for example, it is not uncommon tosee words like “Lebensversicherungsgesellschaftsangestellter” (life insurance company em-ployee).

Token 17566:
Languages such as Finnish, Turkish, Inuit, and Yupik have recursive morphologicalrules that in principle generate words of unbounded length.

Token 17567:
The next step is to recognize synonyms , such as “sofa” for “couch.” As with stemming, SYNONYM this has the potential for small gains in recall, but can hurt precision.

Token 17568:
A user who gives the query [Tim Couch] wants to see results about the football player, not sofas.

Token 17569:
The problem isthat “languages abhor absolute synonyms just as nature abhors a vacuum” (Cruse, 1986).

Token 17570:
Thatis, anytime there are two words that mean the same thing, speakers of the language conspire to evolve the meanings to remove the confusion.

Token 17571:
Related words that are not synonyms also play an important role in ranking—terms like “leather”, “wooden,” or “modern” can serveto conﬁrm that the document really is about “couch.” Synonyms and related words can befound in dictionaries or by looking for correlations in documents or in queries—if we ﬁndthat many users who ask the query [new sofa] follow it up with the query [new couch], wecan in the future alter [new sofa] to be [new sofa OR new couch].

Token 17572:
As a ﬁnal reﬁnement, IR can be improved by considering metadata —data outside of METADATA the text of the document.

Token 17573:
Examples include human-supplied keywords and publication data. On the Web, hypertext links between documents are a crucial source of information.

Token 17574:
LINKS 22.3.4 The PageRank algorithm PageRank3was one of the two original ideas that set Google’s search apart from other Web PAGERANK search engines when it was introduced in 1997.

Token 17575:
(The other innovation was the use of anchor 3The name stands both for Web pages and for coinventor Larry Page (Brin and Page, 1998).

Token 17576:
Section 22.3.

Token 17577:
Information Retrieval 871 function HITS( query )returns pages with hub and authority numbers pages←EXPAND -PAGES (RELEV ANT -PAGES (query )) for each pinpages do p.AUTHORITY←1 p.HUB←1 repeat until convergence do for each pinpages do p.AUTHORITY←/summationtext iINLINK i(p).HUB p.HUB←/summationtext iOUTLINK i(p).AUTHORITY NORMALIZE (pages ) return pages Figure 22.1 The HITS algorithm for computing hubs and authorities with respect to a query.

Token 17578:
R ELEV ANT -PAGES fetches the pages that match the query, and E XPAND -PAGES adds in every page that links to or is linked from one of the relevant pages.

Token 17579:
N ORMALIZE divides each page’s score by the sum of the squares of a ll pages’ scores (separately for both the authority and hubs scores).

Token 17580:
text—the underlined text in a hyperlink—to index a page, even though the anchor text was on adifferent page than the one being indexed.)

Token 17581:
PageRank was invented to solve the problem of the tyranny of TFscores: if the query is [IBM], how do we make sure that IBM’s home page, ibm.com , is the ﬁrst result, even if another page mentions the term “IBM” more frequently?

Token 17582:
The idea is that ibm.com has many in-links (links to the page), so it should be ranked higher: each in-link is a vote for the quality of the linked-to page.

Token 17583:
But if we only counted in-links,then it would be possible for a Web spammer to create a network of pages and have them allpoint to a page of his choosing, increasing the score of that page.

Token 17584:
Therefore, the PageRankalgorithm is designed to weight links from high-quality sites more heavily. What is a high- quality site?

Token 17585:
One that is linked to by other high-quality sites. The deﬁnition is recursive, but we will see that the recursion bottoms out properly.

Token 17586:
The PageRank for a page pis deﬁned as: PR(p)=1−d N+d/summationdisplay iPR(ini) C(ini), where PR(p)is the PageRank of page p,Nis the total number of pages in the corpus, ini are the pages that link in to p,a n dC(ini)is the count of the total number of out-links on pageini.

Token 17587:
The constant dis a damping factor.

Token 17588:
It can be understood through the random surfer model : imagine a Web surfer who starts at some random page and begins exploring.RANDOM SURFER MODEL With probability d(we’ll assume d=0.85) the surfer clicks on one of the links on the page (choosing uniformly among them), and with probability 1−dshe gets bored with the page and restarts on a random page anywhere on the Web.

Token 17589:
The PageRank of page pis then the probability that the random surfer will be at page pat any point in time.

Token 17590:
PageRank can be computed by an iterative procedure: start with all pages having PR(p)=1 , and iterate the algorithm, updating ranks until they converge.

Token 17591:
872 Chapter 22.

Token 17592:
Natural Language Processing 22.3.5 The HITS algorithm The Hyperlink-Induced Topic Search algorithm, also known as “Hubs and Authorities” or HITS, is another inﬂuential link-analysis algorithm (see Figure 22.1).

Token 17593:
HITS differs from PageRank in several ways. First, it is a query-dependent measure: it rates pages with respectto a query.

Token 17594:
That means that it must be computed anew for each query—a computational burden that most search engines have elected not to take on.

Token 17595:
Given a query, HITS ﬁrst ﬁnds a set of pages that are relevant to the query.

Token 17596:
It does that by intersecting hit lists of querywords, and then adding pages in the link neighborhood of these pages—pages that link to orare linked from one of the pages in the original relevant set.

Token 17597:
Each page in this set is considered an authority on the query to the degree that other AUTHORITY pages in the relevant set point to it.

Token 17598:
A page is considered a hub to the degree that it points HUB to other authoritative pages in the relevant set.

Token 17599:
Just as with PageRank, we don’t want to merely count the number of links; we want to give more value to the high-quality hubs andauthorities.

Token 17600:
Thus, as with PageRank, we iterate a process that updates the authority score ofa page to be the sum of the hub scores of the pages that point to it, and the hub score to bethe sum of the authority scores of the pages it points to.

Token 17601:
If we then normalize the scores and repeat ktimes, the process will converge.

Token 17602:
Both PageRank and HITS played important roles in developing our understanding of Web information retrieval.

Token 17603:
These algorithms and their extensions are used in ranking billionsof queries daily as search engines steadily develop better ways of extracting yet ﬁner signalsof search relevance.

Token 17604:
22.3.6 Question answering Information retrieval is the task of ﬁnding documents that are relevant to a query, where thequery may be a question, or just a topic area or concept.

Token 17605:
Question answering is a somewhat QUESTION ANSWERING different task, in which the query really is a question, and the answer is not a ranked list of documents but rather a short response—a sentence, or even just a phrase.

Token 17606:
There havebeen question-answering NLP (natural language processing) systems since the 1960s, butonly since 2001 have such systems used Web information retrieval to radically increase theirbreadth of coverage.

Token 17607:
The A SKMSR system (Banko et al. , 2002) is a typical Web-based question-answering system.

Token 17608:
It is based on the intuition that most questions will be answered many times on theWeb, so question answering should be thought of as a problem in precision, not recall.

Token 17609:
We don’t have to deal with all the different ways that an answer might be phrased—we only have to ﬁnd one of them.

Token 17610:
For example, consider the query [Who killed Abraham Lincoln?

Token 17611:
]Suppose a system had to answer that question with access only to a single encyclopedia,whose entry on Lincoln said John Wilkes Booth altered history with a bullet.

Token 17612:
He will forever be known as the man who ended Abraham Lincoln’s life.

Token 17613:
To use this passage to answer the question, the system would have to know that ending a life can be a killing, that “He” refers to Booth, and several other linguistic and semantic facts.

Token 17614:
Section 22.4.

Token 17615:
Information Extraction 873 ASKMSR does not attempt this kind of sophistication—it knows nothing about pronoun reference, or about killing, or any other verb.

Token 17616:
It does know 15 different kinds of questions, andhow they can be rewritten as queries to a search engine.

Token 17617:
It knows that [Who killed AbrahamLincoln] can be rewritten as the query [* killed Abraham Lincoln] and as [Abraham Lincolnwas killed by *].

Token 17618:
It issues these rewritten queries and examines the results that come back— not the full Web pages, just the short summaries of text that appear near the query terms.

Token 17619:
The results are broken into 1-, 2-, and 3-grams and tallied for frequency in the result sets andfor weight: an n-gram that came back from a very speciﬁc query rewrite (such as the exact phrase match query [“Abraham Lincoln was killed by *”]) would get more weight than onefrom a general query rewrite, such as [Abraham OR Lincoln OR killed].

Token 17620:
We would expectthat “John Wilkes Booth” would be among the highly ranked n-grams retrieved, but so would “Abraham Lincoln” and “the assassination of” and “Ford’s Theatre.” Once the n-grams are scored, they are ﬁltered by expected type.

Token 17621:
If the original query starts with “who,” then we ﬁlter on names of people; for “how many” we ﬁlter on numbers, for“when,” on a date or time.

Token 17622:
There is also a ﬁlter that says the answer should not be part of thequestion; together these should allow us to return “John Wilkes Booth” (and not “AbrahamLincoln”) as the highest-scoring response.

Token 17623:
In some cases the answer will be longer than three words; since the components re- sponses only go up to 3-grams, a longer response would have to be pieced together fromshorter pieces.

Token 17624:
For example, in a system that used only bigrams, the answer “John WilkesBooth” could be pieced together from high-scoring pieces “John Wilkes” and “Wilkes Booth.” At the Text Retrieval Evaluation Conference (TREC), A SKMSR was rated as one of the top systems, beating out competitors with the ability to do far more complex language understanding.

Token 17625:
A SKMSR relies upon the breadth of the content on the Web rather than on its own depth of understanding.

Token 17626:
It won’t be able to handle complex inference patterns likeassociating “who killed” with “ended the life of.” But it knows that the Web is so vast that itcan afford to ignore passages like that and wait for a simple passage it can handle.

Token 17627:
22.4 I NFORMATION EXTRACTION Information extraction is the process of acquiring knowledge by skimming a text and look-INFORMATION EXTRACTION ing for occurrences of a particular class of object and for relationships among objects.

Token 17628:
A typical task is to extract instances of addresses from Web pages, with database ﬁelds forstreet, city, state, and zip code; or instances of storms from weather reports, with ﬁelds fortemperature, wind speed, and precipitation.

Token 17629:
In a limited domain, this can be done with highaccuracy.

Token 17630:
As the domain gets more general, more complex linguistic models and more com-plex learning techniques are necessary.

Token 17631:
We will see in Chapter 23 how to deﬁne complex language models of the phrase structure (noun phrases and verb phrases) of English.

Token 17632:
But so far there are no complete models of this kind, so for the limited needs of information ex-traction, we deﬁne limited models that approximate the full English model, and concentrateon just the parts that are needed for the task at hand.

Token 17633:
The models we describe in this sec-

Token 17634:
874 Chapter 22.

Token 17635:
Natural Language Processing tion are approximations in the same way that the simple 1-CNF logical model in Figure 7.21 (page 271) is an approximations of the full, wiggly, logical model.

Token 17636:
In this section we describe six different approaches to information extraction, in order of increasing complexity on several dimensions: deterministic to stochastic, domain-speciﬁcto general, hand-crafted to learned, and small-scale to large-scale.

Token 17637:
22.4.1 Finite-state automata f or information extraction The simplest type of information extraction system is an attribute-based extraction systemATTRIBUTE-BASED EXTRACTION that assumes that the entire text refers to a single object and the task is to extract attributes of that object.

Token 17638:
For example, we mentioned in Section 12.7 the problem of extracting from thetext “IBM ThinkBook 970.

Token 17639:
Our price: $399.00” the set of attributes {Manufacturer=IBM, Model=ThinkBook970, Price=$399.00 }.

Token 17640:
We can address this problem by deﬁning a tem- plate (also known as a pattern) for each attribute we would like to extract.

Token 17641:
The template is TEMPLATE deﬁned by a ﬁnite state automaton, the simplest example of which is the regular expression ,REGULAR EXPRESSION or regex.

Token 17642:
Regular expressions are used in Unix commands such as grep, in programming languages such as Perl, and in word processors such as Microsoft Word.

Token 17643:
The details varyslightly from one tool to another and so are best learned from the appropriate manual, but here we show how to build up a regular expression template for prices in dollars: [0-9] matches any digit from 0 to 9 [0-9]+ matches one or more digits [.

Token 17644:
][0-9][0-9] matches a period followed by two digits ([.][0-9][0-9])? matches a period followed by two digits, or nothing [$][0-9]+([.][0-9][0-9])?

Token 17645:
matches $249.99 or $1.23 or $1000000 or . . . Templates are often deﬁned with three parts: a preﬁx regex, a target regex, and a postﬁx regex.

Token 17646:
For prices, the target regex is as above, the preﬁx would look for strings such as “price:” andthe postﬁx could be empty.

Token 17647:
The idea is that some clues about an attribute come from theattribute value itself and some come from the surrounding text.

Token 17648:
If a regular expression for an attribute matches the text exactly once, then we can pull out the portion of the text that is the value of the attribute.

Token 17649:
If there is no match, all we can dois give a default value or leave the attribute missing; but if there are several matches, we needa process to choose among them.

Token 17650:
One strategy is to have several templates for each attribute,ordered by priority.

Token 17651:
So, for example, the top-priority template for price might look for thepreﬁx “our price:”; if that is not found, we look for the preﬁx “price:” and if that is not found,the empty preﬁx.

Token 17652:
Another strategy is to take all the matches and ﬁnd some way to chooseamong them.

Token 17653:
For example, we could take the lowest price that is within 50% of the highestprice.

Token 17654:
That will select $78.00 as the target from the text “List price $99.00, special sale price$78.00, shipping $3.00.” One step up from attribute-based extraction systems are relational extraction systems, RELATIONAL EXTRACTION which deal with multiple objects and the relations among them.

Token 17655:
Thus, when these systems see the text “$249.99,” they need to determine not just that it is a price, but also which objecthas that price.

Token 17656:
A typical relational-based extraction system is FASTUS, which handles newsstories about corporate mergers and acquisitions. It can read the story

Token 17657:
Section 22.4.

Token 17658:
Information Extraction 875 Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local concern and a Japanese trading house to produce golf clubs to be shipped to Japan.

Token 17659:
and extract the relations: e∈JointVentures ∧Product (e,“golfclubs ”)∧Date(e,“Friday ”) ∧Member (e,“Bridgestone Sports Co ”)∧Member (e,“al o c a lc o n c e r n ”) ∧Member (e,“a Japanese trading house ”).

Token 17660:
A relational extraction system can be built as a series of cascaded ﬁnite-state transducers .CASCADED FINITE-STATETRANSDUCERS That is, the system consists of a series of small, efﬁcient ﬁnite-state automata (FSAs), where each automaton receives text as input, transduces the text into a different format, and passes it along to the next automaton.

Token 17661:
FASTUS consists of ﬁve stages: 1. Tokenization 2. Complex-word handling3. Basic-group handling4. Complex-phrase handling5.

Token 17662:
Structure merging FASTUS’s ﬁrst stage is tokenization , which segments the stream of characters into tokens (words, numbers, and punctuation).

Token 17663:
For English, tokenization can be fairly simple; just sep- arating characters at white space or punctuation does a fairly good job.

Token 17664:
Some tokenizers also deal with markup languages such as HTML, SGML, and XML.

Token 17665:
The second stage handles complex words , including collocations such as “set up” and “joint venture,” as well as proper names such as “Bridgestone Sports Co.” These are rec- ognized by a combination of lexical entries and ﬁnite-state grammar rules.

Token 17666:
For example, a company name might be recognized by the rule CapitalizedWord+ (“Company” |“Co”|“Inc”|“Ltd”) The third stage handles basic groups , meaning noun groups and verb groups.

Token 17667:
The idea is to chunk these into units that will be managed by the later stages.

Token 17668:
We will see how to write a complex description of noun and verb phrases in Chapter 23, but here we have simplerules that only approximate the complexity of English, but have the advantage of being rep- resentable by ﬁnite state automata.

Token 17669:
The example sentence would emerge from this stage as the following sequence of tagged groups: 1 NG: Bridgestone Sports Co. 10 NG: a local concern 2 VG: said 11 CJ: and 3 NG: Friday 12 NG: a Japanese trading house 4 NG: it 13 VG: to produce5 VG: had set up 14 NG: golf clubs 6 NG: a joint venture 15 VG: to be shipped 7 PR: in 16 PR: to 8 NG: Taiwan 17 NG: Japan 9 PR: with Here NG means noun group, VG is verb group, PR is preposition, and CJ is conjunction.

Token 17670:
876 Chapter 22. Natural Language Processing The fourth stage combines the basic groups into complex phrases .

Token 17671:
Again, the aim is to have rules that are ﬁnite-state and thus can be processed quickly, and that result inunambiguous (or nearly unambiguous) output phrases.

Token 17672:
One type of combination rule dealswith domain-speciﬁc events. For example, the rule Company+ SetUp JointVenture (“with” Company+)?

Token 17673:
captures one way to describe the formation of a joint venture.

Token 17674:
This stage is the ﬁrst one in the cascade where the output is placed into a database template as well as being placed in the output stream.

Token 17675:
The ﬁnal stage merges structures that were built up in the previous step.

Token 17676:
If the next sentence says “The joint venture will start production in January,” then this step willnotice that there are two references to a joint venture, and that they should be merged intoone.

Token 17677:
This is an instance of the identity uncertainty problem discussed in Section 14.6.3.

Token 17678:
In general, ﬁnite-state template-based information extraction works well for a restricted domain in which it is possible to predetermine what subjects will be discussed, and how theywill be mentioned.

Token 17679:
The cascaded transducer model helps modularize the necessary knowl-edge, easing construction of the system.

Token 17680:
These systems work especially well when they arereverse-engineering text that has been generated by a program.

Token 17681:
For example, a shopping siteon the Web is generated by a program that takes database entries and formats them into Web pages; a template-based extractor then recovers the original database.

Token 17682:
Finite-state informa- tion extraction is less successful at recovering information in highly variable format, such astext written by humans on a variety of subjects.

Token 17683:
22.4.2 Probabilistic models for information extraction When information extraction must be attempted from noisy or varied input, simple ﬁnite-stateapproaches fare poorly.

Token 17684:
It is too hard to get all the rules and their priorities right; it is betterto use a probabilistic model rather than a rule-based model.

Token 17685:
The simplest probabilistic modelfor sequences with hidden state is the hidden Markov model, or HMM.

Token 17686:
Recall from Section 15.3 that an HMM models a progression through a sequence of hidden states, x t, with an observation etat each step.

Token 17687:
To apply HMMs to information ex- traction, we can either build one big HMM for all the attributes or build a separate HMM for each attribute.

Token 17688:
We’ll do the second.

Token 17689:
The observations are the words of the text, and the hidden states are whether we are in the target, preﬁx, or postﬁx part of the attribute template,or in the background (not part of a template).

Token 17690:
For example, here is a brief text and the mostprobable (Viterbi) path for that text for two HMMs, one trained to recognize the speaker in atalk announcement, and one trained to recognize dates.

Token 17691:
The “-” indicates a background state: Text: There will be a seminar by Dr. Andrew McCallum on Friday Speaker: - - - - PRE PRE TARGET TARGET TARGET POST - D a t e : - - - - - - --- PRE TARGET HMMs have two big advantages over FSAs for extraction.

Token 17692:
First, HMMs are probabilistic, andthus tolerant to noise.

Token 17693:
In a regular expression, if a single expected character is missing, theregex fails to match; with HMMs there is graceful degradation with missing characters/words,and we get a probability indicating the degree of match, not just a Boolean match/fail.

Token 17694:
Second,

Token 17695:
Section 22.4.

Token 17696:
Information Extraction 877 that by speakers / herewill (receivedhas is1.0 1.00.99 0.76 0.240.99 0.440.56: with; about howwho speaker speak5409 appointment seminarreminder theater artist additionallydr professor robert michael mr w cavalier stevens christel l Prefix Target Postfix Figure 22.2 Hidden Markov model for the speaker of a talk announcement.

Token 17697:
The two square states are the target (note the second target state has a self-loop, so the target canmatch a string of any length), the four circles to the left are the preﬁx, and the one on the right is the postﬁx.

Token 17698:
For each state, only a few of the high-probability words are shown. From Freitag and McCallum (2000).

Token 17699:
HMMs can be trained from data; they don’t require laborious engineering of templates, and thus they can more easily be kept up to date as text changes over time.

Token 17700:
Note that we have assumed a certain level of structure in our HMM templates: they all consist of one or more target states, and any preﬁx states must precede the targets, postﬁxstates most follow the targets, and other states must be background.

Token 17701:
This structure makesit easier to learn HMMs from examples.

Token 17702:
With a partially speciﬁed structure, the forward–backward algorithm can be used to learn both the transition probabilities P(X t|Xt−1)be- tween states and the observation model, P(Et|Xt), which says how likely each word is in each state.

Token 17703:
For example, the word “Friday” would have high probability in one or more ofthe target states of the date HMM, and lower probability elsewhere.

Token 17704:
With sufﬁcient training data, the HMM automatically learns a structure of dates that we ﬁnd intuitive: the date HMM might have one target state in which the high-probability wordsare “Monday,” “Tuesday,” etc., and which has a high-probability transition to a target statewith words “Jan”, “January,” “Feb,” etc.

Token 17705:
Figure 22.2 shows the HMM for the speaker of atalk announcement, as learned from data.

Token 17706:
The preﬁx covers expressions such as “Speaker:”and “seminar by,” and the target has one state that covers titles and ﬁrst names and anotherstate that covers initials and last names.

Token 17707:
Once the HMMs have been learned, we can apply them to a text, using the Viterbi algorithm to ﬁnd the most likely path through the HMM states.

Token 17708:
One approach is to apply each attribute HMM separately; in this case you would expect most of the HMMs to spend most of their time in background states.

Token 17709:
This is appropriate when the extraction is sparse— when the number of extracted words is small compared to the length of the text.

Token 17710:
878 Chapter 22.

Token 17711:
Natural Language Processing The other approach is to combine all the individual attributes into one big HMM, which would then ﬁnd a path that wanders through different target attributes, ﬁrst ﬁnding a speakertarget, then a date target, etc.

Token 17712:
Separate HMMs are better when we expect just one of eachattribute in a text and one big HMM is better when the texts are more free-form and densewith attributes.

Token 17713:
With either approach, in the end we have a collection of target attribute observations, and have to decide what to do with them.

Token 17714:
If every expected attribute has one target ﬁller then the decision is easy: we have an instance of the desired relation.

Token 17715:
If thereare multiple ﬁllers, we need to decide which to choose, as we discussed with template-basedsystems.

Token 17716:
HMMs have the advantage of supplying probability numbers that can help makethe choice.

Token 17717:
If some targets are missing, we need to decide if this is an instance of the desiredrelation at all, or if the targets found are false positives.

Token 17718:
A machine learning algorithm can betrained to make this choice.

Token 17719:
22.4.3 Conditional random ﬁelds for information extraction One issue with HMMs for the information extraction task is that they model a lot of prob-abilities that we don’t really need.

Token 17720:
An HMM is a generative model; it models the full joint probability of observations and hidden states, and thus can be used to generate samples.

Token 17721:
That is, we can use the HMM model not only to parse a text and recover the speaker and date,but also to generate a random instance of a text containing a speaker and a date.

Token 17722:
Since we’renot interested in that task, it is natural to ask whether we might be better off with a modelthat doesn’t bother modeling that possibility.

Token 17723:
All we need in order to understand a text is adiscriminative model , one that models the conditional probability of the hidden attributes given the observations (the text).

Token 17724:
Given a text e 1:N, the conditional model ﬁnds the hidden state sequence X1:Nthat maximizes P(X1:N|e1:N).

Token 17725:
Modeling this directly gives us some freedom.

Token 17726:
We don’t need the independence as- sumptions of the Markov model—we can have an xtthat is dependent on x1.A f r a m e w o r k for this type of model is the conditional random ﬁeld , or CRF, which models a conditionalCONDITIONAL RANDOM FIELD probability distribution of a set of target variables given a set of observed variables.

Token 17727:
Like Bayesian networks, CRFs can represent many different structures of dependencies among thevariables.

Token 17728:
One common structure is the linear-chain conditional random ﬁeld for repre- LINEAR-CHAIN CONDITIONALRANDOM FIELD senting Markov dependencies among variables in a temporal sequence.

Token 17729:
Thus, HMMs are the temporal version of naive Bayes models, and linear-chain CRFs are the temporal version oflogistic regression, where the predicted target is an entire state sequence rather than a singlebinary variable.

Token 17730:
Lete 1:Nbe the observations (e.g., words in a document), and x1:Nbe the sequence of hidden states (e.g., the preﬁx, target, and postﬁx states).

Token 17731:
A linear-chain conditional randomﬁeld deﬁnes a conditional probability distribution: P(x 1:N|e1:N)=αe[PN i=1F(xi−1,xi,e,i)], where αis a normalization factor (to make sure the probabilities sum to 1), and Fis a feature function deﬁned as the weighted sum of a collection of kcomponent feature functions: F(xi−1,xi,e,i)=/summationdisplay kλkfk(xi−1,xi,e,i).

Token 17732:
Section 22.4.

Token 17733:
Information Extraction 879 Theλkparameter values are learned with a MAP (maximum a posteriori) estimation proce- dure that maximizes the conditional likelihood of the training data.

Token 17734:
The feature functions arethe key components of a CRF.

Token 17735:
The function f khas access to a pair of adjacent states, xi−1and xi, but also the entire observation (word) sequence e, and the current position in the temporal sequence, i.

Token 17736:
This gives us a lot of ﬂexibility in deﬁning features.

Token 17737:
We can deﬁne a simple feature function, for example one that produces a value of 1 if the current word is A NDREW and the current state is SPEAKER : f1(xi−1,xi,e,i)=/braceleftbigg1ifxi=SPEAKER andei=ANDREW 0otherwise How are features like these used?

Token 17738:
It depends on their corresponding weights.

Token 17739:
If λ1>0,t h e n whenever f1is true, it increases the probability of the hidden state sequence x1:N.T h i s i s another way of saying “the CRF model should prefer the target state SPEAKER for the word ANDREW .” If on the other hand λ1<0, the CRF model will try to avoid this association, and if λ1=0, this feature is ignored.

Token 17740:
Parameter values can be set manually or can be learned from data.

Token 17741:
Now consider a second feature function: f2(xi−1,xi,e,i)=/braceleftbigg1ifxi=SPEAKER andei+1=SAID 0otherwise This feature is true if the current state is SPEAKER and the next word is “said.” One would therefore expect a positive λ2value to go with the feature.

Token 17742:
More interestingly, note that both f1andf2can hold at the same time for a sentence like “Andrew said . . .

Token 17743:
.” In this case, the two features overlap each other and both boost the belief in x1=SPEAKER .

Token 17744:
Because of the independence assumption, HMMs cannot use overlapping features; CRFs can.

Token 17745:
Furthermore, a feature in a CRF can use any part of the sequence e1:N. Features can also be deﬁned over transitions between states.

Token 17746:
The features we deﬁned here were binary, but in general, a featurefunction can be any real-valued function.

Token 17747:
For domains where we have some knowledge aboutthe types of features we would like to include, the CRF formalism gives us a great deal ofﬂexibility in deﬁning them.

Token 17748:
This ﬂexibility can lead to accuracies that are higher than withless ﬂexible models such as HMMs.

Token 17749:
22.4.4 Ontology extraction from large corpora So far we have thought of information extraction as ﬁnding a speciﬁc set of relations (e.g.,speaker, time, location) in a speciﬁc text (e.g., a talk announcement).

Token 17750:
A different applica-tion of extraction technology is building a large knowledge base or ontology of facts froma corpus.

Token 17751:
This is different in three ways: First it is open-ended—we want to acquire factsabout all types of domains, not just one speciﬁc domain.

Token 17752:
Second, with a large corpus, thistask is dominated by precision, not recall—just as with question answering on the Web (Sec-tion 22.3.6).

Token 17753:
Third, the results can be statistical aggregates gathered from multiple sources,rather than being extracted from one speciﬁc text.

Token 17754:
For example, Hearst (1992) looked at the problem of learning an ontology of concept categories and subcategories from a large corpus.

Token 17755:
(In 1992, a large corpus was a 1000-pageencyclopedia; today it would be a 100-million-page Web corpus.)

Token 17756:
The work concentrated ontemplates that are very general (not tied to a speciﬁc domain) and have high precision (are

Token 17757:
880 Chapter 22. Natural Language Processing almost always correct when they match) but low recall (do not always match).

Token 17758:
Here is one of the most productive templates: NPsuch as NP(,NP)* (,)? ((and|or)NP)? .

Token 17759:
Here the bold words and commas must appear literally in the text, but the parentheses are for grouping, the asterisk means repetition of zero or more , and the question mark means optional.

Token 17760:
NPis a variable standing for a noun phrase; Chapter 23 describes how to identify noun phrases; for now just assume that we know some words are nouns and other words (suchas verbs) that we can reliably assume are not part of a simple noun phrase.

Token 17761:
This templatematches the texts “diseases such as rabies affect your dog” and “supports network protocolssuch as DNS,” concluding that rabies is a disease and DNS is a network protocol.

Token 17762:
Similartemplates can be constructed with the key words “including,” “especially,” and “or other.” Ofcourse these templates will fail to match many relevant passages, like “Rabies is a disease.”That is intentional.

Token 17763:
The “ NPis aNP” template does indeed sometimes denote a subcategory relation, but it often means something else, as in “There is a God” or “She is a little tired.”With a large corpus we can afford to be picky; to use only the high-precision templates.

Token 17764:
We’ll miss many statements of a subcategory relationship, but most likely we’ll ﬁnd a paraphrase of the statement somewhere else in the corpus in a form we can use.

Token 17765:
22.4.5 Automated template construction The subcategory relation is so fundamental that is worthwhile to handcraft a few templates to help identify instances of it occurring in natural language text.

Token 17766:
But what about the thousandsof other relations in the world?

Token 17767:
There aren’t enough AI grad students in the world to createand debug templates for all of them.

Token 17768:
Fortunately, it is possible to learn templates from a few examples, then use the templates to learn more examples, from which more templates can belearned, and so on.

Token 17769:
In one of the ﬁrst experiments of this kind, Brin (1999) started with a dataset of just ﬁve examples: (“Isaac Asimov”, “The Robots of Dawn”) (“David Brin”, “Startide Rising”) (“James Gleick”, “Chaos—Making a New Science”) (“Charles Dickens”, “Great Expectations”)(“William Shakespeare”, “The Comedy of Errors”) Clearly these are examples of the author–title relation, but the learning system had no knowl- edge of authors or titles.

Token 17770:
The words in these examples were used in a search over a Webcorpus, resulting in 199 matches.

Token 17771:
Each match is deﬁned as a tuple of seven strings, (Author, Title, Order, Preﬁx, Middle, Postﬁx, URL ), where Order is true if the author came ﬁrst and false if the title came ﬁrst, Middle is the characters between the author and title, Preﬁx is the 10 characters before the match, Sufﬁx is the 10 characters after the match, and URL is the Web address where the match was made.

Token 17772:
Given a set of matches, a simple template-generation scheme can ﬁnd templates to explain the matches.

Token 17773:
The language of templates was designed to have a close mapping to thematches themselves, to be amenable to automated learning, and to emphasize high precision

Token 17774:
Section 22.4. Information Extraction 881 (possibly at the risk of lower recall). Each template has the same seven components as a match.

Token 17775:
The Author andTitle are regexes consisting of any characters (but beginning and ending in letters) and constrained to have a length from half the minimum length of theexamples to twice the maximum length.

Token 17776:
The preﬁx, middle, and postﬁx are restricted toliteral strings, not regexes.

Token 17777:
The middle is the easiest to learn: each distinct middle string in the set of matches is a distinct candidate template.

Token 17778:
For each such candidate, the template’s Preﬁx is then deﬁned as the longest common sufﬁx of all the preﬁxes in the matches, and the Postﬁx is deﬁned as the longest common preﬁx of all the postﬁxes in the matches.

Token 17779:
If either of these is of length zero, then the template is rejected.

Token 17780:
The URL of the template is deﬁned as the longest preﬁx of the URLs in the matches.

Token 17781:
In the experiment run by Brin, the ﬁrst 199 matches generated three templates.

Token 17782:
The most productive template was <LI><B> Title</B> by Author ( URL :www.sff.net/locus/c The three templates were then used to retrieve 4047 more (author, title) examples.

Token 17783:
The exam- ples were then used to generate more templates, and so on, eventually yielding over 15,000titles.

Token 17784:
Given a good set of templates, the system can collect a good set of examples.

Token 17785:
Given agood set of examples, the system can build a good set of templates. The biggest weakness in this approach is the sensitivity to noise.

Token 17786:
If one of the ﬁrst few templates is incorrect, errors can propagate quickly.

Token 17787:
One way to limit this problem is tonot accept a new example unless it is veriﬁed by multiple templates, and not accept a newtemplate unless it discovers multiple examples that are also found by other templates.

Token 17788:
22.4.6 Machine reading Automated template construction is a big step up from handcrafted template construction, butit still requires a handful of labeled examples of each relation to get started.

Token 17789:
To build a largeontology with many thousands of relations, even that amount of work would be onerous; we would like to have an extraction system with nohuman input of any kind—a system that could read on its own and build up its own database.

Token 17790:
Such a system would be relation-independent; would work for any relation.

Token 17791:
In practice, these systems work on allrelations in parallel, because of the I/O demands of large corpora.

Token 17792:
They behave less like a traditional information-extraction system that is targeted at a few relations and more like a human reader who learnsfrom the text itself; because of this the ﬁeld has been called machine reading .

Token 17793:
MACHINE READING A representative machine-reading system is T EXTRUNNER (Banko and Etzioni, 2008).

Token 17794:
TEXTRUNNER uses cotraining to boost its performance, but it needs something to bootstrap from.

Token 17795:
In the case of Hearst (1992), speciﬁc patterns (e.g., such as ) provided the bootstrap, and for Brin (1998), it was a set of ﬁve author–title pairs.

Token 17796:
For T EXTRUNNER , the original inspi- ration was a taxonomy of eight very general syntactic templates, as shown in Figure 22.3.

Token 17797:
It was felt that a small number of templates like this could cover most of the ways that relation- ships are expressed in English.

Token 17798:
The actual bootsrapping starts from a set of labelled examplesthat are extracted from the Penn Treebank, a corpus of parsed sentences.

Token 17799:
For example, fromthe parse of the sentence “Einstein received the Nobel Prize in 1921,” T EXTRUNNER is able

Token 17800:
882 Chapter 22. Natural Language Processing to extract the relation (“Einstein,” “received,” “Nobel Prize”).

Token 17801:
Given a set of labeled examples of this type, T EXTRUNNER trains a linear-chain CRF to extract further examples from unlabeled text.

Token 17802:
The features in the CRF include functionwords like “to” and “of” and “the,” but not nouns and verbs (and not noun phrases or verbphrases).

Token 17803:
Because T EXTRUNNER is domain-independent, it cannot rely on predeﬁned lists of nouns and verbs.

Token 17804:
Type Template Example Frequency Verb NP1Verb NP 2 X established Y 38% Noun–Prep NP1NP Prep NP 2 X settlement with Y 23% Verb–Prep NP1Verb Prep NP 2 X moved to Y 16% Inﬁnitive NP1toVerb NP 2 X plans to acquire Y 9% Modiﬁer NP1Verb NP 2Noun X is Y winner 5% Noun-Coordinate NP1(,|and|-|:)NP2NP X-Y deal 2% Verb-Coordinate NP1(,|and)NP2Verb X, Y merge 1% Appositive NP1NP(:|,)?NP2 X hometown : Y 1% Figure 22.3 Eight general templates that cover about 95% of the ways that relations are expressed in English.

Token 17805:
TEXTRUNNER achieves a precision of 88% and recall of 45% ( F1of 60%) on a large Web corpus.

Token 17806:
T EXTRUNNER has extracted hundreds of millions of facts from a corpus of a half-billion Web pages.

Token 17807:
For example, even though it has no predeﬁned medical knowledge,it has extracted over 2000 answers to the query [what kills bacteria]; correct answers includeantibiotics, ozone, chlorine, Cipro, and broccoli sprouts.

Token 17808:
Questionable answers include “wa-ter,” which came from the sentence “Boiling water for at least 10 minutes will kill bacteria.”It would be better to attribute this to “boiling water” rather than just “water.” With the techniques outlined in this chapter and continual new inventions, we are start- ing to get closer to the goal of machine reading.

Token 17809:
22.5 S UMMARY The main points of this chapter are as follows: •Probabilistic language models based on n-grams recover a surprising amount of infor- mation about a language.

Token 17810:
They can perform well on such diverse tasks as languageidentiﬁcation, spelling correction, genre classiﬁcation, and named-entity recognition.

Token 17811:
•These language models can have millions of features, so feature selection and prepro- cessing of the data to reduce noise is important.

Token 17812:
•Text classiﬁcation can be done with naive Bayes n-gram models or with any of the classiﬁcation algorithms we have previously discussed.

Token 17813:
Classiﬁcation can also be seenas a problem in data compression.

Token 17814:


Token 17815:
Bibliographical and Historical Notes 883 •Information retrieval systems use a very simple language model based on bags of words, yet still manage to perform well in terms of recall andprecision on very large corpora of text.

Token 17816:
On Web corpora, link-analysis algorithms improve performance.

Token 17817:
•Question answering can be handled by an approach based on information retrieval, for questions that have multiple answers in the corpus.

Token 17818:
When more answers are availablein the corpus, we can use techniques that emphasize precision rather than recall.

Token 17819:
•Information-extraction systems use a more complex model that includes limited no- tions of syntax and semantics in the form of templates.

Token 17820:
They can be built from ﬁnite-state automata, HMMs, or conditional random ﬁelds, and can be learned from examples.

Token 17821:
•In building a statistical language system, it is best to devise a model that can make good use of available data , even if the model seems overly simplistic.

Token 17822:
BIBLIOGRAPHICAL AND HISTORICAL NOTES N-gram letter models for language modeling were proposed by Markov (1913).

Token 17823:
Claude Shannon (Shannon and Weaver, 1949) was the ﬁrst to generate n-gram word models of En- glish.

Token 17824:
Chomsky (1956, 1957) pointed out the limitations of ﬁnite-state models compared withcontext-free models, concluding, “Probabilistic models give no particular insight into someof the basic problems of syntactic structure.” This is true, but probabilistic models doprovide insight into some other basic problems—problems that context-free models ignore.

Token 17825:
Chom- sky’s remarks had the unfortunate effect of scaring many people away from statistical modelsfor two decades, until these models reemerged for use in speech recognition (Jelinek, 1976).

Token 17826:
Kessler et al. (1997) show how to apply character n-gram models to genre classiﬁcation, and Klein et al.

Token 17827:
(2003) describe named-entity recognition with character models.

Token 17828:
Franz and Brants (2006) describe the Google n-gram corpus of 13 million unique words from a trillion words of Web text; it is now publicly available.

Token 17829:
The bag of words model gets its name from a passage from linguist Zellig Harris (1954), “language is not merely a bag of words buta tool with particular properties.” Norvig (2009) gives some examples of tasks that can beaccomplished with n-gram models.

Token 17830:
Add-one smoothing, ﬁrst suggested by Pierre-Simon Laplace (1816), was formalized by Jeffreys (1948), and interpolation smoothing is due to Jelinek and Mercer (1980), who used it for speech recognition.

Token 17831:
Other techniques include Witten–Bell smoothing (1991), Good– Turing smoothing (Church and Gale, 1991) and Kneser–Ney smoothing (1995).

Token 17832:
Chen andGoodman (1996) and Goodman (2001) survey smoothing techniques.

Token 17833:
Simple n-gram letter and word models are not the only possible probabilistic models. Blei et al.

Token 17834:
(2001) describe a probabilistic text model called latent Dirichlet allocation that views a document as a mixture of topics, each with its own distribution of words.

Token 17835:
This model can be seen as an extension and rationalization of the latent semantic indexing model of (Deerwester et al.

Token 17836:
, 1990) (see also Papadimitriou et al. (1998)) and is also related to the multiple-cause mixture model of (Sahami et al. , 1996).

Token 17837:
884 Chapter 22. Natural Language Processing Manning and Sch¨ utze (1999) and Sebastiani (2002) survey text-classiﬁcation techniques.

Token 17838:
Joachims (2001) uses statistical learning theory and support vector machines to give a theo-retical analysis of when classiﬁcation will be successful.

Token 17839:
Apt´ eet al. (1994) report an accuracy of 96% in classifying Reuters news articles into the “Earnings” category.

Token 17840:
Koller and Sahami(1997) report accuracy up to 95% with a naive Bayes classiﬁer, and up to 98.6% with a Bayes classiﬁer that accounts for some dependencies among features.

Token 17841:
Lewis (1998) surveys forty years of application of naive Bayes techniques to text classiﬁcation and retrieval.

Token 17842:
Schapireand Singer (2000) show that simple linear classiﬁers can often achieve accuracy almost asgood as more complex models and are more efﬁcient to evaluate.

Token 17843:
Nigam et al. (2000) show how to use the EM algorithm to label unlabeled documents, thus learning a better classiﬁ-cation model. Witten et al.

Token 17844:
(1999) describe compression algorithms for classiﬁcation, and show the deep connection between the LZW compression algorithm and maximum-entropylanguage models.

Token 17845:
Many of the n-gram model techniques are also used in bioinformatics problems.

Token 17846:
Bio- statistics and probabilistic NLP are coming closer together, as each deals with long, structuredsequences chosen from an alphabet of constituents.

Token 17847:
The ﬁeld of information retrieval is experiencing a regrowth in interest, sparked by the wide usage of Internet searching.

Token 17848:
Robertson (1977) gives an early overview and intro- duces the probability ranking principle. Croft et al. (2009) and Manning et al.

Token 17849:
(2008) are the ﬁrst textbooks to cover Web-based search as well as traditional IR. Hearst (2009) covers user interfaces for Web search.

Token 17850:
The TREC conference, organized by the U.S. government’s National Institute of Standards and Technology (NIST), hosts an annual competition for IR systems and publishes proceedings with results.

Token 17851:
In the ﬁrst seven years of the competition, performance roughly doubled. The most popular model for IR is the vector space model (Salton et al.

Token 17852:
, 1975). Salton’s work dominated the early years of the ﬁeld.

Token 17853:
There are two alternative probabilistic models,one due to Ponte and Croft (1998) and one by Maron and Kuhns (1960) and Robertson andSparck Jones (1976).

Token 17854:
Lafferty and Zhai (2001) show that the models are based on the same joint probability distribution, but that the choice of model has implications for training the parameters.

Token 17855:
Craswell et al.

Token 17856:
(2005) describe the BM25 scoring function and Svore and Burges (2009) describe how BM25 can be improved with a machine learning approach that incorpo-rates click data—examples of past search queies and the results that were clicked on.

Token 17857:
Brin and Page (1998) describe the PageRank algorithm and the implementation of a Web search engine. Kleinberg (1999) describes the HITS algorithm.

Token 17858:
Silverstein et al. (1998) investigate a log of a billion Web searches.

Token 17859:
The journal Information Retrieval and the pro- ceedings of the annual SIGIR conference cover recent developments in the ﬁeld.

Token 17860:
Early information extraction programs include G US(Bobrow et al. , 1977) and F RUMP (DeJong, 1982).

Token 17861:
Recent information extraction has been pushed forward by the annual Mes- sage Understand Conferences (MUC), sponsored by the U.S. government.

Token 17862:
The FASTUS ﬁnite-state system was done by Hobbs et al. (1997).

Token 17863:
It was based in part on the idea from Pereira and Wright (1991) of using FSAs as approximations to phrase-structure grammars.Surveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999),

Token 17864:
Exercises 885 and Muslea (1999). Large databases of facts were extracted by Craven et al. (2000), Pasca et al.

Token 17865:
(2006), Mitchell (2007), and Durme and Pasca (2008). Freitag and McCallum (2000) discuss HMMs for Information Extraction.

Token 17866:
CRFs were introduced by Lafferty et al.

Token 17867:
(2001); an example of their use for information extraction is described in (McCallum, 2003) and a tutorial with practical guidance is given by (Sutton and McCallum, 2007).

Token 17868:
Sarawagi (2007) gives a comprehensive survey. Banko et al.

Token 17869:
(2002) present the A SKMSR question-answering system; a similar sys- tem is due to Kwok et al. (2001).

Token 17870:
Pasca and Harabagiu (2001) discuss a contest-winning question-answering system.

Token 17871:
Two early inﬂuential approaches to automated knowledge engi-neering were by Riloff (1993), who showed that an automatically constructed dictionary per-formed almost as well as a carefully handcrafted domain-speciﬁc dictionary, and by Yarowsky(1995), who showed that the task of word sense classiﬁcation (see page 756) could be accom-plished through unsupervised training on a corpus of unlabeled text with accuracy as good assupervised methods.

Token 17872:
The idea of simultaneously extracting templates and examples from a handful of labeled examples was developed independently and simultaneously by Blum and Mitchell (1998),who called it cotraining and by Brin (1998), who called it DIPRE (Dual Iterative Pattern Relation Extraction).

Token 17873:
You can see why the term cotraining has stuck. Similar early work, under the name of bootstrapping, was done by Jones et al. (1999).

Token 17874:
The method was advanced by the QX TRACT (Agichtein and Gravano, 2003) and K NOW ITALL(Etzioni et al. , 2005) systems.

Token 17875:
Machine reading was introduced by Mitchell (2005) and Etzioni et al. (2006) and is the focus of the T EXTRUNNER project (Banko et al.

Token 17876:
, 2007; Banko and Etzioni, 2008).

Token 17877:
This chapter has focused on natural language text, but it is also possible to do informa- tion extraction based on the physical structure or layout of text rather than on the linguisticstructure.

Token 17878:
HTML lists and tables in both HTML and relational databases are home to datathat can be extracted and consolidated (Hurst, 2000; Pinto et al.

Token 17879:
, 2003; Cafarella et al. , 2008).

Token 17880:
The Association for Computational Linguistics (ACL) holds regular conferences and publishes the journal Computational Linguistics .

Token 17881:
There is also an International Conference on Computational Linguistics (COLING).

Token 17882:
The textbook by Manning and Sch¨ utze (1999) cov- ers statistical language processing, while Jurafsky and Martin (2008) give a comprehensiveintroduction to speech and natural language processing.

Token 17883:
EXERCISES 22.1 This exercise explores the quality of the n-gram model of language. Find or create a monolingual corpus of 100,000 words or more.

Token 17884:
Segment it into words, and compute the fre- quency of each word. How many distinct words are there?

Token 17885:
Also count frequencies of bigrams (two consecutive words) and trigrams (three consecutive words).

Token 17886:
Now use those frequencies to generate language: from the unigram, bigram, and trigram models, in turn, generate a 100-word text by making random choices according to the frequency counts.

Token 17887:
Compare the threegenerated texts with actual language. Finally, calculate the perplexity of each model.

Token 17888:
886 Chapter 22. Natural Language Processing 22.2 Write a program to do segmentation of words without spaces.

Token 17889:
Given a string, such as the URL “thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,” return a list of component words: [“the,” “longest,” “list,” ...].

Token 17890:
This task is useful for parsing URLs, for spelling correction when words runtogether, and for languages such as Chinese that donot have spaces between words.

Token 17891:
It can be solved with a unigram or bigram word model and a dynamic programming algorithm similar to the Viterbi algorithm.

Token 17892:
22.3 (Adapted from Jurafsky and Martin (2000).)

Token 17893:
In this exercise you will develop a classi- ﬁer for authorship: given a text, the classiﬁer predicts which of two candidate authors wrote the text.

Token 17894:
Obtain samples of text from two different authors. Separate them into training andtest sets. Now train a language model on the training set.

Token 17895:
You can choose what features touse;n-grams of words or letters are the easiest, but you can add additional features that you think may help.

Token 17896:
Then compute the probability of the text under each language model andchose the most probable model. Assess the accuracy of this technique.

Token 17897:
How does accuracychange as you alter the set of features?

Token 17898:
This subﬁeld of linguistics is called stylometry ; its STYLOMETRY successes include the identiﬁcation of the author of the disputed Federalist Papers (Mosteller and Wallace, 1964) and some disputed works of Shakespeare (Hope, 1994).

Token 17899:
Khmelev andTweedie (2001) produce good results with a simple letter bigram model. 22.4 This exercise concerns the classiﬁcation of spam email.

Token 17900:
Create a corpus of spam email and one of non-spam mail.

Token 17901:
Examine each corpus and decide what features appear to be useful for classiﬁcation: unigram words? bigrams? message length, sender, time of arrival?

Token 17902:
Then train a classiﬁcation algorithm (decision tree, naive Bayes, SVM, logistic regression, or someother algorithm of your choosing) on a training set and report its accuracy on a test set.

Token 17903:
22.5 Create a test set of ten queries, and pose them to three major Web search engines. Evaluate each one for precision at 1, 3, and 10 documents.

Token 17904:
Can you explain the differencesbetween engines?

Token 17905:
22.6 Try to ascertain which of the search engines from the previous exercise are using case folding, stemming, synonyms, and spelling correction.

Token 17906:
22.7 Write a regular expression or a short program to extract company names. Test it on a corpus of business news articles.

Token 17907:
Report your recall and precision.

Token 17908:
22.8 Consider the problem of trying to evaluate the quality of an IR system that returns a ranked list of answers (like most Web search engines).

Token 17909:
The appropriate measure of qualitydepends on the presumed model of what the searcher is trying to achieve, and what strategyshe employs.

Token 17910:
For each of the following models, propose a corresponding numeric measure. a.

Token 17911:
The searcher will look at the ﬁrst twenty answers returned, with the objective of getting as much relevant information as possible. b.

Token 17912:
The searcher needs only one relevant document, and will go down the list until she ﬁnds the ﬁrst one.

Token 17913:
c. The searcher has a fairly narrow query and is able to examine all the answers retrieved.

Token 17914:
She wants to be sure that she has seen everything in the document collection that is

Token 17915:
Exercises 887 relevant to her query.

Token 17916:
(E.g., a lawyer wants to be sure that she has found allrelevant precedents, and is willing to spend considerable resources on that.)

Token 17917:
d. The searcher needs just one document relevant to the query, and can afford to pay a research assistant for an hour’s work looking through the results.

Token 17918:
The assistant can lookthrough 100 retrieved documents in an hour.

Token 17919:
The assistant will charge the searcher forthe full hour regardless of whether he ﬁnds it immediately or at the end of the hour.

Token 17920:
e. The searcher will look through all the answers.

Token 17921:
Examining a document has cost $ A; ﬁnding a relevant document has value $ B; failing to ﬁnd a relevant document has cost $Cfor each relevant document not found.

Token 17922:
f. The searcher wants to collect as many relevant documents as possible, but needs steady encouragement. She looks through the documents in order.

Token 17923:
If the documents she haslooked at so far are mostly good, she will continue; otherwise, she will stop.

Token 17924:


Token 17925:
23NATURAL LANGUAGE FOR COMMUNICATION In which we see how humans communicate with one another in natural language, and how computer agents might join in the conversation.

Token 17926:
Communication is the intentional exchange of information brought about by the production COMMUNICATION and perception of signs drawn from a shared system of conventional signs.

Token 17927:
Most animals use SIGN signs to represent important messages: food here, predator nearby, approach, withdraw, let’s mate.

Token 17928:
In a partially observable world, communication can help agents be successful becausethey can learn information that is observed or inferred by others.

Token 17929:
Humans are the most chattyof all species, and if computer agents are to be helpful, they’ll need to learn to speak thelanguage.

Token 17930:
In this chapter we look at language models for communication.

Token 17931:
Models aimed at deep understanding of a conversation necessarily need to be more complex than the simple models aimed at, say, spam classiﬁcation.

Token 17932:
We start with grammatical models of the phrasestructure of sentences, add semantics to the model, and then apply it to machine translationand speech recognition.

Token 17933:
23.1 P HRASE STRUCTURE GRAMMARS Then-gram language models of Chapter 22 were based on sequences of words.

Token 17934:
The big issue for these models is data sparsity —with a vocabulary of, say, 105words, there are 1015 trigram probabilities to estimate, and so a corpus of even a trillion words will not be able to supply reliable estimates for all of them.

Token 17935:
We can address the problem of sparsity throughgeneralization.

Token 17936:
From the fact that “black dog” is more frequent than “dog black” and similarobservations, we can form the generalization that adjectives tend to come before nouns inEnglish (whereas they tend to follow nouns in French: “chien noir” is more frequent).

Token 17937:
Ofcourse there are always exceptions; “galore” is an adjective that follows the noun it modiﬁes.Despite the exceptions, the notion of a lexical category (also known as a part of speech )s u c h LEXICAL CATEGORY asnoun oradjective is a useful generalization—useful in its own right, but more so when we string together lexical categories to form syntactic categories such as noun phrase orverbSYNTACTIC CATEGORIES phrase , and combine these syntactic categories into trees representing the phrase structure PHRASE STRUCTURE of sentences: nested phrases, each marked with a category.

Token 17938:
888

Token 17939:
Section 23.1.

Token 17940:
Phrase Structure Grammars 889 GENERATIVE CAPACITY Grammatical formalisms can be classiﬁed by their generative capacity : the set of languages they can represent.

Token 17941:
Chomsky (1957) describes four classes of grammat-ical formalisms that differ only in the form of the rewrite rules.

Token 17942:
The classes canbe arranged in a hierarchy, where each class can be used to describe all the lan-guages that can be described by a less powerful class, as well as some additionallanguages.

Token 17943:
Here we list the hierarchy, most powerful class ﬁrst: Recursively enumerable grammars use unrestricted rules: both sides of the rewrite rules can have any number of terminal and nonterminal symbols, as in theruleABC→DE .

Token 17944:
These grammars are equivalent to Turing machines in their expressive power.

Token 17945:
Context-sensitive grammars are restricted only in that the right-hand side must contain at least as many symbols as the left-hand side.

Token 17946:
The name “context-sensitive” comes from the fact that a rule such as AXB→AY B says that anXcan be rewritten as a Yin the context of a preceding Aand a following B. Context-sensitive grammars can represent languages such as a nbncn(a sequence ofncopies of afollowed by the same number of bsa n dt h e n cs).

Token 17947:
Incontext-free grammars (orCFG s), the left-hand side consists of a sin- gle nonterminal symbol.

Token 17948:
Thus, each rule licenses rewriting the nonterminal asthe right-hand side in anycontext.

Token 17949:
CFGs are popular for natural-language and programming-language grammars, although it is now widely accepted that at least some natural languages have constructions that are not context-free (Pullum, 1991).

Token 17950:
Context-free grammars can represent a nbn, but not anbncn. Regular grammars are the most restricted class.

Token 17951:
Every rule has a single non- terminal on the left-hand side and a terminal symbol optionally followed by a non-terminal on the right-hand side.

Token 17952:
Regular grammars are equivalent in power to ﬁnite-state machines.

Token 17953:
They are poorly suited for programming languages, because theycannot represent constructs such as balanced opening and closing parentheses (avariation of the a nbnlanguage).

Token 17954:
The closest they can come is representing a∗b∗,a sequence of any number of as followed by any number of bs.

Token 17955:
The grammars higher up in the hierarchy have more expressive power, but the algorithms for dealing with them are less efﬁcient.

Token 17956:
Up to the 1980s, linguists focused on context-free and context-sensitive languages.

Token 17957:
Since then, there has been renewed interest in regular grammars, brought about by the need to process andlearn from gigabytes or terabytes of online text very quickly, even at the cost ofa less complete analysis.

Token 17958:
As Fernando Pereira put it, “The older I get, the furtherdown the Chomsky hierarchy I go.” To see what he means, compare Pereira andWarren (1980) with Mohri, Pereira, and Riley (2002) (and note that these threeauthors all now work on large text corpora at Google).

Token 17959:
890 Chapter 23.

Token 17960:
Natural Language for Communication There have been many competing language models based on the idea of phrase struc- ture; we will describe a popular model called the probabilistic context-free grammar ,o rPROBABILISTIC CONTEXT-FREE GRAMMARPCFG.1Agrammar is a collection of rules that deﬁnes a language as a set of allowable GRAMMAR LANGUAGE strings of words.

Token 17961:
“Context-free” is described in the sidebar on page 889, and “probabilistic” means that the grammar assigns a probability to every string.

Token 17962:
Here is a PCFG rule: VP→Verb[0.70] |VP NP [0.30]. HereVP(verb phrase )a n dNP(noun phrase )a r e non-terminal symbols .

Token 17963:
The grammarNON-TERMINAL SYMBOLS also refers to actual words, which are called terminal symbols .

Token 17964:
This rule is saying that with TERMINAL SYMBOL probability 0.70 a verb phrase consists solely of a verb, and with probability 0.30 it is a VP followed by an NP.

Token 17965:
Appendix B describes non-probabilistic context-free grammars.

Token 17966:
We now deﬁne a grammar for a tiny fragment of English that is suitable for communi- cation between agents exploring the wumpus world.

Token 17967:
We call this language E0. Later sections improve onE0to make it slightly closer to real English.

Token 17968:
We are unlikely ever to devise a complete grammar for English, if only because no two persons would agree entirely on what constitutes valid English.

Token 17969:
23.1.1 The lexicon of E0 First we deﬁne the lexicon , or list of allowable words.

Token 17970:
The words are grouped into the lexical LEXICON categories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs to denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:articles (such as the), prepositions ( in), and conjunctions ( and).

Token 17971:
Figure 23.1 shows a small lexicon for the language E 0. Each of the categories ends in ...to indicate that there are other words in the category.

Token 17972:
For nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list allthe words.

Token 17973:
Not only are there tens of thousands of members in each class, but new ones–like iPod orbiodiesel —are being added constantly.

Token 17974:
These ﬁve categories are called open classes .

Token 17975:
For the categories of pronoun, relative pronoun, article, preposition, and conjunction OPEN CLASS we could have listed all the words with a little more work.

Token 17976:
These are called closed classes ; CLOSED CLASS they have a small number of words (a dozen or so).

Token 17977:
Closed classes change over the course of centuries, not months.

Token 17978:
For example, “thee” and “thou” were commonly used pronouns in the 17th century, were on the decline in the 19th, and are seen today only in poetry and someregional dialects.

Token 17979:
23.1.2 The Grammar of E0 The next step is to combine the words into phrases.

Token 17980:
Figure 23.2 shows a grammar for E0, with rules for each of the six syntactic categories and an example for each rewrite rule.2 Figure 23.3 shows a parse tree for the sentence “Every wumpus smells.” The parse tree PARSE TREE 1PCFGs are also known as stochastic context-free grammars, or SCFGs.

Token 17981:
2A relative clause follows and modiﬁes a noun phrase. It consists of a relative pronoun (such as “who” or “that”) followed by a verb phrase.

Token 17982:
An example of a relative clause is that stinks in “The wumpus that stinks is in 2 2.” Another kind of relative clause has no relative pronoun, e.g., I know in “the man I know .”

Token 17983:
Section 23.1.

Token 17984:
Phrase Structure Grammars 891 Noun→stench [0.05]|breeze [0.10]|wumpus [0.15]|pits[0.05]|... Verb→is[0.10]|feel[0.10]|smells [0.10]|stinks [0.05]|... Adjective→right[0.10]|dead[0.05]|smelly [0.02]|breezy [0.02]... Adverb→here[0.05]|ahead [0.05]|nearby [0.02]|... Pronoun→me[0.10]|you[0.03]|I[0.10]|it[0.10]|... RelPro→that[0.40]|which [0.15]|who[0.20]|whom [0.02]∨... Name→John[0.01]|Mary [0.01]|Boston [0.01]|... Article→the[0.40]|a[0.30]|an[0.10]|every [0.05]|... Prep→to[0.20]|in[0.10]|on[0.05]|near[0.10]|... Conj→and[0.50]|or[0.10]|but[0.20]|yet[0.02]∨... Digit→0[0.20]|1[0.20]|2[0.20]|3[0.20]|4[0.20]|...

Token 17985:
Figure 23.1 The lexicon for E0.RelPro is short for relative pronoun, Prep for preposition, andConj for conjunction.

Token 17986:
The sum of the probabilities for each category is 1.

Token 17987:
E0: S→NP VP [0.90]I + feel a breeze |SC o n jS [0.10]I feel a breeze + and + It stinks NP→Pronoun [0.30]I |Name [0.10]John |Noun [0.10]pits |Article Noun [0.25]the + wumpus |Article Adjs Noun [0.05]the + smelly dead + wumpus |Digit Digit [0.05]34 |NP PP [0.10]the wumpus + in 1 3 |NP RelClause [0.05]the wumpus + that is smelly VP→Verb [0.40]stinks |VP NP [0.35]feel + a breeze |VP Adjective [0.05]smells + dead |VP PP [0.10]is + in 1 3 |VP Adverb [0.10]go + ahead Adjs→Adjective [0.80]smelly |Adjective Adjs [0.20]smelly + dead PP→Prep NP [1.00]to + the east RelClause→RelPro VP [1.00]that + is smelly Figure 23.2 The grammar for E0, with example phrases for each rule.

Token 17988:
The syntactic cat- egories are sentence ( S), noun phrase ( NP), verb phrase ( VP), list of adjectives ( Adjs ), prepositional phrase ( PP), and relative clause ( RelClause ).

Token 17989:
892 Chapter 23.

Token 17990:
Natural Language for Communication Article Noun wumpusVerbNP VPS Every smells0.250.90 0.05 0.15 0.10 0.40 Figure 23.3 Parse tree for the sentence “Every wumpus smells” according to the grammar E0.

Token 17991:
Each interior node of the tree is labeled with its probability. The probability of the tree a saw h o l ei s 0.9×0.25×0.05×0.15×0.40×0.10=0 .0000675 .

Token 17992:
Since this tree is the only parse of the sentence, that number is also the probability of the sentence.

Token 17993:
The tree can also be written in linear form as [S[NP[Article every][Noun wumpus ]][VP[Verb smells ]]].

Token 17994:
gives a constructive proof that the string of words is indeed a sentence according to the rules ofE0.T h eE0grammar generates a wide range of English sentences such as the following: John is in the pitThe wumpus that stinks is in 2 2 Mary is in Boston and the wumpus is near 3 2 Unfortunately, the grammar overgenerates : that is, it generates sentences that are not gram- OVERGENERATION matical, such as “Me go Boston” and “I smell pits wumpus John.” It also undergenerates : UNDERGENERATION there are many sentences of English that it rejects, such as “I think the wumpus is smelly.” We will see how to learn a better grammar later; for now we concentrate on what we can dowith the grammar we have.

Token 17995:
23.2 S YNTACTIC ANALYSIS (PARSING ) Parsing is the process of analyzing a string of words to uncover its phrase structure, according PARSING to the rules of a grammar.

Token 17996:
Figure 23.4 shows that we can start with the Ssymbol and search top down for a tree that has the words as its leaves, or we can start with the words and searchbottom up for a tree that culminates in an S. Both top-down and bottom-up parsing can be inefﬁcient, however, because they can end up repeating effort in areas of the search space that lead to dead ends.

Token 17997:
Consider the following two sentences: Have the students in section 2 of Computer Science 101 take the exam.

Token 17998:
Have the students in section 2 of Computer Science 101 taken the exam?

Token 17999:
Even though they share the ﬁrst 10 words, these sentences have very different parses, because the ﬁrst is a command and the second is a question.

Token 18000:
A left-to-right parsing algorithm wouldhave to guess whether the ﬁrst word is part of a command or a question and will not be ableto tell if the guess is correct until at least the eleventh word, take ortaken.

Token 18001:
If the algorithm guesses wrong, it will have to backtrack all the way to the ﬁrst word and reanalyze the whole sentence under the other interpretation.

Token 18002:
Section 23.2.

Token 18003:
Syntactic Analysis (Parsing) 893 List of items Rule S NP VP S →NP VP NP VP Adjective VP →VP Adjective NP Verb Adjective VP →Verb NP Verb dead Adjective→dead NPis dead Verb→is Article Noun is dead NP→Article Noun Article wumpus is dead Noun→wumpus the wumpus is dead Article→the Figure 23.4 Trace of the process of ﬁnding a parse for the string “The wumpus is dead” as a sentence, according to the grammar E0.

Token 18004:
Viewed as a top-down parse, we start with the list of items being Sand, on each step, match an item Xwith a rule of the form ( X→...) and replace Xin the list of items with (.

Token 18005:
. . ). Viewed as a bottom-up parse, we start with the list of items being the words of the sentence, and, on each step, match a string of tokens (. .

Token 18006:
. ) in the list against a rule of the form ( X→...) a n dr e p l a c e( ...) w i t h X.

Token 18007:
To avoid this source of inefﬁciency we can use dynamic programming: every time we analyze a substring, store the results so we won’t have to reanalyze it later.

Token 18008:
For example, once we discover that “the students in section 2 of Computer Science 101” is an NP, we can record that result in a data structure known as a chart .

Token 18009:
Algorithms that do this are called chart CHART parsers .

Token 18010:
Because we are dealing with context-free grammars, any phrase that was found in the context of one branch of the search space can work just as well in any other branch of the search space.

Token 18011:
There are many types of chart parsers; we describe a bottom-up version called theCYK algorithm , after its inventors, John Cocke, Daniel Younger, and Tadeo Kasami.

Token 18012:
CYK ALGORITHM The CYK algorithm is shown in Figure 23.5.

Token 18013:
Note that it requires a grammar with all rules in one of two very speciﬁc formats: lexical rules of the form X→word , and syntactic rules of the form X→YZ .

Token 18014:
This grammar format, called Chomsky Normal Form ,m a yCHOMSKY NORMAL FORM seem restrictive, but it is not: any context-free grammar can be automatically transformed into Chomsky Normal Form.

Token 18015:
Exercise 23.8 leads you through the process.

Token 18016:
The CYK algorithm uses space of O(n2m)for the Ptable, where nis the number of words in the sentence, and mis the number of nonterminal symbols in the grammar, and takes timeO(n3m).

Token 18017:
( S i n c e mis constant for a particular grammar, this is commonly described as O(n3).)

Token 18018:
No algorithm can do better for general context-free grammars, although there are faster algorithms on more restricted grammars.

Token 18019:
In fact, it is quite a trick for the algorithm to complete in O(n3)time, given that it is possible for a sentence to have an exponential number of parse trees.

Token 18020:
Consider the sentence Fall leaves fall and spring leaves spring.

Token 18021:
It is ambiguous because each word (except “and”) can be either a noun or a verb, and “fall” and “spring” can be adjectives as well.

Token 18022:
(For example, one meaning of “Fall leaves fall” is

Token 18023:
894 Chapter 23.

Token 18024:
Natural Language for Communication function CYK-P ARSE (words ,grammar )returns P, a table of probabilities N←LENGTH (words ) M←the number of nonterminal symbols in grammar P←an array of size [ M,N,N], initially all 0 /*Insert lexical rules for each word */ fori=1toNdo for each rule of form ( X→words i[p])do P[X,i,1 ]←p /*Combine ﬁrst and second parts of right-hand sides of rules, from short to long */ forlength =2toNdo forstart =1toN−length +1do forlen1 =1toN−1do len2←length−len1 for each rule of the form ( X→YZ [p])do P[X,start ,length ]←MAX(P[X,start,length ], P[Y,start,len1]×P[Z,start+len1,len2]×p) return P Figure 23.5 The CYK algorithm for parsing.

Token 18025:
Given a sequence of words, it ﬁnds the most probable derivation for the whole seque nce and for each subsequence.

Token 18026:
It returns the whole table, P, in which an entry P[X,start ,len] is the probability of the most probable Xof length lenstarting at position start .

Token 18027:
If there is no Xof that size at that location, the probability is 0. equivalent to “Autumn abandons autumn.)

Token 18028:
With E0the sentence has four parses: [S[S[NPFall leaves] fall] and [ S[NPspring leaves] spring] [S[S[NPFall leaves] fall] and [ Sspring [ VPleaves spring]] [S[SFall [VPleaves fall]] and [ S[NPspring leaves] spring] [S[SFall [VPleaves fall]] and [ Sspring [ VPleaves spring]] .

Token 18029:
If we had ctwo-ways-ambiguous conjoined subsentences, we would have 2cways of choos- ing parses for the subsentences.3How does the CYK algorithm process these 2cparse trees inO(c3)time?

Token 18030:
The answer is that it doesn’t examine all the parse trees; all it has to do is compute the probability of the most probable tree.

Token 18031:
The subtrees are all represented in the P table, and with a little work we could enumerate them all (in exponential time), but the beauty of the CYK algorithm is that we don’t have to enumerate them unless we want to.

Token 18032:
In practice we are usually not interested in all parses; just the best one or best few.

Token 18033:
Think of the CYK algorithm as deﬁning the complete state space deﬁned by the “apply grammar rule” operator.

Token 18034:
It is possible to search just part of this space using A∗search.

Token 18035:
Each state in this space is a list of items (words or categories), as shown in the bottom-up parse table(Figure 23.4).

Token 18036:
The start state is a list of words, and a goal state is the single item S.T h e 3T h e r ea l s ow o u l db e O(c!

Token 18037:
)ambiguity in the way the components conjoin—for example, ( Xand (YandZ)) versus (( XandY)a n dZ).

Token 18038:
But that is another story, one told well by Church and Patil (1982).

Token 18039:
Section 23.2.

Token 18040:
Syntactic Analysis (Parsing) 895 [ [S [NP-SBJ-2 Her eyes] [VP were [VP glazed [NP *-2][SBAR-ADV as if [S [NP-SBJ she] [VP did n’t [VP [VP hear [NP *-1]] or [VP [ADVP even] see [NP *-1]] [NP-1 him]]]]]]]] .]

Token 18041:
Figure 23.6 Annotated tree for the sentence “Her eyes were glazed as if she didn’t hear or even see him.” from the Penn Treebank.

Token 18042:
Note that in this grammar there is a distinction between an object noun phrase ( NP) and a subject noun phrase ( NP-SBJ ).

Token 18043:
Note also a gram- matical phenomenon we have not covered yet: the movement of a phrase from one part ofthe tree to another.

Token 18044:
This tree analyzes the phrase “hear or even see him” as consisting of two constituent VPs, [VP hear [NP *-1]] and [VP [ADVP even] see [NP *-1]], both of which have a missing object, denoted *-1, which refers to the NPlabeled elsewhere in the tree as [NP-1 him].

Token 18045:
cost of a state is the inverse of its probability as deﬁned by the rules applied so far, and there are various heuristics to estimate the remaining distance to the goal; the best heuristics comefrom machine learning applied to a corpus of sentences.

Token 18046:
With the A ∗algorithm we don’t have to search the entire state space, and we are guaranteed that the ﬁrst parse found will be themost probable.

Token 18047:
23.2.1 Learning probabilities for PCFGs A PCFG has many rules, with a probability for each rule.

Token 18048:
This suggests that learning the grammar from data might be better than a knowledge engineering approach.

Token 18049:
Learning is eas- iest if we are given a corpus of correctly parsed sentences, commonly called a treebank .T h e TREEBANK Penn Treebank (Marcus et al.

Token 18050:
, 1993) is the best known; it consists of 3 million words which have been annotated with part of speech and parse-tree structure, using human labor assistedby some automated tools.

Token 18051:
Figure 23.6 shows an annotated tree from the Penn Treebank. Given a corpus of trees, we can create a PCFG just by counting (and smoothing).

Token 18052:
In the example above, there are two nodes of the form [S[NP...][VP...]]. We would count these, and all the other subtrees with root Sin the corpus.

Token 18053:
If there are 100,000 Snodes of which 60,000 are of this form, then we create the rule: S→NP VP [0.60].

Token 18054:
What if a treebank is not available, but we have a corpus of raw unlabeled sentences?

Token 18055:
It is still possible to learn a grammar from such a corpus, but it is more difﬁcult.

Token 18056:
First of all, we actually have two problems: learning the structure of the grammar rules and learning the

Token 18057:
896 Chapter 23. Natural Language for Communication probabilities associated with each rule. (We have the same distinction in learning Bayes nets.)

Token 18058:
We’ll assume that we’re given the lexical and syntactic category names.

Token 18059:
(If not, we can justassume categories X 1,...X nand use cross-validation to pick the best value of n.) We can then assume that the grammar includes every possible ( X→YZ )o r(X→word )r u l e , although many of these rules will have probability 0 or close to 0.

Token 18060:
We can then use an expectation–maximization (EM) approach, just as we did in learning HMMs.

Token 18061:
The parameters we are trying to learn are the rule probabilities; we start them off atrandom or uniform values.

Token 18062:
The hidden variables are the parse trees: we don’t know whethera string of words w i...w jis or is not generated by a rule ( X→...).

Token 18063:
The E step estimates the probability that each subsequence is generated by each rule. The M step then estimatesthe probability of each rule.

Token 18064:
The whole computation can be done in a dynamic-programmingfashion with an algorithm called the inside–outside algorithm in analogy to the forward– INSIDE–OUTSIDE ALGORITHM backward algorithm for HMMs.

Token 18065:
The inside–outside algorithm seems magical in that it induces a grammar from unparsed text. But it has several drawbacks.

Token 18066:
First, the parses that are assigned by the induced grammarsare often difﬁcult to understand and unsatisfying to linguists.

Token 18067:
This makes it hard to combinehandcrafted knowledge with automated induction.

Token 18068:
Second, it is slow: O(n 3m3),w h e r e nis the number of words in a sentence and mis the number of grammar categories.

Token 18069:
Third, the space of probability assignments is very large, and empirically it seems that getting stuck inlocal maxima is a severe problem.

Token 18070:
Alternatives such as simulated annealing can get closer tothe global maximum, at a cost of even more computation.

Token 18071:
Lari and Young (1990) concludethat inside–outside is “computationally intractable for realistic problems.” However, progress can be made if we are willing to step outside the bounds of learning solely from unparsed text.

Token 18072:
One approach is to learn from prototypes : to seed the process with a dozen or two rules, similar to the rules in E 1.

Token 18073:
From there, more complex rules can be learned more easily, and the resulting grammar parses English with an overall recall and precision forsentences of about 80% (Haghighi and Klein, 2006).

Token 18074:
Another approach is to use treebanks,but in addition to learning PCFG rules directly from the bracketings, also learning distinctions that are not in the treebank.

Token 18075:
For example, not that the tree in Figure 23.6 makes the distinction between NPandNP−SBJ .

Token 18076:
The latter is used for the pronoun “she,” the former for the pronoun “her.” We will explore this issue in Section 23.6; for now let us just say that thereare many ways in which it would be useful to split a category like NP—grammar induction systems that use treebanks but automatically split categories do better than those that stickwith the original category set (Petrov and Klein, 2007c).

Token 18077:
The error rates for automaticallylearned grammars are still about 50% higher than for hand-constructed grammar, but the gapis decreasing.

Token 18078:
23.2.2 Comparing context-free and Markov models The problem with PCFGs is that they are context-free.

Token 18079:
That means that the difference betweenP(“eat a banana”) and P(“eat a bandanna”) depends only on P(Noun→“banana”) versus P(Noun→“bandanna”) and not on the relation between “eat” and the respective objects.

Token 18080:
A Markov model of order two or more, given a sufﬁciently large corpus, willknow that “eat

Token 18081:
Section 23.3. Augmented Grammars and Semantic Interpretation 897 a banana” is more probable.

Token 18082:
We can combine a PCFG and Markov model to get the best of both.

Token 18083:
The simplest approach is to estimate the probability of a sentence with the geometricmean of the probabilities computed by both models.

Token 18084:
Then we would know that “eat a banana”is probable from both the grammatical and lexical point of view.

Token 18085:
But it still wouldn’t pick upthe relation between “eat” and “banana” in “eat a slightly aging but still palatable banana” because here the relation is more than two words away.

Token 18086:
Increasing the order of the Markov model won’t get at the relation precisely; to do that we can use a lexicalized PCFG, as described in the next section.

Token 18087:
Another problem with PCFGs is that they tend to have too strong a preference for shorter sentences.

Token 18088:
In a corpus such as the Wall Street Journal , the average length of a sentence is about 25 words.

Token 18089:
But a PCFG will usually assign fairly high probability to many shortsentences, such as “He slept,” whereas in the Journal we’re more likely to see something like “It has been reported by a reliable source that the allegation that he slept is credible.” It seemsthat the phrases in the Journal really are not context-free; instead the writers have an idea of the expected sentence length and use that length as a soft global constraint on their sentences.This is hard to reﬂect in a PCFG.

Token 18090:
23.3 A UGMENTED GRAMMARS AND SEMANTIC INTERPRETATION In this section we see how to extend context-free grammars—to say that, for example, noteveryNPis independent of context, but rather, certain NPs are more likely to appear in one context, and others in another context.

Token 18091:
23.3.1 Lexicalized PCFGs To get at the relationship between the verb “eat” and the nouns “banana” versus “bandanna,”we can use a lexicalized PCFG , in which the probabilities for a rule depend on the relation- LEXICALIZED PCFG ship between words in the parse tree, not just on the adjacency of words in a sentence.

Token 18092:
Of course, we can’t have the probability depend on every word in the tree, because we won’t have enough training data to estimate all those probabilities.

Token 18093:
It is useful to introduce the no- tion of the head of a phrase—the most important word.

Token 18094:
Thus, “eat” is the head of the VP HEAD “eat a banana” and “banana” is the head of the NP“a banana.” We use the notation VP(v) to denote a phrase with category VPwhose head word is v. We say that the category VP isaugmented with the head variable v.H e r ei sa n augmented grammar that describes theAUGMENTED GRAMMAR verb–object relation: VP(v)→Verb(v)NP(n)[ P1(v,n)] VP(v)→Verb(v)[ P2(v)] NP(n)→Article (a)Adjs(j)Noun(n)[P3(n,a)] Noun(banana )→banana [pn] ... ...

Token 18095:
Here the probability P1(v,n)depends on the head words vandn.

Token 18096:
We would set this proba- bility to be relatively high when vis “eat” and nis “banana,” and low when nis “bandanna.”

Token 18097:
898 Chapter 23.

Token 18098:
Natural Language for Communication Note that since we are considering only heads, the distinction between “eat a banana” and “eat a rancid banana” will not be caught by these probabilities.

Token 18099:
Another issue with this ap-proach is that, in a vocabulary with, say, 20,000 nouns and 5,000 verbs, P 1needs 100 million probability estimates.

Token 18100:
Only a few percent of these can come from a corpus; the rest will haveto come from smoothing (see Section 22.1.2).

Token 18101:
For example, we can estimate P 1(v,n)for a (v,n)pair that we have not seen often (or at all) by backing off to a model that depends only on v. These objectless probabilities are still very useful; they can capture the distinction between a transitive verb like “eat”—which will have a high value for P1and a low value for P2—and an intransitive verb like “sleep,” which will have the reverse.

Token 18102:
It is quite feasible to learn these probabilities from a treebank.

Token 18103:
23.3.2 Formal deﬁnition of augmented grammar rules Augmented rules are complicated, so we will give them a formal deﬁnition by showing howan augmented rule can be translated into a logical sentence.

Token 18104:
The sentence will have the formof a deﬁnite clause (see page 256), so the result is called a deﬁnite clause grammar , or DCG.

Token 18105:
DEFINITE CLAUSE GRAMMAR We’ll use as an example a version of a rule from the lexicalized grammar for NPwith one new piece of notation: NP(n)→Article (a)Adjs(j)Noun(n){Compatible (j, n)}.

Token 18106:
The new aspect here is the notation {constraint}to denote a logical constraint on some of the variables; the rule only holds when the constraint is true.

Token 18107:
Here the predicate Compatible (j, n) is meant to test whether adjective jand noun nare compatible; it would be deﬁned by a series of assertions such as Compatible (black,dog).

Token 18108:
We can convert this grammar rule into a def- inite clause by (1) reversing the order of right- and left-hand sides, (2) making a conjunctionof all the constituents and constraints, (3) adding a variable s ito the list of arguments for each constituent to represent the sequence of words spanned by the constituent, (4) adding a term for the concatenation of words, Append (s1,...), to the list of arguments for the root of the tree.

Token 18109:
That gives us Article (a,s1)∧Adjs(j, s2)∧Noun(n,s3)∧Compatible (j, n) ⇒NP(n,Append (s1,s2,s3)).

Token 18110:
This deﬁnite clause says that if the predicate Article is true of a head word aand a string s1, andAdjs is similarly true of a head word jand a string s2,a n dNoun is true of a head word nand a string s3,a n di f jandnare compatible, then the predicate NP is true of the head wordnand the result of appending strings s1,s2,a n ds3.

Token 18111:
The DCG translation left out the probabilities, but we could put them back in: just aug- ment each constituent with one more variable representing the probability of the constituent,and augment the root with a variable that is the product of the constituent probabilities timesthe rule probability.

Token 18112:
The translation from grammar rule to deﬁnite clause allows us to talk about parsing as logical inference.

Token 18113:
This makes it possible to reason about languages and strings in manydifferent ways.

Token 18114:
For example, it means we can do bottom-up parsing using forward chaining ortop-down parsing using backward chaining.

Token 18115:
In fact, parsing natural language with DCGs was

Token 18116:
Section 23.3.

Token 18117:
Augmented Grammars and Semantic Interpretation 899 one of the ﬁrst applications of (and motivations for) the Prolog logic programming language.

Token 18118:
It is sometimes possible to run the process backward and do language generation as well asLANGUAGE GENERATION parsing.

Token 18119:
For example, skipping ahead to Figure 23.10 (page 903), a logic program could be given the semantic form Loves(John,Mary)and apply the deﬁnite-clause rules to deduce S(Loves (John,Mary),[John,loves,Mary]).

Token 18120:
This works for toy examples, but serious language-generation systems need more control over the process than is afforded by the DCG rules alone.

Token 18121:
E1: S→NPSVP|... NPS→Pronoun S|Name|Noun|... NPO→Pronoun O|Name|Noun|... VP→VP NP O|... PP→Prep NP O Pronoun S→I|you|he|she|it|... Pronoun O→me|you|him|her|it|... ... E2: S(head)→NP(Sbj,pn,h)VP(pn,head)|... NP(c,pn,head)→Pronoun (c,pn,head)|Noun(c,pn,head)|... VP(pn,head)→VP(pn,head)NP(Obj,p,h)|... PP(head)→Prep(head)NP(Obj,pn,h) Pronoun (Sbj,1S,I)→I Pronoun (Sbj,1P,we)→we Pronoun (Obj,1S,me)→me Pronoun (Obj,3P,them)→them ...

Token 18122:
Figure 23.7 Top: part of a grammar for the language E1, which handles subjective and objective cases in noun phrases and thus does not overgenerate quite as badly as E0.T h e portions that are identical to E0have been omitted.

Token 18123:
Bottom: part of an augmented grammar forE2, with three augmentations: case agreement, subject–verb agreement, and head word.

Token 18124:
Sbj, Obj, 1S, 1P and3Pare constants, and lowercase names are variables.

Token 18125:
23.3.3 Case agreement and subject–verb agreement We saw in Section 23.1 that the simple grammar for E0overgenerates, producing nonsen- tences such as “Me smell a stench.” To avoid this problem, our grammar would have to knowthat “me” is not a valid NPwhen it is the subject of a sentence.

Token 18126:
Linguists say that the pronoun “I” is in the subjective case, and “me” is in the objective case.

Token 18127:
4We can account for this by 4The subjective case is also sometimes called the nominative case and the objective case is sometimes called the accusative case.

Token 18128:
Many languages also have a dat ive case for words in the indirect obj ect position.

Token 18129:
900 Chapter 23.

Token 18130:
Natural Language for Communication splitting NPinto two categories, NPSandNPO, to stand for noun phrases in the subjective and objective case, respectively.

Token 18131:
We would also need to split the category Pronoun into the two categories Pronoun S(which includes “I”) and Pronoun O(which includes “me”).

Token 18132:
The top part of Figure 23.7 shows the grammar for case agreement ; we call the resulting language CASE AGREEMENT E1.

Token 18133:
Notice that all the NPrules must be duplicated, once for NPSand once for NPO. Unfortunately, E1still overgenerates.

Token 18134:
English requires subject–verb agreement forSUBJECT–VERB AGREEMENT person and number of the subject and main verb of a sentence.

Token 18135:
For example, if “I” is the subject, then “I smell” is grammatical, but “I smells” is not. If “it” is the subject, we get thereverse.

Token 18136:
In English, the agreement distinctions are minimal: most verbs have one form forthird-person singular subjects (he, she, or it), and a second form for all other combinationsof person and number.

Token 18137:
There is one exception: the verb “to be” has three forms, “I am / youare / he is.” So one distinction (case) splits NP two ways, another distinction (person and number) splits NP three ways, and as we uncover other distinctions we would end up with an exponential number of subscripted NP forms if we took the approach of E 1.

Token 18138:
Augmentations are a better approach: they can represent an exponential number of forms as a single rule.

Token 18139:
In the bottom of Figure 23.7 we see (part of) an augmented grammar for the language E2, which handles case agreement, subject–verb agreement, and head words.

Token 18140:
We have just oneNP category, but NP(c,pn,head)has three augmentations: cis a parameter for case, pnis a parameter for person and number, and head is a parameter for the head word of the phrase.

Token 18141:
The other categories also are augmented with heads and other arguments. Let’s consider one rule in detail: S(head)→NP(Sbj,pn,h)VP(pn,head).

Token 18142:
This rule is easiest to understand right-to-left: when an NPand a VPare conjoined they form anS, but only if the NPhas the subjective ( Sbj) case and the person and number ( pn)o ft h e NPand VPare identical.

Token 18143:
If that holds, then we have an Swhose head is the same as the head of the VP.

Token 18144:
Note the head of the NP, denoted by the dummy variable h, is not part of the augmentation of the S. The lexical rules for E2ﬁll in the values of the parameters and are also best read right-to-left.

Token 18145:
For example, the rule Pronoun (Sbj,1S,I)→I says that “I” can be interpreted as a Pronoun in the subjective case, ﬁrst-person singular, with head “I.” For simplicity we have omitted the probabilities for these rules, but augmentationdoes work with probabilities.

Token 18146:
Augmentation can also work with automated learning mecha-nisms.

Token 18147:
Petrov and Klein (2007c) show how a learning algorithm can automatically split the NPcategory into NP SandNPO.

Token 18148:
23.3.4 Semantic interpretation To show how to add semantics to a grammar, we start with an example that is simpler than English: the semantics of arithmetic expressions.

Token 18149:
Figure 23.8 shows a grammar for arithmetic expressions, where each rule is augmented with a variable indicating the semantic interpreta- tion of the phrase.

Token 18150:
The semantics of a digit such as “3” is the digit itself.

Token 18151:
The semantics of anexpression such as “3 + 4” is the operator “+” applied to the semantics of the phrase “3” and

Token 18152:
Section 23.3.

Token 18153:
Augmented Grammars and Semantic Interpretation 901 Exp(x)→Exp(x1)Operator (op)Exp(x2){x=Apply (op,x1,x2)} Exp(x)→(Exp(x)) Exp(x)→Number (x) Number (x)→Digit(x) Number (x)→Number (x1)Digit(x2){x=1 0×x1+x2} Digit(x)→x{0≤x≤9} Operator (x)→x{x∈{+,−,÷,×}} Figure 23.8 A grammar for arithmetic expressions, augmented with semantics.

Token 18154:
Each vari- ablexirepresents the semantics of a constituent.

Token 18155:
Note the use of the {test}notation to deﬁne logical predicates that must be satisﬁed, but that are not constituents.

Token 18156:
Operator (÷) 3( ) 42 +Number (2) Digit (2)Number (4) Digit (4) Operator (+) Digit (3)Number (3)Exp(5) Exp(2) Exp(2) Exp(4) Exp(2) Exp(3) ÷ Figure 23.9 Parse tree with semantic interpretations for the string “ 3+( 4÷2)”.

Token 18157:
the phrase “4.” The rules obey the principle of compositional semantics —the semantics ofCOMPOSITIONAL SEMANTICS a phrase is a function of the semantics of the subphrases.

Token 18158:
Figure 23.9 shows the parse tree for 3+( 4÷2)according to this grammar.

Token 18159:
The root of the parse tree is Exp(5), an expression whose semantic interpretation is 5.

Token 18160:
Now let’s move on to the semantics of English, or at least of E0.

Token 18161:
We start by determin- ing what semantic representations we want to associate with what phrases.

Token 18162:
We use the simpleexample sentence “John loves Mary.” The NP“John” should have as its semantic interpreta- tion the logical term John , and the sentence as a whole should have as its interpretation the logical sentence Loves(John,Mary).

Token 18163:
That much seems clear.

Token 18164:
The complicated part is the VP“loves Mary.” The semantic interpretation of this phrase is neither a logical term nor a complete logical sentence.

Token 18165:
Intuitively, “loves Mary” is a description that might or might not

Token 18166:
902 Chapter 23. Natural Language for Communication apply to a particular person. (In this case, it applies to John.)

Token 18167:
This means that “loves Mary” is apredicate that, when combined with a term that represents a person (the person doing the loving), yields a complete logical sentence.

Token 18168:
Using the λ-notation (see page 294), we can represent “loves Mary” as the predicate λxLoves(x,Mary).

Token 18169:
Now we need a rule that says “an NPwith semantics objfollowed by a VPwith semantics pred yields a sentence whose semantics is the result of applying pred toobj:” S(pred(obj))→NP(obj)VP(pred).

Token 18170:
The rule tells us that the semantic interpretation of “John loves Mary” is (λxLoves(x,Mary))(John), which is equivalent to Loves(John,Mary).

Token 18171:
The rest of the semantics follows in a straightforward way from the choices we have made so far.

Token 18172:
Because VPs are represented as predicates, it is a good idea to be consistent and represent verbs as predicates as well.

Token 18173:
The verb “loves” is represented as λy λx Loves(x,y), the predicate that, when given the argument Mary , returns the predicate λxLoves(x,Mary).

Token 18174:
We end up with the grammar shown in Figure 23.10 and the parse tree shown in Figure 23.11.We could just as easily have added semantics to E 2; we chose to work with E0so that the reader can focus on one type of augmentation at a time.

Token 18175:
Adding semantic augmentations to a grammar by hand is laborious and error prone.

Token 18176:
Therefore, there have been several projects to learn semantic augmentations from examples.

Token 18177:
CHILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that learns a grammar and a specialized parser for that grammar from examples.

Token 18178:
The target domain is natural language database queries.

Token 18179:
The training examples consist of pairs of word strings and corresponding semantic forms—for example; What is the capital of the state with the largest population?

Token 18180:
Answer (c,Capital (s,c)∧Largest (p,State(s)∧Population (s,p))) CHILL ’s task is to learn a predicate Parse (words ,semantics )that is consistent with the ex- amples and, hopefully, generalizes well to other examples.

Token 18181:
Applying ILP directly to learnthis predicate results in poor performance: the induced parser has only about 20% accuracy.Fortunately, ILP learners can improve by adding knowledge.

Token 18182:
In this case, most of the Parse predicate was deﬁned as a logic program, and C HILL ’s task was reduced to inducing the control rules that guide the parser to select one parse over another.

Token 18183:
With this additional back-ground knowledge, C HILL can learn to achieve 70% to 85% accuracy on various database query tasks.

Token 18184:
23.3.5 Complications The grammar of real English is endlessly complex. We will brieﬂy mention some examples.

Token 18185:
Time and tense : Suppose we want to represent the difference between “John loves TIME AND TENSE Mary” and “John loved Mary.” English uses verb tenses (past, present, and future) to indicate

Token 18186:
Section 23.3.

Token 18187:
Augmented Grammars and Semantic Interpretation 903 S(pred(obj))→NP(obj)VP(pred) VP(pred(obj))→Verb(pred)NP(obj) NP(obj)→Name (obj) Name (John)→John Name (Mary)→Mary Verb(λy λx Loves(x,y))→loves Figure 23.10 A grammar that can derive a parse tree and semantic interpretation for “John loves Mary” (and three other sentences).

Token 18188:
Each category is augmented with a single argument representing the semantics.

Token 18189:
John loves MaryName (John ) Name (Mary )NP(Mary )NP (John )S(Loves (John,Mary )) Verb (λy λx Loves (x,y))VP(λx Loves (x,Mary )) Figure 23.11 A parse tree with semantic interpretations for the string “John loves Mary”.

Token 18190:
the relative time of an event. One good choice to represent the time of events is the event calculus notation of Section 12.3.

Token 18191:
In event calculus we have John loves mary: E1∈Loves(John,Mary)∧During (Now,Extent (E1)) John loved mary: E2∈Loves(John,Mary)∧After(Now,Extent (E2)).

Token 18192:
This suggests that our two lexical rules for the words “loves” and “loved” should be these: Verb(λy λx e∈Loves(x,y)∧During (Now,e))→loves Verb(λy λx e∈Loves(x,y)∧After(Now,e))→loved .

Token 18193:
Other than this change, everything else about the grammar remains the same, which is en- couraging news; it suggests we are on the right track if we can so easily add a complicationlike the tense of verbs (although we have just scratched the surface of a complete grammarfor time and tense).

Token 18194:
It is also encouraging that the distinction between processes and discreteevents that we made in our discussion of knowledge representation in Section 12.3.1 is actu-ally reﬂected in language use.

Token 18195:
We can say “John slept a lot last night,” where Sleeping is a process category, but it is odd to say “John found a unicorn a lot last night,” where Finding is a discrete event category.

Token 18196:
A grammar would reﬂect that fact by having a low probability for adding the adverbial phrase “a lot” to discrete events.

Token 18197:
Quantiﬁcation : Consider the sentence “Every agent feels a breeze.” The sentence has QUANTIFICATION only one syntactic parse under E0, but it is actually semantically ambiguous; the preferred

Token 18198:
904 Chapter 23.

Token 18199:
Natural Language for Communication meaning is “For every agent there exists a breeze that the agent feels,” but an acceptable alternative meaning is “There exists a breeze that every agent feels.”5The two interpretations can be represented as ∀aa∈Agents⇒ ∃bb∈Breezes∧∃ee∈Feel(a,b)∧During (Now,e); ∃bb∈Breezes∀aa∈Agents⇒ ∃ee∈Feel(a,b)∧During (Now,e).

Token 18200:
The standard approach to quantiﬁcation is for the grammar to deﬁne not an actual logical semantic sentence, but rather a quasi-logical form that is then turned into a logical sentenceQUASI-LOGICAL FORM by algorithms outside of the parsing process.

Token 18201:
Those algorithms can have preference rules for preferring one quantiﬁer scope over another—preferences that need not be reﬂected directlyin the grammar.

Token 18202:
Pragmatics : We have shown how an agent can perceive a string of words and use a PRAGMATICS grammar to derive a set of possible semantic interpretations.

Token 18203:
Now we address the problem of completing the interpretation by adding context-dependent information about the currentsituation.

Token 18204:
The most obvious need for pragmatic information is in resolving the meaning ofindexicals , which are phrases that refer directly to the current situation.

Token 18205:
For example, in the INDEXICAL sentence “I am in Boston today,” both “I” and “today” are indexicals.

Token 18206:
The word “I” would be represented by the ﬂuent Speaker , and it would be up to the hearer to resolve the meaning of the ﬂuent—that is not considered part of the grammar but rather an issue of pragmatics; ofusing the context of the current situation to interpret ﬂuents.

Token 18207:
Another part of pragmatics is interpreting the speaker’s intent.

Token 18208:
The speaker’s action is considered a speech act , and it is up to the hearer to decipher what type of action it is—a SPEECH ACT question, a statement, a promise, a warning, a command, and so on.

Token 18209:
A command such as “go to 2 2” implicitly refers to the hearer. So far, our grammar for Scovers only declarative sentences.

Token 18210:
We can easily extend it to cover commands. A command can be formed from aVP, where the subject is implicitly the hearer.

Token 18211:
We need to distinguish commands from statements, so we alter the rules for Sto include the type of speech act: S(Statement (Speaker ,pred(obj)))→NP(obj)VP(pred) S(Command (Speaker ,pred(Hearer )))→VP(pred).

Token 18212:
Long-distance dependencies : Questions introduce a new grammatical complexity.

Token 18213:
InLONG-DISTANCE DEPENDENCIES “Who did the agent tell you to give the gold to?” the ﬁnal word “to” should be parsed as [PPto ], where the “ ” denotes a gap or trace where an NPis missing; the missing NP TRACE is licensed by the ﬁrst word of the sentence, “who.” A complex system of augmentations is used to make sure that the missing NPs match up with the licensing words in just the right way, and prohibit gaps in the wrong places.

Token 18214:
For example, you can’t have a gap in one branchof anNPconjunction: “What did he play [ NPDungeons and ]?” is ungrammatical.

Token 18215:
But you can have the same gap in both branches of a VPconjunction: “What did you [ VP[VP smell ]a n d[VPshoot an arrow at ]]?” Ambiguity : In some cases, hearers are consciously aware of ambiguity in an utterance.

Token 18216:
AMBIGUITY Here are some examples taken from newspaper headlines: 5If this interpretation seems unlikely, consider “Every Protestant believes in a just God.”

Token 18217:
Section 23.3. Augmented Grammars and Semantic Interpretation 905 Squad helps dog bite victim.

Token 18218:
Police begin campaign to run down jaywalkers.Helicopter powered by human ﬂies.

Token 18219:
Once-sagging cloth diaper industry saved by full dumps.Portable toilet bombed; police have nothing to go on. Teacher strikes idle kids.

Token 18220:
Include your children when baking cookies.Hospitals are sued by 7 foot doctors.Milk drinkers are turning to powder.Safety experts say school bus passengers should be belted.

Token 18221:
But most of the time the language we hear seems unambiguous.

Token 18222:
Thus, when researchers ﬁrst began to use computers to analyze language in the 1960s, they were quite surprised to learnthat almost every utterance is highly ambiguous, even though the alternative interpretations might not be apparent to a native speaker.

Token 18223:
A system with a large grammar and lexicon might ﬁnd thousands of interpretations for a perfectly ordinary sentence.

Token 18224:
Lexical ambiguity ,i n LEXICAL AMBIGUITY which a word has more than one meaning, is quite common; “back” can be an adverb (go back), an adjective (back door), a noun (the back of the room) or a verb (back up your ﬁles).“Jack” can be a name, a noun (a playing card, a six-pointed metal game piece, a nautical ﬂag,a ﬁsh, a socket, or a device for raising heavy objects), or a verb (to jack up a car, to hunt witha light, or to hit a baseball hard).

Token 18225:
Syntactic ambiguity refers to a phrase that has multiple SYNTACTIC AMBIGUITY parses: “I smelled a wumpus in 2,2” has two parses: one where the prepositional phrase “in 2,2” modiﬁes the noun and one where it modiﬁes the verb.

Token 18226:
The syntactic ambiguity leads to a semantic ambiguity , because one parse means that the wumpus is in 2,2 and the other meansSEMANTIC AMBIGUITY that a stench is in 2,2.

Token 18227:
In this case, getting the wrong interpretation could be a deadly mistake for the agent.

Token 18228:
Finally, there can be ambiguity between literal and ﬁgurative meanings.

Token 18229:
Figures of speech are important in poetry, but are surprisingly common in everyday speech as well.

Token 18230:
A metonymy is a ﬁgure of speech in which one object is used to stand for another.

Token 18231:
When METONYMY we hear “Chrysler announced a new model,” we do not interpret it as saying that compa- nies can talk; rather we understand that a spokesperson representing the company made theannouncement.

Token 18232:
Metonymy is common and is often interpreted unconsciously by human hear-ers. Unfortunately, our grammar as it is written is not so facile.

Token 18233:
To handle the semantics of metonymy properly, we need to introduce a whole new level of ambiguity.

Token 18234:
We do this by pro- viding twoobjects for the semantic interpretation of every phrase in the sentence: one for the object that the phrase literally refers to (Chrysler) and one for the metonymic reference (thespokesperson).

Token 18235:
We then have to say that there is a relation between the two.

Token 18236:
In our currentgrammar, “Chrysler announced” gets interpreted as x=Chrysler∧e∈Announce (x)∧After(Now,Extent (e)).

Token 18237:
We need to change that to x=Chrysler∧e∈Announce (m)∧After(Now,Extent (e)) ∧Metonymy (m,x).

Token 18238:
906 Chapter 23.

Token 18239:
Natural Language for Communication This says that there is one entity xthat is equal to Chrysler, and another entity mthat did the announcing, and that the two are in a metonymy relation.

Token 18240:
The next step is to deﬁne whatkinds of metonymy relations can occur.

Token 18241:
The simplest case is when there is no metonymy atall—the literal object xand the metonymic object mare identical: ∀m,x(m=x)⇒Metonymy (m,x).

Token 18242:
For the Chrysler example, a reasonable generalization is that an organization can be used to stand for a spokesperson of that organization: ∀m,x x∈Organizations ∧Spokesperson (m,x)⇒Metonymy (m,x).

Token 18243:
Other metonymies include the author for the works (I read Shakespeare ) or more generally the producer for the product (I drive a Honda ) and the part for the whole (The Red Sox need a strong arm).

Token 18244:
Some examples of metonymy, such as “The ham sandwich on Table 4 wants another beer,” are more novel and are interpreted with respect to a situation.

Token 18245:
Ametaphor is another ﬁgure of speech, in which a phrase with one literal meaning is METAPHOR used to suggest a different meaning by way of an analogy.

Token 18246:
Thus, metaphor can be seen as a kind of metonymy where the relation is one of similarity.

Token 18247:
Disambiguation is the process of recovering the most probable intended meaning of DISAMBIGUATION an utterance.

Token 18248:
In one sense we already have a framework for solving this problem: each rule has a probability associated with it, so the probability of an interpretation is the product ofthe probabilities of the rules that led to the interpretation.

Token 18249:
Unfortunately, the probabilitiesreﬂect how common the phrases are in the corpus from which the grammar was learned,and thus reﬂect general knowledge, not speciﬁc knowledge of the current situation.

Token 18250:
To do disambiguation properly, we need to combine four models: 1. The world model : the likelihood that a proposition occurs in the world.

Token 18251:
Given what we know about the world, it is more likely that a speaker who says “I’m dead” means “Iam in big trouble” rather than “My life ended, and yet I can still talk.” 2.

Token 18252:
The mental model : the likelihood that the speaker forms the intention of communicat- ing a certain fact to the hearer.

Token 18253:
This approach combines models of what the speakerbelieves, what the speaker believes the hearer believes, and so on.

Token 18254:
For example, whena politician says, “I am not a crook,” the world model might assign a probability ofonly 50% to the proposition that the politician is not a criminal, and 99.999% to theproposition that he is not a hooked shepherd’s staff.

Token 18255:
Nevertheless, we select the formerinterpretation because it is a more likely thing to say. 3.

Token 18256:
The language model : the likelihood that a certain string of words will be chosen, given that the speaker has the intention of communicating a certain fact.

Token 18257:
4.

Token 18258:
The acoustic model : for spoken communication, the likelihood that a particular se- quence of sounds will be generated, given that the speaker has chosen a given string ofwords.

Token 18259:
Section 23.5 covers speech recognition.

Token 18260:
Section 23.4.

Token 18261:
Machine Translation 907 23.4 M ACHINE TRANSLATION Machine translation is the automatic translation of text from one natural language (the source) to another (the target).

Token 18262:
It was one of the ﬁrst application areas envisioned for computers(Weaver, 1949), but it is only in the past decade that the technology has seen widespreadusage.

Token 18263:
Here is a passage from page 1 of this book: AI is one of the newest ﬁelds in science and engineering.

Token 18264:
Work started in earnest soon after World War II, and the name itself was coined in 1956.

Token 18265:
Along with molecular biol- ogy, AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.

Token 18266:
And here it is translated from English to Danish by an online tool, Google Translate: AI er en af de nyeste omr˚ ader inden for videnskab og teknik.

Token 18267:
Arbejde startede for alvor lige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956.

Token 18268:
Sammen med molekylær biologi, er AI jævnligt nævnt som “feltet Jeg ville de ﬂeste gerne være i” af forskere i andre discipliner.

Token 18269:
For those who don’t read Danish, here is the Danish translated back to English.

Token 18270:
The words that came out different are in italics: AI is one of the newest ﬁelds ofscience and engineering.

Token 18271:
Work began in earnest justafter theSecond World War, and the name itself was invented in 1956.

Token 18272:
Together with molecular biology, AI is frequently mentioned as “ﬁeld I would most like to be in” by researchers in other disciplines.

Token 18273:
The differences are all reasonable paraphrases, such as frequently mentioned forregularly cited .

Token 18274:
The only real error is the omission of the article the, denoted by the symbol.

Token 18275:
This is typical accuracy: of the two sentences, one has an error that would not be made by a nativespeaker, yet the meaning is clearly conveyed.

Token 18276:
Historically, there have been three main applications of machine translation.

Token 18277:
Rough translation , as provided by free online services, gives the “gist” of a foreign sentence or document, but contains errors.

Token 18278:
Pre-edited translation is used by companies to publish their documentation and sales materials in multiple languages.

Token 18279:
The original source text is writtenin a constrained language that is easier to translate automatically, and the results are usually edited by a human to correct any errors.

Token 18280:
Restricted-source translation works fully automati- cally, but only on highly stereotypical language, such as a weather report.

Token 18281:
Translation is difﬁcult because, in the fully general case, it requires in-depth understand- ing of the text.

Token 18282:
This is true even for very simple texts—even “texts” of one word. Consider the word “Open” on the door of a store.

Token 18283:
6It communicates the idea that the store is accepting customers at the moment.

Token 18284:
Now consider the same word “Open” on a large banner outside a newly constructed store.

Token 18285:
It means that the store is now in daily operation, but readers of this sign would not feel misled if the store closed at night without removing the banner.

Token 18286:
The twosigns use the identical word to convey different meanings.

Token 18287:
In German the sign on the doorwould be “Offen” while the banner would read “Neu Er¨ offnet.” 6This example is due to Martin Kay.

Token 18288:
908 Chapter 23. Natural Language for Communication The problem is that different languages categorize the world differently.

Token 18289:
For example, the French word “doux” covers a wide range of meanings corresponding approximately tothe English words “soft,” “sweet,” and “gentle.” Similarly, the English word “hard” coversvirtually all uses of the German word “hart” (physically recalcitrant, cruel) and some usesof the word “schwierig” (difﬁcult).

Token 18290:
Therefore, representing the meaning of a sentence is more difﬁcult for translation than it is for single-language understanding.

Token 18291:
An English parsing system could use predicates like Open(x), but for translation, the representation language would have to make more distinctions, perhaps with Open 1(x)representing the “Offen” sense andOpen 2(x)representing the “Neu Er¨ offnet” sense.

Token 18292:
A representation language that makes all the distinctions necessary for a set of languages is called an interlingua .

Token 18293:
INTERLINGUA A translator (human or machine) often needs to understand the actual situation de- scribed in the source, not just the individual words.

Token 18294:
For example, to translate the Englishword “him,” into Korean, a choice must be made between the humble and honoriﬁc form, achoice that depends on the social relationship between the speaker and the referent of “him.”In Japanese, the honoriﬁcs are relative, so the choice depends on the social relationships be-tween the speaker, the referent, and the listener.

Token 18295:
Translators (both machine and human) some-times ﬁnd it difﬁcult to make this choice. As another example, to translate “The baseball hit the window.

Token 18296:
It broke.” into French, we must choose the feminine “elle” or the masculine “il” for “it,” so we must decide whether “it” refers to the baseball or the window.

Token 18297:
To get thetranslation right, one must understand physics as well as language.

Token 18298:
Sometimes there is no choice that can yield a completely satisfactory translation.

Token 18299:
For example, an Italian love poem that uses the masculine “il sole” (sun) and feminine “la luna”(moon) to symbolize two lovers will necessarily be altered when translated into German,where the genders are reversed, and further altered when translated into a language where thegenders are the same.

Token 18300:
7 23.4.1 Machine translation systems All translation systems must model the source and target languages, but systems vary in thetype of models they use.

Token 18301:
Some systems attempt to analyze the source language text all the wayinto an interlingua knowledge representation and then generate sentences in the target lan-guage from that representation.

Token 18302:
This is difﬁcult because it involves three unsolved problems:creating a complete knowledge representation of everything; parsing into that representation;and generating sentences from that representation.

Token 18303:
Other systems are based on a transfer model .

Token 18304:
They keep a database of translation rules TRANSFER MODEL (or examples), and whenever the rule (or example) matches, they translate directly.

Token 18305:
Transfer can occur at the lexical, syntactic, or semantic level.

Token 18306:
For example, a strictly syntactic rulemaps English [ Adjective Noun ]t oF r e n c h[ Noun Adjective ].

Token 18307:
A mixed syntactic and lexical rule maps French [ S 1“et puis” S2] to English [ S1“and then” S2]. Figure 23.12 diagrams the various transfer points.

Token 18308:
7Warren Weaver (1949) reports that Max Zeldner points out that the great Hebrew poet H. N. Bialik once said that translation “is like kissing the bride through a veil.”

Token 18309:
Section 23.4.

Token 18310:
Machine Translation 909 Interlingua Semantics Attraction (NamedJohn, NamedMary, High ) English Words John loves MaryFrench Words Jean aime MarieEnglish Syntax S(NP(John), VP(loves, NP(Mary ))) S(NP(Jean), VP(aime, NP(Marie )))French SyntaxEnglish Semantics Loves (John, Mary ) Aime (Jean, Marie )French Semantics Figure 23.12 The Vauquois triangle: schematic diagram of the choices for a machine translation system (Vauquois, 1968).

Token 18311:
We start with English text at the top.

Token 18312:
An interlingua- based system follows the solid lines, parsing English ﬁrst into a syntactic form, then intoa semantic representation and an interlingua representation, and then through generation to a semantic, syntactic, and lexical form in French.

Token 18313:
A transfer-based system uses the dashed lines as a shortcut. Different systems make the transfer at different points; some make it atmultiple points.

Token 18314:
23.4.2 Statistical machine translation Now that we have seen how complex the translation task can be, it should come as no sur- prise that the most successful machine translation systems are built by training a probabilisticmodel using statistics gathered from a large corpus of text.

Token 18315:
This approach does not needa complex ontology of interlingua concepts, nor does it need handcrafted grammars of thesource and target languages, nor a hand-labeled treebank.

Token 18316:
All it needs is data—sample trans-lations from which a translation model can be learned.

Token 18317:
To translate a sentence in, say, English(e)into French (f), we ﬁnd the string of words f ∗that maximizes f∗=a r g m a x fP(f|e) = argmax P(e|f)P(f).

Token 18318:
Here the factor P(f)is the target language model for French; it says how probable a given LANGUAGE MODEL sentence is in French.

Token 18319:
P(e|f)is the translation model ; it says how probable an EnglishTRANSLATION MODEL sentence is as a translation for a given French sentence.

Token 18320:
Similarly, P(f|e)is a translation model from English to French. Should we work directly on P(f|e), or apply Bayes’ rule and work on P(e|f)P(f)?

Token 18321:
Indiagnostic applications like medicine, it is easier to model the domain in the causal di- rection: P(symptoms|disease )rather than P(disease|symptoms ).

Token 18322:
But in translation both directions are equally easy.

Token 18323:
The earliest work in statistical machine translation did applyBayes’ rule—in part because the researchers had a good language model, P(f), and wanted to make use of it, and in part because they came from a background in speech recognition, which isa diagnostic problem.

Token 18324:
We follow their lead in this chapter, but we note that re- cent work in statistical machine translation often optimizes P(f|e)directly, using a more sophisticated model that takes into account many of the features from the language model.

Token 18325:
910 Chapter 23.

Token 18326:
Natural Language for Communication The language model, P(f), could address any level(s) on the right-hand side of Fig- ure 23.12, but the easiest and most common approach is to build an n-gram model from a French corpus, as we have seen before.

Token 18327:
This captures only a partial, local idea of Frenchsentences; however, that is often sufﬁcient for rough translation.

Token 18328:
8 The translation model is learned from a bilingual corpus —a collection of parallel texts, BILINGUAL CORPUS each an English/French pair.

Token 18329:
Now, if we had an inﬁnitely large corpus, then translating a sentence would just be a lookup task: we would have seen the English sentence before in thecorpus, so we could just return the paired French sentence.

Token 18330:
But of course our resources areﬁnite, and most of the sentences we will be asked to translate will be novel.

Token 18331:
However, theywill be composed of phrases that we have seen before (even if some phrases are as short as one word).

Token 18332:
For example, in this book, common phrases include “in this exercise we will,”“size of the state space,” “as a function of the” and “notes at the end of the chapter.” If askedto translate the novel sentence “In this exercise we will compute the size of the state space as afunction of the number of actions.” into French, we should be able to break the sentence intophrases, ﬁnd the phrases in the English corpus (this book), ﬁnd the corresponding Frenchphrases (from the French translation of the book), and then reassemble the French phrasesinto an order that makes sense in French.

Token 18333:
In other words, given a source English sentence, e, ﬁnding a French translation fis a matter of three steps: 1.

Token 18334:
Break the English sentence into phrases e 1,...,e n. 2. For each phrase ei, choose a corresponding French phrase fi.

Token 18335:
We use the notation P(fi|ei)for the phrasal probability that fiis a translation of ei. 3.

Token 18336:
Choose a permutation of the phrases f1,...,f n. We will specify this permutation in a way that seems a little complicated, but is designed to have a simple probability dis- tribution: For each fi, we choose a distortion di, which is the number of words that DISTORTION phrase fihas moved with respect to fi−1; positive for moving to the right, negative for moving to the left, and zero if fiimmediately follows fi−1.

Token 18337:
Figure 23.13 shows an example of the process.

Token 18338:
At the top, the sentence “There is a smelly wumpus sleeping in 2 2” is broken into ﬁve phrases, e1,...,e 5.

Token 18339:
Each of them is translated into a corresponding phrase fi, and then these are permuted into the order f1,f3,f4,f2,f5.

Token 18340:
We specify the permutation in terms of the distortions diof each French phrase, deﬁned as di=START(fi)−END(fi−1)−1, where S TART(fi)is the ordinal number of the ﬁrst word of phrase fiin the French sentence, and E ND(fi−1)is the ordinal number of the last word of phrase fi−1.

Token 18341:
In Figure 23.13 we see thatf5,“ `a 2 2,” immediately follows f4, “qui dort,” and thus d5=0.

Token 18342:
Phrase f2, however, has moved one words to the right of f1,s od2=1.

Token 18343:
As a special case we have d1=0, because f1 starts at position 1 and E ND(f0)is deﬁned to be 0 (even though f0does not exist).

Token 18344:
Now that we have deﬁned the distortion, di, we can deﬁne the probability distribution for distortion, P(di).

Token 18345:
Note that for sentences bounded by length nwe have|di|≤n,a n d 8For the ﬁner points of translation, n-grams are clearly not enough.

Token 18346:
Marcel Proust’s 4000-page novel Al a r´echerche du temps perdu begins and ends with the same word ( longtemps ), so some translators have decided to do the same, thus basing the translation of the ﬁnal w ord on one that appeared roughly 2 million words earlier.

Token 18347:
Section 23.4.

Token 18348:
Machine Translation 911 so the full probability distribution P(di)has only 2n+1 elements, far fewer numbers to learn than the number of permutations, n!.

Token 18349:
That is why we deﬁned the permutation in this circuitous way. Of course, this is a rather impoverished model of distortion.

Token 18350:
It doesn’t saythat adjectives are usually distorted to appear after the noun when we are translating fromEnglish to French—that fact is represented in the French language model, P(f).

Token 18351:
The distor- tion probability is completely independent of the words in the phrases—it depends only on the integer value d i.

Token 18352:
The probability distribution provides a summary of the volatility of the permutations; how likely a distortion of P(d=2 ) is, compared to P(d=0 ), for example.

Token 18353:
We’re ready now to put it all together: we can deﬁne P(f,d|e), the probability that the sequence of phrases fwith distortions dis a translation of the sequence of phrases e.W e make the assumption that each phrase translation and each distortion is independent of theothers, and thus we can factor the expression as P(f,d|e)=/productdisplay iP(fi|ei)P(di) There is a smelly wumpus sleeping in 2 2 Il y a un wumpus qui dort malodorant à 2 2e1e2e3e4e5 d1 = 0d3 = -2 d2 = +1 d4 = +1d5 = 0f1f3f2 f4f5 Figure 23.13 Candidate French phrases for each phrase of an English sentence, with dis- tortion ( d) values for each French phrase.

Token 18354:
That gives us a way to compute the probability P(f,d|e)for a candidate translation f and distortion d. But to ﬁnd the best fanddwe can’t just enumerate sentences; with maybe 100French phrases for each English phrase in the corpus, there are 1005different 5-phrase translations, and 5!reorderings for each of those.

Token 18355:
We will have to search for a good solution.

Token 18356:
A local beam search (see page 125) with a heuristic that estimates probability has proveneffective at ﬁnding a nearly-most-probable translation.

Token 18357:
All that remains is to learn the phrasal and distortion probabilities. We sketch the pro- cedure; see the notes at the end of the chapter for details.

Token 18358:
1.Find parallel texts : First, gather a parallel bilingual corpus. For example, a Hansard 9HANSARD is a record of parliamentary debate.

Token 18359:
Canada, Hong Kong, and other countries pro- duce bilingual Hansards, the European Union publishes its ofﬁcial documents in 11 languages, and the United Nations publishes multilingual documents.

Token 18360:
Bilingual text isalso available online; some Web sites publish parallel content with parallel URLs, for 9Named after William Hansard, who ﬁrst published the British parliamentary debates in 1811.

Token 18361:
912 Chapter 23. Natural Language for Communication example, /en/ for the English page and /fr/ for the corresponding French page.

Token 18362:
The leading statistical translation systems train on hundreds of millions of words of paralleltext and billions of words of monolingual text.

Token 18363:
2.Segment into sentences : The unit of translation is a sentence, so we will have to break the corpus into sentences.

Token 18364:
Periods are strong indicators of the end of a sentence, butconsider “Dr.

Token 18365:
J. R. Smith of Rodeo Dr. paid $29.99 on 9.9.09.”; only the ﬁnal periodends a sentence.

Token 18366:
One way to decide if a period ends a sentence is to train a modelthat takes as features the surrounding words and their parts of speech.

Token 18367:
This approachachieves about 98% accuracy.

Token 18368:
3.Align sentences : For each sentence in the English version, determine what sentence(s) it corresponds to in the French version.

Token 18369:
Usually, the next sentence of English corre-sponds to the next sentence of French in a 1:1 match, but sometimes there is variation:one sentence in one language will be split into a 2:1 match, or the order of two sentenceswill be swapped, resulting in a 2:2 match.

Token 18370:
By looking at the sentence lengths alone (i.e.

Token 18371:
short sentences should align with short sentences), it is possible to align them (1:1, 1:2, or 2:2, etc.)

Token 18372:
with accuracy in the 90% to 99% range using a variation on the Viterbialgorithm.

Token 18373:
Even better alignment can be achieved by using landmarks that are commonto both languages, such as numbers, dates, proper names, or words that we know froma bilingual dictionary have an unambiguous translation.

Token 18374:
For example, if the 3rd Englishand 4th French sentences contain the string “1989” and neighboring sentences do not,that is good evidence that the sentences should be aligned together.

Token 18375:
4.Align phrases : Within a sentence, phrases can be aligned by a process that is similar to that used for sentence alignment, but requiring iterative improvement.

Token 18376:
When we start, we have no way of knowing that “qui dort” aligns with “sleeping,” but we can arrive atthat alignment by a process of aggregation of evidence.

Token 18377:
Over all the example sentenceswe have seen, we notice that “qui dort” and “sleeping” co-occur with high frequency,and that in the pair of aligned sentences, no phrase other than “qui dort” co-occurs sofrequently in other sentences with “sleeping.” A complete phrase alignment over ourcorpus gives us the phrasal probabilities (after appropriate smoothing).

Token 18378:
5.Extract distortions : Once we have an alignment of phrases we can deﬁne distortion probabilities.

Token 18379:
Simply count how often distortion occurs in the corpus for each distanced=0,±1,±2,..., and apply smoothing.

Token 18380:
6.Improve estimates with EM : Use expectation–maximization to improve the estimates ofP(f|e)andP(d)values.

Token 18381:
We compute the best alignments with the current values of these parameters in the E step, then update the estimates in the M step and iterate theprocess until convergence.

Token 18382:
23.5 S PEECH RECOGNITION Speech recognition is the task of identifying a sequence of words uttered by a speaker, givenSPEECH RECOGNITION the acoustic signal.

Token 18383:
It has become one of the mainstream applications of AI—millions of

Token 18384:
Section 23.5.

Token 18385:
Speech Recognition 913 people interact with speech recognition systems every day to navigate voice mail systems, search the Web from mobile phones, and other applications.

Token 18386:
Speech is an attractive optionwhen hands-free operation is necessary, as when operating machinery.

Token 18387:
Speech recognition is difﬁcult because the sounds made by a speaker are ambiguous and, well, noisy.

Token 18388:
As a well-known example, the phrase “recognize speech” sounds almost the same as “wreck a nice beach” when spoken quickly.

Token 18389:
Even this short example shows several of the issues that make speech problematic.

Token 18390:
First, segmentation : written words in SEGMENTATION English have spaces between them, but in fast speech there are no pauses in “wreck a nice” that would distinguish it as a multiword phrase as opposed to the single word “recognize.”Second, coarticulation : when speaking quickly the “s” sound at the end of “nice” merges COARTICULATION with the “b” sound at the beginning of “beach,” yielding something that is close to a “sp.” Another problem that does not show up in this example is homophones —words like “to,” HOMOPHONES “too,” and “two” that sound the same but differ in meaning.

Token 18391:
We can view speech recognition as a problem in most-likely-sequence explanation.

Token 18392:
As we saw in Section 15.2, this is the problem of computing the most likely sequence of statevariables, x 1:t, given a sequence of observations e1:t. In this case the state variables are the words, and the observations are sounds.

Token 18393:
More precisely, an observation is a vector of features extracted from the audio signal.

Token 18394:
As usual, the most likely sequence can be computed with the help of Bayes’ rule to be: argmax word 1:tP(word 1:t|sound 1:t) = argmax word 1:tP(sound 1:t|word 1:t)P(word 1:t).

Token 18395:
HereP(sound 1:t|word 1:t)is the acoustic model .

Token 18396:
It describes the sounds of words—that ACOUSTIC MODEL “ceiling” begins with a soft “c” and sounds the same as “sealing.” P(word 1:t)is known as thelanguage model .

Token 18397:
It speciﬁes the prior probability of each utterance—for example, that LANGUAGE MODEL “ceiling fan” is about 500 times more likely as a word sequence than “sealing fan.” This approach was named the noisy channel model by Claude Shannon (1948).

Token 18398:
HeNOISY CHANNEL MODEL described a situation in which an original message (the words in our example) is transmitted over a noisy channel (such as a telephone line) such that a corrupted message (the sounds in our example) are received at the other end.

Token 18399:
Shannon showed that no matter how noisythe channel, it is possible to recover the original message with arbitrarily small error, if weencode the original message in a redundant enough way.

Token 18400:
The noisy channel approach hasbeen applied to speech recognition, machine translation, spelling correction, and other tasks.

Token 18401:
Once we deﬁne the acoustic and language models, we can solve for the most likely sequence of words using the Viterbi algorithm (Section 15.2.3 on page 576).

Token 18402:
Most speech recognition systems use a language model that makes the Markov assumption—that the cur- rent state Word tdepends only on a ﬁxed number nof previous states—and represent Word t as a single random variable taking on a ﬁnite set of values, which makes it a Hidden Markov Model (HMM).

Token 18403:
Thus, speech recognition becomes a simple application of the HMM method- ology, as described in Section 15.3—simple that is, once we deﬁne the acoustic and language models.

Token 18404:
We cover them next.

Token 18405:
914 Chapter 23.

Token 18406:
Natural Language for Communication Vow el s Consonants B–N Consonants P–Z Phone Example Phone Example Phone Example [iy] b ea t [b] b et [p] p et [ih] b i t [ch] Ch et [r] r at [eh] b e t [d] d ebt [s] s et [æ] b a t [f] f at [sh] sh oe [ah] b u t [g] g et [t] t en [ao] b ough t [hh] h at [th] th ick [ow] b oa t [hv] h igh [dh] th at [uh] b oo k [jh] j et [dx] bu tt er [ey] b ai t [k] k ick [v] v et [er] B er t [l] l et [w] w et [ay] b uy [el] bott le [wh] wh ich [oy] b oy [m] m et [y] y et [axr] din er [em] bott om [z] z oo [aw] d ow n [n] n et [zh] mea s ure [ax] a bout [en] butt on [ix] ros e s [ng] si ng [aa] c o t [eng] wash ing [-] silence Figure 23.14 The ARPA phonetic alphabet, or ARPAbet , listing all the phones used in American English.

Token 18407:
There are several alternative notations, including an International Pho- netic Alphabet (IPA), which contai ns the phones in all known languages.

Token 18408:
23.5.1 Acoustic model Sound waves are periodic changes in pressure that propagate through the air.

Token 18409:
When these waves strike the diaphragm of a microphone, the back-and-forth movement generates anelectric current.

Token 18410:
An analog-to-digital converter measures the size of the current—which ap-proximates the amplitude of the sound wave—at discrete intervals called the sampling rate .

Token 18411:
SAMPLING RATE Speech sounds, which are mostly in the range of 100 Hz (100 cycles per second) to 1000 Hz, are typically sampled at a rate of 8 kHz.

Token 18412:
(CDs and mp3 ﬁles are sampled at 44.1 kHz.)

Token 18413:
Theprecision of each measurement is determined by the quantization factor ; speech recognizers QUANTIZATION FACTOR typically keep 8 to 12 bits.

Token 18414:
That means that a low-end system, sampling at 8 kHz with 8-bit quantization, would require nearly half a megabyte per minute of speech.

Token 18415:
Since we only want to know what words were spoken, not exactly what they sounded like, we don’t need to keep all that information.

Token 18416:
We only need to distinguish between differ-ent speech sounds.

Token 18417:
Linguists have identiﬁed about 100 speech sounds, or phones , that can be PHONE composed to form all the words in all known human languages.

Token 18418:
Roughly speaking, a phone is the sound that corresponds to a single vowel or consonant, but there are some complica- tions: combinations of letters, such as “th” and “ng” produce single phones, and some letters produce different phones in different contexts (e.g., the “a” in ratandrate.

Token 18419:
Figure 23.14 lists

Token 18420:
Section 23.5. Speech Recognition 915 all the phones that are used in English, with an example of each.

Token 18421:
A phoneme is the smallest PHONEME unit of sound that has a distinct meaning to speakers of a particular language.

Token 18422:
For example, the “t” in “stick” sounds similar enough to the “t” in “tick” that speakers of English considerthem the same phoneme.

Token 18423:
But the difference is signiﬁcant in the Thai language, so there theyare two phonemes.

Token 18424:
To represent spoken English we want a representation that can distinguish between different phonemes, but one that need not distinguish the nonphonemic variations in sound: loud or soft, fast or slow, male or female voice, etc.

Token 18425:
First, we observe that although the sound frequencies in speech may be several kHz, thechanges in the content of the signal occur much less often, perhaps at no more than 100 Hz.

Token 18426:
Therefore, speech systems summarize the properties of the signal over time slices calledframes .

Token 18427:
A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is short enough FRAME to ensure that few short-duration phenomena will be missed.

Token 18428:
Overlapping frames are used to make sure that we don’t miss a signal because it happens to fall on a frame boundary.

Token 18429:
Each frame is summarized by a vector of features .

Token 18430:
Picking out features from a speech FEATURE signal is like listening to an orchestra and saying “here the French horns are playing loudly and the violins are playing softly.” We’ll give a brief overview of the features in a typicalsystem.

Token 18431:
First, a Fourier transform is used to determine the amount of acoustic energy at about a dozen frequencies.

Token 18432:
Then we compute a measure called the mel frequency cepstral coefﬁcient (MFCC) or MFCC for each frequency.

Token 18433:
We also compute the total energy inMEL FREQUENCY CEPSTRALCOEFFICIENT (MFCC) the frame.

Token 18434:
That gives thirteen features; for each one we compute the difference between this frame and the previous frame, and the difference between differences, for a total of 39features.

Token 18435:
These are continuous-valued; the easiest way to ﬁt them into the HMM frameworkis to discretize the values.

Token 18436:
(It is also possible to extend the HMM model to handle continuousmixtures of Gaussians.)

Token 18437:
Figure 23.15 shows the sequence of transformations from the rawsound to a sequence of frames with discrete features.

Token 18438:
We have seen how to go from the raw acoustic signal to a series of observations, e t. Now we have to describe the (unobservable) states of the HMM and deﬁne the transitionmodel, P(X t|Xt−1), and the sensor model, P(Et|Xt).

Token 18439:
The transition model can be broken into two levels: word and phone.

Token 18440:
We’ll start from the bottom: the phone model describes PHONE MODEL Analog acoustic signal: Sampled, quantized digital signal: Frames with features:10 15 38 52 47 8222 63 24 89 94 1110 12 73 Figure 23.15 Translating the acoustic signal into a sequence of frames.

Token 18441:
In this diagram each frame is described by the discretized values of three acoustic features; a real system would have dozens of features.

Token 18442:
916 Chapter 23.

Token 18443:
Natural Language for Communication Phone HMM for [m]: 0.10.9 0.3 0.60.4 C1: 0.5 C2: 0.2 C3: 0.3C3: 0.2 C4: 0.7 C5: 0.1C4: 0.1 C6: 0.5 C7: 0.4Output probabilities for the phone HMM: Onset: Mid: End:FINAL0.7Mid End Onset Figure 23.16 An HMM for the three-state phone [m].

Token 18444:
Each state has several possible outputs, each with its own probability.

Token 18445:
The MFCC feature labels C1through C7are arbitrary, standing for some combination of feature values.

Token 18446:
0.5 0.5[t] [ow] [m][ey] [ow] [aa][t] 0.50.50.2 0.8[m][ey] [ow] [t] [aa][t] [ah][ow](a) Word model with dialect variation: (b) Word model with coarticulation and dialect variations : 1.01.0 1.01.0 1.01.0 1.01.0 1.0 1.0 Figure 23.17 Two pronunciation models of the word “tomato.” Each model is shown as a transition diagram with states as circles and arrows showing allowed transitions with theirassociated probabilities.

Token 18447:
(a) A model allowing for dialect differences. The 0.5 numbers are estimates based on the two authors’ preferred pronunciations.

Token 18448:
(b) A model with a coarticula- tion effect on the ﬁrst vowel, allowing either the [ow] or the [ah] phone.

Token 18449:
Section 23.5. Speech Recognition 917 a phone as three states, the onset, middle, and end.

Token 18450:
For example, the [t] phone has a silent beginning, a small explosive burst of sound in the middle, and (usually) a hissing at the end.Figure 23.16 shows an example for the phone [m].

Token 18451:
Note that in normal speech, an averagephone has a duration of 50–100 milliseconds, or 5–10 frames.

Token 18452:
The self-loops in each stateallows for variation in this duration.

Token 18453:
By taking many self-loops (especially in the mid state), we can represent a long “ mmmmmmmmmmm” s ound.

Token 18454:
Bypassing the self-loops yields a short “m” sound.

Token 18455:
In Figure 23.17 the phone models are strung together to form a pronunciation model PRONUNCIATION MODEL for a word.

Token 18456:
According to Gershwin (1937), you say [t ow m ey t ow] and I say [t ow m aa t ow].

Token 18457:
Figure 23.17(a) shows a transition model that provides for this dialect variation.

Token 18458:
Eachof the circles in this diagram represents a phone model like the one in Figure 23.16.

Token 18459:
In addition to dialect variation, words can have coarticulation variation.

Token 18460:
For example, the [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has thetongue near the bottom.

Token 18461:
When speaking quickly, the tongue doesn’t have time to get intoposition for the [ow], and we end up with [t ah] rather than [t ow].

Token 18462:
Figure 23.17(b) givesa model for “tomato” that takes this coarticulation effect into account.

Token 18463:
More sophisticatedphone models take into account the context of the surrounding phones.

Token 18464:
There can be substantial variation in pronunciation for a word.

Token 18465:
The most common pronunciation of “because” is [b iy k ah z], but that only accounts for about a quarter ofuses.

Token 18466:
Another quarter (approximately) substitutes [ix], [ih] or [ax] for the ﬁrst vowel, and theremainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the ﬁnal [z], or drop“be” entirely, leaving “cuz.” 23.5.2 Language model For general-purpose speech recognition, the language model can be an n-gram model of text learned from a corpus of written sentences.

Token 18467:
However, spoken language has differentcharacteristics than written language, so it is better to get a corpus of transcripts of spokenlanguage.

Token 18468:
For task-speciﬁc speech recognition, the corpus should be task-speciﬁc: to buildyour airline reservation system, get transcripts of prior calls.

Token 18469:
It also helps to have task-speciﬁcvocabulary, such as a list of all the airports and cities served, and all the ﬂight numbers.

Token 18470:
Part of the design of a voice user interface is to coerce the user into saying things from a limited set of options, so that the speech recognizer will have a tighter probability distributionto deal with.

Token 18471:
For example, asking “What city do you want to go to?” elicits a response with a highly constrained language model, while asking “How can I help you?” does not.

Token 18472:
23.5.3 Building a speech recognizer The quality of a speech recognition system depends on the quality of all of its components— the language model, the word-pronunciation models, the phone models, and the signal-processing algorithms used to extract spectral features from the acoustic signal.

Token 18473:
We have discussed how the language model can be constructed from a corpus of written text, and we leave the details of signal processing to other textbooks.

Token 18474:
We are left with the pronunciationand phone models. The structure of the pronunciation models—such as the tomato models in

Token 18475:
918 Chapter 23. Natural Language for Communication Figure 23.17—is usually developed by hand.

Token 18476:
Large pronunciation dictionaries are now avail- able for English and other languages, although their accuracy varies greatly.

Token 18477:
The structureof the three-state phone models is the same for all phones, as shown in Figure 23.16. Thatleaves the probabilities themselves.

Token 18478:
As usual, we will acquire the probabilities from a corpus, this time a corpus of speech.

Token 18479:
The most common type of corpus to obtain is one that includes the speech signal for each sentence paired with a transcript of the words.

Token 18480:
Building a model from this corpus is moredifﬁcult than building an n-gram model of text, because we have to build a hidden Markov model—the phone sequence for each word and the phone state for each time frame are hiddenvariables.

Token 18481:
In the early days of speech recognition, the hidden variables were provided bylaborious hand-labeling of spectrograms.

Token 18482:
Recent systems use expectation–maximization toautomatically supply the missing data.

Token 18483:
The idea is simple: given an HMM and an observationsequence, we can use the smoothing algorithms from Sections 15.2 and 15.3 to compute theprobability of each state at each time step and, by a simple extension, the probability of eachstate–state pair at consecutive time steps.

Token 18484:
These probabilities can be viewed as uncertain labels .

Token 18485:
From the uncertain labels, we can estimate new transition and sensor probabilities, and the EM procedure repeats.

Token 18486:
The method is guaranteed to increase the ﬁt between model and data on each iteration, and it generally converges to a much better set of parameter values than those provided by the initial, hand-labeled estimates.

Token 18487:
The systems with the highest accuracy work by training a different model for each speaker, thereby capturing differences in dialect as well as male/female and other variations.

Token 18488:
This training can require several hours of interaction with the speaker, so the systems with the most widespread adoption do not create speaker-speciﬁc models.

Token 18489:
The accuracy of a system depends on a number of factors.

Token 18490:
First, the quality of the signal matters: a high-quality directional microphone aimed at a stationary mouth in a padded roomwill do much better than a cheap microphone transmitting a signal over phone lines from acar in trafﬁc with the radio playing.

Token 18491:
The vocabulary size matters: when recognizing digitstrings with a vocabulary of 11 words (1-9 plus “oh” and “zero”), the word error rate will be below 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary, and 20% on a corpus with a 64,000-word vocabulary.

Token 18492:
The task matters too: when the systemis trying to accomplish a speciﬁc task—book a ﬂight or give directions to a restaurant—thetask can often be accomplished perfectly even with a word error rate of 10% or more.

Token 18493:
23.6 S UMMARY Natural language understanding is one of the most important subﬁelds of AI.

Token 18494:
Unlike mostother areas of AI, natural language understanding requires an empirical investigation of actualhuman behavior—which turns out to be complex and interesting.

Token 18495:
•Formal language theory and phrase structure grammars (and in particular, context- free grammar) are useful tools for dealing with some aspects of natural language.

Token 18496:
The probabilistic context-free grammar (PCFG) formalism is widely used.

Token 18497:


Token 18498:
Bibliographical and Historical Notes 919 •Sentences in a context-free language can be parsed in O(n3)time by a chart parser such as the CYK algorithm , which requires grammar rules to be in Chomsky Normal Form .

Token 18499:
•Atreebank can be used to learn a grammar. It is also possible to learn a grammar from an unparsed corpus of sentences, but this is less successful.

Token 18500:
•Alexicalized PCFG allows us to represent that some relationships between words are more common than others.

Token 18501:
•It is convenient to augment a grammar to handle such problems as subject–verb agree- ment and pronoun case.

Token 18502:
Deﬁnite clause grammar (DCG) is a formalism that allows for augmentations.

Token 18503:
With DCG, parsing and semantic interpretation (and even generation)can be done using logical inference.

Token 18504:
•Semantic interpretation can also be handled by an augmented grammar.

Token 18505:
•Ambiguity is a very important problem in natural language understanding; most sen- tences have many possible interpretations, but usually only one is appropriate.

Token 18506:
Disam-biguation relies on knowledge about the world, about the current situation, and aboutlanguage use.

Token 18507:
•Machine translation systems have been implemented using a range of techniques, from full syntactic and semantic analysis to statistical techniques based on phrase fre-quencies.

Token 18508:
Currently the statistical models are most popular and most successful. •Speech recognition systems are also primarily based on statistical principles.

Token 18509:
Speech systems are popular and useful, albeit imperfect.

Token 18510:
•Together, machine translation and speech recognition are two of the big successes of natural language technology.

Token 18511:
One reason that the models perform well is that largecorpora are available—both translation and speech are tasks that are performed “in thewild” by people every day.

Token 18512:
In contrast, tasks like parsing sentences have been lesssuccessful, in part because no large corpora of parsed sentences are available “in the wild” and in part because parsing is not useful in and of itself.

Token 18513:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Like semantic networks, context-free grammars (also known as phrase structure grammars) are a reinvention of a technique ﬁrst used by ancient Indian grammarians (especially Panini,ca.

Token 18514:
350 B.C.) studying Shastric Sanskrit (Ingerman, 1967).

Token 18515:
They were reinvented by Noam Chomsky (1956) for the analysis of English syntax and independently by John Backus forthe analysis of Algol-58 syntax.

Token 18516:
Peter Naur extended Backus’s notation and is now credited(Backus, 1996) with the “N” in BNF, which originally stood for “Backus Normal Form.” Knuth (1968) deﬁned a kind of augmented grammar called attribute grammar that is use- ATTRIBUTE GRAMMAR ful for programming languages.

Token 18517:
Deﬁnite clause grammars were introduced by Colmer- auer (1975) and developed and popularized by Pereira and Shieber (1987).

Token 18518:
Probabilistic context-free grammars were investigated by Booth (1969) and Salo- maa (1969).

Token 18519:
Other algorithms for PCFGs are presented in the excellent short monograph by

Token 18520:
920 Chapter 23.

Token 18521:
Natural Language for Communication Charniak (1993) and the excellent long textbooks by Manning and Sch¨ utze (1999) and Juraf- sky and Martin (2008).

Token 18522:
Baker (1979) introduces the inside–outside algorithm for learning aPCFG, and Lari and Young (1990) describe its uses and limitations.

Token 18523:
Stolcke and Omohundro(1994) show how to learn grammar rules with Bayesian model merging; Haghighi and Klein(2006) describe a learning system based on prototypes.

Token 18524:
Lexicalized PCFGs (Charniak, 1997; Hwa, 1998) combine the best aspects of PCFGs andn-gram models.

Token 18525:
Collins (1999) describes PCFG parsing that is lexicalized with head features.

Token 18526:
Petrov and Klein (2007a) show how to get the advantages of lexicalization withoutactual lexical augmentations by learning speciﬁc syntactic categories from a treebank that hasgeneral categories; for example, the treebank has the category NP, from which more speciﬁc categories such as NP OandNPScan be learned.

Token 18527:
There have been many attempts to write formal grammars of natural languages, both in “pure” linguistics and in computational linguistics.

Token 18528:
There are several comprehensive butinformal grammars of English (Quirk et al. , 1985; McCawley, 1988; Huddleston and Pullum, 2002).

Token 18529:
Since the mid-1980s, there has been a trend toward putting more information in thelexicon and less in the grammar.

Token 18530:
Lexical-functional grammar, or LFG (Bresnan, 1982) wasthe ﬁrst major grammar formalism to be highly lexicalized.

Token 18531:
If we carry lexicalization to an extreme, we end up with categorial grammar (Clark and Curran, 2004), in which there can be as few as two grammar rules, or with dependency grammar (Smith and Eisner, 2008; K¨ubler et al.

Token 18532:
, 2009) in which there are no syntactic categories, only relations between words. Sleator and Temperley (1993) describe a dependency parser.

Token 18533:
Paskin (2001) shows that a version of dependency grammar is easier to learn than PCFGs.

Token 18534:
The ﬁrst computerized parsing algorithms were demonstrated by Yngve (1955).

Token 18535:
Ef- ﬁcient algorithms were developed in the late 1960s, with a few twists since then (Kasami,1965; Younger, 1967; Earley, 1970; Graham et al. , 1980).

Token 18536:
Maxwell and Kaplan (1993) show how chart parsing with augmentations can be made efﬁcient in the average case.

Token 18537:
Churchand Patil (1982) address the resolution of syntactic ambiguity.

Token 18538:
Klein and Manning (2003)describe A ∗parsing, and Pauls and Klein (2009) extend that to K-best A∗parsing, in which the result is not a single parse but the Kbest.

Token 18539:
Leading parsers today include those by Petrov and Klein (2007b), which achieved 90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), whichachieved 92.0%, and Koo et al.

Token 18540:
(2008), which achieved 93.2% on the Penn treebank.

Token 18541:
These numbers are not directly comparable, and there is some criticism of the ﬁeld that it is focusingtoo narrowly on a few select corpora, and perhaps overﬁtting on them.

Token 18542:
Formal semantic interpretation of natural languages originates within philosophy and formal logic, particularly Alfred Tarski’s (1935) work on the semantics of formal languages.Bar-Hillel (1954) was the ﬁrst to consider the problems of pragmatics and propose that theycould be handled by formal logic.

Token 18543:
For example, he introduced C. S. Peirce’s (1902) termindexical into linguistics.

Token 18544:
Richard Montague’s essay “English as a formal language” (1970) is a kind of manifesto for the logical analysis of language, but the books by Dowty et al.

Token 18545:
(1991) and Portner and Partee (2002) are more readable.

Token 18546:
The ﬁrst NLP system to solve an actual task was probably the B ASEBALL question answering system (Green et al.

Token 18547:
, 1961), which handled questions about a database of baseball

Token 18548:
Bibliographical and Historical Notes 921 statistics.

Token 18549:
Close after that was Woods’s (1973) L UNAR , which answered questions about the rocks brought back from the moon by the Apollo program.

Token 18550:
Roger Schank and his studentsbuilt a series of programs (Schank and Abelson, 1977; Schank and Riesbeck, 1981) thatall had the task of understanding language.

Token 18551:
Modern approaches to semantic interpretationusually assume that the mapping from syntax to semantics will be learned from examples (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005).

Token 18552:
Hobbs et al. (1993) describes a quantitative nonprobabilistic framework for interpreta- tion.

Token 18553:
More recent work follows an explicitly probabilistic framework (Charniak and Gold-man, 1992; Wu, 1993; Franz, 1996).

Token 18554:
In linguistics, optimality theory (Kager, 1999) is basedon the idea of building soft constraints into the grammar, giving a natural ranking to inter-pretations (similar to a probability distribution), rather than having the grammar generate allpossibilities with equal rank.

Token 18555:
Norvig (1988) discusses the problems of considering multiplesimultaneous interpretations, rather than settling for a single maximum-likelihood interpre-tation.

Token 18556:
Literary critics (Empson, 1953; Hobbs, 1990) have been ambiguous about whetherambiguity is something to be resolved or cherished.

Token 18557:
Nunberg (1979) outlines a formal model of metonymy. Lakoff and Johnson (1980) give an engaging analysis and catalog of common metaphors in English.

Token 18558:
Martin (1990) and Gibbs (2006) offer computational models of metaphor interpretation.

Token 18559:
The ﬁrst important result on grammar induction was a negative one: Gold (1967) showed that it is not possible to reliably learn a correct context-free grammar, given a set of strings from that grammar.

Token 18560:
Prominent linguists, such as Chomsky (1957) and Pinker (2003), have used Gold’s result to argue that there must be an innate universal grammar that all UNIVERSAL GRAMMAR children have from birth.

Token 18561:
The so-called Poverty of the Stimulus argument says that children aren’t given enough input to learn a CFG, so they must already “know” the grammar and bemerely tuning some of its parameters.

Token 18562:
While this argument continues to hold sway throughoutmuch of Chomskyan linguistics, it has been dismissed by some other linguists (Pullum, 1996;Elman et al.

Token 18563:
, 1997) and most computer scientists.

Token 18564:
As early as 1969, Horning showed that it ispossible to learn, in the sense of PAC learning, a probabilistic context-free grammar.

Token 18565:
Since then, there have been many convincing empirical demonstrations of learning from positive examples alone, such as the ILP work of Mooney (1999) and Muggleton and De Raedt (1994),the sequence learning of Nevill-Manning and Witten (1997), and the remarkable Ph.D. thesesof Sch¨ utze (1995) and de Marcken (1996).

Token 18566:
There is an annual International Conference on Grammatical Inference (ICGI).

Token 18567:
It is possible to learn other grammar formalisms, such asregular languages (Denis, 2001) and ﬁnite state automata (Parekh and Honavar, 2001).

Token 18568:
Abney(2007) is a textbook introduction to semi-supervised learning for language models.

Token 18569:
Wordnet (Fellbaum, 2001) is a publicly available dictionary of about 100,000 words and phrases, categorized into parts of speech and linked by semantic relations such as synonym,antonym, and part-of.

Token 18570:
The Penn Treebank (Marcus et al. , 1993) provides parse trees for a 3-million-word corpus of English.

Token 18571:
Charniak (1996) and Klein and Manning (2001) discuss parsing with treebank grammars. The British National Corpus (Leech et al.

Token 18572:
, 2001) contains 100 million words, and the World Wide Web contains several trillion words; (Brants et al.

Token 18573:
, 2007) describe n-gram models over a 2-trillion-word Web corpus.

Token 18574:
922 Chapter 23.

Token 18575:
Natural Language for Communication In the 1930s Petr Troyanskii applied for a patent for a “translating machine,” but there were no computers available to implement his ideas.

Token 18576:
In March 1947, the Rockefeller Founda-tion’s Warren Weaver wrote to Norbert Wiener, suggesting that machine translation might bepossible.

Token 18577:
Drawing on work in cryptography and information theory, Weaver wrote, “When Ilook at an article in Russian, I say: ‘This is really written in English, but it has been coded in strange symbols.

Token 18578:
I will now proceed to decode.”’ For the next decade, the community tried to decode in this way. IBM exhibited a rudimentary system in 1954.

Token 18579:
Bar-Hillel (1960) de-scribes the enthusiasm of this period.

Token 18580:
However, the U.S. government subsequently reported(ALPAC, 1966) that “there is no immediate or predictable prospect of useful machine trans-lation.” However, limited work continued, and starting in the 1980s, computer power hadincreased to the point where the ALPAC ﬁndings were no longer correct.

Token 18581:
The basic statistical approach we describe in the chapter is based on early work by the IBM group (Brown et al.

Token 18582:
, 1988, 1993) and the recent work by the ISI and Google research groups (Och and Ney, 2004; Zollmann et al. , 2008).

Token 18583:
A textbook introduction on statistical machine translation is given by Koehn (2009), and a short tutorial by Kevin Knight (1999) hasbeen inﬂuential.

Token 18584:
Early work on sentence segmentation was done by Palmer and Hearst (1994).Och and Ney (2003) and Moore (2005) cover bilingual sentence alignment.

Token 18585:
The prehistory of speech recognition began in the 1920s with Radio Rex, a voice- activated toy dog.

Token 18586:
Rex jumped out of his doghouse in response to the word “Rex!” (oractually almost any sufﬁciently loud word).

Token 18587:
Somewhat more serious work began after WorldWar II. At AT&T Bell Labs, a system was built for recognizing isolated digits (Davis et al.

Token 18588:
, 1952) by means of simple pattern matching of acoustic features.

Token 18589:
Starting in 1971, the Defense Advanced Research Projects Agency (DARPA) of the United States Department of Defense funded four competing ﬁve-year projects to develop high-performance speech recognitionsystems.

Token 18590:
The winner, and the only system to meet the goal of 90% accuracy with a 1000-wordvocabulary, was the H ARPY system at CMU (Lowerre and Reddy, 1980).

Token 18591:
The ﬁnal version of H ARPY was derived from a system called D RAGON built by CMU graduate student James Baker (1975); D RAGON was the ﬁrst to use HMMs for speech.

Token 18592:
Almost simultaneously, Je- linek (1976) at IBM had developed another HMM-based system.

Token 18593:
Recent years have beencharacterized by steady incremental progress, larger data sets and models, and more rigor-ous competitions on more realistic speech tasks.

Token 18594:
In 1997, Bill Gates predicted, “The PC ﬁveyears from now—you won’t recognize it, because speech will come into the interface.” Thatdidn’t quite happen, but in 2008 he predicted “In ﬁve years, Microsoft expects more Internetsearches to be done through speech than through typing on a keyboard.” History will tell ifhe is right this time around.

Token 18595:
Several good textbooks on speech recognition are available (Rabiner and Juang, 1993; Jelinek, 1997; Gold and Morgan, 2000; Huang et al. , 2001).

Token 18596:
The presentation in this chapter drew on the survey by Kay, Gawron, and Norvig (1994) and on the textbook by Jurafsky and Martin (2008).

Token 18597:
Speech recognition research is published in Computer Speech and Language , Speech Communications , and the IEEE Transactions on Acoustics, Speech, and Signal Pro- cessing and at the DARPA Workshops on Speech and Natural Language Processing and the Eurospeech, ICSLP, and ASRU conferences.

Token 18598:


Token 18599:
Exercises 923 Ken Church (2004) shows that natural language research has cycled between concen- trating on the data (empiricism) and concentrating on theories (rationalism).

Token 18600:
The linguistJohn Firth (1957) proclaimed “You shall know a word by the company it keeps,” and linguis-tics of the 1940s and early 1950s was based largely on word frequencies, although withoutthe computational power we have available today.

Token 18601:
Then Noam (Chomsky, 1956) showed the limitations of ﬁnite-state models, and sparked an interest in theoretical studies of syntax, disregarding frequency counts.

Token 18602:
This approach dominated for twenty years, until empiricismmade a comeback based on the success of work in statistical speech recognition (Jelinek,1976).

Token 18603:
Today, most work accepts the statistical framework, but there is great interest in build-ing statistical models that consider higher-level models, such as syntactic trees and semanticrelations, not just sequences of words.

Token 18604:
Work on applications of language processing is presented at the biennial Applied Natu- ral Language Processing conference (ANLP), the conference on Empirical Methods in Natu-ral Language Processing (EMNLP), and the journal Natural Language Engineering .

Token 18605:
A broad range of NLP work appears in the journal Computational Linguistics and its conference, ACL, and in the Computational Linguistics (COLING) conference.

Token 18606:
EXERCISES 23.1 Read the following text once for understanding, and remember as much of it as you can. There will be a test later.

Token 18607:
The procedure is actually quite simple. First you arrange things into different groups.

Token 18608:
Of course, one pile may be sufﬁcient dependi ng on how much there is to do.

Token 18609:
If you have to go somewhere else due to lack of facilities that is the next step, otherwise you are pretty wellset.

Token 18610:
It is important not to overdo things. That is, it is better to do too few things at once than too many.

Token 18611:
In the short run this may not seem important but complications can easily arise. A mistake is expensive as well.

Token 18612:
At ﬁrst the whole procedure will seem complicated.Soon, however, it will become just another facet of life.

Token 18613:
It is difﬁcult to foresee any end to the necessity for this task in the immediate future, but then one can never tell.

Token 18614:
After the procedure is completed one arranges the material into different groups again. Then they can be put into their appropriate places.

Token 18615:
Eventually they will be used once more and the whole cycle will have to be repeated. However, this is part of life.

Token 18616:
23.2 AnHMM grammar is essentially a standard HMM whose state variable is N(nonter- minal, with values such as Det,Adjective ,Noun and so on) and whose evidence variable is W(word, with values such as is,duck , and so on).

Token 18617:
The HMM model includes a prior P(N0), a transition model P(Nt+1|Nt), and a sensor model P(Wt|Nt).

Token 18618:
Show that every HMM gram- mar can be written as a PCFG.

Token 18619:
[Hint: start by thinking about how the HMM prior can be represented by PCFG rules for the sentence symbol.

Token 18620:
You may ﬁnd it helpful to illustrate for the particular HMM with values A,BforNand values x,yforW.]

Token 18621:
924 Chapter 23.

Token 18622:
Natural Language for Communication 23.3 Consider the following PCFG for simple verb phrases: 0.1:VP→Ve r b 0.2:VP→CopulaAd jective 0.5:VP→Ve r bt h eNo u n 0.2:VP→VP A d v e r b 0.5:Ve r b→is 0.5:Ve r b→shoots 0.8:Copula→is 0.2:Copula→seems 0.5:Adjective→unwell 0.5:Adjective→well 0.5:Adverb→well 0.5:Adverb→badly 0.6:Noun→duck 0.4:Noun→well a.

Token 18623:
Which of the following have a nonzero probability as a VP?

Token 18624:
(i) shoots the duck well well well (ii) seems the well well (iii) shoots the unwell well badly b.

Token 18625:
What is the probability of generating “is well well”? c. What types of ambiguity are exhibited by the phrase in (b)?

Token 18626:
d. Given any PCFG, is it possible to calculate the probability that the PCFG generates a string of exactly 10 words?

Token 18627:
23.4 Outline the major differences between Java (or any other computer language with which you are familiar) and English, commenting on the “understanding” problem in eachcase.

Token 18628:
Think about such things as grammar, syntax, semantics, pragmatics, compositional-ity, context-dependence, lexical ambiguity, syntactic ambiguity, reference ﬁnding (includingpronouns), background knowledge, and what it means to “understand” in the ﬁrst place.

Token 18629:
23.5 This exercise concerns grammars for very simple languages. a. Write a context-free grammar for the language anbn. b.

Token 18630:
Write a context-free grammar for the palindrome language: the set of all strings whose second half is the reverse of the ﬁrst half.

Token 18631:
c. Write a context-sensitive grammar for the duplicate language: the set of all strings whose second half is the same as the ﬁrst half.

Token 18632:
23.6 Consider the sentence “Someone walked slowly to the supermarket” and a lexicon consisting of the following words: Pronoun→someone Verb→walked Adv→slowly Prep→to Article→the Noun→supermarket Which of the following three grammars, combined with the lexicon, generates the given sen- tence?

Token 18633:
Show the corresponding parse tree(s).

Token 18634:


Token 18635:
Exercises 925 (A): (B): (C): S→NP VP S →NP VP S →NP VP NP→Pronoun NP →Pronoun NP →Pronoun NP→Article Noun NP →Noun NP →Article NP VP→VP PP NP →Article NP VP →Verb Adv VP→VP Adv Adv VP →Verb Vmod Adv →Adv Adv VP→Verb Vmod →Adv Vmod Adv →PP PP→Prep NP Vmod →Adv PP →Prep NP NP→Noun Adv →PP NP →Noun PP→Prep NP For each of the preceding three grammars, write down three sentences of English and three sentences of non-English generated by the grammar.

Token 18636:
Each sentence should be signiﬁcantlydifferent, should be at least six words long, and should include some new lexical entries(which you should deﬁne).

Token 18637:
Suggest ways to improve each grammar to avoid generating the non-English sentences.

Token 18638:
23.7 Collect some examples of time expressions, such as “two o’clock,” “midnight,” and “12:46.” Also think up some examples that are ungrammatical, such as “thirteen o’clock” or “half past two ﬁfteen.” Write a grammar for the time language.

Token 18639:
23.8 In this exercise you will transform E 0into Chomsky Normal Form (CNF).

Token 18640:
There are ﬁve steps: (a) Add a new start symbol, (b) Eliminate /epsilon1rules, (c) Eliminate multiple words on right-hand sides, (d) Eliminate rules of the form ( X→Y), (e) Convert long right-hand sides into binary rules.

Token 18641:
a. The start symbol, S, can occur only on the left-hand side in CNF. Add a new rule of the formS/prime→S, using a new symbol S/prime. b.

Token 18642:
The empty string, /epsilon1cannot appear on the right-hand side in CNF. E0does not have any rules with /epsilon1, so this is not an issue.

Token 18643:
c. A word can appear on the right-hand side in a rule only of the form ( X→word ).

Token 18644:
Replace each rule of the form ( X→...word ...) w i t h ( X→...W/prime...) a n d ( W/prime →word ), using a new symbol W/prime.

Token 18645:
d.A r u l e ( X→Y) is not allowed in CNF; it must be ( X→YZ )o r(X→word ).

Token 18646:
Replace each rule of the form ( X→Y) with a set of rules of the form ( X→...) , one for each rule ( Y→. . . ), where (. . . )

Token 18647:
indicates one or more symbols. e. Replace each rule of the form ( X→YZ . . . )

Token 18648:
with two rules, ( X→YZ/prime)a n d(Z/prime →Z...) ,w h e r e Z/primeis a new symbol. Show each step of the process and the ﬁnal set of rules.

Token 18649:
23.9 Using DCG notation, write a grammar for a language that is just like E1, except that it enforces agreement between the subject and verb of a sentence and thus does not generateungrammatical sentences such as “I smells the wumpus.”

Token 18650:
926 Chapter 23.

Token 18651:
Natural Language for Communication 23.10 Consider the following PCFG: S→NP VP [1.0] NP→Noun [0.6]|Pronoun [0.4] VP→Verb NP [0.8]|Modal Verb [0.2] Noun→can[0.1]|ﬁsh[0.3]|... Pronoun→I[0.4]|... Verb→can[0.01]|ﬁsh[0.1]|... Modal→can[0.3]|...

Token 18652:
The sentence “I can ﬁsh” has two parse trees with this grammar.

Token 18653:
Show the two trees, their prior probabilities, and their conditional probabilities, given the sentence.

Token 18654:
23.11 An augmented context-free grammar can represent languages that a regular context- free grammar cannot.

Token 18655:
Show an augmented context-free grammar for the language anbncn.

Token 18656:
The allowable values for augmentation variables are 1 and S UCCESSOR (n),w h e r e nis a value.

Token 18657:
The rule for a sentence in this language is S(n)→A(n)B(n)C(n). Show the rule(s) for each of A,B,a n dC.

Token 18658:
23.12 Augment theE1grammar so that it handles article–noun agreement.

Token 18659:
That is, make sure that “agents” and “an agent” are NPs, but “agent” and “an agents” are not.

Token 18660:
23.13 Consider the following sentence (from The New York Times, July 28, 2008): Banks struggling to recover from multibillion-dollar loans on real estate are cur- tailing loans to American businesses, depriving even healthy companies of moneyfor expansion and hiring.

Token 18661:
a. Which of the words in this sentence are lexically ambiguous? b. Find two cases of syntactic ambiguity in this sentence (there are more than two.)

Token 18662:
c. Give an instance of metaphor in this sentence. d. Can you ﬁnd semantic ambiguity?

Token 18663:
23.14 Without looking back at Exercise 23.1, answer the following questions: a. What are the four steps that are mentioned? b. What step is left out?

Token 18664:
c. What is “the material” that is mentioned in the text? d. What kind of mistake would be expensive? e. Is it better to do too few things or too many?

Token 18665:
Why? 23.15 Select ﬁve sentences and submit them to an online translation service. Translate them from English to another language and back to English.

Token 18666:
Rate the resulting sentences for grammaticality and preservation of meaning. Repeat the process; does the second round of

Token 18667:
Exercises 927 iteration give worse results or the same results?

Token 18668:
Does the choice of intermediate language make a difference to the quality of the results?

Token 18669:
If you know a foreign language, look at thetranslation of one paragraph into that language.

Token 18670:
Count and describe the errors made, andconjecture why these errors were made. 23.16 TheD ivalues for the sentence in Figure 23.13 sum to 0.

Token 18671:
Will that be true of every translation pair? Prove it or give a counterexample. 23.17 (Adapted from Knight (1999).)

Token 18672:
Our translation model assumes that, after the phrase translation model selects phrases and the distortion model permutes them, the language modelcan unscramble the permutation.

Token 18673:
This exercise investigates how sensible that assumption is.Try to unscramble these proposed lists of phrases into the correct order: a. have, programming, a, seen, never, I, language, better b. loves, john, mary c. is the, communication, exchange of, intentional, information brought, by, about, the production, perception of, and signs, from, drawn, a, of, system, signs, conventional,shared d. created, that, we hold these, to be, all men, truths, are, equal, self-evident Which ones could you do?

Token 18674:
What type of knowledge did you draw upon?

Token 18675:
Train a bigram model from a training corpus, and use it to ﬁnd the highest-probability permutation of somesentences from a test corpus.

Token 18676:
Report on the accuracy of this model.

Token 18677:
23.18 Calculate the most probable path through the HMM in Figure 23.16 for the output sequence [C 1,C2,C3,C4,C4,C6,C7]. Also give its probability.

Token 18678:
23.19 We forgot to mention that the text in Exercise 23.1 is entitled “Washing Clothes.” Reread the text and answer the questions in Exercise 23.14.

Token 18679:
Did you do better this time?Bransford and Johnson (1973) used this text in a controlled experiment and found that the titlehelped signiﬁcantly.

Token 18680:
What does this tell you about how language and memory works?

Token 18681:
24PERCEPTION In which we connect the computer to the raw, unwashed world.

Token 18682:
Perception provides agents with information about the world they inhabit by interpreting the PERCEPTION response of sensors .

Token 18683:
A sensor measures some aspect of the environment in a form that can SENSOR be used as input by an agent program.

Token 18684:
The sensor could be as simple as a switch, which gives one bit telling whether it is on or off, or as complex as the eye.

Token 18685:
A variety of sensory modalitiesare available to artiﬁcial agents. Those they share with humans include vision, hearing, andtouch.

Token 18686:
Modalities that are not available to the unaided human include radio, infrared, GPS,and wireless signals.

Token 18687:
Some robots do active sensing , meaning they send out a signal, such as radar or ultrasound, and sense the reﬂection of this signal off of the environment.

Token 18688:
Rather thantrying to cover all of these, this chapter will cover one modality in depth: vision.

Token 18689:
We saw in our description of POMDPs (Section 17.4, page 658) that a model-based decision-theoretic agent in a partially observable environment has a sensor model —a prob- ability distribution P(E|S)over the evidence that its sensors provide, given a state of the world.

Token 18690:
Bayes’ rule can then be used to update the estimation of the state.

Token 18691:
For vision, the sensor model can be broken into two components: An object model OBJECT MODEL describes the objects that inhabit the visual world—people, buildings, trees, cars, etc.

Token 18692:
The object model could include a precise 3D geometric model taken from a computer-aided design(CAD) system, or it could be vague constraints, such as the fact that human eyes are usually 5to 7 cm apart.

Token 18693:
A rendering model describes the physical, geometric, and statistical processes RENDERINGMODEL that produce the stimulus from the world.

Token 18694:
Rendering models are quite accurate, but they are ambiguous.

Token 18695:
For example, a white object under low light may appear as the same color as a black object under intense light.

Token 18696:
A small nearby object may look the same as a large distantobject.

Token 18697:
Without additional evidence, we cannot tell if the image that ﬁlls the frame is a toyGodzilla or a real monster.

Token 18698:
Ambiguity can be managed with prior knowledge—we know Godzilla is not real, so the image must be a toy—or by selectively choosing to ignore the ambiguity.

Token 18699:
For example, the vision system for an autonomous car may not be able to interpret objects that are far inthe distance, but the agent can choose to ignore the problem, because it is unlikely to crashinto an object that is miles away.

Token 18700:
928

Token 18701:
Section 24.1. Image Formation 929 A decision-theoretic agent is not the only architecture that can make use of vision sen- sors.

Token 18702:
For example, fruit ﬂies ( Drosophila ) are in part reﬂex agents: they have cervical giant ﬁbers that form a direct pathway from their visual system to the wing muscles that initiate anescape response—an immediate reaction, without deliberation.

Token 18703:
Flies and many other ﬂyinganimals make used of a closed-loop control architecture to land on an object.

Token 18704:
The visual system extracts an estimate of the distance to the object, and the control system adjusts the wing muscles accordingly, allowing very fast changes of direction, with no need for a detailedmodel of the object.

Token 18705:
Compared to the data from other sensors (such as the single bit that tells the vacuum robot that it has bumped into a wall), visual observations are extraordinarily rich, both inthe detail they can reveal and in the sheer amount of data they produce.

Token 18706:
A video camerafor robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 GB perminute.

Token 18707:
The problem for a vision-capable agent then is: Which aspects of the rich visual stimulus should be considered to help the agent make good action choices, and which aspects should be ignored?

Token 18708:
Vision—and all perception—serves to further the agent’s goals, not as an end to itself. We can characterize three broad approaches to the problem.

Token 18709:
The feature extractionFEATURE EXTRACTION approach, as exhibited by Drosophila , emphasizes simple computations applied directly to the sensor observations.

Token 18710:
In the recognition approach an agent draws distinctions among the RECOGNITION objects it encounters based on visual and other information.

Token 18711:
Recognition could mean labeling each image with a yes or no as to whether it contains food that we should forage, or containsGrandma’s face.

Token 18712:
Finally, in the reconstruction approach an agent builds a geometric model RECONSTRUCTION of the world from an image or a set of images.

Token 18713:
The last thirty years of research have produced powerful tools and methods for ad- dressing these approaches.

Token 18714:
Understanding these methods requires an understanding of the processes by which images are formed.

Token 18715:
Therefore, we now cover the physical and statistical phenomena that occur in the production of an image.

Token 18716:
24.1 I MAGE FORMATION Imaging distorts the appearance of objects.

Token 18717:
For example, a picture taken looking down along straight set of railway tracks will suggest that the rails converge and meet.

Token 18718:
As anotherexample, if you hold your hand in front of your eye, you can block out the moon, which isnot smaller than your hand.

Token 18719:
As you move your hand back and forth or tilt it, your hand willseem to shrink and grow in the image , but it is not doing so in reality (Figure 24.1).

Token 18720:
Models of these effects are essential for both recognition and reconstruction.

Token 18721:
24.1.1 Images without lenses: The pinhole camera Image sensors gather light scattered from objects in a scene and create a two-dimensional SCENE image .

Token 18722:
In the eye, the image is formed on the retina, which consists of two types of cells: IMAGE about 100 million rods, which are sensitive to light at a wide range of wavelengths, and 5

Token 18723:
930 Chapter 24. Perception Figure 24.1 Imaging distorts geometry.

Token 18724:
Parallel lines appear to meet in the distance, as in the image of the railway tracks on the left.

Token 18725:
I n the center, a small hand blocks out most of a large moon.

Token 18726:
On the right is a foreshortening effect: the hand is tilted away from the eye, making it appear shorter than in the center ﬁgure. million cones.

Token 18727:
Cones, which are essential for color vision, are of three main types, each of which is sensitive to a different set of wavelengths.

Token 18728:
In cameras, the image is formed on animage plane, which can be a piece of ﬁlm coated with silver halides or a rectangular gridof a few million photosensitive pixels , each a complementary metal-oxide semiconductor PIXEL (CMOS) or charge-coupled device (CCD).

Token 18729:
Each photon arriving at the sensor produces an effect, whose strength depends on the wavelength of the photon.

Token 18730:
The output of the sensoris the sum of all effects due to photons observed in some time window, meaning that imagesensors report a weighted average of the intensity of light arriving at the sensor.

Token 18731:
To see a focused image, we must ensure that all the photons from approximately the same spot in the scene arrive at approximately the same point in the image plane.

Token 18732:
The simplestway to form a focused image is to view stationary objects with a pinhole camera ,w h i c h PINHOLE CAMERA consists of a pinhole opening, O, at the front of a box, and an image plane at the back of the box (Figure 24.2).

Token 18733:
Photons from the scene must pass through the pinhole, so if it is smallenough then nearby photons in the scene will be nearby in the image plane, and the imagewill be in focus.

Token 18734:
The geometry of scene and image is easiest to understand with the pinhole camera.

Token 18735:
We use a three-dimensional coordinate system with the origin at the pinhole, and consider a point Pin the scene, with coordinates (X,Y,Z ).Pgets projected to the point P /primein the image plane with coordinates (x,y,z).I ffis the distance from the pinhole to the image plane, then by similar triangles, we can derive the following equations: −x f=X Z,−y f=Y Z⇒x=−fX Z,y=−fY Z.

Token 18736:
These equations deﬁne an image-formation process known as perspective projection .N o t ePERSPECTIVE PROJECTION that the Zin the denominator means that the farther away an object is, the smaller its image

Token 18737:
Section 24.1.

Token 18738:
Image Formation 931 fImage plane P′Y X ZP Pinhole Figure 24.2 Each light-sensitive element in the image plane at the back of a pinhole cam- era receives light from a the small range of directions that passes through the pinhole.

Token 18739:
If thepinhole is small enough, the result is a focused image at the back of the pinhole.

Token 18740:
The process of projection means that large, distant objects look the same as smaller, nearby objects. Note that the image is projected upside down.

Token 18741:
will be. Also, note that the minus signs mean that the image is inverted , both left–right and up–down, compared with the scene.

Token 18742:
Under perspective projection, distant objects look small. This is what allows you to cover the moon with your hand (Figure 24.1).

Token 18743:
An important result of this effect is that parallel lines converge to a point on the horizon. (Think of railway tracks, Figure 24.1.)

Token 18744:
A line in the scene in the direction (U,V,W )and passing through the point (X0,Y0,Z0)can be described as the set of points (X0+λU,Y 0+λV,Z 0+λW), with λvarying between −∞ and+∞.

Token 18745:
Different choices of (X0,Y0,Z0)yield different lines parallel to one another.

Token 18746:
The projection of a point Pλfrom this line onto the image plane is given by/parenleftbigg fX0+λU Z0+λW,fY0+λV Z0+λW/parenrightbigg .

Token 18747:
Asλ→∞ orλ→−∞ , this becomes p∞=(fU/W,fV/W )ifW/negationslash=0.T h i sm e a n st h a t two parallel lines leaving different points in space will converge in the image—for large λ, the image points are nearly the same, whatever the value of (X0,Y0,Z0)(again, think railway tracks, Figure 24.1).

Token 18748:
We call p∞thevanishing point associated with the family of straight VANISHING POINT lines with direction (U,V,W ).

Token 18749:
Lines with the same direction share the same vanishing point.

Token 18750:
24.1.2 Lens systems The drawback of the pinhole camera is that we need a small pinhole to keep the image in focus.

Token 18751:
But the smaller the pinhole, the fewer photons get through, meaning the image will bedark.

Token 18752:
We can gather more photons by keeping the pinhole open longer, but then we will getmotion blur —objects in the scene that move will appear blurred because they send photons MOTION BLUR to multiple locations on the image plane.

Token 18753:
If we can’t keep the pinhole open longer, we can try to make it bigger.

Token 18754:
More light will enter, but light from a small patch of object in the scenewill now be spread over a patch on the image plane, causing a blurred image.

Token 18755:
932 Chapter 24.

Token 18756:
Perception Iris Cornea Fovea Visual Axis Optical AxisLens RetinaOptic Nerve Lens SystemImage plane Light Source Figure 24.3 Lenses collect the light leaving a scene point in a range of directions, and steer it all to arrive at a single point on the image plane.

Token 18757:
Focusing works for points lying close to a focal plane in space; other points will not be focused properly.

Token 18758:
In cameras, elements of the lens system move to change the focal plane, whereas in the eye, the shape of the lens ischanged by specialized muscles.

Token 18759:
Vertebrate eyes and modern cameras use a lens system to gather sufﬁcient light while LENS keeping the image in focus.

Token 18760:
A large opening is covered with a lens that focuses light from nearby object locations down to nearby locations in the image plane.

Token 18761:
However, lens systems have a limited depth of ﬁeld : they can focus light only from points that lie within a range DEPTH OF FIELD of depths (centered around a focal plane ).

Token 18762:
Objects outside this range will be out of focus in FOCAL PLANE the image.

Token 18763:
To move the focal plane, the lens in the eye can change shape (Figure 24.3); in a camera, the lenses move back and forth.

Token 18764:
24.1.3 Scaled orthographic projection Perspective effects aren’t always pronounced.

Token 18765:
For example, spots on a distant leopard maylook small because the leopard is far away, but two spots that are next to each other will haveabout the same size.

Token 18766:
This is because the difference in distance to the spots is small comparedto the distance to them, and so we can simplify the projection model.

Token 18767:
The appropriate modelisscaled orthographic projection .

Token 18768:
The idea is as follows: If the depth Zof points on the SCALED ORTHOGRAPHICPROJECTION object varies within some range Z0±ΔZ, with ΔZ/lessmuchZ0, then the perspective scaling factor f/ Z can be approximated by a constant s=f/ Z0.

Token 18769:
The equations for projection from the scene coordinates (X,Y,Z )to the image plane become x=sXandy=sY.

Token 18770:
Scaled orthographic projection is an approximation that is valid only for those parts of the scene withnot much internal depth variation.

Token 18771:
For example, scaled orthographic projection can be a good model for the features on the front of a distant building.

Token 18772:
24.1.4 Light and shading The brightness of a pixel in the image is a function of the brightness of the surface patch in the scene that projects to the pixel.

Token 18773:
We will assume a linear model (current cameras have non-linearities at the extremes of light and dark, but are linear in the middle).

Token 18774:
Image brightness is

Token 18775:
Section 24.1.

Token 18776:
Image Formation 933 Specularities Cast shadowDiffuse reflection, bright Diffuse reflection, dark Figure 24.4 A variety of illumination effects.

Token 18777:
There are specularities on the metal spoon and on the milk. The bright diffuse surface is br ight because it faces the light direction.

Token 18778:
The dark diffuse surface is dark because it is tange ntial to the illumination direction.

Token 18779:
The shadows appear at surface points that cannot see the light source. Photo by Mike Linksvayer (mlinksvaon ﬂickr).

Token 18780:
a strong, if ambiguous, cue to the shape of an object, and from there to its identity.

Token 18781:
People are usually able to distinguish the three main causes of varying brightness and reverse-engineerthe object’s properties.

Token 18782:
The ﬁrst cause is overall intensity of the light.

Token 18783:
Even though a white OVERALL INTENSITY object in shadow may be less bright than a black object in direct sunlight, the eye can distin- guish relative brightness well, and perceive the white object as white.

Token 18784:
Second, different pointsin the scene may reﬂect more or less of the light.

Token 18785:
Usually, the result is that people perceive REFLECT these points as lighter or darker, and so see texture or markings on the object.

Token 18786:
Third, surface patches facing the light are brighter than surface patches tilted away from the light, an effect known as shading .

Token 18787:
Typically, people can tell that this shading comes from the geometry of SHADING the object, but sometimes get shading and markings mixed up.

Token 18788:
For example, a streak of dark makeup under a cheekbone will often look like a shading effect, making the face look thinner.

Token 18789:
Most surfaces reﬂect light by a process of diffuse reﬂection .

Token 18790:
Diffuse reﬂection scat-DIFFUSE REFLECTION ters light evenly across the directions leaving a surface, so the brightness of a diffuse surface doesn’t depend on the viewing direction.

Token 18791:
Most cloth, paints, rough wooden surfaces, vegeta-tion, and rough stone are diffuse.

Token 18792:
Mirrors are not diffuse, because what you see depends onthe direction in which you look at the mirror.

Token 18793:
The behavior of a perfect mirror is known asspecular reﬂection .

Token 18794:
Some surfaces—such as brushed metal, plastic, or a wet ﬂoor—display SPECULAR REFLECTION small patches where specular reﬂection has occurred, called specularities .

Token 18795:
These are easy to SPECULARITIES identify, because they are small and bright (Figure 24.4).

Token 18796:
For almost all purposes, it is enough to model all surfaces as being diffuse with specularities.

Token 18797:
934 Chapter 24. Perception A Bθθ Figure 24.5 Two surface patches are illuminated by a distant point source, whose rays are shown as gray arrowheads.

Token 18798:
Patch A is tilted away from the source ( θis close to 900)a n d collects less energy, because it cuts fewer light rays per unit surface area.

Token 18799:
Patch B, facing the source ( θis close to 00), collects more energy.

Token 18800:
The main source of illumination outside is the sun, whose rays all travel parallel to one another.

Token 18801:
We model this behavior as a distant point light source .

Token 18802:
This is the most importantDISTANT POINT LIGHT SOURCE model of lighting, and is quite effective for indoor scenes as well as outdoor scenes.

Token 18803:
The amount of light collected by a surface patch in this model depends on the angle θbetween the illumination direction and the normal to the surface.

Token 18804:
A diffuse surface patch illuminated by a distant point light source will reﬂect some fraction of the light it collects; this fraction is called the diffuse albedo .

Token 18805:
White paper and DIFFUSE ALBEDO snow have a high albedo, about 0.90, whereas ﬂat black velvet and charcoal have a low albedo of about 0.05 (which means that 95% of the incoming light is absorbed within the ﬁbers ofthe velvet or the pores of the charcoal).

Token 18806:
Lambert’s cosine law states that the brightness of a LAMBERT’S COSINE LAW diffuse patch is given by I=ρI0cosθ, where ρis the diffuse albedo, I0is the intensity of the light source and θis the angle between the light source direction and the surface normal (see Figure 24.5).

Token 18807:
Lampert’s law predictsbright image pixels come from surface patches that face the light directly and dark pixelscome from patches that see the light only tangentially, so that the shading on a surface pro-vides some shape information.

Token 18808:
We explore this cue in Section 24.4.5. If the surface is notreached by the light source, then it is in shadow .

Token 18809:
Shadows are very seldom a uniform black, SHADOW because the shadowed surface receives some light from other sources.

Token 18810:
Outdoors, the most important such source is the sky, which is quite bright. Indoors, light reﬂected from other surfaces illuminates shadowed patches.

Token 18811:
These interreﬂections can have a signiﬁcant effect INTERREFLECTIONS on the brightness of other surfaces, too.

Token 18812:
These effects are sometimes modeled by adding a constant ambient illumination term to the predicted intensity.AMBIENT ILLUMINATION

Token 18813:
Section 24.2. Early Image-Processing Operations 935 24.1.5 Color Fruit is a bribe that a tree offers to animals to carry its seeds around.

Token 18814:
Trees have evolved to have fruit that turns red or yellow when ripe, and animals have evolved to detect these colorchanges.

Token 18815:
Light arriving at the eye has different amounts of energy at different wavelengths;this can be represented by a spectral energy density function.

Token 18816:
Human eyes respond to light inthe 380–750nm wavelength region, with three different types of color receptor cells, whichhave peak receptiveness at 420mm (blue), 540nm (green), and 570nm (red).

Token 18817:
The human eyecan capture only a small fraction of the full spectral energy density function—but it is enough to tell when the fruit is ripe.

Token 18818:
Theprinciple of trichromacy states that for any spectral energy density, no matter how PRINCIPLE OF TRICHROMACY complicated, it is possible to construct another spectral energy density consisting of a mixture of just three colors—usually red, green, and blue—such that a human can’t tell the differencebetween the two.

Token 18819:
That means that our TVs and computer displays can get by with just thethree red/green/blue (or R/G/B) color elements.

Token 18820:
It makes our computer vision algorithmseasier, too. Each surface can be modeled with three different albedos for R/G/B.

Token 18821:
Similarly,each light source can be modeled with three R/G/B intensities. We then apply Lambert’scosine law to each to get three R/G/B pixel values.

Token 18822:
This model predicts, correctly, that thesame surface will produce different colored image patches under different-colored lights.

Token 18823:
Infact, human observers are quite good at ignoring the effects of different colored lights and areable to estimate the color of the surface under white light, an effect known as color constancy .

Token 18824:
COLOR CONSTANCY Quite accurate color constancy algorithms are now available; simple versions show up in the “auto white balance” function of your camera.

Token 18825:
Note that if we wanted to build a camera for mantis shrimp, we would need 12 different pixel colors, corresponding to the 12 types ofcolor receptors of the crustacean.

Token 18826:
24.2 E ARLY IMAGE -PROCESSING OPERATIONS We have seen how light reﬂects off objects in the scene to form an image consisting of, say,ﬁve million 3-byte pixels.

Token 18827:
With all sensors there will be noise in the image, and in any casethere is a lot of data to deal with.

Token 18828:
So how do we get started on analyzing this data?

Token 18829:
In this section we will study three useful image-processing operations: edge detection, texture analysis, and computation of optical ﬂow.

Token 18830:
These are called “early” or “low-level”operations because they are the ﬁrst in a pipeline of operations.

Token 18831:
Early vision operations are characterized by their local nature (they can be carried out in one part of the image without regard for anything more than a few pixels away) and by their lack of knowledge: we canperform these operations without consideration of the objects that might be present in thescene.

Token 18832:
This makes the low-level operations good candidates for implementation in parallelhardware—either in a graphics processor unit (GPU) or an eye.

Token 18833:
We will then look at onemid-level operation: segmenting the image into regions.

Token 18834:
936 Chapter 24.

Token 18835:
Perception AB12 42 11 3 Figure 24.6 Different kinds of edges: (1) depth discontinuities; (2) surface orientation discontinuities; (3) reﬂectance discontinuities; (4) illumination discontinuities (shadows).

Token 18836:
24.2.1 Edge detection Edges are straight lines or curves in the image plane across which there is a “signiﬁcant” EDGE change in image brightness.

Token 18837:
The goal of edge detection is to abstract away from the messy, multimegabyte image and toward a more compact, abstract representation, as in Figure 24.6.The motivation is that edge contours in the image correspond to important scene contours.In the ﬁgure we have three examples of depth discontinuity, labeled 1; two surface-normaldiscontinuities, labeled 2; a reﬂectance discontinuity, labeled 3; and an illumination discon-tinuity (shadow), labeled 4.

Token 18838:
Edge detection is concerned only with the image, and thus doesnot distinguish between these different types of scene discontinuities; later processing will.

Token 18839:
Figure 24.7(a) shows an image of a scene containing a stapler resting on a desk, and (b) shows the output of an edge-detection algorithm on this image.

Token 18840:
As you can see, there is a difference between the output and an ideal line drawing.

Token 18841:
There are gaps where no edge appears, and there are “noise” edges that do not correspond to anything of signiﬁcance in the scene.

Token 18842:
Later stages of processing will have to correct for these errors. How do we detect edges in an image?

Token 18843:
Consider the proﬁle of image brightness along a one-dimensional cross-section perpendicular to an edge—for example, the one between theleft edge of the desk and the wall.

Token 18844:
It looks something like what is shown in Figure 24.8 (top).

Token 18845:
Edges correspond to locations in images where the brightness undergoes a sharp change, so a naive idea would be to differentiate the image and look for places where the magnitudeof the derivative I /prime(x)is large.

Token 18846:
That almost works.

Token 18847:
In Figure 24.8 (middle), we see that there is indeed a peak at x=5 0 , but there are also subsidiary peaks at other locations (e.g., x=7 5 ).

Token 18848:
These arise because of the presence of noise in the image.

Token 18849:
If we smooth the image ﬁrst, thespurious peaks are diminished, as we see in the bottom of the ﬁgure.

Token 18850:
Section 24.2. Early Image-Processing Operations 937 (a) (b) Figure 24.7 (a) Photograph of a stapler. (b) Edges computed from (a).

Token 18851:
0 10 20 30 40 50 60 70 80 90 100−1012 0 10 20 30 40 50 60 70 80 90 100−101 0 10 20 30 40 50 60 70 80 90 100−101 Figure 24.8 Top: Intensity proﬁle I(x)along a one-dimensional section across an edge at x=50 .

Token 18852:
Middle: The derivative of intensity, I/prime(x). Large values of this function correspond to edges, but the function is noisy.

Token 18853:
Bottom: The derivative of a smoothed version of the intensity, (I∗Gσ)/prime, which can be computed in one step as the convolution I∗G/prime σ.

Token 18854:
The noisy candidate edge at x=7 5 has disappeared.

Token 18855:
The measurement of brightness at a pixel in a CCD camera is based on a physical process involving the absorption of photons and the release of electrons; inevitably therewill be statistical ﬂuctuations of the measurement—noise.

Token 18856:
The noise can be modeled with

Token 18857:
938 Chapter 24. Perception a Gaussian probability distribution, with each pixel independent of the others.

Token 18858:
One way to smooth an image is to assign to each pixel the average of its neighbors. This tends to cancelout extreme values.

Token 18859:
But how many neighbors should we consider—one pixel away, or two, ormore?

Token 18860:
One good answer is a weighted average that weights the nearest pixels the most, thengradually decreases the weight for more distant pixels.

Token 18861:
The Gaussian ﬁlter does just that. GAUSSIAN FILTER (Users of Photoshop recognize this as the Gaussian blur operation.)

Token 18862:
Recall that the Gaussian function with standard deviation σand mean 0 is Nσ(x)=1 √ 2πσe−x2/2σ2in one dimension, or Nσ(x,y)=1 2πσ2e−(x2+y2)/2σ2in two dimensions.

Token 18863:
The application of the Gaussian ﬁlter replaces the intensity I(x0,y0)with the sum, over all (x,y)pixels, of I(x,y)Nσ(d),w h e r e dis the distance from (x0,y0)to(x,y).T h i s k i n d o f weighted sum is so common that there is a special name and notation for it.

Token 18864:
We say that thefunction his the convolution of two functions fandg(denoted f∗g)i fw eh a v e CONVOLUTION h(x)=(f∗g)(x)=+∞/summationdisplay u=−∞f(u)g(x−u) in one dimension, or h(x,y)=(f∗g)(x,y)=+∞/summationdisplay u=−∞+∞/summationdisplay v=−∞f(u,v)g(x−u,y−v) in two.

Token 18865:
So the smoothing function is achieved by convolving the image with the Gaussian, I∗Nσ.A σof 1 pixel is enough to smooth over a small amount of noise, whereas 2 pixels will smooth a larger amount, but at the loss of some detail.

Token 18866:
Because the Gaussian’s inﬂuence fades quicklyat a distance, we can replace the ±∞ in the sums with ±3σ.

Token 18867:
We can optimize the computation by combining smoothing and edge ﬁnding into a sin- gle operation.

Token 18868:
It is a theorem that for any functions fandg, the derivative of the convolution, (f∗g) /prime, is equal to the convolution with the derivative, f∗(g/prime).

Token 18869:
So rather than smoothing the image and then differentiating, we can just convolve the image with the derivative of thesmoothing function, N /prime σ.

Token 18870:
We then mark as edges those peaks in the response that are above some threshold.

Token 18871:
There is a natural generalization of this algorithm from one-dimensional cross sections to general two-dimensional images.

Token 18872:
In two dimensions edges may be at any angle θ. Consid- ering the image brightness as a scalar function of the variables x,y, its gradient is a vector ∇I=/parenleftBigg∂I ∂x ∂I ∂y/parenrightBigg =/parenleftbiggIx Iy/parenrightbigg .

Token 18873:
Edges correspond to locations in images where the brightness undergoes a sharp change, and so the magnitude of the gradient, /bardbl∇I/bardbl, should be large at an edge point.

Token 18874:
Of independent interest is the direction of the gradient ∇I /bardbl∇I/bardbl=/parenleftbiggcosθ sinθ/parenrightbigg .

Token 18875:
T h i sg i v e su sa θ=θ(x,y)at every pixel, which deﬁnes the edge orientation at that pixel. ORIENTATION

Token 18876:
Section 24.2.

Token 18877:
Early Image-Processing Operations 939 As in one dimension, to form the gradient we don’t compute ∇I,b u tr a t h e r∇(I∗Nσ), the gradient after smoothing the image by convolving it with a Gaussian.

Token 18878:
And again, theshortcut is that this is equivalent to convolving the image with the partial derivatives of aGaussian.

Token 18879:
Once we have computed the gradient, we can obtain edges by ﬁnding edge pointsand linking them together.

Token 18880:
To tell whether a point is an edge point, we must look at other points a small distance forward and back along the direction of the gradient.

Token 18881:
If the gradient magnitude at one of these points is larger, then we could get a better edge point by shiftingthe edge curve very slightly.

Token 18882:
Furthermore, if the gradient magnitude is too small, the pointcannot be an edge point.

Token 18883:
So at an edge point, the gradient magnitude is a local maximumalong the direction of the gradient, and the gradient magnitude is above a suitable threshold.

Token 18884:
Once we have marked edge pixels by this algorithm, the next stage is to link those pixels that belong to the same edge curves.

Token 18885:
This can be done by assuming that any two neighboringedge pixels with consistent orientations must belong to the same edge curve.

Token 18886:
24.2.2 Texture In everyday language, texture is the visual feel of a surface—what you see evokes what TEXTURE the surface might feel like if you touched it (“texture” has the same root as “textile”).

Token 18887:
In computational vision, texture refers to a spatially repeating pattern on a surface that can besensed visually.

Token 18888:
Examples include the pattern of windows on a building, stitches on a sweater,spots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.Sometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,such as pebbles on a beach, the regularity is only statistical.

Token 18889:
Whereas brightness is a property of individual pixels, the concept of texture makes sense only for a multipixel patch.

Token 18890:
Given such a patch, we could compute the orientation at each pixel, and then characterize the patch by a histogram of orientations.

Token 18891:
The texture of bricks in a wall would have two peaks in the histogram (one vertical and one horizontal), whereas thetexture of spots on a leopard’s skin would have a more uniform distribution of orientations.

Token 18892:
Figure 24.9 shows that orientations are largely invariant to changes in illumination.

Token 18893:
This makes texture an important clue for object recognition, because other clues, such as edges,can yield different results in different lighting conditions.

Token 18894:
In images of textured objects, edge detection does not work as well as it does for smooth objects.

Token 18895:
This is because the most important edges can be lost among the texture elements.Quite literally, we may miss the tiger for the stripes.

Token 18896:
The solution is to look for differences in texture properties, just the way we look for differences in brightness.

Token 18897:
A patch on a tiger and a patch on the grassy background will have very different orientation histograms, allowing usto ﬁnd the boundary curve between them.

Token 18898:
24.2.3 Optical ﬂow Next, let us consider what happens when we have a video sequence, instead of just a single static image.

Token 18899:
When an object in the video is moving, or when the camera is moving relative to an object, the resulting apparent motion in the image is called optical ﬂow .

Token 18900:
Optical ﬂow OPTICAL FLOW describes the direction and speed of motion of features in the image —the optical ﬂow of a

Token 18901:
940 Chapter 24. Perception (a) (b) Figure 24.9 Two images of the same texture of crumpled rice paper, with different illumi- nation levels.

Token 18902:
The gradient vector ﬁeld (at ev ery eighth pixel) is plotted on top of each one.

Token 18903:
Notice that, as the light gets darker, all the gradient vectors get shorter. The vectors do not rotate, so the gradient orientations do not change.

Token 18904:
video of a race car would be measured in pixels per second, not miles per hour. The optical ﬂow encodes useful information about scene structure.

Token 18905:
For example, in a video of scenerytaken from a moving train, distant objects have slower apparent motion than close objects;thus, the rate of apparent motion can tell us something about distance.

Token 18906:
Optical ﬂow alsoenables us to recognize actions. In Figure 24.10(a) and (b), we show two frames from a videoof a tennis player.

Token 18907:
In (c) we display the optical ﬂow vectors computed from these images, showing that the racket and front leg are moving fastest.

Token 18908:
The optical ﬂow vector ﬁeld can be represented at any point (x,y)by its components v x(x,y)in thexdirection and vy(x,y)in theydirection.

Token 18909:
To measure optical ﬂow we need to ﬁnd corresponding points between one time frame and the next.

Token 18910:
A simple-minded techniqueis based on the fact that image patches around corresponding points have similar intensitypatterns.

Token 18911:
Consider a block of pixels centered at pixel p,(x 0,y0), at time t0.T h i s b l o c k of pixels is to be compared with pixel blocks centered at various candidate pixels at (x0+ Dx,y0+Dy)at time t0+Dt.

Token 18912:
One possible measure of similarity is the sum of squared differences (SSD):SUM OF SQUARED DIFFERENCES SSD(Dx,Dy)=/summationdisplay (x,y)(I(x,y,t)−I(x+Dx,y+Dy,t+Dt))2.

Token 18913:
Here, (x,y)ranges over pixels in the block centered at (x0,y0).W e ﬁ n d t h e (Dx,Dy)that minimizes the SSD.

Token 18914:
The optical ﬂow at (x0,y0)is then (vx,vy)=(Dx/Dt,Dy/Dt).N o t e that for this to work, there needs to be some texture or variation in the scene.

Token 18915:
If one is looking at a uniform white wall, then the SSD is going to be nearly the same for the different can-

Token 18916:
Section 24.2. Early Image-Processing Operations 941 Figure 24.10 Two frames of a video sequence.

Token 18917:
On the right is the optical ﬂow ﬁeld cor- responding to the displacement from one frame to the other.

Token 18918:
Note how the movement ofthe tennis racket and the front leg is captured by the directions of the arrows. (Courtesy of Thomas Brox.)

Token 18919:
didate matches, and the algorithm is reduced to making a blind guess.

Token 18920:
The best-performing algorithms for measuring optical ﬂow rely on a variety of additional constraints when thescene is only partially textured.

Token 18921:
24.2.4 Segmentation of images Segmentation is the process of breaking an image into regions of similar pixels.

Token 18922:
Each image SEGMENTATION REGIONS pixel can be associated with certain visual properties, such as brightness, color, and texture.

Token 18923:
Within an object, or a single part of an object, these attributes vary relatively little, whereasacross an inter-object boundary there is typically a large change in one or more of these at-tributes.

Token 18924:
There are two approaches to segmentation, one focusing on detecting the boundariesof these regions, and the other on detecting the regions themselves (Figure 24.11).

Token 18925:
A boundary curve passing through a pixel (x,y)will have an orientation θ, so one way to formalize the problem of detecting boundary curves is as a machine learning classiﬁcation problem.

Token 18926:
Based on features from a local neighborhood, we want to compute the probability P b(x,y,θ)that indeed there is a boundary curve at that pixel along that orientation.

Token 18927:
Consider a circular disk centered at (x,y), subdivided into two half disks by a diameter oriented at θ.

Token 18928:
If there is a boundary at (x,y,θ)the two half disks might be expected to differ signiﬁcantly in their brightness, color, and texture.

Token 18929:
Martin, Fowlkes, and Malik (2004) used features based on differences in histograms of brightness, color, and texture values measured in these twohalf disks, and then trained a classiﬁer.

Token 18930:
For this they used a data set of natural images wherehumans had marked the “ground truth” boundaries, and the goal of the classiﬁer was to markexactly those boundaries marked by humans and no others.

Token 18931:
Boundaries detected by this technique turn out to be signiﬁcantly better than those found using the simple edge-detection technique described previously.

Token 18932:
But still there are two limita-tions.

Token 18933:
(1) The boundary pixels formed by thresholding P b(x,y,θ)are not guaranteed to form closed curves, so this approach doesn’t deliver regions, and (2) the decision making exploits only local context and does not use global consistency constraints.

Token 18934:
942 Chapter 24. Perception (a) (b) (c) (d) Figure 24.11 (a) Original image.

Token 18935:
(b) Boundary contours, where the higher the Pbvalue, the darker the contour.

Token 18936:
(c) Segmentation int o regions, correspondi ng to a ﬁne partition of the image. Regions are rendered in their mean colors.

Token 18937:
(d) Segmentation into regions, corre- sponding to a coarser partition of the image, resulting in fewer regions.

Token 18938:
(Courtesy of PabloArbelaez, Michael Maire, Charles Fowlkes, and Jitendra Malik) The alternative approach is based on trying to “cluster” the pixels into regions based on their brightness, color, and texture.

Token 18939:
Shi and Malik (2000) set this up as a graph partitioning problem. The nodes of the graph correspond to pixels, and edges to connections betweenpixels.

Token 18940:
The weight W ijon the edge connecting a pair of pixels iandjis based on how similar the two pixels are in brightness, color, texture, etc.

Token 18941:
Partitions that minimize a normalized cut criterion are then found.

Token 18942:
Roughly speaking, the criterion for partitioning the graph is tominimize the sum of weights of connections across the groups of pixels and maximize thesum of weights of connections within the groups.

Token 18943:
Segmentation based purely on low-level, local attributes such as brightness and color cannot be expected to deliver the ﬁnal correct boundaries of all the objects in the scene.

Token 18944:
Toreliably ﬁnd object boundaries we need high-level knowledge of the likely kinds of objectsin the scene.

Token 18945:
Representing this knowledge is a topic of active research.

Token 18946:
A popular strategy is to produce an over-segmentation of an image, containing hundreds of homogeneous regions known as superpixels .

Token 18947:
From there, knowledge-based algorithms can take over; they will SUPERPIXELS ﬁnd it easier to deal with hundreds of superpixels rather than millions of raw pixels.

Token 18948:
How to exploit high-level knowledge of objects is the subject of the next section.

Token 18949:
24.3 O BJECT RECOGNITION BY APPEARANCE Appearance is shorthand for what an object tends to look like.

Token 18950:
Some object categories—for APPEARANCE example, baseballs—vary rather little in appearance; all of the objects in the category look about the same under most circumstances.

Token 18951:
In this case, we can compute a set of featuresdescribing each class of images likely to contain the object, then test it with a classiﬁer.

Token 18952:
Section 24.3. Object Recognition by Appearance 943 Other object categories—for example, houses or ballet dancers—vary greatly.

Token 18953:
A house can have different size, color, and shape and can look different from different angles.

Token 18954:
A dancerlooks different in each pose, or when the stage lights change colors.

Token 18955:
A useful abstraction is tosay that some objects are made up of local patterns which tend to move around with respect toone another.

Token 18956:
We can then ﬁnd the object by looking at local histograms of detector responses, which expose whether some part is present but suppress the details of where it is.

Token 18957:
Testing each class of images with a learned classiﬁer is an important general recipe.

Token 18958:
It works extremely well for faces looking directly at the camera, because at low resolutionand under reasonable lighting, all such faces look quite similar.

Token 18959:
The face is round, and quitebright compared to the eye sockets; these are dark, because they are sunken, and the mouth isa dark slash, as are the eyebrows.

Token 18960:
Major changes of illumination can cause some variations inthis pattern, but the range of variation is quite manageable.

Token 18961:
That makes it possible to detectface positions in an image that contains faces.

Token 18962:
Once a computational challenge, this featureis now commonplace in even inexpensive digital cameras.

Token 18963:
For the moment, we will consider only faces where the nose is oriented vertically; we will deal with rotated faces below.

Token 18964:
We sweep a round window of ﬁxed size over the image,compute features for it, and present the features to a classiﬁer.

Token 18965:
This strategy is sometimes called the sliding window .

Token 18966:
Features need to be robust to shadows and to changes in brightness SLIDINGWINDOW caused by illumination changes.

Token 18967:
One strategy is to build features out of gradient orientations. Another is to estimate and correct the illumination in each image window.

Token 18968:
To ﬁnd faces ofdifferent sizes, repeat the sweep over larger or smaller versions of the image.

Token 18969:
Finally, wepostprocess the responses across scales and locations to produce the ﬁnal set of detections.

Token 18970:
Postprocessing is important, because it is unlikely that we have chosen a window size that is exactly the right size for a face (even if we use multiple sizes).

Token 18971:
Thus, we will likelyhave several overlapping windows that each report a match for a face.

Token 18972:
However, if we usea classiﬁer that can report strength of response (for example, logistic regression or a supportvector machine) we can combine these partial overlapping matches at nearby locations toyield a single high-quality match.

Token 18973:
That gives us a face detector that can search over locations and scales. To search rotations as well, we use two steps.

Token 18974:
We train a regression procedure to estimate the best orientation of any face present in a window.

Token 18975:
Now, for each window, weestimate the orientation, reorient the window, then test whether a vertical face is present withour classiﬁer.

Token 18976:
All this yields a system whose architecture is sketched in Figure 24.12. Training data is quite easily obtained.

Token 18977:
There are several data sets of marked-up face images, and rotated face windows are easy to build (just rotate a window from a trainingdata set).

Token 18978:
One trick that is widely used is to take each example window, then produce newexamples by changing the orientation of the window, the center of the window, or the scalevery slightly.

Token 18979:
This is an easy way of getting a bigger data set that reﬂects real images fairlywell; the trick usually improves performance signiﬁcantly.

Token 18980:
Face detectors built along theselines now perform very well for frontal faces (side views are harder).

Token 18981:
944 Chapter 24.

Token 18982:
Perception Image Responses Detections Non-maximal suppresion Correct illuminationEstimate orientation Rotate windowFeatures Classifier Figure 24.12 Face ﬁnding systems vary, but most follow the architecture illustrated in two parts here.

Token 18983:
On the top, we go from images to responses, then apply non-maximumsuppression to ﬁnd the strongest local response.

Token 18984:
The responses are obtained by the process illustrated on the bottom.

Token 18985:
We sweep a window of ﬁxed size over larger and smaller versions of the image, so as to ﬁnd smaller or larger faces, respectively.

Token 18986:
The illumination in the window is corrected, and then a regression engine (quite often, a neural net) predicts the orientation of the face.

Token 18987:
The window is corrected to this orientation and then presented to aclassiﬁer.

Token 18988:
Classiﬁer outputs are then postprocessed to ensure that only one face is placed at each location in the image.

Token 18989:
24.3.1 Complex appearance and pattern elements Many objects produce much more complex patterns than faces do.

Token 18990:
This is because several effects can move features around in an image of the object.

Token 18991:
Effects include (Figure 24.13) •Foreshortening , which causes a pattern viewed at a slant to be signiﬁcantly distorted.

Token 18992:
•Aspect , which causes objects to look different when seen from different directions.

Token 18993:
Even as simple an object as a doughnut has several aspects; seen from the side, it lookslike a ﬂattened oval, but from above it is an annulus.

Token 18994:
•Occlusion , where some parts are hidden from some viewing directions.

Token 18995:
Objects can occlude one another, or parts of an object can occlude other parts, an effect known as self-occlusion.

Token 18996:
•Deformation , where internal degrees of freedom of the object change its appearance.

Token 18997:
For example, people can move their arms and legs around, generating a very wide rangeof different body conﬁgurations.

Token 18998:
However, our recipe of searching across location and scale can still work.

Token 18999:
This is because some structure will be present in the images produced by the object.

Token 19000:
For example, a pictureof a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, thoughthey may be in somewhat different arrangements in different pictures.

Token 19001:
This suggests modelingobjects with pattern elements—collections of parts. These pattern elements may move around

Token 19002:
Section 24.3. Object Recognition by Appearance 945 Foreshortening Aspect Occlusion Deformation Figure 24.13 Sources of appearance variation.

Token 19003:
First, elements can foreshorten, like the circular patch on the top left. This patch is viewed at a slant, and so is elliptical in the image.

Token 19004:
Second, objects viewed from different directions can change shape quite dramatically, a phenomenon known as aspect.

Token 19005:
On the top right are three different aspects of a doughnut.Occlusion causes the handle of the mug on the bottom left to disappear when the mug is rotated.

Token 19006:
In this case, because the body and handle belong to the same mug, we have self- occlusion.

Token 19007:
Finally, on the bottom right, some objects can deform dramatically.

Token 19008:
with respect to one another, but if most of the pattern elements are present in about the right place, then the object is present.

Token 19009:
An object recognizer is then a collection of features that cantell whether the pattern elements are present, and whether they are in about the right place.

Token 19010:
The most obvious approach is to represent the image window with a histogram of the pattern elements that appear there.

Token 19011:
This approach does not work particularly well, becausetoo many patterns get confused with one another.

Token 19012:
For example, if the pattern elements are color pixels, the French, UK, and Netherlands ﬂags will get confused because they have approximately the same color histograms, though the colors are arranged in very differentways.

Token 19013:
Quite simple modiﬁcations of histograms yield very useful features.

Token 19014:
The trick is topreserve some spatial detail in the representation; for example, headlights tend to be at thefront of a car and wheels tend to be at the bottom.

Token 19015:
Histogram-based features have beensuccessful in a wide variety of recognition applications; we will survey pedestrian detection.

Token 19016:
24.3.2 Pedestrian detection with HOG features The World Bank estimates that each year car accidents kill about 1.2 million people, of whomabout two thirds are pedestrians.

Token 19017:
This means that detecting pedestrians is an important appli-cation problem, because cars that can automatically detect and avoid pedestrians might savemany lives.

Token 19018:
Pedestrians wear many different kinds of clothing and appear in many differentconﬁgurations, but, at relatively low resolution, pedestrians can have a fairly characteristicappearance.

Token 19019:
The most usual cases are lateral or frontal views of a walk. In these cases,

Token 19020:
946 Chapter 24.

Token 19021:
Perception Image Orientation histogramsPositive componentsNegative components Figure 24.14 Local orientation histograms are a powerful feature for recognizing even quite complex objects.

Token 19022:
On the left, an image of a pedestrian. On the center left, local orien- tation histograms for patches.

Token 19023:
We then apply a classiﬁer such as a support vector machine to ﬁnd the weights for each histogram that best separate the positive examples of pedestriansfrom non-pedestrians.

Token 19024:
We see that the positive ly weighted components look like the outline of a person.

Token 19025:
The negative components are less clear; they represent all the patterns that are not pedestrians.

Token 19026:
Figure from Dalal and Triggs (2005) c/circlecopyrtIEEE.

Token 19027:
we see either a “lollipop” shape — the torso is wider than the legs, which are together in the stance phase of the walk — or a “scissor” shape — where the legs are swinging in thewalk.

Token 19028:
We expect to see some evidence of arms and legs, and the curve around the shouldersand head also tends to visible and quite distinctive.

Token 19029:
This means that, with a careful featureconstruction, we can build a useful moving-window pedestrian detector.

Token 19030:
There isn’t always a strong contrast between the pedestrian and the background, so it is better to use orientations than edges to represent the image window.

Token 19031:
Pedestrians can move their arms and legs around, so we should use a histogram to suppress some spatial detail inthe feature.

Token 19032:
We break up the window into cells, which could overlap, and build an orientationhistogram in each cell.

Token 19033:
Doing so will produce a feature that can tell whether the head-and-shoulders curve is at the top of the window or at the bottom, but will not change if the headmoves slightly.

Token 19034:
One further trick is required to make a good feature.

Token 19035:
Because orientation features are not affected by illumination brightness, we cannot treat high-contrast edges specially.

Token 19036:
Thismeans that the distinctive curves on the boundary of a pedestrian are treated in the same wayas ﬁne texture detail in clothing or in the background, and so the signal may be submergedin noise.

Token 19037:
We can recover contrast information by counting gradient orientations with weights that reﬂect how signiﬁcant a gradient is compared to other gradients in the same cell.

Token 19038:
We will write|| ∇I x||for the gradient magnitude at point xin the image, write Cfor the cell whose histogram we wish to compute, and write wx,Cfor the weight that we will use for the

Token 19039:
Section 24.4.

Token 19040:
Reconstructing the 3D World 947 Figure 24.15 Another example of object recognition, this one using the SIFT feature (Scale Invariant Feature Transform), an earlier version of the HOG feature.

Token 19041:
On the left,i m - ages of a shoe and a telephone that serve as object models. In the center , a test image.

Token 19042:
On the right , the shoe and the telephone have been detected by: ﬁnding points in the image whose SIFT feature descriptions match a model; computing an estimate of pose of the model; andverifying that estimate.

Token 19043:
A strong match is usually veriﬁed with rare false positives. Images from Lowe (1999) c/circlecopyrtIEEE. orientation at xfor this cell.

Token 19044:
A natural choice of weight is wx,C=||∇Ix|| /summationtext u∈C||∇Iu||.

Token 19045:
This compares the gradient magnitude to others in the cell, so gradients that are large com- pared to their neighbors get a large weight.

Token 19046:
The resulting feature is usually called a HOG feature (for Histogram Of Gradient orientations).

Token 19047:
HOG FEATURE This feature construction is the main way in which pedestrian detection differs from face detection.

Token 19048:
Otherwise, building a pedestrian detector is very like building a face detector.The detector sweeps a window across the image, computes features for that window, thenpresents it to a classiﬁer.

Token 19049:
Non-maximum suppression needs to be applied to the output. Inmost applications, the scale and orientation of typical pedestrians is known.

Token 19050:
For example, indriving applications in which a camera is ﬁxed to the car, we expect to view mainly verticalpedestrians, and we are interested only in nearby pedestrians.

Token 19051:
Several pedestrian data setshave been published, and these can be used for training the classiﬁer.

Token 19052:
Pedestrians are not the only type of object we can detect.

Token 19053:
In Figure 24.15 we see that similar techniques can be used to ﬁnd a variety of objects in different contexts.

Token 19054:
24.4 R ECONSTRUCTING THE 3D W ORLD In this section we show how to go from the two-dimensional image to a three-dimensionalrepresentation of the scene.

Token 19055:
The fundamental question is this: Given that all points in thescene that fall along a ray to the pinhole are projected to the same point in the image, how dowe recover three-dimensional information?

Token 19056:
Two ideas come to our rescue:

Token 19057:
948 Chapter 24.

Token 19058:
Perception •If we have two (or more) images from different camera positions, then we can triangu- late to ﬁnd the position of a point in the scene.

Token 19059:
•We can exploit background knowledge about the physical scene that gave rise to the image.

Token 19060:
Given an object model P(Scene )and a rendering model P(Image|Scene ),w e can compute a posterior distribution P(Scene|Image ).

Token 19061:
There is as yet no single uniﬁed theory for scene reconstruction.

Token 19062:
We survey eight commonly used visual cues: motion ,binocular stereopsis ,multiple views ,texture ,shading ,contour , andfamiliar objects .

Token 19063:
24.4.1 Motion parallax If the camera moves relative to the three-dimensional scene, the resulting apparent motion in the image, optical ﬂow, can be a source of information for both the movement of the camera and depth in the scene.

Token 19064:
To understand this, we state (without proof) an equation that relatesthe optical ﬂow to the viewer’s translational velocity Tand the depth in the scene.

Token 19065:
The components of the optical ﬂow ﬁeld are v x(x,y)=−Tx+xTz Z(x,y),v y(x,y)=−Ty+yTz Z(x,y), where Z(x,y)is thez-coordinate of the point in the scene corresponding to the point in the image at (x,y).

Token 19066:
Note that both components of the optical ﬂow, vx(x,y)andvy(x,y), are zero at the pointx=Tx/Tz,y=Ty/Tz.

Token 19067:
This point is called the focus of expansion of the ﬂowFOCUS OF EXPANSION ﬁeld.

Token 19068:
Suppose we change the origin in the x–yplane to lie at the focus of expansion; then the expressions for optical ﬂow take on a particularly simple form.

Token 19069:
Let (x/prime,y/prime)be the new coordinates deﬁned by x/prime=x−Tx/Tz,y/prime=y−Ty/Tz.T h e n vx(x/prime,y/prime)=x/primeTz Z(x/prime,y/prime),v y(x/prime,y/prime)=y/primeTz Z(x/prime,y/prime).

Token 19070:
Note that there is a scale-factor ambiguity here.

Token 19071:
If the camera was moving twice as fast, and every object in the scene was twice as big and at twice the distance to the camera, the opticalﬂow ﬁeld would be exactly the same.

Token 19072:
But we can still extract quite useful information. 1.

Token 19073:
Suppose you are a ﬂy trying to land on a wall and you want to know the time-to- contact at the current velocity. This time is given by Z/T z.

Token 19074:
Note that although the instantaneous optical ﬂow ﬁeld cannot provide either the distance Zor the velocity component Tz, it can provide the ratio of the two and can therefore be used to control the landing approach.

Token 19075:
There is considerable experimental evidence that many differentanimal species exploit this cue. 2. Consider two points at depths Z 1,Z2, respectively.

Token 19076:
We may not know the absolute value of either of these, but by considering the inverse of the ratio of the optical ﬂow magnitudes at these points, we can determine the depth ratio Z1/Z2.

Token 19077:
This is the cue of motion parallax, one we use when we look out of the side window of a moving car ortrain and infer that the slower moving parts of the landscape are farther away.

Token 19078:
Section 24.4.

Token 19079:
Reconstructing the 3D World 949 Perceived object Right image (a) (b)Left image DisparityLeft Right Figure 24.16 Translating a camera parallel to the image plane causes image features to move in the camera plane.

Token 19080:
The disparity in positions that results is a cue to depth. If we superimpose left and right image, as in (b), we see the disparity.

Token 19081:
24.4.2 Binocular stereopsis Most vertebrates have twoeyes. This is useful for redundancy in case of a lost eye, but it helps in other ways too.

Token 19082:
Most prey have eyes on the side of the head to enable a wider ﬁeld of vision.

Token 19083:
Predators have the eyes in the front, enabling them to use binocular stereopsis .BINOCULAR STEREOPSIS The idea is similar to motion parallax, except that instead of using images over time, we use two (or more) images separated in space.

Token 19084:
Because a given feature in the scene will be in adifferent place relative to the z-axis of each image plane, if we superpose the two images, there will be a disparity in the location of the image feature in the two images.

Token 19085:
You can see DISPARITY this in Figure 24.16, where the nearest point of the pyramid is shifted to the left in the right image and to the right in the left image.

Token 19086:
Note that to measure disparity we need to solve the correspondence problem, that is, determine for a point in the left image, the point in the right image that results from theprojection of the same scene point.

Token 19087:
This is analogous to what one has to do in measuringoptical ﬂow, and the most simple-minded approaches are somewhat similar and based on comparing blocks of pixels around corresponding points using the sum of squared differences.

Token 19088:
In practice, we use much more sophisticated algorithms, which exploit additional constraints.

Token 19089:
Assuming that we can measure disparity, how does this yield information about depth in the scene?

Token 19090:
We will need to work out the geometrical relationship between disparity anddepth.

Token 19091:
First, we will consider the case when both the eyes (or cameras) are looking forwardwith their optical axes parallel.

Token 19092:
The relationship of the right camera to the left camera is thenjust a displacement along the x-axis by an amount b, the baseline.

Token 19093:
We can use the optical ﬂow equations from the previous section, if we think of this as resulting from a translation

Token 19094:
950 Chapter 24. Perception bδθ/2 δZP P0 PRPLLeft eye ZRight eyeθ Figure 24.17 The relation between disparity and depth in stereopsis.

Token 19095:
The centers of pro- jection of the two eyes are bapart, and the optical axes in tersect at the ﬁxation point P0.T h e pointPin the scene projects to points PLandPRin the two eyes.

Token 19096:
In angular terms, the disparity between these is δθ.S e et e x t . vector Tacting for time δt, with Tx=b/δt andTy=Tz=0.

Token 19097:
The horizontal and vertical disparity are given by the optical ﬂow components, multiplied by the time step δt,H=vxδt, V=vyδt.

Token 19098:
Carrying out the substitutions, we get the result that H=b/Z,V=0.

Token 19099:
In words, the horizontal disparity is equal to the ratio of the baseline to the depth, and the verticaldisparity is zero.

Token 19100:
Given that we know b, we can measure Hand recover the depth Z.

Token 19101:
Under normal viewing conditions, humans ﬁxate ; that is, there is some point in the FIXATE scene at which the optical axes of the two eyes intersect.

Token 19102:
Figure 24.17 shows two eyes ﬁxated at a point P0, which is at a distance Zfrom the midpoint of the eyes.

Token 19103:
For convenience, we will compute the angular disparity, measured in radians. The disparity at the point of ﬁxation P0is zero.

Token 19104:
For some other point Pin the scene that is δZfarther away, we can compute the angular displacements of the left and right images of P, which we will call PL andPR, respectively.

Token 19105:
If each of these is displaced by an angle δθ/2relative to P0, then the displacement between PLandPR, which is the disparity of P,i sj u s t δθ.

Token 19106:
From Figure 24.17, tanθ=b/2 Zandtan(θ−δθ/2) =b/2 Z+δZ, but for small angles, tanθ≈θ,s o δθ/2=b/2 Z−b/2 Z+δZ≈bδZ 2Z2 and, since the actual disparity is δθ,w eh a v e disparity =bδZ Z2.

Token 19107:
In humans, b(thebaseline distance between the eyes) is about 6 cm. Suppose that Zis about BASELINE 100 cm.

Token 19108:
If the smallest detectable δθ(corresponding to the pixel size) is about 5 seconds of arc, this gives a δZof 0.4 mm.

Token 19109:
For Z=3 0 cm, we get the impressively small value δZ=0.036mm.

Token 19110:
That is, at a distance of 30 cm, humans can discriminate depths that differ by as little as 0.036 mm, enabling us to thread needles and the like.

Token 19111:
Section 24.4.

Token 19112:
Reconstructing the 3D World 951 Figure 24.18 (a) Four frames from a video sequence in which the camera is moved and rotated relative to the object.

Token 19113:
(b) The ﬁrst frame of the sequence, annotated with small boxes highlighting the features found by the feature detector. (Courtesy of Carlo Tomasi.)

Token 19114:
24.4.3 Multiple views Shape from optical ﬂow or binocular disparity are two instances of a more general framework, that of exploiting multiple views for recovering depth.

Token 19115:
In computer vision, there is no reasonfor us to be restricted to differential motion or to only use two cameras converging at a ﬁxationpoint.

Token 19116:
Therefore, techniques have been developed that exploit the information available inmultiple views, even from hundreds or thousands of cameras.

Token 19117:
Algorithmically, there arethree subproblems that need to be solved: •The correspondence problem, i.e., identifying features in the different images that are projections of the same feature in the three-dimensional world.

Token 19118:
•The relative orientation problem, i.e., determining the transformation (rotation and translation) between the coordinate systems ﬁxed to the different cameras.

Token 19119:
•The depth estimation problem, i.e., determining the depths of various points in the world for which image plane projections were available in at least two views The development of robust matching procedures for the correspondence problem, accompa- nied by numerically stable algorithms for solving for relative orientations and scene depth, isone of the success stories of computer vision.

Token 19120:
Results from one such approach due to Tomasiand Kanade (1992) are shown in Figures 24.18 and 24.19.

Token 19121:
24.4.4 Texture Earlier we saw how texture was used for segmenting objects. It can also be used to estimatedistances.

Token 19122:
In Figure 24.20 we see that a homogeneous texture in the scene results in varyingtexture elements, or texels , in the image.

Token 19123:
All the paving tiles in (a) are identical in the scene. TEXEL They appear different in the image for two reasons:

Token 19124:
952 Chapter 24.

Token 19125:
Perception (a) (b) Figure 24.19 (a) Three-dimensional reconstruction of the locations of the image features in Figure 24.18, shown from above.

Token 19126:
(b) The real house, taken from the same position. 1.Differences in the distances of the texels from the camera.

Token 19127:
Distant objects appear smaller by a scaling factor of 1/Z. 2.Differences in the foreshortening of the texels.

Token 19128:
If all the texels are in the ground plane then distance ones are viewed at an angle that is farther off the perpendicular, and soare more foreshortened.

Token 19129:
The magnitude of the foreshortening effect is proportional tocosσ,w h e r e σis the slant, the angle between the Z-axis and n, the surface normal to the texel.

Token 19130:
Researchers have developed various algorithms that try to exploit the variation in the appearance of the projected texels as a basis for determining surface normals.

Token 19131:
However, the accuracy and applicability of these algorithms is not anywhere as general as those based onusing multiple views.

Token 19132:
24.4.5 Shading Shading—variation in the intensity of light received from different portions of a surface in ascene—is determined by the geometry of the scene and by the reﬂectance properties of thesurfaces.

Token 19133:
In computer graphics, the objective is to compute the image brightness I(x,y), given the scene geometry and reﬂectance properties of the objects in the scene.

Token 19134:
Computervision aims to invert the process—that is, to recover the geometry and reﬂectance properties,given the image brightness I(x,y).

Token 19135:
This has proved to be difﬁcult to do in anything but the simplest cases.

Token 19136:
From the physical model of section 24.1.4, we know that if a surface normal points toward the light source, the surface is brighter, and if it points away, the surface is darker.We cannot conclude that a dark patch has its normal pointing away from the light; instead,it could have low albedo.

Token 19137:
Generally, albedo changes quite quickly in images, and shading

Token 19138:
Section 24.4. Reconstructing the 3D World 953 (a) (b) Figure 24.20 (a) A textured scene.

Token 19139:
Assuming that the real texture is uniform allows recov- ery of the surface orientation.

Token 19140:
The computed surface orientation is indicated by overlaying a black circle and pointer, transformed as if the circle were painted on the surface at that point.

Token 19141:
(b) Recovery of shape from texture for a curved surface (white circle and pointer this time).Images courtesy of Jitendra Malik and Ruth Rosenholtz (1994).

Token 19142:
changes rather slowly, and humans seem to be quite good at using this observation to tell whether low illumination, surface orientation, or albedo caused a surface patch to be dark.To simplify the problem, let us assume that the albedo is known at every surface point.

Token 19143:
It is still difﬁcult to recover the normal, because the image brightness is one measurement but the normal has two unknown parameters, so we cannot simply solve for the normal.

Token 19144:
The keyto this situation seems to be that nearby normals will be similar, because most surfaces aresmooth—they do not have sharp changes.

Token 19145:
The real difﬁculty comes in dealing with interreﬂections.

Token 19146:
If we consider a typical indoor scene, such as the objects inside an ofﬁce, surfaces are illuminated not only by the lightsources, but also by the light reﬂected from other surfaces in the scene that effectively serveas secondary light sources.

Token 19147:
These mutual illumination effects are quite signiﬁcant and makeit quite difﬁcult to predict the relationship between the normal and the image brightness.

Token 19148:
Twosurface patches with the same normal might have quite different brightnesses, because onereceives light reﬂected from a large white wall and the other faces only a dark bookcase.

Token 19149:
Despite these difﬁculties, the problem is important.

Token 19150:
Humans seem to be able to ignore the effects of interreﬂections and get a useful perception of shape from shading, but we know frustratingly little about algorithms to do this.

Token 19151:
24.4.6 Contour When we look at a line drawing, such as Figure 24.21, we get a vivid perception of three-dimensional shape and layout. How?

Token 19152:
It is a combination of recognition of familiar objects inthe scene and the application of generic constraints such as the following: •Occluding contours, such as the outlines of the hills.

Token 19153:
One side of the contour is nearer to the viewer, the other side is farther away. Features such as local convexity and sym-

Token 19154:
954 Chapter 24. Perception Figure 24.21 An evocative line drawing. (Courtesy of Isha Malik.)

Token 19155:
metry provide cues to solving the ﬁgure-ground problem—assigning which side of the FIGURE-GROUND contour is ﬁgure (nearer), and which is ground (farther).

Token 19156:
At an occluding contour, the line of sight is tangential to the surface in the scene. •T-junctions.

Token 19157:
When one object occludes another, the contour of the farther object is interrupted, assuming that the nearer object is opaque.

Token 19158:
A T-junction results in the image. •Position on the ground plane.

Token 19159:
Humans, like many other terrestrial animals, are very often in a scene that contains a ground plane , with various objects at different locations GROUND PLANE on this plane.

Token 19160:
Because of gravity, typical objects don’t ﬂoat in air but are supported by this ground plane, and we can exploit the very special geometry of this viewing scenario.

Token 19161:
Let us work out the projection of objects of different heights and at different loca- tions on the ground plane.

Token 19162:
Suppose that the eye, or camera, is at a height hcabove the ground plane.

Token 19163:
Consider an object of height δYresting on the ground plane, whose bottom is at (X,−hc,Z)and top is at (X,δY−hc,Z).

Token 19164:
The bottom projects to the image point (fX/ Z,−fhc/Z)and the top to (fX/ Z,f (δY−hc)/Z).

Token 19165:
The bottoms of nearer objects (small Z) project to points lower in the image plane; farther objects have bottoms closer to the horizon.

Token 19166:
24.4.7 Objects and the geometric structure of scenes A typical adult human head is about 9 inches long.

Token 19167:
This means that for someone who is 43feet away, the angle subtended by the head at the camera is 1 degree.

Token 19168:
If we see a person whosehead appears to subtend just half a degree, Bayesian inference suggests we are looking at anormal person who is 86 feet away, rather than someone with a half-size head.

Token 19169:
This line ofreasoning supplies us with a method to check the results of a pedestrian detector, as well as amethod to estimate the distance to an object.

Token 19170:
For example, all pedestrians are about the sameheight, and they tend to stand on a ground plane.

Token 19171:
If we know where the horizon is in an image,we can rank pedestrians by distance to the camera. This works because we know where their

Token 19172:
Section 24.4.

Token 19173:
Reconstructing the 3D World 955 Image plane Horizon Ground plane ABC AC B Figure 24.22 In an image of people standing on a ground plane, the people whose feet are closer to the horizon in the image must be farther away (top drawing).

Token 19174:
This means they must look smaller in the image (left lower drawing).

Token 19175:
This means that the size and location of real pedestrians in an image depend upon one a nother and on the location of the horizon.

Token 19176:
To exploit this, we need to identify the ground plane, which is done using shape-from-texture methods.

Token 19177:
From this information, and from some likely pedestrians, we can recover a horizon as shown in the center image.

Token 19178:
On the right, acceptable pedestrian boxes given this geometriccontext. Notice that pedestrians who are higher in the scene must be smaller.

Token 19179:
If they are not, then they are false positives. Images from Hoiem et al. (2008) c/circlecopyrtIEEE.

Token 19180:
feet are, and pedestrians whose feet are closer to the horizon in the image are farther away from the camera (Figure 24.22).

Token 19181:
Pedestrians who are farther away from the camera must alsobe smaller in the image.

Token 19182:
This means we can rule out some detector responses — if a detectorﬁnds a pedestrian who is large in the image and whose feet are close to the horizon, it hasfound an enormous pedestrian; these don’t exist, so the detector is wrong.

Token 19183:
In fact, many ormost image windows are not acceptable pedestrian windows, and need not even be presentedto the detector.

Token 19184:
There are several strategies for ﬁnding the horizon, including searching for a roughly horizontal line with a lot of blue above it, and using surface orientation estimates obtained from texture deformation.

Token 19185:
A more elegant strategy exploits the reverse of our geometricconstraints.

Token 19186:
A reasonably reliable pedestrian detector is capable of producing estimates of thehorizon, if there are several pedestrians in the scene at different distances from the camera.This is because the relative scaling of the pedestrians is a cue to where the horizon is.

Token 19187:
So wecan extract a horizon estimate from the detector, then use this estimate to prune the pedestriandetector’s mistakes.

Token 19188:
956 Chapter 24.

Token 19189:
Perception If the object is familiar, we can estimate more than just the distance to it, because what it looks like in the image depends very strongly on its pose, i.e., its position and orientation withrespect to the viewer.

Token 19190:
This has many applications. For instance, in an industrial manipulationtask, the robot arm cannot pick up an object until the pose is known.

Token 19191:
In the case of rigidobjects, whether three-dimensional or two-dimensional, this problem has a simple and well- deﬁned solution based on the alignment method , which we now develop.

Token 19192:
ALIGNMENT METHOD The object is represented by Mfeatures or distinguished points m1,m2,...,m Min three-dimensional space—perhaps the vertices of a polyhedral object.

Token 19193:
These are measuredin some coordinate system that is natural for the object.

Token 19194:
The points are then subjected toan unknown three-dimensional rotation R, followed by translation by an unknown amount t and then projection to give rise to image feature points p 1,p2,...,p Non the image plane.

Token 19195:
In general, N/negationslash=M, because some model points may be occluded, and the feature detector could miss some features (or invent false ones due to noise).

Token 19196:
We can express this as pi=Π ( Rmi+t)=Q(mi) for a three-dimensional model point miand the corresponding image point pi.

Token 19197:
Here, R is a rotation matrix, tis a translation, and Πdenotes perspective projection or one of its approximations, such as scaled orthographic projection.

Token 19198:
The net result is a transformation Q that will bring the model point miinto alignment with the image point pi.

Token 19199:
Although we do not know Qinitially, we do know (for rigid objects) that Qmust be the same for all the model points.

Token 19200:
We can solve for Q, given the three-dimensional coordinates of three model points and their two-dimensional projections.

Token 19201:
The intuition is as follows: we can write down equations relating the coordinates of pito those of mi.

Token 19202:
In these equations, the unknown quantities correspond to the parameters of the rotation matrix Rand the translation vector t.I fw eh a v e enough equations, we ought to be able to solve for Q.

Token 19203:
We will not give a proof here; we merely state the following result: Given three noncollinear points m1,m2,a n dm3in the model, and their scaled orthographic projections p1,p2,a n dp3on the image plane, there exist exactly two transformations from the three-dimensional model coordinate frame to a two- dimensional image coordinate frame.

Token 19204:
These transformations are related by a reﬂection around the image plane and can be computed by a simple closed-form solution.

Token 19205:
If we could identify the corresponding model features forthree features in the image, we could compute Q, the pose of the object.

Token 19206:
Let us specify position and orientation in mathematical terms.

Token 19207:
The position of a point P in the scene is characterized by three numbers, the (X,Y,Z )coordinates of Pin a coordinate frame with its origin at the pinhole and the Z-axis along the optical axis (Figure 24.2 on page 931).

Token 19208:
What we have available is the perspective projection (x,y)of the point in the image.

Token 19209:
This speciﬁes the ray from the pinhole along which Plies; what we do not know is the distance.

Token 19210:
The term “orientation” could be used in two senses: 1.The orientation of the object as a whole.

Token 19211:
This can be speciﬁed in terms of a three- dimensional rotation relating its coordinate frame to that of the camera.

Token 19212:
Section 24.5.

Token 19213:
Object Recognition from Structural Information 957 2.The orientation of the surface of the object at P.This can be speciﬁed by a normal vector, n—which is a vector specifying the direction that is perpendicular to the surface.

Token 19214:
Often we express the surface orientation using the variables slant andtilt.

Token 19215:
Slant is the SLANT TILT angle between the Z-axis and n. Tilt is the angle between the X-axis and the projection ofnon the image plane.

Token 19216:
When the camera moves relative to an object, both the object’s distance and its orientation change. What is preserved is the shape of the object.

Token 19217:
If the object is a cube, that fact is SHAPE not changed when the object moves.

Token 19218:
Geometers have been attempting to formalize shape for centuries, the basic concept being that shape is what remains unchanged under some group of transformations—for example, combinations of rotations and translations.

Token 19219:
The difﬁculty lies in ﬁnding a representation of global shape that is general enough to deal with the wide varietyof objects in the real world—not just simple forms like cylinders, cones, and spheres—and yetcan be recovered easily from the visual input.

Token 19220:
The problem of characterizing the local shape of a surface is much better understood.

Token 19221:
Essentially, this can be done in terms of curvature:how does the surface normal change as one moves in different directions on the surface?

Token 19222:
Fora plane, there is no change at all.

Token 19223:
For a cylinder, if one moves parallel to the axis, there isno change, but in the perpendicular direction, the surface normal rotates at a rate inverselyproportional to the radius of the cylinder, and so on.

Token 19224:
All this is studied in the subject calleddifferential geometry.

Token 19225:
The shape of an object is relevant for some manipulation tasks (e.g., deciding where to grasp an object), but its most signiﬁcant role is in object recognition, where geometric shape along with color and texture provide the most signiﬁcant cues to enable us to identify objects, classify what is in the image as an example of some class one has seen before, and so on.

Token 19226:
24.5 O BJECT RECOGNITION FROM STRUCTURAL INFORMATION Putting a box around pedestrians in an image may well be enough to avoid driving into them.We have seen that we can ﬁnd a box by pooling the evidence provided by orientations, usinghistogram methods to suppress potentially confusing spatial detail.

Token 19227:
If we want to know moreabout what someone is doing, we will need to know where their arms, legs, body, and head liein the picture.

Token 19228:
Individual body parts are quite difﬁcult to detect on their own using a movingwindow method, because their color and texture can vary widely and because they are usuallysmall in images.

Token 19229:
Often, forearms and shins are as small as two to three pixels wide.

Token 19230:
Bodyparts do not usually appear on their own, and representing what is connected to what could be quite powerful, because parts that are easy to ﬁnd might tell us where to look for parts that are small and hard to detect.

Token 19231:
Inferring the layout of human bodies in pictures is an important task in vision, because the layout of the body often reveals what people are doing.

Token 19232:
A model called a deformable template can tell us which conﬁgurations are acceptable: the elbow can bend but the head is DEFORMABLE TEMPLATE never joined to the foot.

Token 19233:
The simplest deformable template model of a person connects lower arms to upper arms, upper arms to the torso, and so on.

Token 19234:
There are richer models: for example,

Token 19235:
958 Chapter 24.

Token 19236:
Perception we could represent the fact that left and right upper arms tend to have the same color and texture, as do left and right legs.

Token 19237:
These richer models remain difﬁcult to work with, however.

Token 19238:
24.5.1 The geometry of bodies: Finding arms and legs For the moment, we assume that we know what the person’s body parts look like (e.g., weknow the color and texture of the person’s clothing).

Token 19239:
We can model the geometry of thebody as a tree of eleven segments (upper and lower left and right arms and legs respectively,a torso, a face, and hair on top of the face) each of which is rectangular.

Token 19240:
We assume thatthe position and orientation ( pose ) of the left lower arm is independent of all other segments POSE given the pose of the left upper arm; that the pose of the left upper arm is independent of all segments given the pose of the torso; and extend these assumptions in the obvious way to include the right arm and the legs, the face, and the hair.

Token 19241:
Such models are often called “cardboard people” models. The model forms a tree, which is usually rooted at the torso.

Token 19242:
Wewill search the image for the best match to this cardboard person using inference methods fora tree-structured Bayes net (see Chapter 14).

Token 19243:
There are two criteria for evaluating a conﬁguration. First, an image rectangle should look like its segment.

Token 19244:
For the moment, we will remain vague about precisely what that means,but we assume we have a function φ ithat scores how well an image rectangle matches a body segment.

Token 19245:
For each pair of related segments, we have another function ψthat scores how well relations between a pair of image rectangles match those to be expected from the bodysegments.

Token 19246:
The dependencies between segments form a tree, so each segment has only oneparent, and we could write ψ i,pa(i).

Token 19247:
All the functions will be larger if the match is better, so we can think of them as being like a log probability.

Token 19248:
The cost of a particular match that allocates image rectangle mito body segment iis then /summationdisplay i∈segmentsφi(mi)+/summationdisplay i∈segmentsψi,pa(i)(mi,mpa(i)).

Token 19249:
Dynamic programming can ﬁnd the best match, because the relational model is a tree.

Token 19250:
It is inconvenient to search a continuous space, and we will discretize the space of image rectangles.

Token 19251:
We do so by discretizing the location and orientation of rectangles of ﬁxed size(the sizes may be different for different segments).

Token 19252:
Because ankles and knees are different,we need to distinguish between a rectangle and the same rectangle rotated by 180 ◦.O n e could visualize the result as a set of very large stacks of small rectangles of image, cut out atdifferent locations and orientations.

Token 19253:
There is one stack per segment. We must now ﬁnd thebest allocation of rectangles to segments.

Token 19254:
This will be slow, because there are many imagerectangles and, for the model we have given, choosing the right torso will be O(M 6)if there areMimage rectangles.

Token 19255:
However, various speedups are available for an appropriate choice ofψ, and the method is practical (Figure 24.23).

Token 19256:
The model is usually known as a pictorial structure model .PICTORIAL STRUCTURE MODEL Recall our assumption that we know what we need to know about what the person looks like.

Token 19257:
If we are matching a person in a single image, the most useful feature for scoring seg-ment matches turns out to be color.

Token 19258:
Texture features don’t work well in most cases, becausefolds on loose clothing produce strong shading patterns that overlay the image texture. These

Token 19259:
Section 24.5.

Token 19260:
Object Recognition from Structural Information 959 Figure 24.23 A pictorial structure model evaluates a match between a set of image rect- angles and a cardboard person (shown on the left) by scoring the similarity in appearance between body segments and image segments and the spatial relations between the image seg-ments.

Token 19261:
Generally, a match is better if the image segments have about the right appearance and are in about the right place with respect to one another.

Token 19262:
The appearance model uses average colors for hair, head, torso, and upper and lower arms and legs. The relevant relations areshown as arrows.

Token 19263:
On the right, the best match for a particular image, obtained using dynamic programming. The match is a fair estimate of the conﬁguration of the body.

Token 19264:
Figure from Felzenszwalb and Huttenlocher (2000) c/circlecopyrtIEEE. patterns are strong enough to disrupt the true texture of the cloth.

Token 19265:
In current work, ψtypically reﬂects the need for the ends of the segments to be reasonably close together, but there areusually no constraints on the angles.

Token 19266:
Generally, we don’t know what a person looks like,and must build a model of segment appearances.

Token 19267:
We call the description of what a personlooks like the appearance model .

Token 19268:
If we must report the conﬁguration of a person in a single APPEARANCE MODEL image, we can start with a poorly tuned appearance model, estimate conﬁguration with this, then re-estimate appearance, and so on.

Token 19269:
In video, we have many frames of the same person, and this will reveal their appearance.

Token 19270:
24.5.2 Coherent appearance: Tracking people in video Tracking people in video is an important practical problem.

Token 19271:
If we could reliably report the location of arms, legs, torso, and head in video sequences, we could build much improved game interfaces and surveillance systems.

Token 19272:
Filtering methods have not had much success with this problem, because people can produce large accelerations and move quite fast.

Token 19273:
Thismeans that for 30 Hz video, the conﬁguration of the body in frame idoesn’t constrain the conﬁguration of the body in frame i+1all that strongly.

Token 19274:
Currently, the most effective methods exploit the fact that appearance changes very slowly from frame to frame.

Token 19275:
If we can infer anappearance model of an individual from the video, then we can use this information in apictorial structure model to detect that person in each frame of the video.

Token 19276:
We can then linkthese locations across time to make a track.

Token 19277:
960 Chapter 24.

Token 19278:
Perception Lateral walking detectorAppearance modelBody part mapsDetected figure torso arm motion blur & interlacing Figure 24.24 We can track moving people with a pictorial structure model by ﬁrst ob- taining an appearance model, then applying it.

Token 19279:
To obtain the appearance model, we scan the image to ﬁnd a lateral walking pose.

Token 19280:
The detector does not need to be very accurate, but should produce few false positives.

Token 19281:
From the detector response, we can read off pixels thatlie on each body segment, and others that do not lie on that segment.

Token 19282:
This makes it possible to build a discriminative model of the appearance of each body part, and these are tied together into a pictorial structure model of the person being tracked.

Token 19283:
Finally, we can reliably track by detecting this model in each frame.

Token 19284:
As the frames in the lower part of the image suggest, this procedure can track complicated, fast-changing body conﬁgurations, despite degradation ofthe video signal due to motion blur.

Token 19285:
Figure from Ramanan et al. (2007) c/circlecopyrtIEEE. There are several ways to infer a good appearance model.

Token 19286:
We regard the video as a large stack of pictures of the person we wish to track.

Token 19287:
We can exploit this stack by lookingfor appearance models that explain many of the pictures.

Token 19288:
This would work by detecting body segments in each frame, using the fact that segments have roughly parallel edges.

Token 19289:
Such detectors are not particularly reliable, but the segments we want to ﬁnd are special.

Token 19290:
Theywill appear at least once in most of the frames of video; such segments can be found byclustering the detector responses.

Token 19291:
It is best to start with the torso, because it is big andbecause torso detectors tend to be reliable.

Token 19292:
Once we have a torso appearance model, upperleg segments should appear near the torso, and so on.

Token 19293:
This reasoning yields an appearancemodel, but it can be unreliable if people appear against a near-ﬁxed background where thesegment detector generates lots of false positives.

Token 19294:
An alternative is to estimate appearancefor many of the frames of video by repeatedly reestimating conﬁguration and appearance; wethen see if one appearance model explains many frames.

Token 19295:
Another alternative, which is quite

Token 19296:
Section 24.6. Using Vision 961 Figure 24.25 Some complex human actions produce consistent patterns of appearance and motion.

Token 19297:
For example, drinking involves movements of the hand in front of the face.

Token 19298:
The ﬁrst three images are correct detections of drinking; the fourth is a false-positive (the cook islooking into the coffee pot, but not drinking from it).

Token 19299:
Figure from Laptev and Perez (2007) c/circlecopyrtIEEE.

Token 19300:
reliable in practice, is to apply a detector for a ﬁxed body conﬁguration to all of the frames.

Token 19301:
A good choice of conﬁguration is one that is easy to detect reliably, and where there is a strong chance the person will appear in that conﬁguration even in a short sequence (lateral walkingis a good choice).

Token 19302:
We tune the detector to have a low false positive rate, so we know when itresponds that we have found a real person; and because we have localized their torso, arms,legs, and head, we know what these segments look like.

Token 19303:
24.6 U SING VISION If vision systems could analyze video and understood what people are doing, we would beable to: design buildings and public places better by collecting and using data about whatpeople do in public; build more accurate, more secure, and less intrusive surveillance systems;build computer sports commentators; and build human-computer interfaces that watch peopleand react to their behavior.

Token 19304:
Applications for reactive interfaces range from computer gamesthat make a player get up and move around to systems that save energy by managing heat and light in a building to match where the occupants are and what they are doing.

Token 19305:
Some problems are well understood.

Token 19306:
If people are relatively small in the video frame, and the background is stable, it is easy to detect the people by subtracting a background imagefrom the current frame.

Token 19307:
If the absolute value of the difference is large, this background subtraction declares the pixel to be a foreground pixel; by linking foreground blobs over BACKGROUND SUBTRACTION time, we obtain a track.

Token 19308:
Structured behaviors like ballet, gymnastics, or tai chi have speciﬁc vocabularies of ac- tions.

Token 19309:
When performed against a simple background, videos of these actions are easy to dealwith.

Token 19310:
Background subtraction identiﬁes the major moving regions, and we can build HOGfeatures (keeping track of ﬂow rather than orientation) to present to a classiﬁer.

Token 19311:
We can detectconsistent patterns of action with a variant of our pedestrian detector, where the orientation features are collected into histogram buckets over time as well as space (Figure 24.25).

Token 19312:
More general problems remain open.

Token 19313:
The big research question is to link observations of the body and the objects nearby to the goals and intentions of the moving people.

Token 19314:
One source of difﬁculty is that we lack a simple vocabulary of human behavior. Behavior is a lot

Token 19315:
962 Chapter 24.

Token 19316:
Perception like color, in that people tend to think they know a lot of behavior names but can’t produce long lists of such words on demand.

Token 19317:
There is quite a lot of evidence that behaviors combine—you can, for example, drink a milkshake while visiting an ATM—but we don’t yet knowwhat the pieces are, how the composition works, or how many composites there might be.A second source of difﬁculty is that we don’t know what features expose what is happening.

Token 19318:
For example, knowing someone is close to an ATM may be enough to tell that they’re visiting the ATM.

Token 19319:
A third difﬁculty is that the usual reasoning about the relationship between trainingand test data is untrustworthy.

Token 19320:
For example, we cannot argue that a pedestrian detector issafe simply because it performs well on a large data set, because that data set may well omitimportant, but rare, phenomena (for example, people mounting bicycles).

Token 19321:
We wouldn’t wantour automated driver to run over a pedestrian who happened to do something unusual.

Token 19322:
24.6.1 Words and pictures Many Web sites offer collections of images for viewing. How can we ﬁnd the images wewant?

Token 19323:
Let’s suppose the user enters a text query, such as “bicycle race.” Some of the imageswill have keywords or captions attached, or will come from Web pages that contain text nearthe image.

Token 19324:
For these, image retrieval can be like text retrieval: ignore the images and matchthe image’s text against the query (see Section 22.3 on page 867).

Token 19325:
However, keywords are usually incomplete.

Token 19326:
For example, a picture of a cat playing in the street might be tagged with words like “cat” and “street,” but it is easy to forget to mention the “garbage can” or the “ﬁsh bones.” Thus an interesting task is to annotate an image (which may already have a few keywords) with additional appropriate keywords.

Token 19327:
In the most straightforward version of this task, we have a set of correctly tagged ex- ample images, and we wish to tag some test images.

Token 19328:
This problem is sometimes known as auto-annotation. The most accurate solutions are obtained using nearest-neighbors methods.

Token 19329:
One ﬁnds the training images that are closest to the test image in a feature space metric thatis trained using examples, then reports their tags.

Token 19330:
Another version of the problem involves predicting which tags to attach to which re- gions in a test image.

Token 19331:
Here we do not know which regions produced which tags for the train-ing data.

Token 19332:
We can use a version of expectation maximization to guess an initial correspondencebetween text and regions, and from that estimate a better decomposition into regions, and soon.

Token 19333:
24.6.2 Reconstruction from many views Binocular stereopsis works because for each point we have four measurements constrainingthree unknown degrees of freedom.

Token 19334:
The four measurements are the (x,y)positions of the point in each view, and the unknown degrees of freedom are the (x,y,z)coordinate values of the point in the scene.

Token 19335:
This rather crude argument suggests, correctly, that there are geometricconstraints that prevent most pairs of points from being acceptable matches.

Token 19336:
Many images of a set of points should reveal their positions unambiguously.

Token 19337:
We don’t always need a second picture to get a second view of a set of points.

Token 19338:
If we believe the original set of points comes from a familiar rigid 3D object, then we might have

Token 19339:
Section 24.6. Using Vision 963 an object model available as a source of information.

Token 19340:
If this object model consists of a set of 3D points or of a set of pictures of the object, and if we can establish point correspondences,we can determine the parameters of the camera that produced the points in the original image.This is very powerful information.

Token 19341:
We could use it to evaluate our original hypothesis thatthe points come from an object model.

Token 19342:
We do this by using some points to determine the parameters of the camera, then projecting model points in this camera and checking to see whether there are image points nearby.

Token 19343:
We have sketched here a technology that is now very highly developed.

Token 19344:
The technology can be generalized to deal with views that are not orthographic; to deal with points that areobserved in only some views; to deal with unknown camera properties like focal length; toexploit various sophisticated searches for appropriate correspondences; and to do reconstruc-tion from very large numbers of points and of views.

Token 19345:
If the locations of points in the imagesare known with some accuracy and the viewing directions are reasonable, very high accuracycamera and point information can be obtained.

Token 19346:
Some applications are •Model-building: For example, one might build a modeling system that takes a video sequence depicting an object and produces a very detailed three-dimensional mesh oftextured polygons for use in computer graphics and virtual reality applications.

Token 19347:
Modelslike this can now be built from apparently quite unpromising sets of pictures.

Token 19348:
For ex-ample, Figure 24.26 shows a model of the Statue of Liberty built from pictures foundon the Internet.

Token 19349:
•Matching moves: To place computer graphics characters into real video, we need to know how the camera moved for the real video, so that we can render the charactercorrectly.

Token 19350:
•Path reconstruction: Mobile robots need to know where they have been.

Token 19351:
If they are moving in a world of rigid objects, then performing a reconstruction and keeping thecamera information is one way to obtain a path.

Token 19352:
24.6.3 Using vision for controlling movement One of the principal uses of vision is to provide information both for manipulating objects—picking them up, grasping them, twirling them, and so on—and for navigating while avoidingobstacles.

Token 19353:
The ability to use vision for these purposes is present in the most primitive ofanimal visual systems.

Token 19354:
In many cases, the visual system is minimal, in the sense that it extracts from the available light ﬁeld just the information the animal needs to inform its behavior.

Token 19355:
Quite probably, modern vision systems evolved from early, primitive organismsthat used a photosensitive spot at one end to orient themselves toward (or away from) thelight.

Token 19356:
We saw in Section 24.4 that ﬂies use a very simple optical ﬂow detection system toland on walls.

Token 19357:
A classic study, What the Frog’s Eye Tells the Frog’s Brain (Lettvin et al.

Token 19358:
, 1959), observes of a frog that, “He will starve to death surrounded by food if it is not moving.His choice of food is determined only by size and movement.” Let us consider a vision system for an automated vehicle driving on a freeway.

Token 19359:
The tasks faced by the driver include the following:

Token 19360:
964 Chapter 24. Perception a b c (a) (b) (c) Figure 24.26 The state of the art in multiple-view reconstruction is now highly advanced.

Token 19361:
This ﬁgure outlines a system built by Michael Goesele and colleagues from the University of Washington, TU Darmstadt, and Microsoft Research.

Token 19362:
From a collection of pictures of a monument taken by a large community of users and posted on the Internet (a), their systemcan determine the viewing directions for those pictures, shown by the small black pyramids in (b) and a comprehensive 3D reconstruction shown in (c).

Token 19363:
1. Lateral control—ensure that the vehicle remains securely within its lane or changes lanes smoothly when required. 2.

Token 19364:
Longitudinal control—ensure that there is a safe distance to the vehicle in front.3.

Token 19365:
Obstacle avoidance—monitor vehicles in neighboring lanes and be prepared for evasive maneuvers if one of them decides to change lanes.

Token 19366:
The problem for the driver is to generate appropriate steering, acceleration, and braking ac- tions to best accomplish these tasks.

Token 19367:
For lateral control, one needs to maintain a representation of the position and orientation of the car relative to the lane.

Token 19368:
We can use edge-detection algorithms to ﬁnd edges correspond-ing to the lane-marker segments. We can then ﬁt smooth curves to these edge elements.

Token 19369:
Theparameters of these curves carry information about the lateral position of the car, the direc-tion it is pointing relative to the lane, and the curvature of the lane.

Token 19370:
This information, alongwith information about the dynamics of the car, is all that is needed by the steering-controlsystem.

Token 19371:
If we have good detailed maps of the road, then the vision system serves to conﬁrmour position (and to watch for obstacles that are not on the map).

Token 19372:
For longitudinal control, one needs to know distances to the vehicles in front. This can be accomplished with binocular stereopsis or optical ﬂow.

Token 19373:
Using these techniques, vision- controlled cars can now drive reliably at highway speeds.

Token 19374:
The more general case of mobile robots navigating in various indoor and outdoor envi- ronments has been studied, too.

Token 19375:
One particular problem, localizing the robot in its environ- ment, now has pretty good solutions.

Token 19376:
A group at Sarnoff has developed a system based on two cameras looking forward that track feature points in 3D and use that to reconstruct the

Token 19377:
Section 24.7. Summary 965 position of the robot relative to the environment.

Token 19378:
In fact, they have two stereoscopic camera systems, one looking front and one looking back—this gives greater robustness in case therobot has to go through a featureless patch due to dark shadows, blank walls, and the like.

Token 19379:
It isunlikely that there are no features either in the front or in the back.

Token 19380:
Now of course, that couldhappen, so a backup is provided by using an inertial motion unit (IMU) somewhat akin to the mechanisms for sensing acceleration that we humans have in our inner ears.

Token 19381:
By integrating the sensed acceleration twice, one can keep track of the change in position.

Token 19382:
Combining thedata from vision and the IMU is a problem of probabilistic evidence fusion and can be tackledusing techniques, such as Kalman ﬁltering, we have studied elsewhere in the book.

Token 19383:
In the use of visual odometry (estimation of change in position), as in other problems of odometry, there is the problem of “drift,” positional errors accumulating over time.

Token 19384:
Thesolution for this is to use landmarks to provide absolute position ﬁxes: as soon as the robotpasses a location in its internal map, it can adjust its estimate of its position appropriately.Accuracies on the order of centimeters have been demonstrated with the these techniques.

Token 19385:
The driving example makes one point very clear: for a speciﬁc task, one does not need to recover all the information that, in principle, can be recovered from an image.

Token 19386:
One does not need to recover the exact shape of every vehicle, solve for shape-from-texture on the grass surface adjacent to the freeway, and so on.

Token 19387:
Instead, a vision system should compute just what is needed to accomplish the task.

Token 19388:
24.7 S UMMARY Although perception appears to be an effortless activity for humans, it requires a signiﬁcantamount of sophisticated computation.

Token 19389:
The goal of vision is to extract information needed fortasks such as manipulation, navigation, and object recognition.

Token 19390:
•The process of image formation is well understood in its geometric and physical as- pects.

Token 19391:
Given a description of a three-dimensional scene, we can easily produce a pictureof it from some arbitrary camera position (the graphics problem).

Token 19392:
Inverting the processby going from an image to a description of the scene is more difﬁcult.

Token 19393:
•To extract the visual information necessary for the tasks of manipulation, navigation, and recognition, intermediate representations have to be constructed.

Token 19394:
Early vision image-processing algorithms extract primitive features from the image, such as edges and regions.

Token 19395:
•There are various cues in the image that enable one to obtain three-dimensional in- formation about the scene: motion, stereopsis, texture, shading, and contour analysis.Each of these cues relies on background assumptions about physical scenes to providenearly unambiguous interpretations.

Token 19396:
•Object recognition in its full generality is a very hard problem. We discussed brightness- based and feature-based approaches.

Token 19397:
We also presented a simple algorithm for poseestimation. Other possibilities exist.

Token 19398:
966 Chapter 24.

Token 19399:
Perception BIBLIOGRAPHICAL AND HISTORICAL NOTES The eye developed in the Cambrian explosion (530 million years ago), apparently in a com- mon ancestor.

Token 19400:
Since then, endless variations have developed in different creatures, but thesame gene, Pax-6, regulates the development of the eye in animals as diverse as humans,mice, and Drosophila .

Token 19401:
Systematic attempts to understand human vision can be traced back to ancient times. Euclid (ca. 300 B.C.)

Token 19402:
wrote about natural perspective—the mapping that associates, with each point Pin the three-dimensional world, the direction of the ray OPjoining the center of projection Oto the point P. He was well aware of the notion of motion parallax.

Token 19403:
The use of perspective in art was developed in ancient Roman culture, as evidenced by art found in the ruins of Pompeii (A.D. 79), but was then largely lost for 1300 years.

Token 19404:
The mathematical under-standing of perspective projection, this time in the context of projection onto planar surfaces,had its next signiﬁcant advance in the 15th-century in Renaissance Italy.

Token 19405:
Brunelleschi (1413)is usually credited with creating the ﬁrst paintings based on geometrically correct projectionof a three-dimensional scene.

Token 19406:
In 1435, Alberti codiﬁed the rules and inspired generations ofartists whose artistic achievements amaze us to this day.

Token 19407:
Particularly notable in their develop-ment of the science of perspective, as it was called in those days, were Leonardo da Vinci andAlbrecht D¨ urer.

Token 19408:
Leonardo’s late 15th century descriptions of the interplay of light and shade (chiaroscuro), umbra and penumbra regions of shadows, and aerial perspective are still worth reading in translation (Kemp, 1989).

Token 19409:
Stork (2004) analyzes the creation of various pieces of Renaissance art using computer vision techniques.

Token 19410:
Although perspective was known to the ancient Greeks, they were curiously confused by the role of the eyes in vision.

Token 19411:
Aristotle thought of the eyes as devices emitting rays, rather in the manner of modern laser range ﬁnders.

Token 19412:
This mistaken view was laid to rest by the workof Arab scientists, such as Abu Ali Alhazen, in the 10th century.

Token 19413:
Alhazen also developed thecamera obscura , a room ( camera is Latin for “room” or “chamber”) with a pinhole that casts an image on the opposite wall.

Token 19414:
Of course the image was inverted, which caused no end ofconfusion.

Token 19415:
If the eye was to be thought of as such an imaging device, how do we see right-side up?

Token 19416:
This enigma exercised the greatest minds of the era (including Leonardo).

Token 19417:
Keplerﬁrst proposed that the lens of the eye focuses an image on the retina, and Descartes surgicallyremoved an ox eye and demonstrated that Kepler was right.

Token 19418:
There was still puzzlement as to why we do not see everything upside down; today we realize it is just a question of accessing the retinal data structure in the right way.

Token 19419:
In the ﬁrst half of the 20th century, the most signiﬁcant research results in vision were obtained by the Gestalt school of psychology, led by Max Wertheimer.

Token 19420:
They pointed out theimportance of perceptual organization: for a human observer, the image is not a collectionof pointillist photoreceptor outputs (pixels in computer vision terminology); rather it is or-ganized into coherent groups.

Token 19421:
One could trace the motivation in computer vision of ﬁndingregions and curves back to this insight.

Token 19422:
The Gestaltists also drew attention to the “ﬁgure–ground” phenomenon—a contour separating two image regions that, in the world, are atdifferent depths, appears to belong only to the nearer region, the “ﬁgure,” and not the farther

Token 19423:


Token 19424:
Bibliographical and Historical Notes 967 region, the “ground.” The computer vision problem of classifying image curves according to their signiﬁcance in the scene can be thought of as a generalization of this insight.

Token 19425:
The period after World War II was marked by renewed activity.

Token 19426:
Most signiﬁcant was the work of J. J. Gibson (1950, 1979), who pointed out the importance of optical ﬂow, as wellas texture gradients in the estimation of environmental variables such as surface slant and tilt.

Token 19427:
He reemphasized the importance of the stimulus and how rich it was.

Token 19428:
Gibson emphasized the role of the active observer whose self-directed movement facilitates the pickup of informationabout the external environment.

Token 19429:
Computer vision was founded in the 1960s.

Token 19430:
Roberts’s (1963) thesis at MIT was one of the earliest publications in the ﬁeld, introducing key ideas such as edge detection andmodel-based matching.

Token 19431:
There is an urban legend that Marvin Minsky assigned the problemof “solving” computer vision to a graduate student as a summer project.

Token 19432:
According to Minskythe legend is untrue—it was actually an undergraduate student.

Token 19433:
But it was an exceptionalundergraduate, Gerald Jay Sussman (who is now a professor at MIT) and the task was not to“solve” vision, but to investigate some aspects of it.

Token 19434:
In the 1960s and 1970s, progress was slow, hampered considerably by the lack of com- putational and storage resources.

Token 19435:
Low-level visual processing received a lot of attention. The widely used Canny edge-detection technique was introduced in Canny (1986).

Token 19436:
Techniques for ﬁnding texture boundaries based on multiscale, multiorientation ﬁltering of images date towork such as Malik and Perona (1990).

Token 19437:
Combining multiple clues—brightness, texture andcolor—for ﬁnding boundary curves in a learning framework was shown by Martin, Fowlkesand Malik (2004) to considerably improve performance.

Token 19438:
The closely related problem of ﬁnding regions of coherent brightness, color, and tex- ture, naturally lends itself to formulations in which ﬁnding the best partition becomes anoptimization problem.

Token 19439:
Three leading examples are the Markov Random Fields approach ofGeman and Geman (1984), the variational formulation of Mumford and Shah (1989), andnormalized cuts by Shi and Malik (2000).

Token 19440:
Through much of the 1960s, 1970s and 1980s, there were two distinct paradigms in which visual recognition was pursued, dictated by different perspectives on what was per- ceived to be the primary problem.

Token 19441:
Computer vision research on object recognition largely fo-cused on issues arising from the projection of three-dimensional objects onto two-dimensionalimages.

Token 19442:
The idea of alignment, also ﬁrst introduced by Roberts, resurfaced in the 1980s in thework of Lowe (1987) and Huttenlocher and Ullman (1990).

Token 19443:
Also popular was an approachbased on describing shapes in terms of volumetric primitives, with generalized cylinders , GENERALIZED CYLINDER introduced by Tom Binford (1971), proving particularly popular.

Token 19444:
In contrast, the pattern recognition community viewed the 3D-to-2D aspects of the prob- lem as not signiﬁcant.

Token 19445:
Their motivating examples were in domains such as optical characterrecognition and handwritten zip code recognition where the primary concern is that of learn-ing the typical variations characteristic of a class of objects and separating them from other classes.

Token 19446:
See LeCun et al. (1995) for a comparison of approaches.

Token 19447:
In the late 1990s, these two paradigms started to converge, as both sides adopted the probabilistic modeling and learning techniques that were becoming popular throughout AI.Two lines of work contributed signiﬁcantly.

Token 19448:
One was research on face detection, such as that

Token 19449:
968 Chapter 24.

Token 19450:
Perception of Rowley, Baluja and Kanade (1996), and of Viola and Jones (2002b) which demonstrated the power of pattern recognition techniques on clearly important and useful tasks.

Token 19451:
The otherwas the development of point descriptors, which enable one to construct feature vectors fromparts of objects.

Token 19452:
This was pioneered by Schmid and Mohr (1996). Lowe’s (2004) SIFTdescriptor is widely used. The HOG descriptor is due to Dalal and Triggs (2005).

Token 19453:
Ullman (1979) and Longuet-Higgins (1981) are inﬂuential early works in reconstruc- tion from multiple images.

Token 19454:
Concerns about the stability of structure from motion were sig-niﬁcantly allayed by the work of Tomasi and Kanade (1992) who showed that with the use ofmultiple frames shape could be recovered quite accurately.

Token 19455:
In the 1990s, with great increasein computer speed and storage, motion analysis found many new applications.

Token 19456:
Building geo-metrical models of real-world scenes for rendering by computer graphics techniques provedparticularly popular, led by reconstruction algorithms such as the one developed by Debevec,Taylor, and Malik (1996).

Token 19457:
The books by Hartley and Zisserman (2000) and Faugeras et al. (2001) provide a comprehensive treatment of the geometry of multiple views.

Token 19458:
For single images, inferring shape from shading was ﬁrst studied by Horn (1970), and Horn and Brooks (1989) present an extensive survey of the main papers from a period whenthis was a much-studied problem.

Token 19459:
Gibson (1950) was the ﬁrst to propose texture gradients as a cue to shape, though a comprehensive analysis for curved surfaces ﬁrst appears in Gard- ing (1992) and Malik and Rosenholtz (1997).

Token 19460:
The mathematics of occluding contours, andmore generally understanding the visual events in the projection of smooth curved objects,owes much to the work of Koenderink and van Doorn, which ﬁnds an extensive treatment inKoenderink’s (1990) Solid Shape .

Token 19461:
In recent years, attention has turned to treating the problem of shape and surface recovery from a single image as a probabilistic inference problem, where geometrical cues are not modeled explicitly, but used implicitly in a learning framework.

Token 19462:
Agood representative is the work of Hoiem, Efros, and Hebert (2008).

Token 19463:
For the reader interested in human vision, Palmer (1999) provides the best comprehen- sive treatment; Bruce et al. (2003) is a shorter textbook.

Token 19464:
The books by Hubel (1988) and Rock (1984) are friendly introductions centered on neurophysiology and perception respec- tively.

Token 19465:
David Marr’s book Vision (Marr, 1982) played a historical role in connecting computer vision to psychophysics and neurobiology.

Token 19466:
While many of his speciﬁc models haven’t stoodthe test of time, the theoretical perspective from which each task is analyzed at an informa-tional, computational, and implementation level is still illuminating.

Token 19467:
For computer vision, the most comprehensive textbook is Forsyth and Ponce (2002). Trucco and Verri (1998) is a shorter account.

Token 19468:
Horn (1986) and Faugeras (1993) are two olderand still useful textbooks.

Token 19469:
The main journals for computer vision are IEEE Transactions on Pattern Analysis and Machine Intelligence and International Journal of Computer Vision .

Token 19470:
Computer vision con- ferences include ICCV (International Conference on Computer Vision), CVPR (ComputerVision and Pattern Recognition), and ECCV (European Conference on Computer Vision).

Token 19471:
Research with a machine learning component is also published in the NIPS (Neural Informa- tion Processing Systems) conference, and work on the interface with computer graphics oftenappears at the ACM SIGGRAPH (Special Interest Group in Graphics) conference.

Token 19472:
Exercises 969 EXERCISES 24.1 In the shadow of a tree with a dense, leafy canopy, one sees a number of light spots.

Token 19473:
Surprisingly, they all appear to be circular. Why? After all, the gaps between the leavesthrough which the sun shines are not likely to be circular.

Token 19474:
24.2 Consider a picture of a white sphere ﬂoating in front of a black backdrop.

Token 19475:
The im- age curve separating white pixels from black pixels is sometimes called the “outline” of the sphere.

Token 19476:
Show that the outline of a sphere, viewed in a perspective camera, can be an ellipse.Why do spheres not look like ellipses to you?

Token 19477:
24.3 Consider an inﬁnitely long cylinder of radius roriented with its axis along the y-axis.

Token 19478:
The cylinder has a Lambertian surface and is viewed by a camera along the positive z-axis.

Token 19479:
What will you expect to see in the image if the cylinder is illuminated by a point source at inﬁnity located on the positive x-axis?

Token 19480:
Draw the contours of constant brightness in the projected image. Are the contours of equal brightness uniformly spaced?

Token 19481:
24.4 Edges in an image can correspond to a variety of events in a scene.

Token 19482:
Consider Fig- ure 24.4 (page 933), and assume that it is a picture of a real three-dimensional scene.

Token 19483:
Identifyten different brightness edges in the image, and for each, state whether it corresponds to adiscontinuity in (a) depth, (b) surface orientation, (c) reﬂectance, or (d) illumination.

Token 19484:
24.5 A stereoscopic system is being contemplated for terrain mapping.

Token 19485:
It will consist of two CCD cameras, each having 512×512pixels on a 10 cm ×10 cm square sensor.

Token 19486:
The lenses to be used have a focal length of 16 cm, with the focus ﬁxed at inﬁnity.

Token 19487:
For correspondingpoints ( u 1,v1) in the left image and ( u2,v2) in the right image, v1=v2because the x-axes in the two image planes are parallel to the epipolar lines—the lines from the object to the camera.

Token 19488:
The optical axes of the two cameras are parallel. The baseline between the camerasis 1 meter. a.

Token 19489:
If the nearest distance to be measured is 16 meters, what is the largest disparity that will occur (in pixels)? b.

Token 19490:
What is the distance resolution at 16 meters, due to the pixel spacing? c. What distance corresponds to a disparity of one pixel?

Token 19491:
24.6 Which of the following are true, and which are false? a.

Token 19492:
Finding corresponding points in stereo images is the easiest phase of the stereo depth- ﬁnding process.

Token 19493:
b. Shape-from-texture can be done by projecting a grid of light-stripes onto the scene.

Token 19494:
c. Lines with equal lengths in the scene always project to equal lengths in the image.

Token 19495:
d. Straight lines in the image necessarily correspond to straight lines in the scene.

Token 19496:
970 Chapter 24. Perception EA B CX YD Figure 24.27 Top view of a two-camera vision system observing a bottle with a wall behind it.

Token 19497:
24.7 (Courtesy of Pietro Perona.) Figure 24.27 shows two cameras at X and Y observing a scene.

Token 19498:
Draw the image seen at each camera, assuming that all named points are in the same horizontal plane.

Token 19499:
What can be concluded from these two images about the relative distancesof points A, B, C, D, and E from the camera baseline, and on what basis?

Token 19500:
25ROBOTICS In which agents are endowed with physical effectors with which to do mischief.

Token 19501:
25.1 I NTRODUCTION Robots are physical agents that perform tasks by manipulating the physical world.

Token 19502:
To do so, ROBOT they are equipped with effectors such as legs, wheels, joints, and grippers.

Token 19503:
Effectors have EFFECTOR a single purpose: to assert physical forces on the environment.1Robots are also equipped with sensors , which allow them to perceive their environment.

Token 19504:
Present day robotics em- SENSOR ploys a diverse set of sensors, including cameras and lasers to measure the environment, and gyroscopes and accelerometers to measure the robot’s own motion.

Token 19505:
Most of today’s robots fall into one of three primary categories.

Token 19506:
Manipulators , or robot MANIPULATOR arms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory assembly line or on the International Space Station.

Token 19507:
Manipulator motion usually involvesa chain of controllable joints, enabling such robots to place their effectors in any positionwithin the workplace.

Token 19508:
Manipulators are by far the most common type of industrial robots,with approximately one million units installed worldwide.

Token 19509:
Some mobile manipulators areused in hospitals to assist surgeons.

Token 19510:
Few car manufacturers could survive without roboticmanipulators, and some manipulators have even been used to generate original artwork.

Token 19511:
The second category is the mobile robot . Mobile robots move about their environment MOBILE ROBOT using wheels, legs, or similar mechanisms.

Token 19512:
They have been put to use delivering food in hospitals, moving containers at loading docks, and similar tasks.

Token 19513:
Unmanned ground vehi- cles, or UGVs, drive au tonomously on streets, highways, and off-road.

Token 19514:
The planetary rover UGV PLANETARY ROVER shown in Figure 25.2(b) explored Mars for a period of 3 months in 1997.

Token 19515:
Subsequent NASA robots include the twin Mars Exploration Rovers (one is depicted on the cover of this book),which landed in 2003 and were still operating six years later.

Token 19516:
Other types of mobile robotsinclude unmanned air vehicles (UAVs), commonly used for surveillance, crop-spraying, and UAV 1In Chapter 2 we talked about actuators , not effectors.

Token 19517:
Here we distinguish the effector (the physical device) from the actuator (the control line that communicates a command to the effector). 971

Token 19518:
972 Chapter 25. Robotics (a) (b) Figure 25.1 (a) An industrial robotic manipulator for stacking bags on a pallet.

Token 19519:
Image courtesy of Nachi Robotic Systems. (b) Honda’s P3 and Asimo humanoid robots.

Token 19520:
(a) (b) Figure 25.2 (a) Predator, an unmanned aerial vehicle (UA V) used by the U.S. Military. Image courtesy of General Atomics Aeronautical Systems.

Token 19521:
(b) NASA’s Sojourner, a mobile robot that explored the surface of Mars in July 1997. military operations.

Token 19522:
Figure 25.2(a) shows a UAV commonly used by the U.S. military. Au- tonomous underwater vehicles (AUVs) are used in deep sea exploration.

Token 19523:
Mobile robots AUV deliver packages in the workplace and vacuum the ﬂoors at home.

Token 19524:
The third type of robot combines mobility with manipulation, and is often called a mobile manipulator .Humanoid robots mimic the human torso.

Token 19525:
Figure 25.1(b) shows twoMOBILE MANIPULATOR HUMANOID ROBOT early humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators

Token 19526:
Section 25.2.

Token 19527:
Robot Hardware 973 can apply their effectors further aﬁeld than anchored manipulators can, but their task is made harder because they don’t have the rigidity that the anchor provides.

Token 19528:
The ﬁeld of robotics also includes prosthetic devices (artiﬁcial limbs, ears, and eyes for humans), intelligent environments (such as an entire house that is equipped with sensorsand effectors), and multibody systems, wherein robotic action is achieved through swarms of small cooperating robots.

Token 19529:
Real robots must cope with environments that are partially observable, stochastic, dy- namic, and continuous.

Token 19530:
Many robot environments are sequential and multiagent as well.Partial observability and stochasticity are the result of dealing with a large, complex world.Robot cameras cannot see around corners, and motion commands are subject to uncertaintydue to gears slipping, friction, etc.

Token 19531:
Also, the real world stubbornly refuses to operate fasterthan real time.

Token 19532:
In a simulated environment, it is possible to use simple algorithms (such as theQ-learning algorithm described in Chapter 21) to learn in a few CPU hours from millions oftrials.

Token 19533:
In a real environment, it might take years to run these trials. Furthermore, real crashesreally hurt, unlike simulated ones.

Token 19534:
Practical robotic systems need to embody prior knowledgeabout the robot, its physical environment, and the tasks that the robot will perform so that therobot can learn quickly and perform safely.

Token 19535:
Robotics brings together many of the concepts we have seen earlier in the book, in- cluding probabilistic state estimation, perception, planning, unsupervised learning, and re-inforcement learning.

Token 19536:
For some of these concepts robotics serves as a challenging exampleapplication.

Token 19537:
For other concepts this chapter breaks new ground in introducing the continuousversion of techniques that we previously saw only in the discrete case.

Token 19538:
25.2 R OBOT HARDW ARE So far in this book, we have taken the agent architecture—sensors, effectors, and processors—as given, and we have concentrated on the agent program.

Token 19539:
The success of real robots dependsat least as much on the design of sensors and effectors that are appropriate for the task.

Token 19540:
25.2.1 Sensors Sensors are the perceptual interface between robot and environment.

Token 19541:
Passive sensors ,s u c h PASSIVE SENSOR as cameras, are true observers of the environment: they capture signals that are generated by other sources in the environment.

Token 19542:
Active sensors , such as sonar, send energy into the envi- ACTIVE SENSOR ronment.

Token 19543:
They rely on the fact that this energy is reﬂected back to the sensor.

Token 19544:
Active sensors tend to provide more information than passive sensors, but at the expense of increased powerconsumption and with a danger of interference when multiple active sensors are used at thesame time.

Token 19545:
Whether active or passive, sensors can be divided into three types, depending on whether they sense the environment, the robot’s location, or the robot’s internal conﬁguration.

Token 19546:
Range ﬁnders are sensors that measure the distance to nearby objects.

Token 19547:
In the early RANGE FINDER days of robotics, robots were commonly equipped with sonar sensors .

Token 19548:
Sonar sensors emit SONAR SENSORS directional sound waves, which are reﬂected by objects, with some of the sound making it

Token 19549:
974 Chapter 25. Robotics (a) (b) Figure 25.3 (a) Time of ﬂight camera; image courtesy of Mesa Imaging GmbH.

Token 19550:
(b) 3D range image obtained with this camera. The range image makes it possible to detect obstacles and objects in a robot’s vicinity.

Token 19551:
back into the sensor. The time and intensity of the returning signal indicates the distance to nearby objects.

Token 19552:
Sonar is the technology of choice for autonomous underwater vehicles.Stereo vision (see Section 24.4.2) relies on multiple cameras to image the environment from STEREO VISION slightly different viewpoints, analyzing the resulting parallax in these images to compute the range of surrounding objects.

Token 19553:
For mobile ground robots, sonar and stereo vision are nowrarely used, because they are not reliably accurate.

Token 19554:
Most ground robots are now equipped with optical range ﬁnders.

Token 19555:
Just like sonar sensors, optical range sensors emit active signals (light) and measure the time until a reﬂection of thissignal arrives back at the sensor.

Token 19556:
Figure 25.3(a) shows a time of ﬂight camera .

Token 19557:
This camera TIME OF FLIGHT CAMERA acquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.

Token 19558:
Other range sensors use laser beams and special 1-pixel cameras that can be directed using complex arrangements of mirrors or rotating elements.

Token 19559:
These sensors are called scanning lidars (short for light detection and ranging ).

Token 19560:
Scanning lidars tend to provide longer ranges SCANNINGLIDARS than time of ﬂight cameras, and tend to perform better in bright daylight.

Token 19561:
Other common range sensors include radar, which is often the sensor of choice for UAVs. Radar sensors can measure distances of multiple kilometers.

Token 19562:
On the other extreme end of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitive TACTILE SENSORS skin.

Token 19563:
These sensors measure range based on physical contact, and can be deployed only for sensing objects very close to the robot.

Token 19564:
A second important class of sensors is location sensors .

Token 19565:
Most location sensors use LOCATION SENSORS range sensing as a primary component to determine location.

Token 19566:
Outdoors, the Global Position- ing System (GPS) is the most common solution to the localization problem.

Token 19567:
GPS measuresGLOBAL POSITIONINGSYSTEM the distance to satellites that emit pulsed signals.

Token 19568:
At present, there are 31 satellites in orbit, transmitting signals on multiple frequencies.

Token 19569:
GPS receivers can recover the distance to thesesatellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS

Token 19570:
Section 25.2. Robot Hardware 975 receivers can determine their absolute location on Earth to within a few meters.

Token 19571:
Differential GPS involves a second ground receiver with known location, providing millimeter accuracy DIFFERENTIAL GPS under ideal conditions.

Token 19572:
Unfortunately, GPS does not work indoors or underwater.

Token 19573:
Indoors, localization is often achieved by attaching beacons in the environment at known locations.Many indoor environments are full of wireless base stations, which can help robots localize through the analysis of the wireless signal.

Token 19574:
Underwater, active sonar beacons can provide a sense of location, using sound to inform AUVs of their relative distances to those beacons.

Token 19575:
The third important class is proprioceptive sensors , which inform the robot of its own PROPRIOCEPTIVE SENSOR motion.

Token 19576:
To measure the exact conﬁguration of a robotic joint, motors are often equipped with shaft decoders that count the revolution of motors in small increments.

Token 19577:
On robot arms, SHAFT DECODER shaft decoders can provide accurate information over any period of time.

Token 19578:
On mobile robots, shaft decoders that report wheel revolutions can be used for odometry —the measurement of ODOMETRY distance traveled.

Token 19579:
Unfortunately, wheels tend to drift and slip, so odometry is accurate only over short distances.

Token 19580:
External forces, such as the current for AUVs and the wind for UAVs,increase positional uncertainty.

Token 19581:
Inertial sensors , such as gyroscopes, rely on the resistance INERTIAL SENSOR of mass to the change of velocity. They can help reduce uncertainty.

Token 19582:
Other important aspects of robot state are measured by force sensors andtorque sen- FORCE SENSOR sors.

Token 19583:
These are indispensable when robots handle fragile objects or objects whose exact shape TORQUE SENSOR and location is unknown.

Token 19584:
Imagine a one-ton robotic manipulator screwing in a light bulb. It would be all too easy to apply too much force and break the bulb.

Token 19585:
Force sensors allow therobot to sense how hard it is gripping the bulb, and torque sensors allow it to sense how hardit is turning.

Token 19586:
Good sensors can measure forces in all three translational and three rotationaldirections.

Token 19587:
They do this at a frequency of several hundred times a second, so that a robot canquickly detect unexpected forces and correct its actions before it breaks a light bulb.

Token 19588:
25.2.2 Effectors Effectors are the means by which robots move and change the shape of their bodies.

Token 19589:
Tounderstand the design of effectors, it will help to talk about motion and shape in the abstract,using the concept of a degree of freedom (DOF) We count one degree of freedom for each DEGREE OF FREEDOM independent direction in which a robot, or one of its effectors, can move.

Token 19590:
For example, a rigid mobile robot such as an AUV has six degrees of freedom, three for its (x,y,z)location in space and three for its angular orientation, known as yaw,roll,a n d pitch .

Token 19591:
These six degrees deﬁne the kinematic state2orpose of the robot.

Token 19592:
The dynamic state of a robot includes these KINEMATIC STATE POSE DYNAMIC STATEsix plus an additional six dimensions for the rate of change of each kinematic dimension, that is, their velocities.

Token 19593:
For nonrigid bodies, there are additional degrees of freedom within the robot itself.

Token 19594:
For example, the elbow of a human arm possesses two degree of freedom. It can ﬂex the upper arm towards or away, and can rotate right or left.

Token 19595:
The wrist has three degrees of freedom. It can move up and down, side to side, and can also rotate.

Token 19596:
Robot joints also have one, two,or three degrees of freedom each.

Token 19597:
Six degrees of freedom are required to place an object,such as a hand, at a particular point in a particular orientation.

Token 19598:
The arm in Figure 25.4(a) 2“Kinematic” is from the Greek word for motion , as is “cinema.”

Token 19599:
976 Chapter 25.

Token 19600:
Robotics has exactly six degrees of freedom, created by ﬁve revolute joints that generate rotational REVOLUTE JOINT motion and one prismatic joint that generates sliding motion.

Token 19601:
You can verify that the human PRISMATIC JOINT arm as a whole has more than six degrees of freedom by a simple experiment: put your hand on the table and notice that you still have the freedom to rotate your elbow without changingthe conﬁguration of your hand.

Token 19602:
Manipulators that have extra degrees of freedom are easier to control than robots with only the minimum number of DOFs.

Token 19603:
Many industrial manipulators therefore have seven DOFs, not six.

Token 19604:
RR RP R R θ (x, y) (a) (b) Figure 25.4 (a) The Stanford Manipulator, an early robot arm with ﬁve revolute joints (R) and one prismatic joint (P), for a total of six degrees of freedom.

Token 19605:
(b) Motion of a nonholo-nomic four-wheeled vehicle with front-wheel steering.

Token 19606:
For mobile robots, the DOFs are not necessarily the same as the number of actuated ele- ments.

Token 19607:
Consider, for example, your average car: it can move forward or backward, and it canturn, giving it two DOFs.

Token 19608:
In contrast, a car’s kinematic conﬁguration is three-dimensional:on an open ﬂat surface, one can easily maneuver a car to any (x,y)point, in any orientation.

Token 19609:
(See Figure 25.4(b).) Thus, the car has three effective degrees of freedom but two control- EFFECTIVE DOF lable degrees of freedom .

Token 19610:
We say a robot is nonholonomic if it has more effective DOFs CONTROLLABLE DOF NONHOLONOMIC than controllable DOFs and holonomic if the two numbers are the same.

Token 19611:
Holonomic robots are easier to control—it would be much easier to park a car that could move sideways as wellas forward and backward—but holonomic robots are also mechanically more complex.

Token 19612:
Mostrobot arms are holonomic, and most mobile robots are nonholonomic.

Token 19613:
Mobile robots have a range of mechanisms for locomotion, including wheels, tracks, and legs.

Token 19614:
Differential drive robots possess two independently actuated wheels (or tracks), DIFFERENTIAL DRIVE one on each side, as on a military tank.

Token 19615:
If both wheels move at the same velocity, the robot moves on a straight line. If they move in opposite directions, the robot turns on the spot.

Token 19616:
Analternative is the synchro drive , in which each wheel can move and turn around its own axis.

Token 19617:
SYNCHRO DRIVE To avoid chaos, the wheels are tightly coordinated.

Token 19618:
When moving straight, for example, all wheels point in the same direction and move at the same speed.

Token 19619:
Both differential and synchro drives are nonholonomic.

Token 19620:
Some more expensive robots use holonomic drives, which have three or more wheels that can be oriented and moved independently.

Token 19621:
Some mobile robots possess arms. Figure 25.5(a) displays a two-armed robot.

Token 19622:
This robot’s arms use springs to compensate for gravity, and they provide minimal resistance to

Token 19623:
Section 25.2. Robot Hardware 977 (a) (b) Figure 25.5 (a) Mobile manipulator plugging its charge cable into a wall outlet.

Token 19624:
Image courtesy of Willow Garage, c/circlecopyrt2009. (b) One of Marc Raibert’s legged robots in motion. external forces.

Token 19625:
Such a design minimizes the physical danger to people who might stumble into such a robot.

Token 19626:
This is a key consideration in deploying robots in domestic environments. Legs, unlike wheels, can handle rough terrain.

Token 19627:
However, legs are notoriously slow on ﬂat surfaces, and they are mechanically difﬁcult to build.

Token 19628:
Robotics researchers have tried de-signs ranging from one leg up to dozens of legs.

Token 19629:
Legged robots have been made to walk, run, and even hop—as we see with the legged robot in Figure 25.5(b).

Token 19630:
This robot is dynamically stable , meaning that it can remain upright while hopping around.

Token 19631:
A robot that can remain DYNAMICALLY STABLE upright without moving its legs is called statically stable .

Token 19632:
A robot is statically stable if its STATICALLY STABLE center of gravity is above the polygon spanned by its legs.

Token 19633:
The quadruped (four-legged) robot shown in Figure 25.6(a) may appear statically stable.

Token 19634:
However, it walks by lifting multiplelegs at the same time, which renders it dynamically stable.

Token 19635:
The robot can walk on snow andice, and it will not fall over even if you kick it (as demonstrated in videos available online).Two-legged robots such as those in Figure 25.6(b) are dynamically stable.

Token 19636:
Other methods of movement are possible: air vehicles use propellers or turbines; un- derwater vehicles use propellers or thrusters, similar to those used on submarines.

Token 19637:
Robotic blimps rely on thermal effects to keep themselves aloft. Sensors and effectors alone do not make a robot.

Token 19638:
A complete robot also needs a source of power to drive its effectors.

Token 19639:
The electric motor is the most popular mechanism for both ELECTRIC MOTOR manipulator actuation and locomotion, but pneumatic actuation using compressed gas andPNEUMATIC ACTUATION hydraulic actuation using pressurized ﬂuids also have their application niches.HYDRAULIC ACTUATION

Token 19640:
978 Chapter 25. Robotics (a) (b) Figure 25.6 (a) Four-legged dynamically-stable robot “Big Dog.” Image courtesy Boston Dynamics, c/circlecopyrt2009.

Token 19641:
(b) 2009 RoboCup Standa rd Platform League competition, showing the winning team, B-Human, from the DFKI center at the University of Bremen.

Token 19642:
Throughout the match, B-Human outscored th eir opponents 64:1.

Token 19643:
Their success was built on probabilistic state estimation using particle ﬁlters and Kalman ﬁlters; on machine-learning models for gait optimization; and on dynamic kicking moves.

Token 19644:
Image courtesy DFKI, c/circlecopyrt2009.

Token 19645:
25.3 R OBOTIC PERCEPTION Perception is the process by which robots map sensor measurements into internal representa- tions of the environment.

Token 19646:
Perception is difﬁcult because sensors are noisy, and the environ- ment is partially observable, unpredictable, and often dynamic.

Token 19647:
In other words, robots haveall the problems of state estimation (orﬁltering ) that we discussed in Section 15.2.

Token 19648:
As a rule of thumb, good internal representations for robots have three properties: they containenough information for the robot to make good decisions, they are structured so that they canbe updated efﬁciently, and they are natural in the sense that internal variables correspond tonatural state variables in the physical world.

Token 19649:
In Chapter 15, we saw that Kalman ﬁlters, HMMs, and dynamic Bayes nets can repre- sent the transition and sensor models of a partially observable environment, and we described both exact and approximate algorithms for updating the belief state —the posterior probabil- ity distribution over the environment state variables.

Token 19650:
Several dynamic Bayes net models for this process were shown in Chapter 15.

Token 19651:
For robotics problems, we include the robot’s own past actions as observed variables in the model.

Token 19652:
Figure 25.7 shows the notation used in this chapter: X tis the state of the environment (including the robot) at time t,Ztis the observation received at time t,a n dAtis the action taken after the observation is received.

Token 19653:
Section 25.3.

Token 19654:
Robotic Perception 979 Xt+1 XtAt−2 At−1 At Zt−1Xt−1 Zt Zt+1 Figure 25.7 Robot perception can be viewed as temporal inference from sequences of actions and measurements, as illustra ted by this dynamic Bayes network.

Token 19655:
We would like to compute the new belief state, P(Xt+1|z1:t+1,a1:t), from the current belief state P(Xt|z1:t,a1:t−1)and the new observation zt+1.

Token 19656:
We did this in Section 15.2, but here there are two differences: we condition explicitly on the actions as well as the ob-servations, and we deal with continuous rather than discrete variables.

Token 19657:
Thus, we modify the recursive ﬁltering equation (15.5 on page 572) to use integration rather than summation: P(X t+1|z1:t+1,a1:t) =αP(zt+1|Xt+1)/integraldisplay P(Xt+1|xt,at)P(xt|z1:t,a1:t−1)dxt.

Token 19658:
(25.1) This equation states that the posterior over the state variables Xat time t+1is calculated recursively from the corresponding estimate one time step earlier.

Token 19659:
This calculation involves the previous action atand the current sensor measurement zt+1.

Token 19660:
For example, if our goal is to develop a soccer-playing robot, Xt+1might be the location of the soccer ball relative to the robot.

Token 19661:
The posterior P(Xt|z1:t,a1:t−1)is a probability distribution over all states that captures what we know from past sensor measurements and controls.

Token 19662:
Equation (25.1) tells us how to recursively estimate this location, by incrementally folding in sensor measurements (e.g., camera images) and robot motion commands.

Token 19663:
The probability P(Xt+1|xt,at)is called thetransition model ormotion model ,a n d P(zt+1|Xt+1)is the sensor model .

Token 19664:
MOTION MODEL 25.3.1 Localization and mapping Localization is the problem of ﬁnding out where things are—including the robot itself.

Token 19665:
LOCALIZATION Knowledge about where things are is at the core of any successful physical interaction with the environment.

Token 19666:
For example, robot manipulators must know the location of objects theyseek to manipulate; navigating robots must know where they are to ﬁnd their way around.

Token 19667:
To keep things simple, let us consider a mobile robot that moves slowly in a ﬂat 2D world.

Token 19668:
Let us also assume the robot is given an exact map of the environment. (An example of such a map appears in Figure 25.10.)

Token 19669:
The pose of such a mobile robot is deﬁned by its two Cartesian coordinates with values xandyand its heading with value θ, as illustrated in Figure 25.8(a).

Token 19670:
If we arrange those three values in a vector, then any particular state is givenbyX t=(xt,yt,θt)/latticetop. So far so good.

Token 19671:
980 Chapter 25. Robotics xi, yi vt Δt xt+1h(xt) xtθtt+1θt Δtω Z1Z2Z3Z4 (a) (b) Figure 25.8 (a) A simpliﬁed kinematic model of a mobile robot.

Token 19672:
The robot is shown as a circle with an interior line marking the forward direction.

Token 19673:
The state xtconsists of the (xt,yt) position (shown implicitly) and the orientation θt.

Token 19674:
The new state xt+1is obtained by an update in position of vtΔtand in orientation of ωtΔt.

Token 19675:
Also shown is a landmark at (xi,yi) observed at time t. (b) The range-scan sensor model.

Token 19676:
Two possible robot poses are shown for a given range scan (z1,z2,z3,z4).

Token 19677:
It is much more likely that the pose on the left generated the range scan than the pose on the right.

Token 19678:
In the kinematic approximation, each action consists of the “instantaneous” speciﬁca- tion of two velocities—a translational velocity vtand a rotational velocity ωt.

Token 19679:
For small time intervals Δt, a crude deterministic model of the motion of such robots is given by ˆXt+1=f(Xt,vt,ωt/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright at)=Xt+⎛ ⎝vtΔtcosθt vtΔtsinθt ωtΔt⎞ ⎠.

Token 19680:
The notation ˆXrefers to a deterministic state prediction. Of course, physical robots are somewhat unpredictable.

Token 19681:
This is commonly modeled by a Gaussian distribution with meanf(X t,vt,ωt)and covariance Σx. (See Appendix A for a mathematical deﬁnition.)

Token 19682:
P(Xt+1|Xt,vt,ωt)=N(ˆXt+1,Σx). This probability distribution is the robot’s motion model.

Token 19683:
It models the effects of the motion aton the location of the robot. Next, we need a sensor model. We will consider two kinds of sensor model.

Token 19684:
The ﬁrst assumes that the sensors detect stable ,recognizable features of the environment called landmarks .

Token 19685:
For each landmark, the range and bearing are reported.

Token 19686:
Suppose the robot’s state LANDMARK isxt=(xt,yt,θt)/latticetopand it senses a landmark whose location is known to be (xi,yi)/latticetop.

Token 19687:
Without noise, the range and bearing can be calculated by simple geometry. (See Figure 25.8(a).)

Token 19688:
Theexact prediction of the observed range and bearing would be ˆz t=h(xt)=/parenleftbigg/radicalbig (xt−xi)2+(yt−yi)2 arctanyi−yt xi−xt−θt/parenrightbigg .

Token 19689:
Section 25.3. Robotic Perception 981 Again, noise distorts our measurements.

Token 19690:
To keep things simple, one might assume Gaussian noise with covariance Σz, giving us the sensor model P(zt|xt)=N(ˆzt,Σz).

Token 19691:
A somewhat different sensor model is used for an array of range sensors, each of which has a ﬁxed bearing relative to the robot.

Token 19692:
Such sensors produce a vector of range values zt=(z1,...,z M)/latticetop.

Token 19693:
Given a pose xt,l e tˆzjbe the exact range along the jth beam direction from xtto the nearest obstacle.

Token 19694:
As before, this will be corrupted by Gaussian noise.

Token 19695:
Typically, we assume that the errors for the different beam directions are independent and identicallydistributed, so we have P(z t|xt)=αM/productdisplay j=1e−(zj−ˆzj)/2σ2.

Token 19696:
Figure 25.8(b) shows an example of a four-beam range scan and two possible robot poses, one of which is reasonably likely to have produced the observed scan and one of which is not.Comparing the range-scan model to the landmark model, we see that the range-scan modelhas the advantage that there is no need to identify a landmark before the range scan can be interpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall.

Token 19697:
On the other hand, if there arevisible, identiﬁable landmarks, they may provide instant localization.

Token 19698:
Chapter 15 described the Kalman ﬁlter, which represents the belief state as a single multivariate Gaussian, and the particle ﬁlter, which represents the belief state by a collectionof particles that correspond to states.

Token 19699:
Most modern localization algorithms use one of tworepresentations of the robot’s belief P(X t|z1:t,a1:t−1).

Token 19700:
Localization using particle ﬁltering is called Monte Carlo localization ,o rM C L .T h eMONTE CARLO LOCALIZATION MCL alfgorithm is an instance of the particle-ﬁltering algorithm of Figure 15.17 (page 598).

Token 19701:
All we need to do is supply the appropriate motion model and sensor model. Figure 25.9 shows one version using the range-scan model.

Token 19702:
The operation of the algorithm is illustrated in Figure 25.10 as the robot ﬁnds out where it is inside an ofﬁce building.

Token 19703:
In the ﬁrst image, the particles are uniformly distributed based on the prior, indicating global uncertainty about the robot’s position.

Token 19704:
In the second image, the ﬁrst set of measurements arrives and the particles form clusters in the areas of high posterior belief.

Token 19705:
In the third, enough measurements areavailable to push all the particles to a single location. The Kalman ﬁlter is the other major way to localize.

Token 19706:
A Kalman ﬁlter represents the posterior P(X t|z1:t,a1:t−1)by a Gaussian. The mean of this Gaussian will be denoted μtand its covariance Σt.

Token 19707:
The main problem with Gaussian beliefs is that they are only closed under linear motion models fand linear measurement models h. For nonlinear forh, the result of updating a ﬁlter is in general not Gaussian.

Token 19708:
Thus, localization algorithms using the Kalmanﬁlter linearize the motion and sensor models.

Token 19709:
Linearization is a local approximation of a LINEARIZATION nonlinear function by a linear function.

Token 19710:
Figure 25.11 illustrates the concept of linearization for a (one-dimensional) robot motion model.

Token 19711:
On the left, it depicts a nonlinear motion model f(xt,at)(the control atis omitted in this graph since it plays no role in the linearization).

Token 19712:
On the right, this function is approximated by a linear function ˜f(xt,at).

Token 19713:
This linear function is tangent to fat the point μt, the mean of our state estimate at time t. Such a linearization

Token 19714:
982 Chapter 25.

Token 19715:
Robotics function MONTE -CARLO -LOCALIZATION (a,z,N,P(X/prime|X, v, ω ),P(z|z∗),m)returns a set of samples for the next time step inputs :a, robot velocities vandω z, range scan z1,...,z M P(X/prime|X, v, ω ), motion model P(z|z∗), range sensor noise model m, 2D map of the environment persistent :S, a vector of samples of size N local variables :W, a vector of weights of size N S/prime, a temporary vector of particles of size N W/prime, a vector of weights of size N ifSis empty then /* initialization phase */ fori=1toNdo S[i]←sample from P(X0) fori=1toNdo /* update cycle */ S/prime[i]←sample from P(X/prime|X=S[i],v,ω) W/prime[i]←1 forj=1toMdo z∗←RAYCAST(j,X=S/prime[i],m) W/prime[i]←W/prime[i]·P(zj|z∗) S←WEIGHTED -SAMPLE -WITH-REPLACEMENT (N,S/prime,W/prime) return S Figure 25.9 A Monte Carlo localization algorithm using a range-scan sensor model with independent noise.

Token 19716:
is called (ﬁrst degree) Taylor expansion .

Token 19717:
A Kalman ﬁlter that linearizes fandhvia Taylor TAYLOR EXPANSION expansion is called an extended Kalman ﬁlter (or EKF).

Token 19718:
Figure 25.12 shows a sequence of estimates of a robot running an extended Kalman ﬁlter localization algorithm.

Token 19719:
As the robot moves, the uncertainty in its location estimate increases, as shown by the error ellipses.Its error decreases as it senses the range and bearing to a landmark with known locationand increases again as the robot loses sight of the landmark.

Token 19720:
EKF algorithms work well iflandmarks are easily identiﬁed. Otherwise, the posterior distribution may be multimodal, asin Figure 25.10(b).

Token 19721:
The problem of needing to know the identity of landmarks is an instanceof the data association problem discussed in Figure 15.6.

Token 19722:
In some situations, no map of the environment is available. Then the robot will have to acquire a map.

Token 19723:
This is a bit of a chicken-and-egg problem: the navigating robot will have todetermine its location relative to a map it doesn’t quite know, at the same time building thismap while it doesn’t quite know its actual location.

Token 19724:
This problem is important for many robot applications, and it has been studied extensively under the name simultaneous localization and mapping , abbreviated as SLAM .

Token 19725:
SIMULTANEOUS LOCALIZATION ANDMAPPING SLAM problems are solved using many different probabilistic techniques, including the extended Kalman ﬁlter discussed above.

Token 19726:
Using the EKF is straightforward: just augment

Token 19727:
Section 25.3.

Token 19728:
Robotic Perception 983 Robot position (a) Robot position (b) Robot position (c) Figure 25.10 Monte Carlo localization, a particle ﬁltering algorithm for mobile robot lo- calization.

Token 19729:
(a) Initial, global uncertainty. (b) Approximately bimodal uncertainty after navi- gating in the (symmetric) corridor.

Token 19730:
(c) Unimodal uncertainty after entering a room and ﬁnding it to be distinctive.

Token 19731:
984 Chapter 25.

Token 19732:
Robotics Xt+1 Xtμt Σtf(Xt, at) f(μt, at) Σt+1Xt+1 Xtμt Σtf(Xt, at) f(μt, at) Σt+1 Σt+1~f(Xt, at) = f(μt, at) + Ft(Xt − μt)~ (a) (b) Figure 25.11 One-dimensional illustration of a linearized motion model: (a) The function f, and the projection of a mean μtand a covariance interval (based on Σt) into time t+1.

Token 19733:
(b) The linearized version is the tangent of fatμt. The projection of the mean μtis correct. However, the projected covariance ˜Σt+1differs from Σt+1.

Token 19734:
robot landmark Figure 25.12 Example of localization using the extended Kalman ﬁlter. The robot moves on a straight line.

Token 19735:
As it progresses, its uncertainty increases gradually, as illustrated by theerror ellipses.

Token 19736:
When it observes a landmark with known position, the uncertainty is reduced.

Token 19737:
the state vector to include the locations of the landmarks in the environment.

Token 19738:
Luckily, the EKF update scales quadratically, so for small maps (e.g., a few hundred landmarks) the com-putation is quite feasible.

Token 19739:
Richer maps are often obtained using graph relaxation methods,similar to the Bayesian network inference techniques discussed in Chapter 14.

Token 19740:
Expectation–maximization is also used for SLAM. 25.3.2 Other types of perception Not all of robot perception is about localization or mapping.

Token 19741:
Robots also perceive the tem-perature, odors, acoustic signals, and so on.

Token 19742:
Many of these quantities can be estimated usingvariants of dynamic Bayes networks.

Token 19743:
All that is required for such estimators are conditionalprobability distributions that characterize the evolution of state variables over time, and sen- sor models that describe the relation of measurements to state variables.

Token 19744:
It is also possible to program a robot as a reactive agent, without explicitly reasoning about probability distributions over states.

Token 19745:
We cover that approach in Section 25.6.3. The trend in robotics is clearly towards representations with well-deﬁned semantics.

Token 19746:
Section 25.3. Robotic Perception 985 (a) (b) (c) Figure 25.13 Sequence of “drivable surface” classiﬁer results using adaptive vision.

Token 19747:
In (a) only the road is classiﬁed as drivable (striped area). The V-shaped dark line shows where the vehicle is heading.

Token 19748:
In (b) the vehicle is commanded to drive off the road, onto a grassy surface, and the classiﬁer is beginning to classify some of the grass as drivable.

Token 19749:
In (c) thevehicle has updated its model of drivable surface to correspond to grass as well as road.

Token 19750:
Probabilistic techniques outperform other approaches in many hard perceptual problems such as localization and mapping.

Token 19751:
However, statistical techniques are sometimes too cumbersome,and simpler solutions may be just as effective in practice.

Token 19752:
To help decide which approach totake, experience working with real physical robots is your best teacher.

Token 19753:
25.3.3 Machine learning in robot perception Machine learning plays an important role in robot perception.

Token 19754:
This is particularly the casewhen the best internal representation is not known.

Token 19755:
One common approach is to map high-dimensional sensor streams into lower-dimensional spaces using unsupervised machine learn-ing methods (see Chapter 18).

Token 19756:
Such an approach is called low-dimensional embedding .

Token 19757:
LOW-DIMENSIONAL EMBEDDING Machine learning makes it possible to learn sensor and motion models from data, while si- multaneously discovering a suitable internal representations.

Token 19758:
Another machine learning technique enables robots to continuously adapt to broad changes in sensor measurements.

Token 19759:
Picture yourself walking from a sun-lit space into a dark neon-lit room. Clearly things are darker inside.

Token 19760:
But the change of light source also affects all the colors: Neon light has a stronger component of green light than sunlight.

Token 19761:
Yet somehowwe seem not to notice the change.

Token 19762:
If we walk together with people into a neon-lit room, wedon’t think that suddenly their faces turned green.

Token 19763:
Our perception quickly adapts to the newlighting conditions, and our brain ignores the differences.

Token 19764:
Adaptive perception techniques enable robots to adjust to such changes.

Token 19765:
One example is shown in Figure 25.13, taken from the autonomous driving domain.

Token 19766:
Here an unmannedground vehicle adapts its classiﬁer of the concept “drivable surface.” How does this work?The robot uses a laser to provide classiﬁcation for a small area right in front of the robot.When this area is found to be ﬂat in the laser range scan, it is used as a positive training example for the concept “drivable surface.” A mixture-of-Gaussians technique similar to the EM algorithm discussed in Chapter 20 is then trained to recognize the speciﬁc color andtexture coefﬁcients of the small sample patch.

Token 19767:
The images in Figure 25.13 are the result ofapplying this classiﬁer to the full image.

Token 19768:
986 Chapter 25. Robotics Methods that make robots collect their own training data (with labels!) are called self- supervised .

Token 19769:
In this instance, the robot uses machine learning to leverage a short-range sensorSELF-SUPERVISED LEARNING that works well for terrain classiﬁcation into a sensor that can see much farther.

Token 19770:
That allows the robot to drive faster, slowing down only when the sensor model says there is a change inthe terrain that needs to be examined more carefully by the short-range sensors.

Token 19771:
25.4 P LANNING TO MOVE All of a robot’s deliberations ultimately come down to deciding how to move effectors.

Token 19772:
The point-to-point motion problem is to deliver the robot or its end effector to a designated targetPOINT-TO-POINT MOTION location.

Token 19773:
A greater challenge is the compliant motion problem, in which a robot moves COMPLIANT MOTION while being in physical contact with an obstacle.

Token 19774:
An example of compliant motion is a robot manipulator that screws in a light bulb, or a robot that pushes a box across a table top.

Token 19775:
We begin by ﬁnding a suitable representation in which motion-planning problems can be described and solved.

Token 19776:
It turns out that the conﬁguration space —the space of robot states deﬁned by location, orientation, and joint angles—is a better place to work than the original3D space.

Token 19777:
The path planning problem is to ﬁnd a path from one conﬁguration to another in PATH PLANNING conﬁguration space.

Token 19778:
We have already encountered various versions of the path-planning prob- lem throughout this book; the complication added by robotics is that path planning involvescontinuous spaces.

Token 19779:
There are two main approaches: cell decomposition andskeletonization .

Token 19780:
Each reduces the continuous path-planning problem to a discrete graph-search problem.

Token 19781:
In this section, we assume that motion is deterministic and that localization of the robot is exact.Subsequent sections will relax these assumptions.

Token 19782:
25.4.1 Conﬁguration space We will start with a simple representation for a simple robot motion problem.

Token 19783:
Consider the robot arm shown in Figure 25.14(a). It has two joints that move independently.

Token 19784:
Moving the joints alters the (x,y)coordinates of the elbow and the gripper. (The arm cannot move in the zdirection.)

Token 19785:
This suggests that the robot’s conﬁguration can be described by a four- dimensional coordinate: (xe,ye)for the location of the elbow relative to the environment and (xg,yg)for the location of the gripper.

Token 19786:
Clearly, these four coordinates characterize the full state of the robot.

Token 19787:
They constitute what is known as workspace representation , since theWORKSPACE REPRESENTATION coordinates of the robot are speciﬁed in the same coordinate system as the objects it seeks to manipulate (or to avoid).

Token 19788:
Workspace representations are well-suited for collision checking,especially if the robot and all objects are represented by simple polygonal models.

Token 19789:
The problem with the workspace representation is that not all workspace coordinates are actually attainable, even in the absence of obstacles.

Token 19790:
This is because of the linkage con- straints on the space of attainable workspace coordinates.

Token 19791:
For example, the elbow position LINKAGE CONSTRAINTS (xe,ye)and the gripper position (xg,yg)are always a ﬁxed distance apart, because they are joined by a rigid forearm.

Token 19792:
A robot motion planner deﬁned over workspace coordinates facesthe challenge of generating paths that adhere to these constraints.

Token 19793:
This is particularly tricky

Token 19794:
Section 25.4.

Token 19795:
Planning to Move 987 shouelb s e etabletable left wallvertical obstacle s e s (a) (b) Figure 25.14 (a) Workspace representation of a robot arm with 2 DOFs.

Token 19796:
The workspace is a box with a ﬂat obstacle hanging from the ceiling. (b) Conﬁguration space of the same robot.

Token 19797:
Only white regions in the space are conﬁ gurations that are free of collisions.

Token 19798:
The dot in this diagram corresponds to the conﬁguration of the robot shown on the left.

Token 19799:
because the state space is continuous and the constraints are nonlinear. It turns out to be eas- ier to plan with a conﬁguration space representation.

Token 19800:
Instead of representing the state of theCONFIGURATION SPACE robot by the Cartesian coordinates of its elements, we represent the state by a conﬁguration of the robot’s joints.

Token 19801:
Our example robot possesses two joints.

Token 19802:
Hence, we can represent itsstate with the two angles ϕ sandϕefor the shoulder joint and elbow joint, respectively.

Token 19803:
In the absence of any obstacles, a robot could freely take on any value in conﬁguration space.

Token 19804:
In particular, when planning a path one could simply connect the present conﬁguration and the target conﬁguration by a straight line.

Token 19805:
In following this path, the robot would then move itsjoints at a constant velocity, until a target location is reached.

Token 19806:
Unfortunately, conﬁguration spaces have their own problems.

Token 19807:
The task of a robot is usu- ally expressed in workspace coordinates, not in conﬁguration space coordinates.

Token 19808:
This raises the question of how to map between workspace coordinates and conﬁguration space.

Token 19809:
Trans- forming conﬁguration space coordinates into workspace coordinates is simple: it involves a series of straightforward coordinate transformations.

Token 19810:
These transformations are linear forprismatic joints and trigonometric for revolute joints.

Token 19811:
This chain of coordinate transformationis known as kinematics .

Token 19812:
KINEMATICS The inverse problem of calculating the conﬁguration of a robot whose effector location is speciﬁed in workspace coordinates is known as inverse kinematics .

Token 19813:
Calculating the inverseINVERSE KINEMATICS kinematics is hard, especially for robots with many DOFs. In particular, the solution is seldom unique.

Token 19814:
Figure 25.14(a) shows one of two possible conﬁgurations that put the gripper in thesame location.

Token 19815:
(The other conﬁguration would has the elbow below the shoulder.)

Token 19816:
988 Chapter 25.

Token 19817:
Robotics conf-3 conf-1 conf-2 conf-3conf-2 conf-1 e s s e (a) (b) Figure 25.15 Three robot conﬁgurations, shown in workspace and conﬁguration space.

Token 19818:
In general, this two-link robot arm has between zero and two inverse kinematic solu- tions for any set of workspace coordinates.

Token 19819:
Most industrial robots have sufﬁcient degreesof freedom to ﬁnd inﬁnitely many solutions to motion problems.

Token 19820:
To see how this is possi-ble, simply imagine that we added a third revolute joint to our example robot, one whoserotational axis is parallel to the ones of the existing joints.

Token 19821:
In such a case, we can keep thelocation (but not the orientation!)

Token 19822:
of the gripper ﬁxed and still freely rotate its internal joints,for most conﬁgurations of the robot. With a few more joints (how many?)

Token 19823:
we can achieve thesame effect while keeping the orientation of the gripper constant as well.

Token 19824:
We have already seen an example of this in the “experiment” of placing your hand on the desk and moving your elbow.

Token 19825:
The kinematic constraint of your hand position is insufﬁcient to determine theconﬁguration of your elbow.

Token 19826:
In other words, the inverse kinematics of your shoulder–armassembly possesses an inﬁnite number of solutions.

Token 19827:
The second problem with conﬁguration space representations arises from the obsta- cles that may exist in the robot’s workspace.

Token 19828:
Our example in Figure 25.14(a) shows severalsuch obstacles, including a free-hanging obstacle that protrudes into the center of the robot’sworkspace.

Token 19829:
In workspace, such obstacles take on simple geometric forms—especially inmost robotics textbooks, which tend to focus on polygonal obstacles.

Token 19830:
But how do they lookin conﬁguration space?

Token 19831:
Figure 25.14(b) shows the conﬁguration space for our example robot, under the speciﬁc obstacle conﬁguration shown in Figure 25.14(a).

Token 19832:
The conﬁguration space can be decomposed into two subspaces: the space of all conﬁgurations that a robot may attain, commonly calledfree space , and the space of unattainable conﬁgurations, called occupied space .

Token 19833:
The white FREE SPACE OCCUPIED SPACE area in Figure 25.14(b) corresponds to the free space. All other regions correspond to occu-

Token 19834:
Section 25.4. Planning to Move 989 pied space.

Token 19835:
The different shadings of the occupied space corresponds to the different objects in the robot’s workspace; the black region surrounding the entire free space corresponds toconﬁgurations in which the robot collides with itself.

Token 19836:
It is easy to see that extreme values ofthe shoulder or elbow angles cause such a violation.

Token 19837:
The two oval-shaped regions on bothsides of the robot correspond to the table on which the robot is mounted.

Token 19838:
The third oval region corresponds to the left wall.

Token 19839:
Finally, the most interesting object in conﬁguration space is the vertical obstacle that hangs from the ceiling and impedes the robot’s motions.

Token 19840:
This object hasa funny shape in conﬁguration space: it is highly nonlinear and at places even concave.

Token 19841:
Witha little bit of imagination the reader will recognize the shape of the gripper at the upper leftend.

Token 19842:
We encourage the reader to pause for a moment and study this diagram. The shape ofthis obstacle is not at all obvious!

Token 19843:
The dot inside Figure 25.14(b) marks the conﬁguration ofthe robot, as shown in Figure 25.14(a).

Token 19844:
Figure 25.15 depicts three additional conﬁgurations,both in workspace and in conﬁguration space.

Token 19845:
In conﬁguration conf-1, the gripper enclosesthe vertical obstacle.

Token 19846:
Even if the robot’s workspace is represented by ﬂat polygons, the shape of the free space can be very complicated.

Token 19847:
In practice, therefore, one usually probes a conﬁguration space instead of constructing it explicitly.

Token 19848:
A planner may generate a conﬁguration and then test to see if it is in free space by applying the robot kinematics and then checking for collisions in workspace coordinates.

Token 19849:
25.4.2 Cell decomposition methods The ﬁrst approach to path planning uses cell decomposition —that is, it decomposes theCELL DECOMPOSITION free space into a ﬁnite number of contiguous regions, called cells.

Token 19850:
These regions have the important property that the path-planning problem within a single region can be solved by simple means (e.g., moving along a straight line).

Token 19851:
The path-planning problem then becomes a discrete graph-search problem, very much like the search problems introduced in Chapter 3.

Token 19852:
The simplest cell decomposition consists of a regularly spaced grid.

Token 19853:
Figure 25.16(a) shows a square grid decomposition of the space and a solution path that is optimal for this grid size.

Token 19854:
Grayscale shading indicates the value of each free-space grid cell—i.e., the cost of the shortest path from that cell to the goal.

Token 19855:
(These values can be computed by a deterministicform of the V ALUE -ITERATION algorithm given in Figure 17.4 on page 653.)

Token 19856:
Figure 25.16(b) shows the corresponding workspace trajectory for the arm. Of course, we can also use the A∗ algorithm to ﬁnd a shortest path.

Token 19857:
Such a decomposition has the advantage that it is extremely simple to implement, but it also suffers from three limitations.

Token 19858:
First, it is workable only for low-dimensional conﬁgu-ration spaces, because the number of grid cells increases exponentially with d, the number of dimensions.

Token 19859:
Sounds familiar? This is the curse!dimensionality@of dimensionality.

Token 19860:
Second,there is the problem of what to do with cells that are “mixed”—that is, neither entirely within free space nor entirely within occupied space.

Token 19861:
A solution path that includes such a cell may not be a real solution, because there may be no way to cross the cell in the desired directionin a straight line.

Token 19862:
This would make the path planner unsound .

Token 19863:
On the other hand, if we insist that only completely free cells may be used, the planner will be incomplete , because it might

Token 19864:
990 Chapter 25.

Token 19865:
Robotics startgoalstartgoal (a) (b) Figure 25.16 (a) Value function and path found for a discrete grid cell approximation of the conﬁguration space.

Token 19866:
(b) The same path visualized in workspace coordinates. Notice how the robot bends its elbow to avoid a collision with the vertical obstacle.

Token 19867:
be the case that the only paths to the goal go through mixed cells—especially if the cell size is comparable to that of the passageways and clearances in the space.

Token 19868:
And third, any paththrough a discretized state space will not be smooth.

Token 19869:
It is generally difﬁcult to guarantee thata smooth solution exists near the discrete path.

Token 19870:
So a robot may not be able to execute thesolution found through this decomposition.

Token 19871:
Cell decomposition methods can be improved in a number of ways, to alleviate some of these problems.

Token 19872:
The ﬁrst approach allows further subdivision of the mixed cells—perhaps using cells of half the original size.

Token 19873:
This can be continued recursively until a path is found that lies entirely within free cells.

Token 19874:
(Of course, the method only works if there is a way to decide if a given cell is a mixed cell, which is easy only if the conﬁguration space boundarieshave relatively simple mathematical descriptions.)

Token 19875:
This method is complete provided there isa bound on the smallest passageway through which a solution must pass.

Token 19876:
Although it focusesmost of the computational effort on the tricky areas within the conﬁguration space, it stillfails to scale well to high-dimensional problems because each recursive splitting of a cellcreates 2 dsmaller cells.

Token 19877:
A second way to obtain a complete algorithm is to insist on an exact cell decomposition of the free space.

Token 19878:
This method must allow cells to be irregularly shapedEXACT CELL DECOMPOSITION where they meet the boundaries of free space, but the shapes must still be “simple” in the sense that it should be easy to compute a traversal of any free cell.

Token 19879:
This technique requiressome quite advanced geometric ideas, so we shall not pursue it further here.

Token 19880:
Examining the solution path shown in Figure 25.16(a), we can see an additional difﬁ- culty that will have to be resolved.

Token 19881:
The path contains arbitrarily sharp corners; a robot movingat any ﬁnite speed could not execute such a path.

Token 19882:
This problem is solved by storing certaincontinuous values for each grid cell. Consider an algorithm which stores, for each grid cell,

Token 19883:
Section 25.4. Planning to Move 991 the exact, continuous state that was attained with the cell was ﬁrst expanded in the search.

Token 19884:
Assume further, that when propagating information to nearby grid cells, we use this continu-ous state as a basis, and apply the continuous robot motion model for jumping to nearby cells.In doing so, we can now guarantee that the resulting trajectory is smooth and can indeed beexecuted by the robot.

Token 19885:
One algorithm that implements this is hybrid A* .

Token 19886:
HYBRID A* 25.4.3 Modiﬁed cost functions Notice that in Figure 25.16, the path goes very close to the obstacle.

Token 19887:
Anyone who has driven a car knows that a parking space with one millimeter of clearance on either side is not really aparking space at all; for the same reason, we would prefer solution paths that are robust withrespect to small motion errors.

Token 19888:
This problem can be solved by introducing a potential ﬁeld .

Token 19889:
A potential ﬁeld is a POTENTIAL FIELD function deﬁned over state space, whose value grows with the distance to the closest obstacle.

Token 19890:
Figure 25.17(a) shows such a potential ﬁeld—the darker a conﬁguration state, the closer it isto an obstacle.

Token 19891:
The potential ﬁeld can be used as an additional cost term in the shortest-path calculation. This induces an interesting tradeoff.

Token 19892:
On the one hand, the robot seeks to minimize path lengthto the goal.

Token 19893:
On the other hand, it tries to stay away from obstacles by virtue of minimizing thepotential function.

Token 19894:
With the appropriate weight balancing the two objectives, a resulting pathmay look like the one shown in Figure 25.17(b).

Token 19895:
This ﬁgure also displays the value functionderived from the combined cost function, again calculated by value iteration.

Token 19896:
Clearly, theresulting path is longer, but it is also safer. There exist many other ways to modify the cost function.

Token 19897:
For example, it may be desirable to smooth the control parameters over time.

Token 19898:
For example, when driving a car, a smooth path is better than a jerky one.

Token 19899:
In general, such higher-order constraints are not easyto accommodate in the planning process, unless we make the most recent steering commanda part of the state.

Token 19900:
However, it is often easy to smooth the resulting trajectory after planning,using conjugate gradient methods.

Token 19901:
Such post-planning smoothing is essential in many real-world applications.

Token 19902:
25.4.4 Skeletonization methods The second major family of path-planning algorithms is based on the idea of skeletonization .

Token 19903:
SKELETONIZATION These algorithms reduce the robot’s free space to a one-dimensional representation, for which the planning problem is easier.

Token 19904:
This lower-dimensional representation is called a skeleton of the conﬁguration space.

Token 19905:
Figure 25.18 shows an example skeletonization: it is a Voronoi graph of the free VORONOI GRAPH space—the set of all points that are equidistant to two or more obstacles.

Token 19906:
To do path plan- ning with a Voronoi graph, the robot ﬁrst changes its present conﬁguration to a point on the Voronoi graph.

Token 19907:
It is easy to show that this can always be achieved by a straight-line motion in conﬁguration space.

Token 19908:
Second, the robot follows the Voronoi graph until it reaches the pointnearest to the target conﬁguration.

Token 19909:
Finally, the robot leaves the Voronoi graph and moves tothe target. Again, this ﬁnal step involves straight-line motion in conﬁguration space.

Token 19910:
992 Chapter 25. Robotics start goal (a) (b) Figure 25.17 (a) A repelling potential ﬁeld pushe s the robot away from obstacles.

Token 19911:
(b) Path found by simultaneously minimizing path length and the potential.

Token 19912:
(a) (b) Figure 25.18 (a) The V oronoi graph is the set of points equidistant to two or more obsta- cles in conﬁguration space.

Token 19913:
(b) A probabilistic roadmap, composed of 400 randomly chosen points in free space.

Token 19914:
In this way, the original path-planning problem is reduced to ﬁnding a path on the Voronoi graph, which is generally one-dimensional (except in certain nongeneric cases) andhas ﬁnitely many points where three or more one-dimensional curves intersect.

Token 19915:
Thus, ﬁnding

Token 19916:
Section 25.5.

Token 19917:
Planning Uncertain Movements 993 the shortest path along the Voronoi graph is a discrete graph-search problem of the kind discussed in Chapters 3 and 4.

Token 19918:
Following the Voronoi graph may not give us the shortestpath, but the resulting paths tend to maximize clearance.

Token 19919:
Disadvantages of Voronoi graphtechniques are that they are difﬁcult to apply to higher-dimensional conﬁguration spaces, andthat they tend to induce unnecessarily large detours when the conﬁguration space is wide open.

Token 19920:
Furthermore, computing the Voronoi graph can be difﬁcult, especially in conﬁguration space, where the shapes of obstacles can be complex.

Token 19921:
An alternative to the Voronoi graphs is the probabilistic roadmap , a skeletonization PROBABILISTIC ROADMAP approach that offers more possible routes, and thus deals better with wide-open spaces.

Token 19922:
Fig- ure 25.18(b) shows an example of a probabilistic roadmap.

Token 19923:
The graph is created by randomlygenerating a large number of conﬁgurations, and discarding those that do not fall into freespace.

Token 19924:
Two nodes are joined by an arc if it is “easy” to reach one node from the other–forexample, by a straight line in free space.

Token 19925:
The result of all this is a randomized graph in therobot’s free space.

Token 19926:
If we add the robot’s start and goal conﬁgurations to this graph, pathplanning amounts to a discrete graph search.

Token 19927:
Theoretically, this approach is incomplete, be-cause a bad choice of random points may leave us without any paths from start to goal.

Token 19928:
Itis possible to bound the probability of failure in terms of the number of points generated and certain geometric properties of the conﬁguration space.

Token 19929:
It is also possible to direct the generation of sample points towards the areas where a partial search suggests that a goodpath may be found, working bidirectionally from both the start and the goal positions.

Token 19930:
Withthese improvements, probabilistic roadmap planning tends to scale better to high-dimensionalconﬁguration spaces than most alternative path-planning techniques.

Token 19931:
25.5 P LANNING UNCERTAIN MOVEMENTS None of the robot motion-planning algorithms discussed thus far addresses a key characteris-tic of robotics problems: uncertainty .

Token 19932:
In robotics, uncertainty arises from partial observability of the environment and from the stochastic (or unmodeled) effects of the robot’s actions.

Token 19933:
Er-rors can also arise from the use of approximation algorithms such as particle ﬁltering, whichdoes not provide the robot with an exact belief state even if the stochastic nature of the envi-ronment is modeled perfectly.

Token 19934:
Most of today’s robots use deterministic algorithms for decision making, such as the path-planning algorithms of the previous section.

Token 19935:
To do so, it is common practice to extractthemost likely state from the probability distribution produced by the state estimation al- MOST LIKELY STATE gorithm.

Token 19936:
The advantage of this approach is purely computational.

Token 19937:
Planning paths through conﬁguration space is already a challenging problem; it would be worse if we had to work with a full probability distribution over states.

Token 19938:
Ignoring uncertainty in this way works when the uncertainty is small.

Token 19939:
In fact, when the environment model changes over time as the resultof incorporating sensor measurements, many robots plan paths online during plan execution.This is the online replanning technique of Section 11.3.3.

Token 19940:
ONLINEREPLANNING

Token 19941:
994 Chapter 25. Robotics Unfortunately, ignoring the uncertainty does not always work.

Token 19942:
In some problems the robot’s uncertainty is simply too massive: How can we use a deterministic path planner tocontrol a mobile robot that has no clue where it is?

Token 19943:
In general, if the robot’s true state is notthe one identiﬁed by the maximum likelihood rule, the resulting control will be suboptimal.Depending on the magnitude of the error this can lead to all sorts of unwanted effects, such as collisions with obstacles.

Token 19944:
The ﬁeld of robotics has adopted a range of techniques for accommodating uncertainty.

Token 19945:
Some are derived from the algorithms given in Chapter 17 for decision making under uncer-tainty.

Token 19946:
If the robot faces uncertainty only in its state transition, but its state is fully observable,the problem is best modeled as a Markov decision process (MDP).

Token 19947:
The solution of an MDP isan optimal policy , which tells the robot what to do in every possible state.

Token 19948:
In this way, it can handle all sorts of motion errors, whereas a single-path solution from a deterministic plannerwould be much less robust.

Token 19949:
In robotics, policies are called navigation functions .T h ev a l u e NAVIGATION FUNCTION function shown in Figure 25.16(a) can be converted into such a navigation function simply by following the gradient.

Token 19950:
Just as in Chapter 17, partial observability makes the problem much harder.

Token 19951:
The result- ing robot control problem is a partially observable MDP, or POMDP.

Token 19952:
In such situations, the robot maintains an internal belief state, like the ones discussed in Section 25.3.

Token 19953:
The solution to a POMDP is a policy deﬁned over the robot’s belief state.

Token 19954:
Put differently, the input tothe policy is an entire probability distribution.

Token 19955:
This enables the robot to base its decision notonly on what it knows, but also on what it does not know.

Token 19956:
For example, if it is uncertainabout a critical state variable, it can rationally invoke an information gathering action .T h i s INFORMATION GATHERING ACTION is impossible in the MDP framework, since MDPs assume full observability.

Token 19957:
Unfortunately, techniques that solve POMDPs exactly are inapplicable to robotics—there are no known tech-niques for high-dimensional continuous spaces.

Token 19958:
Discretization produces POMDPs that are fartoo large to handle. One remedy is to make the minimization of uncertainty a control objec-tive.

Token 19959:
For example, the coastal navigation heuristic requires the robot to stay near known COASTAL NAVIGATION landmarks to decrease its uncertainty.

Token 19960:
Another approach applies variants of the probabilis- tic roadmap planning method to the belief space representation.

Token 19961:
Such methods tend to scale better to large discrete POMDPs.

Token 19962:
25.5.1 Robust methods Uncertainty can also be handled using so-called robust control methods (see page 836) rather ROBUST CONTROL than probabilistic methods.

Token 19963:
A robust method is one that assumes a bounded amount of un- certainty in each aspect of a problem, but does not assign probabilities to values within theallowed interval.

Token 19964:
A robust solution is one that works no matter what actual values occur,provided they are within the assumed interval.

Token 19965:
An extreme form of robust method is the con- formant planning approach given in Chapter 11—it produces plans that work with no state information at all.

Token 19966:
Here, we look at a robust method that is used for ﬁne-motion planning (or FMP) in FINE-MOTION PLANNING robotic assembly tasks.

Token 19967:
Fine-motion planning involves moving a robot arm in very close proximity to a static environment object.

Token 19968:
The main difﬁculty with ﬁne-motion planning is

Token 19969:
Section 25.5.

Token 19970:
Planning Uncertain Movements 995 vCv motion envelopeinitial configuration Figure 25.19 A two-dimensional environment, velocity uncertainty cone, and envelope of possible robot motions.

Token 19971:
The intended velocity is v, but with uncertainty the actual velocity could be anywhere in Cv, resulting in a ﬁnal conﬁguration somewhere in the motion envelope, which means we wouldn’t know if we hit the hole or not.

Token 19972:
vCv motion envelopeinitial configuration Figure 25.20 The ﬁrst motion command and the re sulting envelope of possible robot mo- tions.

Token 19973:
No matter what the error, we know the ﬁnal conﬁguration will be to the left of the hole.

Token 19974:
that the required motions and the relevant features of the environment are very small.

Token 19975:
At such small scales, the robot is unable to measure or control its position accurately and may also be uncertain of the shape of the environment itself; we will assume that these uncertainties areall bounded.

Token 19976:
The solutions to FMP problems will typically be conditional plans or policiesthat make use of sensor feedback during execution and are guaranteed to work in all situationsconsistent with the assumed uncertainty bounds.

Token 19977:
A ﬁne-motion plan consists of a series of guarded motions .

Token 19978:
Each guarded motion GUARDED MOTION consists of (1) a motion command and (2) a termination condition, which is a predicate on the robot’s sensor values, and returns true to indicate the end of the guarded move.

Token 19979:
The motioncommands are typically compliant motions that allow the effector to slide if the motion COMPLIANT MOTION command would cause collision with an obstacle.

Token 19980:
As an example, Figure 25.19 shows a two- dimensional conﬁguration space with a narrow vertical hole.

Token 19981:
It could be the conﬁguration space for insertion of a rectangular peg into a hole or a car key into the ignition.

Token 19982:
The motion commands are constant velocities. The termination conditions are contact with a surface.

Token 19983:
Tomodel uncertainty in control, we assume that instead of moving in the commanded direction,the robot’s actual motion lies in the cone C vabout it.

Token 19984:
The ﬁgure shows what would happen

Token 19985:
996 Chapter 25. Robotics vCv motion envelope Figure 25.21 The second motion command and the envelope of possible motions.

Token 19986:
Even with error, we will eventually get into the hole. if we commanded a velocity straight down from the initial conﬁguration.

Token 19987:
Because of the uncertainty in velocity, the robot could move anywhere in the conical envelope, possiblygoing into the hole, but more likely landing to one side of it.

Token 19988:
Because the robot would notthen know which side of the hole it was on, it would not know which way to move.

Token 19989:
A more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the robot deliberately moves to one side of the hole.

Token 19990:
The motion command is shown in the ﬁgure, and the termination test is contact with any surface.

Token 19991:
In Figure 25.21, a motion command isgiven that causes the robot to slide along the surface and into the hole.

Token 19992:
Because all possiblevelocities in the motion envelope are to the right, the robot will slide to the right whenever itis in contact with a horizontal surface.

Token 19993:
It will slide down the right-hand vertical edge of thehole when it touches it, because all possible velocities are down relative to a vertical surface.It will keep moving until it reaches the bottom of the hole, because that is its terminationcondition.

Token 19994:
In spite of the control uncertainty, all possible trajectories of the robot terminatein contact with the bottom of the hole—that is, unless surface irregularities cause the robot tostick in one place.

Token 19995:
As one might imagine, the problem of constructing ﬁne-motion plans is not trivial; in fact, it is a good deal harder than planning with exact motions.

Token 19996:
One can either choose a ﬁxed number of discrete values for each motion or use the environment geometry to choose directions that give qualitatively different behavior.

Token 19997:
A ﬁne-motion planner takes as input theconﬁguration-space description, the angle of the velocity uncertainty cone, and a speciﬁcationof what sensing is possible for termination (surface contact in this case).

Token 19998:
It should produce amultistep conditional plan or policy that is guaranteed to succeed, if such a plan exists.

Token 19999:
Our example assumes that the planner has an exact model of the environment, but it is possible to allow for bounded error in this model as follows.

Token 20000:
If the error can be described interms of parameters, those parameters can be added as degrees of freedom to the conﬁgurationspace.

Token 20001:
In the last example, if the depth and width of the hole were uncertain, we could addthem as two degrees of freedom to the conﬁguration space.

Token 20002:
It is impossible to move the robot in these directions in the conﬁguration space or to sense its position directly.

Token 20003:
But both those restrictions can be incorporated when describing this problem as an FMP problemby appropriately specifying control and sensor uncertainties.

Token 20004:
This gives a complex, four-dimensional planning problem, but exactly the same planning techniques can be applied.

Token 20005:
Section 25.6.

Token 20006:
Moving 997 Notice that unlike the decision-theoretic methods in Chapter 17, this kind of robust approach results in plans designed for the worst-case outcome, rather than maximizing the expectedquality of the plan.

Token 20007:
Worst-case plans are optimal in the decision-theoretic sense only if failureduring execution is much worse than any of the other costs involved in execution.

Token 20008:
25.6 M OVING So far, we have talked about how to plan motions, but not about how to move .

Token 20009:
Our plans— particularly those produced by deterministic path planners—assume that the robot can simply follow any path that the algorithm produces.

Token 20010:
In the real world, of course, this is not the case. Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds.

Token 20011:
Inmost cases, the robot gets to exert forces rather than specify positions. This section discussesmethods for calculating these forces.

Token 20012:
25.6.1 Dynamics and control Section 25.2 introduced the notion of dynamic state , which extends the kinematic state of a robot by its velocity.

Token 20013:
For example, in addition to the angle of a robot joint, the dynamic statealso captures the rate of change of the angle, and possibly even its momentary acceleration.The transition model for a dynamic state representation includes the effect of forces on thisrate of change.

Token 20014:
Such models are typically expressed via differential equations , which are DIFFERENTIAL EQUATION equations that relate a quantity (e.g., a kinematic state) to the change of the quantity over time (e.g., velocity).

Token 20015:
In principle, we could have chosen to plan robot motion using dynamicmodels, instead of our kinematic models.

Token 20016:
Such a methodology would lead to superior robotperformance, if we could generate the plans.

Token 20017:
However, the dynamic state has higher dimen-sion than the kinematic space, and the curse of dimensionality would render many motionplanning algorithms inapplicable for all but the most simple robots.

Token 20018:
For this reason, practical robot system often rely on simpler kinematic path planners.

Token 20019:
A common technique to compensate for the limitations of kinematic plans is to use a separate mechanism, a controller , for keeping the robot on track.

Token 20020:
Controllers are techniques CONTROLLER for generating robot controls in real time using feedback from the environment, so as to achieve a control objective.

Token 20021:
If the objective is to keep the robot on a preplanned path, it is o f t e nr e f e r r e dt oa sa reference controller and the path is called a reference path .

Token 20022:
ControllersREFERENCE CONTROLLER REFERENCE PATH that optimize a global cost function are known as optimal controllers .

Token 20023:
Optimal policies for OPTIMAL CONTROLLERS continuous MDPs are, in effect, optimal controllers.

Token 20024:
On the surface, the problem of keeping a robot on a prespeciﬁed path appears to be relatively straightforward.

Token 20025:
In practice, however, even this seemingly simple problem has itspitfalls.

Token 20026:
Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that attempts to follow a kinematic path.

Token 20027:
Whenever a deviation occurs—whether due to noise or to constraints on the forces the robot can apply—the robot provides an opposing force whosemagnitude is proportional to this deviation.

Token 20028:
Intuitively, this might appear plausible, sincedeviations should be compensated by a counterforce to keep the robot on track. However,

Token 20029:
998 Chapter 25.

Token 20030:
Robotics (a) (b) (c) Figure 25.22 Robot arm control using (a) proportional control with gain factor 1.0, (b) proportional control with gain factor 0.1, and (c) PD (proportional derivative) control with gain factors 0.3 for the proportional component and 0.8 for the differential component.

Token 20031:
In allcases the robot arm tries to follow the path shown in gray.

Token 20032:
as Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently.

Token 20033:
The vibration is the result of a natural inertia of the robot arm: once driven back to its referenceposition the robot then overshoots, which induces a symmetric error with opposite sign.

Token 20034:
Such overshooting may continue along an entire trajectory, and the resulting robot motion is far from desirable.

Token 20035:
Before we can deﬁne a better controller, let us formally describe what went wrong.

Token 20036:
Controllers that provide force in negative proportion to the observed error are known as P controllers .

Token 20037:
The letter ‘P’ stands for proportional , indicating that the actual control is pro- P CONTROLLER portional to the error of the robot manipulator.

Token 20038:
More formally, let y(t)be the reference path, parameterized by time index t. The control atgenerated by a P controller has the form: at=KP(y(t)−xt).

Token 20039:
Herextis the state of the robot at time tandKPis a constant known as the gain parameter of GAIN PARAMETER the controller and its value is called the gain factor); Kpregulates how strongly the controller corrects for deviations between the actual state xtand the desired one y(t).

Token 20040:
In our example, KP=1. At ﬁrst glance, one might think that choosing a smaller value for KPwould remedy the problem.

Token 20041:
Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory forKP=.1, still exhibiting oscillatory behavior.

Token 20042:
Lower values of the gain parameter may simply slow down the oscillation, but do not solve the problem.

Token 20043:
In fact, in the absence offriction, the P controller is essentially a spring law; so it will oscillate indeﬁnitely around aﬁxed target location.

Token 20044:
Traditionally, problems of this type fall into the realm of control theory ,aﬁ e l do f increasing importance to researchers in AI.

Token 20045:
Decades of research in this ﬁeld have led to a large number of controllers that are superior to the simple control law given above.

Token 20046:
In particular, areference controller is said to be stable if small perturbations lead to a bounded error between STABLE the robot and the reference signal.

Token 20047:
It is said to be strictly stable if it is able to return to and STRICTLY STABLE

Token 20048:
Section 25.6. Moving 999 then stay on its reference path upon such perturbations.

Token 20049:
Our P controller appears to be stable but not strictly stable, since it fails to stay anywhere near its reference trajectory.

Token 20050:
The simplest controller that achieves strict stability in our domain is a PD controller .

Token 20051:
PD CONTROLLER The letter ‘P’ stands again for proportional , and ‘D’ stands for derivative .

Token 20052:
PD controllers are described by the following equation: at=KP(y(t)−xt)+KD∂(y(t)−xt) ∂t.

Token 20053:
(25.2) As this equation suggests, PD controllers extend P controllers by a differential component, which adds to the value of ata term that is proportional to the ﬁrst derivative of the error y(t)−xtover time.

Token 20054:
What is the effect of such a term? In general, a derivative term dampens the system that is being controlled.

Token 20055:
To see this, consider a situation where the error (y(t)−xt) is changing rapidly over time, as is the case for our P controller above.

Token 20056:
The derivative of this error will then counteract the proportional term, which will reduce the overall response to the perturbation.

Token 20057:
However, if the same error persists and does not change, the derivative willvanish and the proportional term dominates the choice of control.

Token 20058:
Figure 25.22(c) shows the result of applying this PD controller to our robot arm, using as gain parameters K P=.3andKD=.8.

Token 20059:
Clearly, the resulting path is much smoother, and does not exhibit any obvious oscillations. PD controllers do have failure modes, however.

Token 20060:
In particular, PD controllers may fail to regulate an error down to zero, even in the absence of external perturbations.

Token 20061:
Often sucha situation is the result of a systematic external force that is not part of the model.

Token 20062:
An au-tonomous car driving on a banked surface, for example, may ﬁnd itself systematically pulledto one side.

Token 20063:
Wear and tear in robot arms cause similar systematic errors.

Token 20064:
In such situations, an over-proportional feedback is required to drive the error closer to zero.

Token 20065:
The solution to this problem lies in adding a third term to the control law, based on the integrated error over time: a t=KP(y(t)−xt)+KI/integraldisplay (y(t)−xt)dt+KD∂(y(t)−xt) ∂t.

Token 20066:
(25.3) HereKIis yet another gain parameter. The term/integraltext (y(t)−xt)dtcalculates the integral of the error over time.

Token 20067:
The effect of this term is that long-lasting deviations between the referencesignal and the actual state are corrected.

Token 20068:
If, for example, x tis smaller than y(t)for a long period of time, this integral will grow until the resulting control atforces this error to shrink.

Token 20069:
Integral terms, then, ensure that a controller does not exhibit systematic error, at the expense of increased danger of oscillatory behavior.

Token 20070:
A controller with all three terms is called a PID controller (for proportional integral derivative).

Token 20071:
PID controllers are widely used in industry, PID CONTROLLER for a variety of control problems.

Token 20072:
25.6.2 Potential-ﬁeld control We introduced potential ﬁelds as an additional cost function in robot motion planning, but they can also be used for generating robot motion directly, dispensing with the path planning phase altogether.

Token 20073:
To achieve this, we have to deﬁne an attractive force that pulls the robottowards its goal conﬁguration and a repellent potential ﬁeld that pushes the robot away fromobstacles.

Token 20074:
Such a potential ﬁeld is shown in Figure 25.23. Its single global minimum is

Token 20075:
1000 Chapter 25. Robotics startgoal start goal (a) (b) Figure 25.23 Potential ﬁeld control.

Token 20076:
The robot ascends a potential ﬁeld composed of repelling forces asserted from the obstacles and an attracting force that corresponds to thegoal conﬁguration.

Token 20077:
(a) Successful path. (b) Local optimum.

Token 20078:
the goal conﬁguration, and the value is the sum of the distance to this goal conﬁguration and the proximity to obstacles.

Token 20079:
No planning was involved in generating the potential ﬁeldshown in the ﬁgure.

Token 20080:
Because of this, potential ﬁelds are well suited to real-time control.Figure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potentialﬁeld.

Token 20081:
In many applications, the potential ﬁeld can be calculated efﬁciently for any givenconﬁguration.

Token 20082:
Moreover, optimizing the potential amounts to calculating the gradient of the potential for the present robot conﬁguration.

Token 20083:
These calculations can be extremely efﬁcient, especially when compared to path-planning algorithms, all of which are exponential in thedimensionality of the conﬁguration space (the DOFs) in the worst case.

Token 20084:
The fact that the potential ﬁeld approach manages to ﬁnd a path to the goal in such an efﬁcient manner, even over long distances in conﬁguration space, raises the question as to whether there is a need for planning in robotics at all.

Token 20085:
Are potential ﬁeld techniques sufﬁcient, or were we just lucky in our example? The answer is that we were indeed lucky.

Token 20086:
Potential ﬁelds have many local minima that can trap the robot.

Token 20087:
In Figure 25.23(b), the robotapproaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrongside of the obstacle.

Token 20088:
The potential ﬁeld is not rich enough to make the robot bend its elbowso that the arm ﬁts under the obstacle.

Token 20089:
In other words, potential ﬁeld control is great for local robot motion but sometimes we still need global planning.

Token 20090:
Another important drawback with potential ﬁelds is that the forces they generate depend only on the obstacle and robot positions,not on the robot’s velocity.

Token 20091:
Thus, potential ﬁeld control is really a kinematic method and mayfail if the robot is moving quickly.

Token 20092:
Section 25.6. Moving 1001 S1 S2S4 S3 push backwardlift up set downretract, lift higher move forwardnoyes stuck?

Token 20093:
(a) (b) Figure 25.24 (a) Genghis, a hexapod robot. (b) An augmented ﬁnite state machine (AFSM) for the control of a single leg.

Token 20094:
Notice that this AFSM reacts to sensor feedback: if a leg is stuck during the forward swinging phase, it will be lifted increasingly higher.

Token 20095:
25.6.3 Reactive control So far we have considered control decisions that require some model of the environment for constructing either a reference path or a potential ﬁeld.

Token 20096:
There are some difﬁculties with thisapproach.

Token 20097:
First, models that are sufﬁciently accurate are often difﬁcult to obtain, especiallyin complex or remote environments, such as the surface of Mars, or for robots that havefew sensors.

Token 20098:
Second, even in cases where we can devise a model with sufﬁcient accuracy,computational difﬁculties and localization error might render these techniques impractical.In some cases, a reﬂex agent architecture using reactive control is more appropriate.

Token 20099:
REACTIVE CONTROL For example, picture a legged robot that attempts to lift a leg over an obstacle.

Token 20100:
We could give this robot a rule that says lift the leg a small height hand move it forward, and if the leg encounters an obstacle, move it back and start again at a higher height.

Token 20101:
You could say that h is modeling an aspect of the world, but we can also think of has an auxiliary variable of the robot controller, devoid of direct physical meaning.

Token 20102:
One such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), de- signed for walking through rough terrain.

Token 20103:
The robot’s sensors are inadequate to obtain mod-els of the terrain for path planning.

Token 20104:
Moreover, even if we added sufﬁciently accurate sensors,the twelve degrees of freedom (two for each leg) would render the resulting path planningproblem computationally intractable.

Token 20105:
It is possible, nonetheless, to specify a controller directly without an explicit environ- mental model.

Token 20106:
(We have already seen this with the PD controller, which was able to keep acomplex robot arm on target without an explicit model of the robot dynamics; it did, however, require a reference path generated from a kinematic model.)

Token 20107:
For the hexapod robot we ﬁrstchoose a gait, or pattern of movement of the limbs.

Token 20108:
One statically stable gait is to ﬁrst move GAIT the right front, right rear, and left center legs forward (keeping the other three ﬁxed), and then move the other three.

Token 20109:
This gait works well on ﬂat terrain. On rugged terrain, obstaclesmay prevent a leg from swinging forward.

Token 20110:
This problem can be overcome by a remarkablysimple control rule: when a leg’s forward motion is blocked, simply retract it, lift it higher,

Token 20111:
1002 Chapter 25. Robotics Figure 25.25 Multiple exposures of an RC helicopter executing a ﬂip based on a policy learned with reinforcement learning.

Token 20112:
Images courtesy of Andrew Ng, Stanford University. and try again.

Token 20113:
The resulting controller is shown in Figure 25.24(b) as a ﬁnite state machine; it constitutes a reﬂex agent with state, where the internal state is represented by the index ofthe current machine state ( s 1through s4).

Token 20114:
Variants of this simple feedback-driven controller have been found to generate remark- ably robust walking patterns, capable of maneuvering the robot over rugged terrain.

Token 20115:
Clearly,such a controller is model-free, and it does not deliberate or use search for generating con-trols.

Token 20116:
Environmental feedback plays a crucial role in the controller’s execution.

Token 20117:
The softwarealone does not specify what will actually happen when the robot is placed in an environment.Behavior that emerges through the interplay of a (simple) controller and a (complex) envi-ronment is often referred to as emergent behavior .

Token 20118:
Strictly speaking, all robots discussed EMERGENT BEHAVIOR in this chapter exhibit emergent behavior, due to the fact that no model is perfect.

Token 20119:
Histori- cally, however, the term has been reserved for control techniques that do not utilize explicitenvironmental models.

Token 20120:
Emergent behavior is also characteristic of biological organisms.

Token 20121:
25.6.4 Reinforcement learning control One particularly exciting form of control is based on the policy search form of reinforcement learning (see Section 21.5).

Token 20122:
This work has been enormously inﬂuential in recent years, atis has solved challenging robotics problems for which previously no solution existed.

Token 20123:
An example is acrobatic autonomous helicopter ﬂight. Figure 25.25 shows an autonomous ﬂip of a small RC (radio-controlled) helicopter.

Token 20124:
This maneuver is challenging due to the highlynonlinear nature of the aerodynamics involved.

Token 20125:
Only the most experienced of human pilotsare able to perform it.

Token 20126:
Yet a policy search method (as described in Chapter 21), using only afew minutes of computation, learned a policy that can safely execute a ﬂip every time.

Token 20127:
Policy search needs an accurate model of the domain before it can ﬁnd a policy.

Token 20128:
The input to this model is the state of the helicopter at time t, the controls at time t,a n dt h e resulting state at time t+Δt.

Token 20129:
The state of a helicopter can be described by the 3D coordinates of the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.The controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,and rudder.

Token 20130:
All that remains is the resulting state—how are we going to deﬁne a model that accurately says how the helicopter responds to each control?

Token 20131:
The answer is simple: Let an expert human pilot ﬂy the helicopter, and record the controls that the expert transmits overthe radio and the state variables of the helicopter.

Token 20132:
About four minutes of human-controlledﬂight sufﬁces to build a predictive model that is sufﬁciently accurate to simulate the vehicle.

Token 20133:
Section 25.7.

Token 20134:
Robotic Software Architectures 1003 What is remarkable about this example is the ease with which this learning approach solves a challenging robotics problem.

Token 20135:
This is one of the many successes of machine learningin scientiﬁc ﬁelds previously dominated by careful mathematical analysis and modeling.

Token 20136:
25.7 R OBOTIC SOFTW ARE ARCHITECTURES A methodology for structuring algorithms is called a software architecture .

Token 20137:
An architectureSOFTWARE ARCHITECTURE includes languages and tools for writing programs, as well as an overall philosophy for how programs can be brought together.

Token 20138:
Modern-day software architectures for robotics must decide how to combine reactive control and model-based deliberative planning.

Token 20139:
In many ways, reactive and deliberate tech-niques have orthogonal strengths and weaknesses.

Token 20140:
Reactive control is sensor-driven and ap-propriate for making low-level decisions in real time.

Token 20141:
However, it rarely yields a plausiblesolution at the global level, because global control decisions depend on information that can-not be sensed at the time of decision making.

Token 20142:
For such problems, deliberate planning is amore appropriate choice.

Token 20143:
Consequently, most robot architectures use reactive techniques at the lower levels of control and deliberative techniques at the higher levels.

Token 20144:
We encountered such a combination in our discussion of PD controllers, where we combined a (reactive) PD controller with a (deliberate) path planner.

Token 20145:
Architectures that combine reactive and deliberate techniques are called hybrid architectures .

Token 20146:
HYBRID ARCHITECTURE 25.7.1 Subsumption architecture Thesubsumption architecture (Brooks, 1986) is a framework for assembling reactive con-SUBSUMPTION ARCHITECTURE trollers out of ﬁnite state machines.

Token 20147:
Nodes in these machines may contain tests for certain sensor variables, in which case the execution trace of a ﬁnite state machine is conditioned on the outcome of such a test.

Token 20148:
Arcs can be tagged with messages that will be generated when traversing them, and that are sent to the robot’s motors or to other ﬁnite state machines.

Token 20149:
Addi- tionally, ﬁnite state machines possess internal timers (clocks) that control the time it takes totraverse an arc.

Token 20150:
The resulting machines are refereed to as augmented ﬁnite state machines , AUGMENTED FINITE STATE MACHINE or AFSMs, where the augmentation refers to the use of clocks.

Token 20151:
An example of a simple AFSM is the four-state machine shown in Figure 25.24(b), which generates cyclic leg motion for a hexapod walker.

Token 20152:
This AFSM implements a cycliccontroller, whose execution mostly does not rely on environmental feedback.

Token 20153:
The forwardswing phase, however, does rely on sensor feedback.

Token 20154:
If the leg is stuck, meaning that it hasfailed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, andattempts to execute the forward swing once again.

Token 20155:
Thus, the controller is able to react to contingencies arising from the interplay of the robot and its environment.

Token 20156:
The subsumption architecture offers additional primitives for synchronizing AFSMs, and for combining output values of multiple, possibly conﬂicting AFSMs.

Token 20157:
In this way, itenables the programmer to compose increasingly complex controllers in a bottom-up fashion.

Token 20158:
1004 Chapter 25. Robotics In our example, we might begin with AFSMs for individual legs, followed by an AFSM for coordinating multiple legs.

Token 20159:
On top of this, we might implement higher-level behaviors suchas collision avoidance, which might involve backing up and turning.

Token 20160:
The idea of composing robot controllers from AFSMs is quite intriguing.

Token 20161:
Imagine how difﬁcult it would be to generate the same behavior with any of the conﬁguration-space path-planning algorithms described in the previous section.

Token 20162:
First, we would need an accu- rate model of the terrain.

Token 20163:
The conﬁguration space of a robot with six legs, each of whichis driven by two independent motors, totals eighteen dimensions (twelve dimensions for theconﬁguration of the legs, and six for the location and orientation of the robot relative to itsenvironment).

Token 20164:
Even if our computers were fast enough to ﬁnd paths in such high-dimensionalspaces, we would have to worry about nasty effects such as the robot sliding down a slope.Because of such stochastic effects, a single path through conﬁguration space would almostcertainly be too brittle, and even a PID controller might not be able to cope with such con-tingencies.

Token 20165:
In other words, generating motion behavior deliberately is simply too complex aproblem for present-day robot motion planning algorithms.

Token 20166:
Unfortunately, the subsumption architecture has its own problems.

Token 20167:
First, the AFSMs are driven by raw sensor input, an arrangement that works if the sensor data is reliable and contains all necessary information for decision making, but fails if sensor data has to be inte- grated in nontrivial ways over time.

Token 20168:
Subsumption-style controllers have therefore mostly beenapplied to simple tasks, such as following a wall or moving towards visible light sources.

Token 20169:
Sec-ond, the lack of deliberation makes it difﬁcult to change the task of the robot.

Token 20170:
A subsumption-style robot usually does just one task, and it has no notion of how to modify its controls toaccommodate different goals (just like the dung beetle on page 39).

Token 20171:
Finally, subsumption-style controllers tend to be difﬁcult to understand.

Token 20172:
In practice, the intricate interplay betweendozens of interacting AFSMs (and the environment) is beyond what most human program-mers can comprehend.

Token 20173:
For all these reasons, the subsumption architecture is rarely used inrobotics, despite its great historical importance.

Token 20174:
However, it has had an inﬂuence on otherarchitectures, and on individual components of some architectures.

Token 20175:
25.7.2 Three-layer architecture Hybrid architectures combine reaction with deliberation.

Token 20176:
The most popular hybrid architec-ture is the three-layer architecture , which consists of a reactive layer, an executive layer, THREE-LAYER ARCHITECTURE and a deliberative layer.

Token 20177:
Thereactive layer provides low-level control to the robot. It is characterized by a tight REACTIVE LAYER sensor–action loop.

Token 20178:
Its decision cycle is often on the order of milliseconds.

Token 20179:
Theexecutive layer (or sequencing layer) serves as the glue between the reactive layer EXECUTIVE LAYER and the deliberative layer.

Token 20180:
It accepts directives by the deliberative layer, and sequences them for the reactive layer.

Token 20181:
For example, the executive layer might handle a set of via-points generated by a deliberative path planner, and make decisions as to which reactive behavior to invoke.

Token 20182:
Decision cycles at the executive layer are usually in the order of a second.

Token 20183:
Theexecutive layer is also responsible for integrating sensor information into an internal staterepresentation.

Token 20184:
For example, it may host the robot’s localization and online mapping routines.

Token 20185:
Section 25.7.

Token 20186:
Robotic Software Architectures 1005 Touareg interface Laser mapper Wireless E-Stop Top level control Laser 2 interface Laser 3 interface Laser 4 interface Laser 1 interface Laser 5 interface Camera interface Radar interface Radar mapper Vision mapper UKF Pose estimation Wheel velocity GPS position GPS compass IMU interface Surface assessment Health monitor Road finder Touch screen UI Throttle/brake control Steering control Path planner laser map vehicle state (pose, velocity) velocity limit map vision map vehicle state obstacle list trajectory road center RDDF database driving mode pause/disable command Power server interface clocks emergency stop power on/off Linux processes start/stop heart beats corridor SENSOR INTERFACE PERCEPTION PLANNING&CONTROL USER INTERFACE VEHICLE INTERFACE RDDF corridor (smoothed and original) Process controller GLOBAL SERVICES health status data Data logger File system Communication requests vehicle state (pose, velocity) Brake/steering Communication channels Inter-process communication (IPC) server Time server Figure 25.26 Software architecture of a robot car.

Token 20187:
This software implements a data pipeline, in which all modules process data simultaneously.

Token 20188:
The deliberative layer generates global solutions to complex tasks using planning.

Token 20189:
DELIBERATIVE LAYER Because of the computational complexity involved in generating such solutions, its decision cycle is often in the order of minutes.

Token 20190:
The deliberative layer (or planning layer) uses modelsfor decision making.

Token 20191:
Those models might be either learned from data or supplied and mayutilize state information gathered at the executive layer.

Token 20192:
Variants of the three-layer architecture can be found in most modern-day robot software systems.

Token 20193:
The decomposition into three layers is not very strict.

Token 20194:
Some robot software systems possess additional layers, such as user interface layers that control the interaction with people,or a multiagent level for coordinating a robot’s actions with that of other robots operating inthe same environment.

Token 20195:
25.7.3 Pipeline architecture Another architecture for robots is known as the pipeline architecture .

Token 20196:
Just like the subsump-PIPELINE ARCHITECTURE tion architecture, the pipeline architecture executes multiple process in parallel.

Token 20197:
However, the speciﬁc modules in this architecture resemble those in the three-layer architecture.

Token 20198:
Figure 25.26 shows an example pipeline architecture, which is used to control an au- tonomous car.

Token 20199:
Data enters this pipeline at the sensor interface layer .T h e perception layerSENSOR INTERFACE LAYER PERCEPTION LAYER

Token 20200:
1006 Chapter 25. Robotics (a) (b) Figure 25.27 (a) The Helpmate robot transports food and other medical items in dozens of hospitals worldwide.

Token 20201:
(b) Kiva robots are part of a material-handling system for movingshelves in fulﬁllment centers. Image courtesy of Kiva Systems.

Token 20202:
then updates the robot’s internal models of the environment based on this data.

Token 20203:
Next, these models are handed to the planning and control layer , which adjusts the robot’s internalPLANNINGAND CONTROL LAYER plans turns them into actual controls for the robot.

Token 20204:
Those are then communicated back to the vehicle through the vehicle interface layer .VEHICLE INTERFACE LAYER The key to the pipeline architecture is that this all happens in parallel.

Token 20205:
While the per- ception layer processes the most recent sensor data, the control layer bases its choices onslightly older data.

Token 20206:
In this way, the pipeline architecture is similar to the human brain. Wedon’t switch off our motion controllers when we digest new sensor data.

Token 20207:
Instead, we perceive,plan, and act all at the same time. Processes in the pipeline architecture run asynchronously,and all computation is data-driven.

Token 20208:
The resulting system is robust, and it is fast.

Token 20209:
The architecture in Figure 25.26 also contains other, cross-cutting modules, responsible for establishing communication between the different elements of the pipeline.

Token 20210:
25.8 A PPLICATION DOMAINS Here are some of the prime application domains for robotic technology. Industry and Agriculture.

Token 20211:
Traditionally, robots have been ﬁelded in areas that require difﬁcult human labor, yet are structured enough to be amenable to robotic automation.

Token 20212:
The best example is the assembly line, where manipulators routinely perform tasks such as as- sembly, part placement, material handling, welding, and painting.

Token 20213:
In many of these tasks,robots have become more cost-effective than human workers.

Token 20214:
Outdoors, many of the heavymachines that we use to harvest, mine, or excavate earth have been turned into robots. For

Token 20215:
Section 25.8. Application Domains 1007 (a) (b) Figure 25.28 (a) Robotic car B OSS, which won the DARPA Urban Challenge.

Token 20216:
Courtesy of Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of da Vinci Surgical Systems.

Token 20217:
example, a project at Carnegie Mellon University has demonstrated that robots can strip paint off large ships about 50 times faster than people can, and with a much reduced environmental impact.

Token 20218:
Prototypes of autonomous mining robots have been found to be faster and more pre- cise than people in transporting ore in underground mines.

Token 20219:
Robots have been used to generatehigh-precision maps of abandoned mines and sewer systems.

Token 20220:
While many of these systemsare still in their prototype stages, it is only a matter of time until robots will take over muchof the semimechanical work that is presently performed by people.

Token 20221:
Transportation.

Token 20222:
Robotic transportation has many facets: from autonomous helicopters that deliver payloads to hard-to-reach locations, to automatic wheelchairs that transport peo-ple who are unable to control wheelchairs by themselves, to autonomous straddle carriers thatoutperform skilled human drivers when transporting containers from ships to trucks on load-ing docks.

Token 20223:
A prime example of indoor transportation robots, or gofers, is the Helpmate robotshown in Figure 25.27(a).

Token 20224:
This robot has been deployed in dozens of hospitals to transport food and other items.

Token 20225:
In factory settings, autonomous vehicles are now routinely deployed to transport goods in warehouses and between production lines.

Token 20226:
The Kiva system, shown inFigure 25.27(b), helps workers at fulﬁllment centers package goods into shipping containers.

Token 20227:
Many of these robots require environmental modiﬁcations for their operation.

Token 20228:
The most common modiﬁcations are localization aids such as inductive loops in the ﬂoor, active bea-cons, or barcode tags.

Token 20229:
An open challenge in robotics is the design of robots that can usenatural cues, instead of artiﬁcial devices, to navigate, particularly in environments such as thedeep ocean where GPS is unavailable.

Token 20230:
Robotic cars. Most of use cars every day. Many of us make cell phone calls while driving. Some of us even text.

Token 20231:
The sad result: more than a million people die every year intrafﬁc accidents.

Token 20232:
Robotic cars like B OSSand S TANLEY offer hope: Not only will they make driving much safer, but they will also free us from the need to pay attention to the road during our daily commute.

Token 20233:
Progress in robotic cars was stimulated by the DARPA Grand Challenge, a race over 100 miles of unrehearsed desert terrain, which represented a much more challenging task than

Token 20234:
1008 Chapter 25. Robotics (a) (b) Figure 25.29 (a) A robot mapping an abandoned coal mine. (b) A 3D map of the mine acquired by the robot.

Token 20235:
had ever been accomplished before.

Token 20236:
Stanford’s S TANLEY vehicle completed the course in less than seven hours in 2005, winning a $2 million prize and a place in the National Museum of American History.

Token 20237:
Figure 25.28(a) depicts B OSS, which in 2007 won the DARPA Urban Challenge, a complicated road race on city streets where robots faced other robots and had toobey trafﬁc rules.

Token 20238:
Health care.

Token 20239:
Robots are increasingly used to assist surgeons with instrument placement when operating on organs as intricate as brains, eyes, and hearts.

Token 20240:
Figure 25.28(b) shows sucha system.

Token 20241:
Robots have become indispensable tools in a range of surgical procedures, such aship replacements, thanks to their high precision.

Token 20242:
In pilot studies, robotic devices have beenfound to reduce the danger of lesions when performing colonoscopy.

Token 20243:
Outside the operatingroom, researchers have begun to develop robotic aides for elderly and handicapped people,such as intelligent robotic walkers and intelligent toys that provide reminders to take medica-tion and provide comfort.

Token 20244:
Researchers are also working on robotic devices for rehabilitation that aid people in performing certain exercises. Hazardous environments.

Token 20245:
Robots have assisted people in cleaning up nuclear waste, most notably in Chernobyl and Three Mile Island.

Token 20246:
Robots were present after the collapseof the World Trade Center, where they entered structures deemed too dangerous for humansearch and rescue crews.

Token 20247:
Some countries have used robots to transport ammunition and to defuse bombs—a no- toriously dangerous task.

Token 20248:
A number of research projects are presently developing prototyperobots for clearing mineﬁelds, on land and at sea.

Token 20249:
Most existing robots for these tasks areteleoperated—a human operates them by remote control.

Token 20250:
Providing such robots with auton-omy is an important next step. Exploration.

Token 20251:
Robots have gone where no one has gone before, including the surface of Mars (see Figure 25.2(b) and the cover).

Token 20252:
Robotic arms assist astronauts in deploying and retrieving satellites and in building the International Space Station.

Token 20253:
Robots also helpexplore under the sea. They are routinely used to acquire maps of sunken ships.

Token 20254:
Figure 25.29shows a robot mapping an abandoned coal mine, along with a 3D model of the mine acquired

Token 20255:
Section 25.8. Application Domains 1009 (a) (b) Figure 25.30 (a) Roomba, the world’s best-selling mobile robot, vacuums ﬂoors.

Token 20256:
Image courtesy of iRobot, c/circlecopyrt2009. (b) Robotic hand modeled after human hand.

Token 20257:
Image courtesy of University of Washington and Carnegie Mellon University. using range sensors.

Token 20258:
In 1996, a team of researches released a legged robot into the crater of an active volcano to acquire data for climate research.

Token 20259:
Unmanned air vehicles known asdrones are used in military operations.

Token 20260:
Robots are becoming very effective tools for gathering DRONE information in domains that are difﬁcult (or dangerous) for people to access.

Token 20261:
Personal Services. Service is an up-and-coming application domain of robotics. Ser- vice robots assist individuals in performing daily tasks.

Token 20262:
Commercially available domesticservice robots include autonomous vacuum cleaners, lawn mowers, and golf caddies.

Token 20263:
Theworld’s most popular mobile robot is a personal service robot: the robotic vacuum cleanerRoomba , shown in Figure 25.30(a).

Token 20264:
More than three million Roombas have been sold. ROOMBA Roomba can navigate autonomously and perform its tasks without human help.

Token 20265:
Other service robots operate in public places, such as robotic information kiosks that have been deployed in shopping malls and trade fairs, or in museums as tour guides.

Token 20266:
Ser-vice tasks require human interaction, and the ability to cope robustly with unpredictable anddynamic environments. Entertainment.

Token 20267:
Robots have begun to conquer the entertainment and toy industry.

Token 20268:
In Figure 25.6(b) we see robotic soccer , a competitive game very much like human soc- ROBOTIC SOCCER cer, but played with autonomous mobile robots.

Token 20269:
Robot soccer provides great opportunities for research in AI, since it raises a range of problems relevant to many other, more seriousrobot applications.

Token 20270:
Annual robotic soccer competitions have attracted large numbers of AIresearchers and added a lot of excitement to the ﬁeld of robotics.

Token 20271:
Human augmentation. A ﬁnal application domain of robotic technology is that of human augmentation.

Token 20272:
Researchers have developed legged walking machines that can carry people around, very much like a wheelchair.

Token 20273:
Several research efforts presently focus on thedevelopment of devices that make it easier for people to walk or move their arms by providingadditional forces through extraskeletal attachments.

Token 20274:
If such devices are attached permanently,

Token 20275:
1010 Chapter 25. Robotics they can be thought of as artiﬁcial robotic limbs.

Token 20276:
Figure 25.30(b) shows a robotic hand that may serve as a prosthetic device in the future.

Token 20277:
Robotic teleoperation, or telepresence, is another form of human augmentation.

Token 20278:
Tele- operation involves carrying out tasks over long distances with the aid of robotic devices.A popular conﬁguration for robotic teleoperation is the master–slave conﬁguration, where a robot manipulator emulates the motion of a remote human operator, measured through a haptic interface.

Token 20279:
Underwater vehicles are often teleoperated; the vehicles can go to a depththat would be dangerous for humans but can still be guided by the human operator.

Token 20280:
All thesesystems augment people’s ability to interact with their environments.

Token 20281:
Some projects go as faras replicating humans, at least at a very superﬁcial level.

Token 20282:
Humanoid robots are now availablecommercially through several companies in Japan.

Token 20283:
25.9 S UMMARY Robotics concerns itself with intelligent agents that manipulate the physical world.

Token 20284:
In thischapter, we have learned the following basics of robot hardware and software.

Token 20285:
•Robots are equipped with sensors for perceiving their environment and effectors with which they can assert physical forces on their environment.

Token 20286:
Most robots are either manipulators anchored at ﬁxed locations or mobile robots that can move.

Token 20287:
•Robotic perception concerns itself with estimating decision-relevant quantities from sensor data.

Token 20288:
To do so, we need an internal representation and a method for updatingthis internal representation over time.

Token 20289:
Common examples of hard perceptual problems include localization, mapping, and object recognition .

Token 20290:
•Probabilistic ﬁltering algorithms such as Kalman ﬁlters and particle ﬁlters are useful for robot perception.

Token 20291:
These techniques maintain the belief state, a posterior distributionover state variables.

Token 20292:
•The planning of robot motion is usually done in conﬁguration space , where each point speciﬁes the location and orientation of the robot and its joint angles.

Token 20293:
•Conﬁguration space search algorithms include cell decomposition techniques, which decompose the space of all conﬁgurations into ﬁnitely many cells, and skeletonization techniques, which project conﬁguration spaces onto lower-dimensional manifolds.

Token 20294:
The motion planning problem is then solved using search in these simpler structures.

Token 20295:
•A path found by a search algorithm can be executed by using the path as the reference trajectory for a PID controller .

Token 20296:
Controllers are necessary in robotics to accommodate small perturbations; path planning alone is usually insufﬁcient.

Token 20297:
•Potential ﬁeld techniques navigate robots by potential functions, deﬁned over the dis- tance to obstacles and the goal location.

Token 20298:
Potential ﬁeld techniques may get stuck in local minima, but they can generate motion directly without the need for path planning.

Token 20299:
•Sometimes it is easier to specify a robot controller directly, rather than deriving a path from an explicit model of the environment.

Token 20300:
Such controllers can often be written as simple ﬁnite state machines .

Token 20301:
Bibliographical and Historical Notes 1011 •There exist different architectures for software design.

Token 20302:
The subsumption architec- ture enables programmers to compose robot controllers from interconnected ﬁnite state machines.

Token 20303:
Three-layer architectures are common frameworks for developing robot software that integrate deliberation, sequencing of subgoals, and control.

Token 20304:
The relatedpipeline architecture processes data in parallel through a sequence of modules, corre- sponding to perception, modeling, planning, control, and robot interfaces.

Token 20305:
BIBLIOGRAPHICAL AND HISTORICAL NOTES The word robot was popularized by Czech playwright Karel Capek in his 1921 play R.U.R.

Token 20306:
(Rossum’s Universal Robots).

Token 20307:
The robots, which were grown chemically rather than con-structed mechanically, end up resenting their masters and decide to take over.

Token 20308:
It appears(Glanc, 1978) it was Capek’s brother, Josef, who ﬁrst combined the Czech words “robota”(obligatory work) and “robotnik” (serf) to yield “robot” in his 1917 short story Opilec .

Token 20309:
The term robotics was ﬁrst used by Asimov (1950). Robotics (under other names) has a much longer history, however.

Token 20310:
In ancient Greek mythology, a mechanical man named Talos was supposedly designed and built by Hephaistos, the Greek god of metallurgy.

Token 20311:
Wonderfulautomata were built in the 18th century—Jacques Vaucanson’s mechanical duck from 1738being one early example—but the complex behaviors they exhibited were entirely ﬁxed inadvance.

Token 20312:
Possibly the earliest example of a programmable robot-like device was the Jacquardloom (1805), described on page 14.

Token 20313:
The ﬁrst commercial robot was a robot arm called Unimate , short for universal automa- UNIMATE tion, developed by Joseph Engelberger and George Devol.

Token 20314:
In 1961, the ﬁrst Unimate robot was sold to General Motors, where it was used for manufacturing TV picture tubes.

Token 20315:
1961was also the year when Devol obtained the ﬁrst U.S. patent on a robot.

Token 20316:
Eleven years later, in1972, Nissan Corp. was among the ﬁrst to automate an entire assembly line with robots, de- veloped by Kawasaki with robots supplied by Engelberger and Devol’s company Unimation.

Token 20317:
This development initiated a major revolution that took place mostly in Japan and the U.S.,and that is still ongoing.

Token 20318:
Unimation followed up in 1978 with the development of the PUMA PUMA robot, short for Programmable Universal Machine for Assembly.

Token 20319:
The PUMA robot, initially developed for General Motors, was the de facto standard for robotic manipulation for the two decades that followed.

Token 20320:
At present, the number of operating robots is estimated at one million worldwide, more than half of which are installed in Japan.

Token 20321:
The literature on robotics research can be divided roughly into two parts: mobile robots and stationary manipulators.

Token 20322:
Grey Walter’s “turtle,” built in 1948, could be considered theﬁrst autonomous mobile robot, although its control system was not programmable.

Token 20323:
The “Hop-kins Beast,” built in the early 1960s at Johns Hopkins University, was much more sophisti- cated; it had pattern-recognition hardware and could recognize the cover plate of a standard AC power outlet.

Token 20324:
It was capable of searching for outlets, plugging itself in, and then recharg-ing its batteries! Still, the Beast had a limited repertoire of skills.

Token 20325:
The ﬁrst general-purposemobile robot was “Shakey,” developed at what was then the Stanford Research Institute (now

Token 20326:
1012 Chapter 25. Robotics SRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984).

Token 20327:
Shakey was the ﬁrst robot to integrate perception, planning, and execution, and much subsequent research in AI wasinﬂuenced by this remarkable achievement.

Token 20328:
Shakey appears on the cover of this book withproject leader Charlie Rosen (1917–2002).

Token 20329:
Other inﬂuential projects include the StanfordCart and the CMU Rover (Moravec, 1983).

Token 20330:
Cox and Wilfong (1990) describes classic work on autonomous vehicles. The ﬁeld of robotic mapping has evolved from two distinct origins.

Token 20331:
The ﬁrst thread began with work by Smith and Cheeseman (1986), who applied Kalman ﬁlters to the si-multaneous localization and mapping problem.

Token 20332:
This algorithm was ﬁrst implemented byMoutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);see Dissanayake et al.

Token 20333:
(2001) for an overview of early Kalman ﬁlter variations.

Token 20334:
The second thread began with the development of the occupancy grid representation for probabilistic OCCUPANCY GRID mapping, which speciﬁes the probability that each (x,y)location is occupied by an obsta- cle (Moravec and Elfes, 1985).

Token 20335:
Kuipers and Levitt (1988) were among the ﬁrst to proposetopological rather than metric mapping, motivated by models of human spatial cognition.

Token 20336:
Aseminal paper by Lu and Milios (1997) recognized the sparseness of the simultaneous local-ization and mapping problem, which gave rise to the development of nonlinear optimization techniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical methods by Bosse et al.

Token 20337:
(2004). Shatkay and Kaelbling (1997) and Thrun et al. (1998) intro- duced the EM algorithm into the ﬁeld of robotic mapping for data association.

Token 20338:
An overview of probabilistic mapping methods can be found in (Thrun et al. , 2005).

Token 20339:
Early mobile robot localization techniques are surveyed by Borenstein et al. (1996).

Token 20340:
Although Kalman ﬁltering was well known as a localization method in control theory for decades, the general probabilistic formulation of the localization problem did not appearin the AI literature until much later, through the work of Tom Dean and colleagues (Deanet al.

Token 20341:
, 1990, 1990) and of Simmons and Koenig (1995). The latter work introduced the term Markov localization .

Token 20342:
The ﬁrst real-world application of this technique was by Burgard et al.

Token 20343:
MARKOV LOCALIZATION (1999), through a series of robots that were deployed in museums.

Token 20344:
Monte Carlo localiza- tion based on particle ﬁlters was developed by Fox et al. (1999) and is now widely used.

Token 20345:
TheRao-Blackwellized particle ﬁlter combines particle ﬁltering for robot localization withRAO- BLACKWELLIZEDPARTICLE FILTER exact ﬁltering for map building (Murphy and Russell, 2001; Montemerlo et al.

Token 20346:
, 2002). The study of manipulator robots, originally called hand–eye machines ,h a se v o l v e dHAND–EYE MACHINES along quite different lines.

Token 20347:
The ﬁrst major effort at creating a hand–eye machine was Hein- rich Ernst’s MH-1, described in his MIT Ph.D. thesis (Ernst, 1961).

Token 20348:
The Machine Intelligenceproject at Edinburgh also demonstrated an impressive early system for vision-based assem-bly called F REDDY (Michie, 1972).

Token 20349:
After these pioneering efforts, a great deal of work fo- cused on geometric algorithms for deterministic and fully observable motion planning prob-lems.

Token 20350:
The PSPACE-hardness of robot motion planning was shown in a seminal paper byReif (1979).

Token 20351:
The conﬁguration space representation is due to Lozano-Perez (1983).

Token 20352:
A series of papers by Schwartz and Sharir on what they called piano movers problems (Schwartz PIANO MOVERS et al. , 1987) was highly inﬂuential.

Token 20353:
Recursive cell decomposition for conﬁguration space planning was originated by Brooks and Lozano-Perez (1985) and improved signiﬁcantly by Zhu and Latombe (1991).

Token 20354:
The ear-

Token 20355:


Token 20356:
Bibliographical and Historical Notes 1013 liest skeletonization algorithms were based on Voronoi diagrams (Rowat, 1979) and visi- bility graphs (Wesley and Lozano-Perez, 1979).

Token 20357:
Guibas et al.

Token 20358:
(1992) developed efﬁcient VISIBILITY GRAPH techniques for calculating Voronoi diagrams incrementally, and Choset (1996) generalized Voronoi diagrams to broader motion-planning problems.

Token 20359:
John Canny (1988) established theﬁrst singly exponential algorithm for motion planning.

Token 20360:
The seminal text by Latombe (1991) covers a variety of approaches to motion-planning, as do the texts by Choset et al. (2004) and LaValle (2006).

Token 20361:
Kavraki et al. (1996) developed probabilistic roadmaps, which are currently one of the most effective methods.

Token 20362:
Fine-motion planning with limited sensing was investi-gated by Lozano-Perez et al. (1984) and Canny and Reif (1987).

Token 20363:
Landmark-based naviga- tion (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.Key work applying POMDP methods (Section 17.4) to motion planning under uncertainty inrobotics is due to Pineau et al.

Token 20364:
(2003) and Roy et al. (2005).

Token 20365:
The control of robots as dynamical systems—whether for manipulation or navigation— has generated a huge literature that is barely touched on by this chapter.

Token 20366:
Important worksinclude a trilogy on impedance control by Hogan (1985) and a general study of robot dy-namics by Featherstone (1987).

Token 20367:
Dean and Wellman (1991) were among the ﬁrst to try to tietogether control theory and AI planning systems.

Token 20368:
Three classic textbooks on the mathematics of robot manipulation are due to Paul (1981), Craig (1989), and Yoshikawa (1990).

Token 20369:
The area ofgrasping is also important in robotics—the problem of determining a stable grasp is quite GRASPING difﬁcult (Mason and Salisbury, 1985).

Token 20370:
Competent grasping requires touch sensing, or haptic feedback , to determine contact forces and detect slip (Fearing and Hollerbach, 1985).

Token 20371:
HAPTIC FEEDBACK Potential-ﬁeld control, which attempts to solve the motion planning and control prob- lems simultaneously, was introduced into the robotics literature by Khatib (1986).

Token 20372:
In mobilerobotics, this idea was viewed as a practical solution to the collision avoidance problem, andwas later extended into an algorithm called vector ﬁeld histograms by Borenstein (1991).

Token 20373:
VECTOR FIELD HISTOGRAMS Navigation functions, the robotics version of a control policy for deterministic MDPs, were introduced by Koditschek (1987).

Token 20374:
Reinforcement learning in robotics took off with the semi- nal work by Bagnell and Schneider (2001) and Ng et al.

Token 20375:
(2004), who developed the paradigm in the context of autonomous helicopter control.

Token 20376:
The topic of software architectures for robots engenders much religious debate.

Token 20377:
The good old-fashioned AI candidate—the three-layer architecture—dates back to the design ofShakey and is reviewed by Gat (1998).

Token 20378:
The subsumption architecture is due to Brooks (1986),although similar ideas were developed independently by Braitenberg (1984), whose book,Vehicles , describes a series of simple robots based on the behavioral approach.

Token 20379:
The suc- cess of Brooks’s six-legged walking robot was followed by many other projects.

Token 20380:
Connell,in his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that wasentirely reactive.

Token 20381:
Extensions of the behavior-based paradigm to multirobot systems can befound in (Mataric, 1997) and (Parker, 1996).

Token 20382:
GRL (Horswill, 2000) and C OLBERT (Kono- lige, 1997) abstract the ideas of concurrent behavior-based robotics into general robot control languages.

Token 20383:
Arkin (1998) surveys some of the most popular approaches in this ﬁeld.

Token 20384:
Research on mobile robotics has been stimulated over the last decade by several impor- tant competitions.

Token 20385:
The earliest competition, AAAI’s annual m obile robot competition, began in 1992. The ﬁrst competition winner was C ARMEL (Congdon et al. , 1992).

Token 20386:
Progress has

Token 20387:
1014 Chapter 25.

Token 20388:
Robotics been steady and impressive: in more recent competitions robots entered the conference com- plex, found their way to the registration desk, registered for the conference, and even gave ashort talk.

Token 20389:
The Robocup competition, launched in 1995 by Kitano and colleagues (1997a), ROBOCUP aims to “develop a team of fully autonomous humanoid robots that can win against the hu- man world champion team in soccer” by 2050.

Token 20390:
Play occurs in leagues for simulated robots, wheeled robots of different sizes, and humanoid robots.

Token 20391:
In 2009 teams from 43 countries participated and the event was broadcast to millions of viewers.

Token 20392:
Visser and Burkhard (2007)track the improvements that have been made in perception, team coordination, and low-levelskills over the past decade.

Token 20393:
The DARPA Grand Challenge , organized by DARPA in 2004 and 2005, required DARPA GRAND CHALLENGE autonomous robots to travel more than 100 miles through unrehearsed desert terrain in less than 10 hours (Buehler et al.

Token 20394:
, 2006). In the original event in 2004, no robot traveled more than 8 miles, leading many to believe the prize would never be claimed.

Token 20395:
In 2005, Stanford’srobot S TANLEY won the competition in just under 7 hours of travel (Thrun, 2006).

Token 20396:
DARPA then organized the Urban Challenge , a competition in which robots had to navigate 60 miles URBAN CHALLENGE in an urban environment with other trafﬁc.

Token 20397:
Carnegie Mellon University’s robot B OSS took ﬁrst place and claimed the $2 million prize (Urmson and Whittaker, 2008).

Token 20398:
Early pioneers in the development of robotic cars included Dickmanns and Zapp (1987) and Pomerleau (1993).

Token 20399:
Two early textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics generally. A more recent overview is due to Bekey (2008).

Token 20400:
An excellent book on robotmanipulation addresses advanced topics such as compliant motion (Mason, 2001).

Token 20401:
Robotmotion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005) provide an introduction into probabilistic robotics.

Token 20402:
The premiere conference for robotics isRobotics: Science and Systems Conference, followed by the IEEE International Conferenceon Robotics and Automation.

Token 20403:
Leading robotics journals include IEEE Robotics and Automa- tion,t h e International Journal of Robotics Research ,a n d Robotics and Autonomous Systems .

Token 20404:
EXERCISES 25.1 Monte Carlo localization is biased for any ﬁnite sample size—i.e., the expected value of the location computed by the algorithm differs from the true expected value—because of the way particle ﬁltering works.

Token 20405:
In this question, you are asked to quantify this bias. To simplify, consider a world with four possible robot locations: X={x1,x2,x3,x4}.

Token 20406:
Initially, we draw N≥1samples uniformly from among those locations.

Token 20407:
As usual, it is perfectly acceptable if more than one sample is generated for any of the locations X.L e tZ be a Boolean sensor variable characterized by the following conditional probabilities: P(z|x1)=0 .8 P(¬z|x1)=0 .2 P(z|x2)=0 .4 P(¬z|x2)=0 .6 P(z|x3)=0 .1 P(¬z|x3)=0 .9 P(z|x4)=0 .1 P(¬z|x4)=0 .9.

Token 20408:


Token 20409:
Exercises 1015 B A A B Starting configuration <−0.5, 7> Ending configuration <−0.5, −7> Figure 25.31 A Robot manipulator in two of its possible conﬁgurations.

Token 20410:
MCL uses these probabilities to generate particle weights, which are subsequently normalized and used in the resampling process.

Token 20411:
For simplicity, let us assume we generate only one newsample in the resampling process, regardless of N. This sample might correspond to any of the four locations in X.

Token 20412:
Thus, the sampling process deﬁnes a probability distribution over X. a. What is the resulting probability distribution over Xfor this new sample?

Token 20413:
Answer this question separately for N=1,...,10,a n df o r N=∞. b.

Token 20414:
The difference between two probability distributions PandQcan be measured by the KL divergence, which is deﬁned as KL(P,Q)=/summationdisplay iP(xi)logP(xi) Q(xi).

Token 20415:
What are the KL divergences between the distributions in (a) and the true posterior?

Token 20416:
c. What modiﬁcation of the problem formulation (not the algorithm!)

Token 20417:
would guarantee that the speciﬁc estimator above is unbiased even for ﬁnite values of N?

Token 20418:
Provide at least two such modiﬁcations (each of which should be sufﬁcient).

Token 20419:
25.2 Implement Monte Carlo localization for a simulated robot with range sensors.

Token 20420:
A grid map and range data are available from the code repository at aima.cs.berkeley.edu .

Token 20421:
You should demonstrate successful global localization of the robot. 25.3 Consider a robot with two simple manipulators, as shown in ﬁgure 25.31.

Token 20422:
Manipulator A is a square block of side 2 which can slide back and on a rod that runs along the x-axisfrom x=−10 to x=10.

Token 20423:
Manipulator B is a square block of side 2 which can slide back and on a rod that runs along the y-axis from y= −10 to y=10.

Token 20424:
The rods lie outside the plane of

Token 20425:
1016 Chapter 25. Robotics manipulation, so the rods do not interfere with the movement of the blocks.

Token 20426:
A conﬁguration is then a pair/angbracketleftx,y/angbracketrightwhere xis the x-coordinate of the center of manipulator A and where yis the y-coordinate of the center of manipulator B.

Token 20427:
Draw the conﬁguration space for this robot,indicating the permitted and excluded zones.

Token 20428:
25.4 Suppose that you are working with the robot in Exercise 25.3 and you are given the problem of ﬁnding a path from the starting conﬁguration of ﬁgure 25.31 to the ending con-ﬁguration.

Token 20429:
Consider a potential function D(A,Goal) 2+D(B,Goal)2+1 D(A,B)2 where D(A,B)is the distance between the closest points of A and B. a.

Token 20430:
Show that hill climbing in this potential ﬁeld will get stuck in a local minimum. b.

Token 20431:
Describe a potential ﬁeld where hill climbing will solve this particular problem.

Token 20432:
You need not work out the exact numerical coefﬁcients needed, just the general form of thesolution.

Token 20433:
(Hint: Add a term that “rewards” the hill climber for moving A out of B’sway, even in a case like this where this does not reduce the distance from A to B in theabove sense.)

Token 20434:
25.5 Consider the robot arm shown in Figure 25.14.

Token 20435:
Assume that the robot’s base element is 60cm long and that its upper arm and forearm are each 40cm long.

Token 20436:
As argued on page 987,the inverse kinematics of a robot is often not unique.

Token 20437:
State an explicit closed-form solution ofthe inverse kinematics for this arm. Under what exact conditions is the solution unique?

Token 20438:
25.6 Implement an algorithm for calculating the Voronoi diagram of an arbitrary 2D en- vironment, described by an n×nBoolean array.

Token 20439:
Illustrate your algorithm by plotting the Voronoi diagram for 10 interesting maps. What is the complexity of your algorithm?

Token 20440:
25.7 This exercise explores the relationship between workspace and conﬁguration space using the examples shown in Figure 25.32. a.

Token 20441:
Consider the robot conﬁgurations shown in Figure 25.32(a) through (c), ignoring the obstacle shown in each of the diagrams.

Token 20442:
Draw the corresponding arm conﬁgurations inconﬁguration space.

Token 20443:
( Hint: Each arm conﬁguration maps to a single point in conﬁgura- tion space, as illustrated in Figure 25.14(b).) b.

Token 20444:
Draw the conﬁguration space for each of the workspace diagrams in Figure 25.32(a)– (c).

Token 20445:
( Hint: The conﬁguration spaces share with the one shown in Figure 25.32(a) the region that corresponds to self-collision, but differences arise from the lack of enclosingobstacles and the different locations of the obstacles in these individual ﬁgures.)

Token 20446:
c. For each of the black dots in Figure 25.32(e)–(f), draw the corresponding conﬁgurations of the robot arm in workspace.

Token 20447:
Please ignore the shaded regions in this exercise.

Token 20448:
d. The conﬁguration spaces shown in Figure 25.32(e)–(f) have all been generated by a single workspace obstacle (dark shading), plus the constraints arising from the self-collision constraint (light shading).

Token 20449:
Draw, for each diagram, the workspace obstaclethat corresponds to the darkly shaded area.

Token 20450:


Token 20451:
Exercises 1017 (a) (b) (c) (d) (e) (f) Figure 25.32 Diagrams for Exercise 25.7. e. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace into two disconnected regions.

Token 20452:
What is the maximum number of disconnected re-gions that can be created by inserting a planar obstacle into an obstacle-free, connectedworkspace, for a 2DOF robot?

Token 20453:
Give an example, and argue why no larger number ofdisconnected regions can be created. How about a non-planar obstacle?

Token 20454:
25.8 Consider a mobile robot moving on a horizontal surface.

Token 20455:
Suppose that the robot can execute two kinds of motions: •Rolling forward a speciﬁed distance. •Rotating in place through a speciﬁed angle.

Token 20456:
The state of such a robot can be characterized in terms of three parameters /angbracketleftx,y,φ , the x- coordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the robot’s orientation expressed as the angle from the positive x direction.

Token 20457:
The action “ Roll(D)” has the effect of changing state /angbracketleftx,y,φ to/angbracketleftx+Dcos(φ),y+Dsin(φ),φ/angbracketright, and the action Rotate (θ)has the effect of changing state /angbracketleftx,y,φ/angbracketrightto/angbracketleftx,y,φ +θ/angbracketright.

Token 20458:
a.

Token 20459:
Suppose that the robot is initially at /angbracketleft0,0,0/angbracketrightand then executes the actions Rotate (60 ◦), Roll(1),Rotate (25◦),Roll(2).

Token 20460:
What is the ﬁnal state of the robot?

Token 20461:
1018 Chapter 25. Robotics robot sensor range goal Figure 25.33 Simpliﬁed robot in a maze. See Exercise 25.9. b.

Token 20462:
Now suppose that the robot has imperfect control of its own rotation, and that, if it attempts to rotate by θ, it may actually rotate by any angle between θ−10◦andθ+10◦.

Token 20463:
In that case, if the robot attempts to carry out the sequence of actions in (A), there is a range of possible ending states.

Token 20464:
What are the minimal and maximal values of thex-coordinate, the y-coordinate and the orientation in the ﬁnal state?

Token 20465:
c. Let us modify the model in (B) to a probabilistic model in which, when the robot attempts to rotate by θ, its actual angle of rotation follows a Gaussian distribution with mean θand standard deviation 10 ◦.

Token 20466:
Suppose that the robot executes the actions Rotate (90◦),Roll(1).

Token 20467:
Give a simple argument that (a) the expected value of the loca- tion at the end is not equal to the result of rotating exactly 90◦and then rolling forward 1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.

Token 20468:
(Do not attempt to calculate the true mean or the true distribution.)

Token 20469:
The point of this exercise is that rotational uncertainty quickly gives rise to a lot of positional uncertainty and that dealing with rotational uncertainty is painful, whether uncertainty is treated in terms of hard intervals or probabilistically, due to the fact that the relation between orientation and position is both non-linear and non-monotonic.

Token 20470:
25.9 Consider the simpliﬁed robot shown in Figure 25.33.

Token 20471:
Suppose the robot’s Cartesian coordinates are known at all times, as are those of its goal location.

Token 20472:
However, the locations of the obstacles are unknown. The robot can sense obstacles in its immediate proximity, asillustrated in this ﬁgure.

Token 20473:
For simplicity, let us assume the robot’s motion is noise-free, andthe state space is discrete.

Token 20474:
Figure 25.33 is only one example; in this exercise you are requiredto address all possible grid worlds with a valid path from the start to the goal location.

Token 20475:
a. Design a deliberate controller that guarantees that the robot always reaches its goal location if at all possible.

Token 20476:
The deliberate controller can memorize measurements in the form of a map that is being acquired as the robot moves.

Token 20477:
Between individual moves, it may spend arbitrary time deliberating.

Token 20478:
Exercises 1019 b. Now design a reactive controller for the same task. This controller may not memorize past sensor measurements.

Token 20479:
(It may not build a map!)

Token 20480:
Instead, it has to make all decisionsbased on the current measurement, which includes knowledge of its own location andthat of the goal.

Token 20481:
The time to make a decision must be independent of the environmentsize or the number of past time steps.

Token 20482:
What is the maximum number of steps that it may take for your robot to arrive at the goal?

Token 20483:
c. How will your controllers from (a) and (b) perform if any of the following six conditions apply: continuous state space, noise in perception, noise in motion, noise in both per-ception and motion, unknown location of the goal (the goal can be detected only whenwithin sensor range), or moving obstacles.

Token 20484:
For each condition and each controller, givean example of a situation where the robot fails (or explain why it cannot fail).

Token 20485:
25.10 In Figure 25.24(b) on page 1001, we encountered an augmented ﬁnite state machine for the control of a single leg of a hexapod robot.

Token 20486:
In this exercise, the aim is to design anAFSM that, when combined with six copies of the individual leg controllers, results in efﬁ- cient, stable locomotion.

Token 20487:
For this purpose, you have to augment the individual leg controller to pass messages to your new AFSM and to wait until other messages arrive.

Token 20488:
Argue why yourcontroller is efﬁcient, in that it does not unnecessarily waste energy (e.g., by sliding legs),and in that it propels the robot at reasonably high speeds.

Token 20489:
Prove that your controller satisﬁesthe dynamic stability condition given on page 977.

Token 20490:
25.11 (This exercise was ﬁrst devised by Michael Genesereth and Nils Nilsson. It works for ﬁrst graders through graduate students.)

Token 20491:
Humans are so adept at basic household tasksthat they often forget how complex these tasks are.

Token 20492:
In this exercise you will discover thecomplexity and recapitulate the last 30 years of developments in robotics.

Token 20493:
Consider the task of building an arch out of three blocks. Simulate a robot with four humans as follows: Brain.

Token 20494:
The Brain direct the hands in the execution of a plan to achieve the goal. The Brain receives input from the Eyes, but cannot see the scene directly .

Token 20495:
The brain is the only one who knows what the goal is. Eyes.

Token 20496:
The Eyes report a brief description of the scene to the Brain: “There is a red box standing on top of a green box, which is on its side” Eyes can also answer questions from theBrain such as, “Is there a gap between the Left Hand and the red box?” If you have a videocamera, point it at the scene and allow the eyes to look at the viewﬁnder of the video camera,but not directly at the scene.

Token 20497:
Left hand andright hand. One person plays each Hand.

Token 20498:
The two Hands stand next to each other, each wearing an oven mitt on one hand, Hands execute only simple commands from the Brain—for example, “Left Hand, move two inches forward.” They cannot execute commands other than motions; for example, they cannot be commanded to “Pick up the box.”The Hands must be blindfolded .

Token 20499:
The only sensory capability they have is the ability to tell when their path is blocked by an immovable obstacle such as a table or the other Hand.

Token 20500:
Insuch cases, they can beep to inform the Brain of the difﬁculty.

Token 20501:
26PHILOSOPHICAL FOUNDATIONS In which we consider what it means to think and whether artifacts could and should ever do so.

Token 20502:
Philosophers have been around far longer than computers and have been trying to resolve some questions that relate to AI: How do minds work?

Token 20503:
Is it possible for machines to actintelligently in the way that people do, and if they did, would they have real, consciousminds?

Token 20504:
What are the ethical implications of intelligent machines?

Token 20505:
First, some terminology: the assertion that machines could act as ifthey were intelligent is called the weak AI hypothesis by philosophers, and the assertion that machines that do so WEAK AI areactually thinking (not just simulating thinking) is called the strong AI hypothesis.

Token 20506:
STRONG AI Most AI researchers take the weak AI hypothesis for granted, and don’t care about the strong AI hypothesis—as long as their program works, they don’t care whether you call it asimulation of intelligence or real intelligence.

Token 20507:
All AI researchers should be concerned withthe ethical implications of their work. 26.1 W EAK AI: C ANMACHINES ACTINTELLIGENTLY ?

Token 20508:
The proposal for the 1956 summer workshop that deﬁned the ﬁeld of Artiﬁcial Intelligence(McCarthy et al.

Token 20509:
, 1955) made the assertion that “Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” Thus,AI was founded on the assumption that weak AI is possible.

Token 20510:
Others have asserted that weakAI is impossible: “Artiﬁcial intelligence pursued within the cult of computationalism stands not even a ghost of a chance of producing durable results” (Sayre, 1993).

Token 20511:
Clearly, whether AI is impossible depends on how it is deﬁned.

Token 20512:
In Section 1.1, we de- ﬁned AI as the quest for the best agent program on a given architecture.

Token 20513:
With this formulation, AI is by deﬁnition possible: for any digital architecture with kbits of program storage there are exactly 2 kagent programs, and all we have to do to ﬁnd the best one is enumerate and test them all.

Token 20514:
This might not be feasible for large k, but philosophers deal with the theoretical, not the practical. 1020

Token 20515:
Section 26.1. Weak AI: Can Machines Act Intelligently?

Token 20516:
1021 Our deﬁnition of AI works well for the engineering problem of ﬁnding a good agent, given an architecture.

Token 20517:
Therefore, we’re tempted to end this section right now, answering thetitle question in the afﬁrmative.

Token 20518:
But philosophers are interested in the problem of compar-ing two architectures—human and machine.

Token 20519:
Furthermore, they have traditionally posed thequestion not in terms of maximizing expected utility but rather as, “ Can machines think ?” CAN MACHINES THINK?

Token 20520:
The computer scientist Edsger Dijkstra (1984) said that “The question of whether Ma- chines Can Think ...i sa bout as relevant as the question of whether Submarines Can Swim .”CAN SUBMARINES SWIM?

Token 20521:
The American Heritage Dictionary’s ﬁrst deﬁnition of swim is “To move through water by means of the limbs, ﬁns, or tail,” and most people agree that submarines, being limbless,cannot swim.

Token 20522:
The dictionary also deﬁnes ﬂyas “To move through the air by means of wings or winglike parts,” and most people agree that airplanes, having winglike parts, can ﬂy.

Token 20523:
How-ever, neither the questions nor the answers have any relevance to the design or capabilities ofairplanes and submarines; rather they are about the usage of words in English.

Token 20524:
(The fact thatships doswim in Russian only ampliﬁes this point.).

Token 20525:
The practical possibility of “thinking machines” has been with us for only 50 years or so, not long enough for speakers of English tosettle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.” Alan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug- gested that instead of asking whether machines can think, we should ask whether machines can pass a behavioral intelligence test, which has come to be called the Turing Test .

Token 20526:
The test TURINGTEST is for a program to have a conversation (via online typed messages) with an interrogator for ﬁve minutes.

Token 20527:
The interrogator then has to guess if the conversation is with a program or a person; the program passes the test if it fools the interrogator 30% of the time.

Token 20528:
Turing con- jectured that, by the year 2000, a computer with a storage of 109units could be programmed well enough to pass the test.

Token 20529:
He was wrong—programs have yet to fool a sophisticated judge.

Token 20530:
On the other hand, many people have been fooled when they didn’t know they might be chatting with a computer.

Token 20531:
The E LIZA program and Internet chatbots such as M GONZ (Humphrys, 2008) and N ATACHATA have fooled their correspondents repeatedly, and the chatbot C YBER LOVER has attracted the attention of law enforcement because of its penchant for tricking fellow chatters into divulging enough personal information that their identity can be stolen.

Token 20532:
The Loebner Prize competition, held annually since 1991, is the longest-runningTuring Test-like contest.

Token 20533:
The competitions have led to better models of human typing errors.

Token 20534:
Turing himself examined a wide variety of possible objections to the possibility of in- telligent machines, including virtually all of those that have been raised in the half-centurysince his paper appeared.

Token 20535:
We will look at some of them.

Token 20536:
26.1.1 The argument from disability The “argument from disability” makes the claim that “a machine can never do X.” As exam- ples of X, Turing lists the following: Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someonefall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behavior as man, do something really new.

Token 20537:
1022 Chapter 26.

Token 20538:
Philosophical Foundations In retrospect, some of these are rather easy—we’re all familiar with computers that “make mistakes.” We are also familiar with a century-old technology that has had a proven abilityto “make someone fall in love with it”—the teddy bear.

Token 20539:
Computer chess expert David Levypredicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).As for a robot falling in love, that is a common theme in ﬁction, 1but there has been only lim- ited speculation about whether it is in fact likely (Kim et al.

Token 20540:
, 2007).

Token 20541:
Programs do play chess, checkers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-nose diseases; and do hundreds of other tasks as well as or better than humans.

Token 20542:
Computershave made small but signiﬁcant discoveries in astronomy, mathematics, chemistry, mineral-ogy, biology, computer science, and other ﬁelds.

Token 20543:
Each of these required performance at thelevel of a human expert.

Token 20544:
Given what we now know about computers, it is not surprising that they do well at combinatorial problems such as playing chess.

Token 20545:
But algorithms also perform at human levelson tasks that seemingly involve human judgment, or as Turing put it, “learning from experi-ence” and the ability to “tell right from wrong.” As far back as 1955, Paul Meehl (see alsoGrove and Meehl, 1996) studied the decision-making processes of trained experts at subjec-tive tasks such as predicting the success of a student in a training program or the recidivism of a criminal.

Token 20546:
In 19 out of the 20 studies he looked at, Meehl found that simple statistical learning algorithms (such as linear regression or naive Bayes) predict better than the experts.The Educational Testing Service has used an automated program to grade millions of essayquestions on the GMAT exam since 1999.

Token 20547:
The program agrees with human graders 97% ofthe time, about the same level that two human graders agree (Burstein et al. , 2001).

Token 20548:
It is clear that computers can do many things as well as or better than humans, including things that people believe require great human insight and understanding.

Token 20549:
This does not mean,of course, that computers use insight and understanding in performing these tasks—those arenot part of behavior , and we address such questions elsewhere—but the point is that one’s ﬁrst guess about the mental processes required to produce a given behavior is often wrong.

Token 20550:
Itis also true, of course, that there are many tasks at which computers do not yet excel (to put it mildly), including Turing’s task of carrying on an open-ended conversation.

Token 20551:
26.1.2 The mathematical objection It is well known, through the work of Turing (1936) and G¨ odel (1931), that certain math- ematical questions are in principle unanswerable by particular formal systems.

Token 20552:
G¨ odel’s in- completeness theorem (see Section 9.5) is the most famous example of this.

Token 20553:
Brieﬂy, for anyformal axiomatic system Fpowerful enough to do arithmetic, it is possible to construct a so-called G¨ odel sentence G(F)with the following properties: •G(F)is a sentence of F, but cannot be proved within F. •IfFis consistent, then G(F)is true.

Token 20554:
1For example, the opera Copp´ elia (1870), the novel Do Androids Dream of Electric Sheep?

Token 20555:
(1968), the movies AI(2001) and Wall-E (2008), and in song, Noel Coward’s 1955 version of Let’s Do It: Let’s Fall in Love predicted “probably we’ll live to see machines do it.” He didn’t.

Token 20556:
Section 26.1. Weak AI: Can Machines Act Intelligently?

Token 20557:
1023 Philosophers such as J. R. Lucas (1961) have claimed that this theorem shows that machines are mentally inferior to humans, because machines are formal systems that are limited by theincompleteness theorem—they cannot establish the truth of their own G¨ odel sentence—while humans have no such limitation.

Token 20558:
This claim has caused decades of controversy, spawning avast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994) that repeat the claim with some fresh twists (such as the hypothesis that humans are different because their brains operate by quantum gravity).

Token 20559:
We will examine only three of the problemswith the claim.

Token 20560:
First, G¨ odel’s incompleteness theorem applies only to formal systems that are powerful enough to do arithmetic.

Token 20561:
This includes Turing machines, and Lucas’s claim is in part basedon the assertion that computers are Turing machines.

Token 20562:
This is a good approximation, but is notquite true.

Token 20563:
Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer cantherefore be described as a (very large) system in propositional logic, which is not subject toG¨odel’s incompleteness theorem.

Token 20564:
Second, an agent should not be too ashamed that it cannot establish the truth of some sentence while other agents can.

Token 20565:
Consider the sentence J. R. Lucas cannot consistently assert that this sentence is true.

Token 20566:
If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucascannot consistently assert it, and hence it must be true.

Token 20567:
We have thus demonstrated that thereis a sentence that Lucas cannot consistently assert while other people (and machines) can.

Token 20568:
Butthat does not make us think less of Lucas.

Token 20569:
To take another example, no human could compute the sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in seconds.

Token 20570:
Still, we do not see this as a fundamental limitation in the human’s ability to think.Humans were behaving intelligently for thousands of years before they invented mathematics,so it is unlikely that formal mathematical reasoning plays more than a peripheral role in whatit means to be intelligent.

Token 20571:
Third, and most important, even if we grant that computers have limitations on what they can prove, there is no evidence that humans are immune from those limitations.

Token 20572:
It isall too easy to show rigorously that a formal system cannot do X, and then claim that hu- mans candoXusing their own informal method, without giving any evidence for this claim.

Token 20573:
Indeed, it is impossible to prove that humans are not subject to G¨ odel’s incompleteness theo- rem, because any rigorous proof would require a formalization of the claimed unformalizable human talent, and hence refute itself.

Token 20574:
So we are left with an appeal to intuition that humans can somehow perform superhuman feats of mathematical insight.

Token 20575:
This appeal is expressedwith arguments such as “we must assume our own consistency, if thought is to be possible atall” (Lucas, 1976).

Token 20576:
But if anything, humans are known to be inconsistent.

Token 20577:
This is certainlytrue for everyday reasoning, but it is also true for careful mathematical thought. A famousexample is the four-color map problem.

Token 20578:
Alfred Kempe published a proof in 1879 that waswidely accepted and contributed to his election as a Fellow of the Royal Society.

Token 20579:
In 1890,however, Percy Heawood pointed out a ﬂaw and the theorem remained unproved until 1977.

Token 20580:
1024 Chapter 26.

Token 20581:
Philosophical Foundations 26.1.3 The argument from informality One of the most inﬂuential and persistent criticisms of AI as an enterprise was raised by Tur- ing as the “argument from informality of behavior.” Essentially, this is the claim that human behavior is far too complex to be captured by any simple set of rules and that because com-puters can do no more than follow a set of rules, they cannot generate behavior as intelligentas that of humans.

Token 20582:
The inability to capture everything in a set of logical rules is called thequaliﬁcation problem in AI.

Token 20583:
QUALIFICATION PROBLEM The principal proponent of this view has been the philosopher Hubert Dreyfus, who has produced a series of inﬂuential critiques of artiﬁcial intelligence: What Computers Can’t Do(1972), the sequel What Computers Still Can’t Do (1992), and, with his brother Stuart, Mind Over Machine (1986).

Token 20584:
The position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI ,a term coined by philosopher John Haugeland (1985).

Token 20585:
GOFAI is supposed to claim that all intelligent behavior can be captured by a system that reasons logically from a set of facts and rules describing the domain.

Token 20586:
It therefore corresponds to the simplest logical agent describedin Chapter 7.

Token 20587:
Dreyfus is correct in saying that logical agents are vulnerable to the qualiﬁcationproblem.

Token 20588:
As we saw in Chapter 13, probabilistic reasoning systems are more appropriate foropen-ended domains.

Token 20589:
The Dreyfus critique therefore is not addressed against computers per se, but rather against one particular way of programming them.

Token 20590:
It is reasonable to suppose, however, that a book called What First-Order Logical Rule-Based Systems Without Learning Can’t Do might have had less impact.

Token 20591:
Under Dreyfus’s view, human expertise does include knowledge of some rules, but only as a “holistic context” or “background” within which humans operate.

Token 20592:
He gives the example of appropriate social behavior in giving and receiving gifts: “Normally one simply responds in the appropriate circumstances by giving an appropriate gift.” One apparently has “a direct sense of how things are done and what to expect.” The same claim is made in the context of chess playing: “A mere chess master might need to ﬁgure out what to do, but a grandmasterjust sees the board as demanding a certain move .

Token 20593:
. .

Token 20594:
the right response just pops into his or herhead.” It is certainly true that much of the thought processes of a present-giver or grandmasteris done at a level that is not open to introspection by the conscious mind.

Token 20595:
But that does notmean that the thought processes do not exist.

Token 20596:
The important question that Dreyfus does notanswer is how the right move gets into the grandmaster’s head.

Token 20597:
One is reminded of Daniel Dennett’s (1984) comment, It is rather as if philosophers were to proclaim themselves expert explainers of the meth- ods of stage magicians, and then, when we ask how the magician does the sawing-the- lady-in-half trick, they explain that it is really quite obvious: the magician doesn’t really saw her in half; he simply makes it appear that he does.

Token 20598:
“But how does he do that?” we ask. “Not our department,” say the philosophers.

Token 20599:
Dreyfus and Dreyfus (1986) propose a ﬁve-stage process of acquiring expertise, beginning with rule-based processing (of the sort proposed in GOFAI ) and ending with the ability to select correct responses instantaneously.

Token 20600:
In making this proposal, Dreyfus and Dreyfus ineffect move from being AI critics to AI theorists—they propose a neural network architecture

Token 20601:
Section 26.1. Weak AI: Can Machines Act Intelligently? 1025 organized into a vast “case library,” but point out several problems.

Token 20602:
Fortunately, all of their problems have been addressed, some with partial success and some with total success. Theirproblems include the following: 1.

Token 20603:
Good generalization from examples cannot be achieved without background knowl- edge.

Token 20604:
They claim no one has any idea how to incorporate background knowledge intothe neural network learning process.

Token 20605:
In fact, we saw in Chapters 19 and 20 that there are techniques for using prior knowledge in learning algorithms.

Token 20606:
Those techniques, however, rely on the availability of knowledge in explicit form, something that Dreyfusand Dreyfus strenuously deny.

Token 20607:
In our view, this is a good reason for a serious redesignof current models of neural processing so that they cantake advantage of previously learned knowledge in the way that other learning algorithms do.

Token 20608:
2.

Token 20609:
Neural network learning is a form of supervised learning (see Chapter 18), requiring the prior identiﬁcation of relevant inputs and correct outputs.

Token 20610:
Therefore, they claim,it cannot operate autonomously without the help of a human trainer.

Token 20611:
In fact, learningwithout a teacher can be accomplished by unsupervised learning (Chapter 20) and reinforcement learning (Chapter 21). 3.

Token 20612:
Learning algorithms do not perform well with many features, and if we pick a subset of features, “there is no known way of adding new features should the current set proveinadequate to account for the learned facts.” In fact, new methods such as supportvector machines handle large feature sets very well.

Token 20613:
With the introduction of largeWeb-based data sets, many applications in areas such as language processing (Sha andPereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millionsof features.

Token 20614:
We saw in Chapter 19 that there are also principled ways to generate newfeatures, although much more work is needed. 4.

Token 20615:
The brain is able to direct its sensors to seek relevant information and to process it to extract aspects relevant to the current situation.

Token 20616:
But, Dreyfus and Dreyfus claim, “Currently, no details of this mechanism are understood or even hypothesized in a way that could guide AI research.” In fact, the ﬁeld of active vision, underpinned by thetheory of information value (Chapter 16), is concerned with exactly the problem ofdirecting sensors, and already some robots have incorporated the theoretical resultsobtained.

Token 20617:
S TANLEY ’s 132-mile trip through the desert (page 28) was made possible in large part by an active sensing system of this kind.

Token 20618:
In sum, many of the issues Dreyfus has focused on—background commonsense knowledge, the qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—areindeed important issues, and have by now been incorporated into standard intelligent agentdesign.

Token 20619:
In our view, this is evidence of AI’s progress, not of its impossibility.

Token 20620:
One of Dreyfus’ strongest arguments is for situated agents rather than disembodied logical inference engines.

Token 20621:
An agent whose understanding of “dog” comes only from a limited set of logical sentences such as “ Dog(x)⇒Mammal (x)” is at a disadvantage compared to an agent that has watched dogs run, has played fetch with them, and has been licked byone.

Token 20622:
As philosopher Andy Clark (1998) says, “Biological brains are ﬁrst and foremost thecontrol systems for biological bodies.

Token 20623:
Biological bodies move and act in rich real-world

Token 20624:
1026 Chapter 26.

Token 20625:
Philosophical Foundations surroundings.” To understand how human (or other animal) agents work, we have to consider the whole agent, not just the agent program.

Token 20626:
Indeed, the embodied cognition approach claimsEMBODIED COGNITION that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment.

Token 20627:
We need to study the system as a whole; the brainaugments its reasoning by referring to the environment, as the reader does in perceiving (and creating) marks on paper to transfer knowledge.

Token 20628:
Under the embodied cognition program, robotics, vision, and other sensors become central, not peripheral. 26.2 S TRONG AI: C ANMACHINES REALLY THINK ?

Token 20629:
Many philosophers have claimed that a machine that passes the Turing Test would still notbeactually thinking, but would be only a simulation of thinking.

Token 20630:
Again, the objection was foreseen by Turing.

Token 20631:
He cites a speech by Professor Geoffrey Jefferson (1949): Not until a machine could write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equalsbrain—that is, not only write it but know that it had written it.

Token 20632:
Turing calls this the argument from consciousness —the machine has to be aware of its own mental states and actions.

Token 20633:
While consciousness is an important subject, Jefferson’s key pointactually relates to phenomenology , or the study of direct experience: the machine has to actually feel emotions.

Token 20634:
Others focus on intentionality —that is, the question of whether the machine’s purported beliefs, desires, and other representations are actually “about” some-thing in the real world.

Token 20635:
Turing’s response to the objection is interesting.

Token 20636:
He could have presented reasons that machines can in fact be conscious (or have phenomenology, or have intentions).

Token 20637:
Instead, hemaintains that the question is just as ill-deﬁned as asking, “Can machines think?” Besides,why should we insist on a higher standard for machines than we do for humans?

Token 20638:
After all,in ordinary life we never have anydirect evidence about the internal mental states of other humans.

Token 20639:
Nevertheless, Turing says, “Instead of arguing continually over this point, it is usualto have the polite convention that everyone thinks.” Turing argues that Jefferson would be willing to extend the polite convention to ma- chines if only he had experience with ones that act intelligently.

Token 20640:
He cites the following dialog, which has become such a part of AI’s oral tradition that we simply have to include it: HUMAN : In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s day,” would not a “spring day” do as well or better?

Token 20641:
MACHINE : It wouldn’t scan. HUMAN : How about “a winter’s day.” That would scan all right.

Token 20642:
MACHINE : Yes, but nobody wants to be compared to a winter’s day. HUMAN : Would you say Mr. Pickwick reminded you of Christmas? MACHINE :I naw a y .

Token 20643:
HUMAN : Yet Christmas is a winter’s day, and I do not think Mr. Pickwick would mind the comparison.

Token 20644:
Section 26.2. Strong AI: Can Machines Really Think? 1027 MACHINE : I don’t think you’re serious.

Token 20645:
By a winter’s day one means a typical winter’s day, rather than a special one like Christmas.

Token 20646:
One can easily imagine some future time in which such conversations with machines are commonplace, and it becomes customary to make no linguistic distinction between “real”and “artiﬁcial” thinking.

Token 20647:
A similar transition occurred in the years after 1848, when artiﬁcialurea was synthesized for the ﬁrst time by Frederick W¨ ohler.

Token 20648:
Prior to this event, organic and inorganic chemistry were essentially disjoint enterprises and many thought that no process could exist that would convert inorganic chemicals into organic material.

Token 20649:
Once the synthesiswas accomplished, chemists agreed that artiﬁcial urea wasurea, because it had all the right physical properties.

Token 20650:
Those who had posited an intrinsic property possessed by organic ma-terial that inorganic material could never have were faced with the impossibility of devisingany test that could reveal the supposed deﬁciency of artiﬁcial urea.

Token 20651:
For thinking, we have not yet reached our 1848 and there are those who believe that artiﬁcial thinking, no matter how impressive, will never be real.

Token 20652:
For example, the philosopherJohn Searle (1980) argues as follows: No one supposes that a computer simulation of a storm will leave us all wet ...Why on earth would anyone in his right mind suppose a computer simulation of mental processes actually had mental processes?

Token 20653:
(pp.

Token 20654:
37–38) While it is easy to agree that computer simulations of storms do not make us wet, it is not clear how to carry this analogy over to computer simulations of mental processes.

Token 20655:
Afterall, a Hollywood simulation of a storm using sprinklers and wind machines does make the actors wet, and a video game simulation of a storm does make the simulated characters wet.

Token 20656:
Most people are comfortable saying that a computer simulation of addition is addition, andof chess is chess.

Token 20657:
In fact, we typically speak of an implementation of addition or chess, not a simulation .

Token 20658:
Are mental processes more like storms, or more like addition?

Token 20659:
Turing’s answer—the polite convention—suggests that the issue will eventually go away by itself once machines reach a certain level of sophistication.

Token 20660:
This would have the effect of dissolving the difference between weak and strong AI.

Token 20661:
Against this, one may insist that there is a factual issue at stake: humans do have real minds, and machines might or might not.

Token 20662:
To address this factual issue, we need to understand how it is that humans havereal minds, not just bodies that generate neurophysiological processes.

Token 20663:
Philosophical effortsto solve this mind–body problem are directly relevant to the question of whether machines MIND–BODY PROBLEM could have real minds.

Token 20664:
The mind–body problem was considered by the ancient Greek philosophers and by var- ious schools of Hindu thought, but was ﬁrst analyzed in depth by the 17th-century Frenchphilosopher and mathematician Ren´ e Descartes.

Token 20665:
His Meditations on First Philosophy (1641) considered the mind’s activity of thinking (a process with no spatial extent or material prop-erties) and the physical processes of the body, concluding that the two must exist in separate realms—what we would now call a dualist theory.

Token 20666:
The mind–body problem faced by du- DUALISM alists is the question of how the mind can control the body if the two are really separate.

Token 20667:
Descartes speculated that the two might interact through the pineal gland, which simply begsthe question of how the mind controls the pineal gland.

Token 20668:
1028 Chapter 26.

Token 20669:
Philosophical Foundations Themonist theory of mind, often called physicalism , avoids this problem by asserting MONISM PHYSICALISM the mind is not separate from the body—that mental states arephysical states.

Token 20670:
Most modern philosophers of mind are physicalists of one form or another, and physicalism allows, at leastin principle, for the possibility of strong AI.

Token 20671:
The problem for physicalists is to explain howphysical states—in particular, the molecular conﬁgurations and electrochemical processes of the brain—can simultaneously be mental states , such as being in pain, enjoying a hamburger, MENTAL STATES knowing that one is riding a horse, or believing that Vienna is the capital of Austria.

Token 20672:
26.2.1 Mental states and the brain in a vat Physicalist philosophers have attempted to explicate what it means to say that a person—and, by extension, a computer—is in a particular mental state.

Token 20673:
They have focused in particular onintentional states .

Token 20674:
These are states, such as believing, knowing, desiring, fearing, and so on, INTENTIONAL STATE that refer to some aspect of the external world.

Token 20675:
For example, the knowledge that one is eating a hamburger is a belief about the hamburger and what is happening to it.

Token 20676:
If physicalism is correct, it must be the case that the proper description of a person’s mental state is determined by that person’s brain state.

Token 20677:
Thus, if I am currently focused on eating a hamburger in a mindful way, my instantaneous brain state is an instance of the class ofmental states “knowing that one is eating a hamburger.” Of course, the speciﬁc conﬁgurationsof all the atoms of my brain are not essential: there are many conﬁgurations of my brain, orof other people’s brain, that would belong to the same class of mental states.

Token 20678:
The key point isthat the same brain state could not correspond to a fundamentally distinct mental state, suchas the knowledge that one is eating a banana.

Token 20679:
The simplicity of this view is challenged by some simple thought experiments.

Token 20680:
Imag- ine, if you will, that your brain was removed from your body at birth and placed in a mar- velously engineered vat.

Token 20681:
The vat sustains your brain, allowing it to grow and develop.

Token 20682:
At the same time, electronic signals are fed to your brain from a computer simulation of an entirely ﬁctitious world, and motor signals from your brain are intercepted and used to modify the simulation as appropriate.

Token 20683:
2In fact, the simulated life you live replicates exactly the life you would have lived, had your brain not been placed in the vat, including simulated eating ofsimulated hamburgers.

Token 20684:
Thus, you could have a brain state identical to that of someone who isreally eating a real hamburger, but it would be literally false to say that you have the mentalstate “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you havenever even experienced a hamburger, and you could not, therefore, have such a mental state.

Token 20685:
This example seems to contradict the view that brain states determine mental states.

Token 20686:
One way to resolve the dilemma is to say that the content of mental states can be interpreted fromtwo different points of view.

Token 20687:
The “ wide content ” view interprets it from the point of view WIDE CONTENT of an omniscient outside observer with access to the whole situation, who can distinguish differences in the world.

Token 20688:
Under this view, the content of mental states involves both the brain state and the environment history.

Token 20689:
Narrow content , on the other hand, considers only the NARROW CONTENT brain state.

Token 20690:
The narrow content of the brain states of a real hamburger-eater and a brain-in-a- vat “hamburger”-“eater” is the same in both cases.

Token 20691:
2This situation may be familiar to those who have seen the 1999 ﬁlm The Matrix .

Token 20692:
Section 26.2. Strong AI: Can Machines Really Think?

Token 20693:
1029 Wide content is entirely appropriate if one’s goals are to ascribe mental states to others who share one’s world, to predict their likely behavior and its effects, and so on.

Token 20694:
This is thesetting in which our ordinary language about mental content has evolved.

Token 20695:
On the other hand,if one is concerned with the question of whether AI systems are really thinking and reallydo have mental states, then narrow content is appropriate; it simply doesn’t make sense to say that whether or not an AI system is really thinking depends on conditions outside that system.

Token 20696:
Narrow content is also relevant if we are thinking about designing AI systems orunderstanding their operation, because it is the narrow content of a brain state that determineswhat will be the (narrow content of the) next brain state.

Token 20697:
This leads naturally to the idea thatwhat matters about a brain state—what makes it have one kind of mental content and notanother—is its functional role within the mental operation of the entity involved.

Token 20698:
26.2.2 Functionalism and the brain replacement experiment The theory of functionalism says that a mental state is any intermediate causal condition FUNCTIONALISM between input and output.

Token 20699:
Under functionalist theory, any two systems with isomorphic causal processes would have the same mental states.

Token 20700:
Therefore, a computer program couldhave the same mental states as a person.

Token 20701:
Of course, we have not yet said what “isomorphic”really means, but the assumption is that there is some level of abstraction below which thespeciﬁc implementation does not matter.

Token 20702:
The claims of functionalism are illustrated most clearly by the brain replacement ex- periment.

Token 20703:
This thought experiment was introduced by the philosopher Clark Glymour andwas touched on by John Searle (1980), but is most commonly associated with roboticist Hans Moravec (1988).

Token 20704:
It goes like this: Suppose neurophysiology has developed to the point where the input–output behavior and connectivity of all the neurons in the human brain are perfectly understood.

Token 20705:
Suppose further that we can build microscopic electronic devices that mimic this behavior and can be smoothly interfaced to neural tissue.

Token 20706:
Lastly, suppose that some mirac-ulous surgical technique can replace individual neurons with the corresponding electronicdevices without interrupting the operation of the brain as a whole.

Token 20707:
The experiment consistsof gradually replacing all the neurons in someone’s head with electronic devices.

Token 20708:
We are concerned with both the external behavior and the internal experience of the subject, during and after the operation.

Token 20709:
By the deﬁnition of the experiment, the subject’sexternal behavior must remain unchanged compared with what would be observed if theoperation were not carried out.

Token 20710:
3Now although the presence or absence of consciousness cannot easily be ascertained by a third party, the subject of the experiment ought at least to be able to record any changes in his or her own conscious experience.

Token 20711:
Apparently, there isa direct clash of intuitions as to what would happen.

Token 20712:
Moravec, a robotics researcher andfunctionalist, is convinced his consciousness would remain unaffected.

Token 20713:
Searle, a philosopherand biological naturalist, is equally convinced his consciousness would vanish: You ﬁnd, to your total amazement, that you are indeed losing control of your external behavior.

Token 20714:
You ﬁnd, for example, that when doctors test your vision, you hear them say “We are holding up a red object in front of you; please tell us what you see.” You want 3One can imagine using an identical “control” subject who is given a placebo operation, for comparison.

Token 20715:
1030 Chapter 26. Philosophical Foundations to cry out “I can’t see anything.

Token 20716:
I’m going totally blind.” But you hear your voice saying in a way that is completely out of your control, “I see a red object in front of me.” ... your conscious experience slowly shrinks to nothing, while your externally observable behavior remains the same.

Token 20717:
(Searle, 1992) One can do more than argue from intuition.

Token 20718:
First, note that, for the external behavior to re- main the same while the subject gradually becomes unconscious, it must be the case that thesubject’s volition is removed instantaneously and totally; otherwise the shrinking of aware-ness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.This instantaneous removal of volition as a result of gradual neuron-at-a-time replacementseems an unlikely claim to have to make.

Token 20719:
Second, consider what happens if we do ask the subject questions concerning his or her conscious experience during the period when no real neurons remain.

Token 20720:
By the conditionsof the experiment, we will get responses such as “I feel ﬁne.

Token 20721:
I must say I’m a bit surprised because I believed Searle’s argument.” Or we might poke the subject with a pointed stick and observe the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic candismiss such outputs from AI programs as mere contrivances.

Token 20722:
Certainly, it is easy enough touse a rule such as “If sensor 12 reads ‘High’ then output ‘Ouch.’ ” But the point here is that,because we have replicated the functional properties of a normal human brain, we assumethat the electronic brain contains no such contrivances.

Token 20723:
Then we must have an explanation ofthe manifestations of consciousness produced by the electronic brain that appeals only to thefunctional properties of the neurons.

Token 20724:
And this explanation must also apply to the real brain, which has the same functional properties. There are three possible conclusions: 1.

Token 20725:
The causal mechanisms of consciousness that generate these kinds of outputs in normal brains are still operating in the electronic version, which is therefore conscious.

Token 20726:
2.

Token 20727:
The conscious mental events in the normal brain have no causal connection to behavior, and are missing from the electronic brain, which is therefore not conscious.

Token 20728:
3. The experiment is impossible, and therefore speculation about it is meaningless.

Token 20729:
Although we cannot rule out the second possibility, it reduces consciousness to what philoso- phers call an epiphenomenal role—something that happens, but casts no shadow, as it were, EPIPHENOMENON on the observable world.

Token 20730:
Furthermore, if consciousness is indeed epiphenomenal, then it cannot be the case that the subject says “Ouch” because it hurts —that is, because of the con- scious experience of pain.

Token 20731:
Instead, the brain must contain a second, unconscious mechanismthat is responsible for the “Ouch.” Patricia Churchland (1986) points out that the functionalist arguments that operate at the level of the neuron can also operate at the level of any larger functional unit—a clumpof neurons, a mental module, a lobe, a hemisphere, or the whole brain.

Token 20732:
That means that ifyou accept the notion that the brain replacement experiment shows that the replacement brainis conscious, then you should also believe that consciousness is maintained when the entire brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge lookup table.

Token 20733:
This is disconcerting to many people (including Turing himself), who havethe intuition that lookup tables are not conscious—or at least, that the conscious experiencesgenerated during table lookup are not the same as those generated during the operation of a

Token 20734:
Section 26.2. Strong AI: Can Machines Really Think?

Token 20735:
1031 system that might be described (even in a simple-minded, computational sense) as accessing and generating beliefs, introspections, goals, and so on.

Token 20736:
26.2.3 Biological naturalism and the Chinese Room A strong challenge to functionalism has been mounted by John Searle’s (1980) biological naturalism , according to which mental states are high-level emergent features that are causedBIOLOGICAL NATURALISM by low-level physical processes in the neurons , and it is the (unspeciﬁed) properties of the neurons that matter.

Token 20737:
Thus, mental states cannot be duplicated just on the basis of some pro- gram having the same functional structure with the same input–output behavior; we wouldrequire that the program be running on an architecture with the same causal power as neurons.To support his view, Searle describes a hypothetical system that is clearly running a programand passes the Turing Test, but that equally clearly (according to Searle) does not understand anything of its inputs and outputs.

Token 20738:
His conclusion is that running the appropriate program(i.e., having the right outputs) is not a sufﬁcient condition for being a mind.

Token 20739:
The system consists of a human, who understands only English, equipped with a rule book, written in English, and various stacks of paper, some blank, some with indecipherableinscriptions.

Token 20740:
(The human therefore plays the role of the CPU, the rule book is the program,and the stacks of paper are the storage device.)

Token 20741:
The system is inside a room with a small opening to the outside. Through the opening appear slips of paper with indecipherable sym- bols.

Token 20742:
The human ﬁnds matching symbols in the rule book, and follows the instructions.

Token 20743:
Theinstructions may include writing symbols on new slips of paper, ﬁnding symbols in the stacks,rearranging the stacks, and so on.

Token 20744:
Eventually, the instructions will cause one or more symbolsto be transcribed onto a piece of paper that is passed back to the outside world.

Token 20745:
So far, so good.

Token 20746:
But from the outside, we see a system that is taking input in the form of Chinese sentences and generating answers in Chinese that are as “intelligent” as thosein the conversation imagined by Turing.

Token 20747:
4Searle then argues: the person in the room does not understand Chinese (given).

Token 20748:
The rule book and the stacks of paper, being just pieces ofpaper, do not understand Chinese. Therefore, there is no understanding of Chinese.

Token 20749:
Hence, according to Searle, running the right program does not necessarily generate understanding.

Token 20750:
Like Turing, Searle considered and attempted to rebuff a number of replies to his ar- gument.

Token 20751:
Several commentators, including John McCarthy and Robert Wilensky, proposedwhat Searle calls the systems reply.

Token 20752:
The objection is that asking if the human in the roomunderstands Chinese is analogous to asking if the CPU can take cube roots.

Token 20753:
In both cases,the answer is no, and in both cases, according to the systems reply, the entire system does have the capacity in question.

Token 20754:
Certainly, if one asks the Chinese Room whether it understandsChinese, the answer would be afﬁrmative (in ﬂuent Chinese).

Token 20755:
By Turing’s polite convention,this should be enough.

Token 20756:
Searle’s response is to reiterate the point that the understanding is notin the human and cannot be in the paper, so there cannot be any understanding.

Token 20757:
He seems tobe relying on the argument that a property of the whole must reside in one of the parts.

Token 20758:
Yet 4The fact that the stacks of paper might contain trillions of pages and the generation of answers would take millions of years has no bearing on the logical structure of the argument.

Token 20759:
One aim of philosophical training is to develop a ﬁnely honed sense of which objections are germane and which are not.

Token 20760:
1032 Chapter 26. Philosophical Foundations water is wet, even though neither H nor O 2is.

Token 20761:
The real claim made by Searle rests upon the following four axioms (Searle, 1990): 1. Computer programs are formal (syntactic). 2.

Token 20762:
Human minds have mental contents (semantics).3. Syntax by itself is neither constitutive of nor sufﬁcient for semantics. 4. Brains cause minds.

Token 20763:
From the ﬁrst three axioms Searle concludes that programs are not sufﬁcient for minds.

Token 20764:
In other words, an agent running a program might be a mind, but it is not necessarily am i n dj u s t by virtue of running the program.

Token 20765:
From the fourth axiom he concludes “Any other system capable of causing minds would have to have causal powers (at least) equivalent to those of brains.” From there he infers that any artiﬁcial brain would have to duplicate the causalpowers of brains, not just run a particular program, and that human brains do not producemental phenomena solely by virtue of running a program.

Token 20766:
The axioms are controversial.

Token 20767:
For example, axioms 1 and 2 rely on an unspeciﬁed distinction between syntax and semantics that seems to be closely related to the distinctionbetween narrow and wide content.

Token 20768:
On the one hand, we can view computers as manipulating syntactic symbols; on the other, we can view them as manipulating electric current, which happens to be what brains mostly do (according to our current understanding).

Token 20769:
So it seemswe could equally say that brains are syntactic.

Token 20770:
Assuming we are generous in interpreting the axioms, then the conclusion—that pro- grams are not sufﬁcient for minds— does follow.

Token 20771:
But the conclusion is unsatisfactory—all Searle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),then you can’t necessarily conclude that non-brains are minds.

Token 20772:
This is reasonable enough—almost tautological—so the whole argument comes down to whether axiom 3 can be ac-cepted.

Token 20773:
According to Searle, the point of the Chinese Room argument is to provide intuitionsfor axiom 3.

Token 20774:
The public reaction shows that the argument is acting as what Daniel Dennett(1991) calls an intuition pump : it ampliﬁes one’s prior intuitions, so biological naturalists INTUITION PUMP are more convinced of their positions, and functionalists are convinced only that axiom 3 is unsupported, or that in general Searle’s argument is unconvincing.

Token 20775:
The argument stirs upcombatants, but has done little to change anyone’s opinion.

Token 20776:
Searle remains undeterred, andhas recently started calling the Chinese Room a “refutation” of strong AI rather than just an“argument” (Snell, 2008).

Token 20777:
Even those who accept axiom 3, and thus accept Searle’s argument, have only their in- tuitions to fall back on when deciding what entities are minds.

Token 20778:
The argument purports to show that the Chinese Room is not a mind by virtue of running the program , but the argument says nothing about how to decide whether the room (or a computer, some other type of machine,or an alien) is a mind by virtue of some other reason .

Token 20779:
Searle himself says that some machines do have minds: humans are biological machines with minds.

Token 20780:
According to Searle, human brains may or may not be running something like an AI program, but if they are, that is not the reason they are minds.

Token 20781:
It takes more to make a mind—according to Searle, somethingequivalent to the causal powers of individual neurons.

Token 20782:
What these powers are is left unspec-iﬁed. It should be noted, however, that neurons evolved to fulﬁll functional roles—creatures

Token 20783:
Section 26.2. Strong AI: Can Machines Really Think? 1033 with neurons were learning and deciding long before consciousness appeared on the scene.

Token 20784:
It would be a remarkable coincidence if such neurons just happened to generate consciousnessbecause of some causal powers that are irrelevant to their functional capabilities; after all, itis the functional capabilities that dictate survival of the organism.

Token 20785:
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the room; what’s there to be a mind?

Token 20786:
But one could make the same argument about the brain: just look at this collection of cells (or of atoms), blindly operating according to the laws ofbiochemistry (or of physics)—what’s there to be a mind?

Token 20787:
Why can a hunk of brain be a mindwhile a hunk of liver cannot? That remains the great mystery.

Token 20788:
26.2.4 Consciousness, qualia, and the explanatory gap Running through all the debates about strong AI—the elephant in the debating room, soto speak—is the issue of consciousness .

Token 20789:
Consciousness is often broken down into aspects CONSCIOUSNESS such as understanding and self-awareness.

Token 20790:
The aspect we will focus on is that of subjective experience : why it is that it feels like something to have certain brain states (e.g., while eating a hamburger), whereas it presumably does not feel like anything to have other physical states (e.g., while being a rock).

Token 20791:
The technical term for the intrinsic nature of experiences is qualia QUALIA (from the Latin word meaning, roughly, “such things”).

Token 20792:
Qualia present a challenge for functionalist accounts of the mind because different qualia could be involved in what are otherwise isomorphic causal processes.

Token 20793:
Consider, forexample, the inverted spectrum thought experiment, which the subjective experience of per- INVERTED SPECTRUM sonXwhen seeing red objects is the same experience that the rest of us experience when seeing green objects, and vice versa.

Token 20794:
Xstill calls red objects “red,” stops for red trafﬁc lights, and agrees that the redness of red trafﬁc lights is a more intense red than the redness of thesetting sun.

Token 20795:
Yet, X’s subjective experience is just different. Qualia are challenging not just for functionalism but for all of science.

Token 20796:
Suppose, for the sake of argument, that we have completed the process of scientiﬁc research on the brain—we have found that neural process P 12in neuron N177transforms molecule Ainto molecule B, and so on, and on.

Token 20797:
There is simply no currently accepted form of reasoning that would leadfrom such ﬁndings to the conclusion that the entity owning those neurons has any particularsubjective experience.

Token 20798:
This explanatory gap has led some philosophers to conclude that EXPLANATORY GAP humans are simply incapable of forming a proper understanding of their own consciousness.

Token 20799:
Others, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia, attributing them to a philosophical confusion.

Token 20800:
Turing himself concedes that the question of consciousness is a difﬁcult one, but denies that it has much relevance to the practice of AI: “I do not wish to give the impression that Ithink there is no mystery about consciousness ...But I do not think these mysteries neces- sarily need to be solved before we can answer the question with which we are concerned in this paper.” We agree with Turing—we are interested in creating programs that behave intel-ligently.

Token 20801:
The additional project of making them conscious is not one that we are equipped totake on, nor one whose success we would be able to determine.

Token 20802:
1034 Chapter 26.

Token 20803:
Philosophical Foundations 26.3 T HEETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE So far, we have concentrated on whether we candevelop AI, but we must also consider whether we should .

Token 20804:
If the effects of AI technology are more likely to be negative than positive, then it would be the moral responsibility of workers in the ﬁeld to redirect their research.Many new technologies have had unintended negative side effects: nuclear ﬁssion broughtChernobyl and the threat of global destruction; the internal combustion engine brought airpollution, global warming, and the paving-over of paradise.

Token 20805:
In a sense, automobiles arerobots that have conquered the world by making themselves indispensable.

Token 20806:
All scientists and engineers face ethical considerations of how they should act on the job, what projects should or should not be done, and how they should be handled.

Token 20807:
See thehandbook on the Ethics of Computing (Berleur and Brunnstein, 2001).

Token 20808:
AI, however, seems to pose some fresh problems beyond that of, say, building bridges that don’t fall down: •People might lose their jobs to automation.

Token 20809:
•People might have too much (or too little) leisure time. •People might lose their sense of being unique.

Token 20810:
•AI systems might be used toward undesirable ends. •The use of AI systems might result in a loss of accountability.

Token 20811:
•The success of AI might mean the end of the human race. We will look at each issue in turn. People might lose their jobs to automation.

Token 20812:
The modern industrial economy has be- come dependent on computers in general, and select AI programs in particular.

Token 20813:
For example,much of the economy, especially in the United States, depends on the availability of con-sumer credit.

Token 20814:
Credit card applications, charge approvals, and fraud detection are now doneby AI programs.

Token 20815:
One could say that thousands of workers have been displaced by these AIprograms, but in fact if you took away the AI programs these jobs would not exist, because human labor would add an unacceptable cost to the transactions.

Token 20816:
So far, automation through information technology in general and AI in particular has created more jobs than it haseliminated, and has created more interesting, higher-paying jobs.

Token 20817:
Now that the canonical AIprogram is an “intelligent agent” designed to assist a human, loss of jobs is less of a concernthan it was when AI focused on “expert systems” designed to replace humans.

Token 20818:
But someresearchers think that doing the complete job is the right goal for AI.

Token 20819:
In reﬂecting on the 25thAnniversary of t he AAAI, Nils Nil sson (2005) set as a challenge the creation of human-level AI that could pass the employment test rather than the Turing Test—a robot that could learnto do any one of a range of jobs.

Token 20820:
We may end up in a future where unemployment is high, buteven the unemployed serve as managers of their own cadre of robot workers.

Token 20821:
People might have too much (or too little) leisure time.

Token 20822:
Alvin Tofﬂer wrote in Future Shock (1970), “The work week has been cut by 50 percent since the turn of the century.

Token 20823:
It is not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.Clarke (1968b) wrote that people in 2001 might be “faced with a future of utter boredom,where the main problem in life is deciding which of several hundred TV channels to select.”

Token 20824:
Section 26.3.

Token 20825:
The Ethics and Risks of Developing Artiﬁcial Intelligence 1035 The only one of these predictions that has come close to panning out is the number of TV channels.

Token 20826:
Instead, people working in knowledge-intensive industries have found themselvespart of an integrated computerized system that operates 24 hours a day; to keep up, they havebeen forced to work longer hours.

Token 20827:
In an industrial economy, rewards are roughly proportional to the time invested; working 10% more would tend to mean a 10% increase in income.

Token 20828:
In an information economy marked by high-bandwidth communication and easy replication of intellectual property (what Frank and Cook (1996) call the “Winner-Take-All Society”), thereis a large reward for being slightly better than the competition; working 10% more could meana 100% increase in income.

Token 20829:
So there is increasing pressure on everyone to work harder.

Token 20830:
AIincreases the pace of technological innovation and thus contributes to this overall trend, butAI also holds the promise of allowing us to take some time off and let our automated agentshandle things for a while.

Token 20831:
Tim Ferriss (2007) recommends using automation and outsourcingto achieve a four-hour work week. People might lose their sense of being unique.

Token 20832:
InComputer Power and Human Rea- son, Weizenbaum (1976), the author of the E LIZA program, points out some of the potential threats that AI poses to society.

Token 20833:
One of Weizenbaum’s principal arguments is that AI researchmakes possible the idea that humans are automata—an idea that results in a loss of autonomy or even of humanity.

Token 20834:
We note that the idea has been around much longer than AI, going back at least to L’Homme Machine (La Mettrie, 1748).

Token 20835:
Humanity has survived other setbacks to our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put Homo sapiens at the same level as other species.

Token 20836:
AI, if widely successful, may be at least as threatening to the moral assumptions of 21st-century society as Darwin’s theory of evolution was to those of the 19th century.

Token 20837:
AI systems might be used toward undesirable ends. Advanced technologies have often been used by the powerful to suppress their rivals.

Token 20838:
As the number theorist G. H. Hardywrote (Hardy, 1940), “A science is said to be useful if its development tends to accentuate theexisting inequalities in the distribution of wealth, or more directly promotes the destruction of human life.” This holds for all sciences, AI being no exception.

Token 20839:
Autonomous AI systems are now commonplace on the battleﬁeld; the U.S. military deployed over 5,000 autonomousaircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009).

Token 20840:
One moral theoryholds that military robots are like medieval armor taken to its logical extreme: no one wouldhave moral objections to a soldier wanting to wear a helmet when being attacked by large,angry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor.

Token 20841:
Onthe other hand, robotic weapons pose additional risks.

Token 20842:
To the extent that human decisionmaking is taken out of the ﬁring loop, robots may end up making decisions that lead to thekilling of innocent civilians.

Token 20843:
At a larger scale, the possession of powerful robots (like thepossession of sturdy helmets) may give a nation overconﬁdence, causing it to go to war morerecklessly than necessary.

Token 20844:
In most wars, at least one party is overconﬁdent in its military abilities—otherwise the conﬂict would have been resolved peacefully.

Token 20845:
Weizenbaum (1976) also pointed out that speech recognition technology could lead to widespread wiretapping, and hence to a loss of civil liberties.

Token 20846:
He didn’t foresee a world withterrorist threats that would change the balance of how much surveillance people are willing to

Token 20847:
1036 Chapter 26. Philosophical Foundations accept, but he did correctly recognize that AI has the potential to mass-produce surveillance.

Token 20848:
His prediction has in part come true: the U.K. now has an extensive network of surveillancecameras, and other countries routinely monitor Web trafﬁc and telephone calls.

Token 20849:
Some acceptthat computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy hassaid “You have zero privacy anyway.

Token 20850:
Get over it.” David Brin (1998) argues that loss of privacy is inevitable, and the way to combat the asymmetry of power of the state over the individual is to make the surveillance accessible to all citizens.

Token 20851:
Etzioni (2004) argues for abalancing of privacy and security; individual rights and community.

Token 20852:
The use of AI systems might result in a loss of accountability.

Token 20853:
In the litigious atmo- sphere that prevails in the United States, legal liability becomes an important issue.

Token 20854:
When aphysician relies on the judgment of a medical expert system for a diagnosis, who is at fault ifthe diagnosis is wrong?

Token 20855:
Fortunately, due in part to the growing inﬂuence of decision-theoreticmethods in medicine, it is now accepted that negligence cannot be shown if the physicianperforms medical procedures that have high expected utility, even if the actual result is catas- trophic for the patient.

Token 20856:
The question should therefore be “Who is at fault if the diagnosis isunreasonable?” So far, courts have held that medical expert systems play the same role asmedical textbooks and reference books; physicians are responsible for understanding the rea- soning behind any decision and for using their own judgment in deciding whether to accept the system’s recommendations.

Token 20857:
In designing medical expert systems as agents, therefore,the actions should be thought of not as directly affecting the patient but as inﬂuencing thephysician’s behavior.

Token 20858:
If expert systems become reliably more accurate than human diagnosti-cians, doctors might become legally liable if they don’t use the recommendations of an expert system.

Token 20859:
Atul Gawande (2002) explores this premise. Similar issues are beginning to arise regarding the use of intelligent agents on the Inter- net.

Token 20860:
Some progress has been made in incorporating constraints into intelligent agents so thatthey cannot, for example, damage the ﬁles of other users (Weld and Etzioni, 1994).

Token 20861:
The prob-lem is magniﬁed when money changes hands.

Token 20862:
If monetary transactions are made “on one’sbehalf” by an intelligent agent, is one liable for the debts incurred?

Token 20863:
Would it be possible for an intelligent agent to have assets itself and to perform electronic trades on its own behalf?

Token 20864:
So far, these questions do not seem to be well understood.

Token 20865:
To our knowledge, no programhas been granted legal status as an individual for the purposes of ﬁnancial transactions; atpresent, it seems unreasonable to do so.

Token 20866:
Programs are also not considered to be “drivers”for the purposes of enforcing trafﬁc regulations on real highways.

Token 20867:
In California law, at least,there do not seem to be any legal sanctions to prevent an automated vehicle from exceedingthe speed limits, although the designer of the vehicle’s control mechanism would be liable inthe case of an accident.

Token 20868:
As with human reproductive technology, the law has yet to catch upwith the new developments. The success of AI might mean the end of the human race.

Token 20869:
Almost any technology has the potential to cause harm in the wrong hands, but with AI and robotics, we have the new problem that the wrong hands might belong to the technology itself.

Token 20870:
Countless science ﬁction stories have warned about robots or robot–human cyborgs running amok. Early examples

Token 20871:
Section 26.3.

Token 20872:
The Ethics and Risks of Developing Artiﬁcial Intelligence 1037 include Mary Shelley’s Frankenstein, or the Modern Prometheus (1818)5and Karel Capek’s play R.U.R.

Token 20873:
(1921), in which robots conquer the world.

Token 20874:
In movies, we have The Terminator (1984), which combines the cliches of robots-conquer-the-world with time travel, and The Matrix (1999), which combines robots-conquer-the-world with brain-in-a-vat.

Token 20875:
It seems that robots are the protagonists of so many conquer-the-world stories because they represent the unknown, just like the witches and ghosts of tales from earlier eras, or the Martians from The War of the Worlds (Wells, 1898).

Token 20876:
The question is whether an AI system poses a bigger risk than traditional software. We will look at three sources of risk.

Token 20877:
First, the AI system’s state estimation may be incorrect, causing it to do the wrong thing.

Token 20878:
For example, an autonomous car might incorrectly estimate the position of a car in theadjacent lane, leading to an accident that might kill the occupants.

Token 20879:
More seriously, a missiledefense system might erroneously detect an attack and launch a counterattack, leading tothe death of billions.

Token 20880:
These risks are not really risks of AI systems—in both cases the samemistake could just as easily be made by a human as by a computer.

Token 20881:
The correct way to mitigatethese risks is to design a system with checks and balances so that a single state-estimationerror does not propagate through the system unchecked.

Token 20882:
Second, specifying the right utility function for an AI system to maximize is not so easy.

Token 20883:
For example, we might propose a utility function designed to minimize human suffering , expressed as an additive reward function over time as in Chapter 17.

Token 20884:
Given the way humansare, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision forthe AI system is to terminate the human race as soon as possible—no humans, no suffering.With AI systems, then, we need to be very careful what we ask for, whereas humans wouldhave no trouble realizing that the proposed utility function cannot be taken literally.

Token 20885:
On theother hand, computers need not be tainted by the irrational behaviors described in Chapter 16.Humans sometimes use their intelligence in aggressive ways because humans have someinnately aggressive tendencies, due to natural selection.

Token 20886:
The machines we build need not beinnately aggressive, unless we decide to build them that way (or unless they emerge as theend product of a mechanism design that encourages aggressive behavior).

Token 20887:
Fortunately, there are techniques, such as apprenticeship learning, that allows us to specify a utility function by example.

Token 20888:
One can hope that a robot that is smart enough to ﬁgure out how to terminate thehuman race is also smart enough to ﬁgure out that that was not the intended utility function.

Token 20889:
Third, the AI system’s learning function may cause it to evolve into a system with unintended behavior.

Token 20890:
This scenario is the most serious, and is unique to AI systems, so wewill cover it in more depth. I. J.

Token 20891:
Good wrote (1965), Let an ultraintelligent machine be deﬁned as a machine that can far surpass all theULTRAINTELLIGENT MACHINE intellectual activities of any man however clever.

Token 20892:
Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines;there would then unquestionably be an “intellig ence explosion,” and the intelligence of man would be left far behind.

Token 20893:
Thus the ﬁrst ultraintelligent machine is the lastinvention that man need ever make, provided that the machine is docile enough to tell us how tokeep it under control.

Token 20894:
5As a young man, Charles Babbage was inﬂuenced by reading Frankenstein .

Token 20895:
1038 Chapter 26.

Token 20896:
Philosophical Foundations The “intelligence explosion” has also been called the technological singularity by mathe-TECHNOLOGICAL SINGULARITY matics professor and science ﬁction author Vernor Vinge, who writes (1993), “Within thirty years, we will have the technological means to create superhuman intelligence.

Token 20897:
Shortly after,the human era will be ended.” Good and Vinge (and many others) correctly note that the curveof technological progress (on many measures) is growing exponentially at present (consider Moore’s Law).

Token 20898:
However, it is a leap to extrapolate that the curve will continue to a singularity of near-inﬁnite growth.

Token 20899:
So far, every other technology has followed an S-shaped curve, wherethe exponential growth eventually tapers off.

Token 20900:
Sometimes new technologies step in when theold ones plateau; sometimes we hit hard limits.

Token 20901:
With less than a century of high-technologyhistory to go on, it is difﬁcult to extrapolate hundreds of years ahead.

Token 20902:
Note that the concept of ultraintelligent machines assumes that intelligence is an es- pecially important attribute, and if you have enough of it, all problems can be solved.

Token 20903:
Butwe know there are limits on computability and computational complexity.

Token 20904:
If the problemof deﬁning ultraintelligent machines (or even approximations to them) happens to fall in theclass of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, theneven exponential progress in technology won’t help—the speed of light puts a strict upperbound on how much computing can be done; problems beyond that limit will not be solved.

Token 20905:
We still don’t know where those upper bounds are.

Token 20906:
Vinge is concerned about the coming singularity, but some computer scientists and futurists relish it.

Token 20907:
Hans Moravec (2000) encourages us to give every advantage to our “mind children,” the robots we create, which may surpass us in intelligence.

Token 20908:
There is even a new word— transhumanism —for the active social movement that looks forward to this future in TRANSHUMANISM which humans are merged with—or replaced by—robotic and biotech inventions.

Token 20909:
Sufﬁce it to say that such issues present a challenge for most moral theorists, who take the preservationof human life and the human species to be a good thing.

Token 20910:
Ray Kurzweil is currently the mostvisible advocate for the singularity view, writing in The Singularity is Near (2005): The Singularity will allow us to transcend these limitations of our biological bodies and brain.

Token 20911:
We will gain power over our fates. Our mortality will be in our own hands.

Token 20912:
Wewill be able to live as long as we want (a subtly different statement from saying we will live forever).

Token 20913:
We will fully understand human thinking and will vastly extend and expand its reach.

Token 20914:
By the end of this century, the nonbi ological portion of our intelligence will be trillions of trillions of times more powerful than unaided human intelligence.

Token 20915:
Kurzweil also notes the potential dangers, writing “But the Singularity will also amplify the ability to act on our destructive inclinations, so its full story has not yet been written.” If ultraintelligent machines are a possibility, we humans would do well to make sure that we design their predecessors in such a way that they design themselves to treat us well.Science ﬁction writer Isaac Asimov (1942) was the ﬁrst to address this issue, with his threelaws of robotics: 1.

Token 20916:
A robot may not injure a human being or, through inaction, allow a human being to come to harm. 2.

Token 20917:
A robot must obey orders given to it by human beings, except where such orders would conﬂict with the First Law.

Token 20918:
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1039 3.

Token 20919:
A robot must protect its own existence as long as such protection does not conﬂict with the First or Second Law.

Token 20920:
These laws seem reasonable, at least to us humans.6But the trick is how to implement these laws.

Token 20921:
In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the robot is found wandering in a circle around the selenium source.

Token 20922:
Every time it heads towardthe source, it senses a danger, and the third law causes it to veer away.

Token 20923:
But every time itveers away, the danger recedes, and the power of the second law takes over, causing it toveer back towards the selenium.

Token 20924:
The set of points that deﬁne the balancing point betweenthe two laws deﬁnes a circle.

Token 20925:
This suggests that the laws are not logical absolutes, but ratherare weighed against each other, with a higher weighting for the earlier laws.

Token 20926:
Asimov was probably thinking of an architecture based on control theory—perhaps a linear combination of factors—while today the most likely architecture would be a probabilistic reasoning agentthat reasons over probability distributions of outcomes, and maximizes utility as deﬁned bythe three laws.

Token 20927:
But presumably we don’t want our robots to prevent a human from crossingthe street because of the nonzero chance of harm.

Token 20928:
That means that the negative utility forharm to a human must be much greater than for disobeying, but that each of the utilities isﬁnite, not inﬁnite.

Token 20929:
Yudkowsky (2008) goes into more detail about how to design a Friendly AI .

Token 20930:
He asserts FRIENDLYAI that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be ﬂawed, and that the robotwill learn and evolve over time.

Token 20931:
Thus the challenge is one of mechanism design—to deﬁne a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.

Token 20932:
We can’t just give a program a static utility function, because circumstances, and our de- sired responses to circumstances, change over time.

Token 20933:
For example, if technology had allowed us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of the time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.

Token 20934:
On the other hand, if we build an AI agent today and tell it to evolve its utility function, howcan we assure that it won’t reason that “Humans think it is moral to kill annoying insects, inpart because insect brains are so primitive.

Token 20935:
But human brains are primitive compared to mypowers, so it must be moral for me to kill humans.” Omohundro (2008) hypothesizes that even an innocuous chess program could pose a risk to society.

Token 20936:
Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.

Token 20937:
The moral is that even if you onlywant your program to play chess or prove theorems, if you give it the capability to learnand alter itself, you need safeguards.

Token 20938:
Omohundro concludes that “Social structures whichcause individuals to bear the cost of their negative externalities would go a long way towardensuring a stable and positive future,” This seems to be an excellent idea for society in general,regardless of the possibility of ultraintelligent machines.

Token 20939:
6A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required to sacriﬁce its own life to save a human.

Token 20940:
1040 Chapter 26. Philosophical Foundations We should note that the idea of safeguards against change in utility function is not a new one.

Token 20941:
In the Odyssey , Homer (ca. 700 B.C.)

Token 20942:
described Ulysses’ encounter with the sirens, whose song was so alluring it compelled sailors to cast themselves into the sea.

Token 20943:
Knowing itwould have that effect on him, Ulysses ordered his crew to bind him to the mast so that hecould not perform the self-destructive act.

Token 20944:
It is interesting to think how similar safeguards could be built into AI systems. Finally, let us consider the robot’s point of view.

Token 20945:
If robots become conscious, then to treat them as mere “machines” (e.g., to take them apart) might be immoral.

Token 20946:
Science ﬁctionwriters have addressed the issue of robot rights. The movie A.I.

Token 20947:
(Spielberg, 2001) was based on a story by Brian Aldiss about an intelligent robot who was programmed to believe thathe was human and fails to understand his eventual abandonment by his owner–mother.

Token 20948:
Thestory (and the movie) argue for the need for a civil rights movement for robots.

Token 20949:
26.4 S UMMARY This chapter has addressed the following issues: •Philosophers use the term weak AI for the hypothesis that machines could possibly behave intelligently, and strong AI for the hypothesis that such machines would count as having actual minds (as opposed to simulated minds).

Token 20950:
•Alan Turing rejected the question “Can machines think?” and replaced it with a be- havioral test.

Token 20951:
He anticipated many objections to the possibility of thinking machines.Few AI researchers pay attention to the Turing Test, preferring to concentrate on theirsystems’ performance on practical tasks, rather than the ability to imitate humans.

Token 20952:
•There is general agreement in modern times that mental states are brain states. •Arguments for and against strong AI are inconclusive.

Token 20953:
Few mainstream AI researchers believe that anything signiﬁcant hinges on the outcome of the debate. •Consciousness remains a mystery.

Token 20954:
•We identiﬁed six potential threats to society posed by AI and related technology.

Token 20955:
We concluded that some of the threats are either unlikely or differ little from threats posedby “unintelligent” technologies.

Token 20956:
One threat in particular is worthy of further consider- ation: that ultraintelligent machines might lead to a future that is very different from today—we may not like it, and at that point we may not have a choice.

Token 20957:
Such consid- erations lead inevitably to the conclusion that we must weigh carefully, and soon, the possible consequences of AI research.

Token 20958:
BIBLIOGRAPHICAL AND HISTORICAL NOTES Sources for the various responses to Turing’s 1950 paper and for the main critics of weakAI were given in the chapter.

Token 20959:
Although it became fashionable in the post-neural-network era

Token 20960:
Bibliographical and Historical Notes 1041 to deride symbolic approaches, not all philosophers are critical of GOFAI .

Token 20961:
Some are, in fact, ardent advocates and even practitioners.

Token 20962:
Zenon Pylyshyn (1984) has argued that cognitioncan best be understood through a computational model, not only in principle but also as away of conducting research at present, and has speciﬁcally rebutted Dreyfus’s criticisms ofthe computational model of human cognition (Pylyshyn, 1974).

Token 20963:
Gilbert Harman (1983), in analyzing belief revision, makes connections with AI research on truth maintenance systems.

Token 20964:
Michael Bratman has applied his “belief-desire-intention” model of human psychology (Brat-man, 1987) to AI research on planning (Bratman, 1992).

Token 20965:
At the extreme end of strong AI,Aaron Sloman (1978, p. xiii) has even described as “racialist” the claim by Joseph Weizen-baum (1976) that intelligent machines can never be regarded as persons.

Token 20966:
Proponents of the importance of embodiment in cognition include the philosophers Merleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the body and the subjective interpretation of reality afforded by our senses, and Heidegger, whoseBeing and Time (1927) asked what it means to actually be an agent, and criticized all of the history of philosophy for taking this notion for granted.

Token 20967:
In the computer age, Alva Noe (2009)and Andy Clark (1998, 2008) propose that our brains form a rather minimal representationof the world, use the world itself in a just-in-time basis to maintain the illusion of a detailed internal model, use props in the world (such as paper and pencil as well as computers) to increase the capabilities of the mind.

Token 20968:
Pfeifer et al. (2006) and Lakoff and Johnson (1999) present arguments for how the body helps shape cognition.

Token 20969:
The nature of the mind has been a standard topic of philosophical theorizing from an- cient times to the present.

Token 20970:
In the Phaedo , Plato speciﬁcally considered and rejected the idea that the mind could be an “attunement” or pattern of organization of the parts of the body, a viewpoint that approximates the functionalist viewpoint in modern philosophy of mind.

Token 20971:
Hedecided instead that the mind had to be an immortal, immaterial soul, separable from thebody and different in substance—the viewpoint of dualism.

Token 20972:
Aristotle distinguished a varietyof souls (Greek ψυχη ) in living things, some of which, at least, he described in a functionalist manner.

Token 20973:
(See Nussbaum (1978) for more on Aristotle’s functionalism.)

Token 20974:
Descartes is notorious for his dualistic view of the human mind, but ironically his histor- ical inﬂuence was toward mechanism and physicalism.

Token 20975:
He explicitly conceived of animals asautomata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]should produce different arrangements of words so as to give an appropriately meaningfulanswer to whatever is said in its presence, as even the dullest of men can do” (Descartes,1637).

Token 20976:
Descartes’s spirited defense of the animals-as-automata viewpoint actually had theeffect of making it easier to conceive of humans as automata as well, even though he himselfdid not take this step.

Token 20977:
The book L’Homme Machine (La Mettrie, 1748) did explicitly argue that humans are automata.

Token 20978:
Modern analytic philosophy has typically accepted physicalism, but the variety of views on the content of mental states is bewildering.

Token 20979:
The identiﬁcation of mental states with brain states is usually attributed to Place (1956) and Smart (1959).

Token 20980:
The debate between narrow- content and wide-content views of mental states was triggered by Hilary Putnam (1975), whointroduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a TWIN EARTHS device to generate identical brain states with different (wide) content.

Token 20981:
1042 Chapter 26. Philosophical Foundations Functionalism is the philosophy of mind most naturally suggested by AI.

Token 20982:
The idea that mental states correspond to classes of brain states deﬁned functionally is due to Putnam(1960, 1967) and Lewis (1966, 1980).

Token 20983:
Perhaps the most forceful proponent of functional-ism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett, 1991) has attracted many attempted rebuttals.

Token 20984:
Metzinger (2009) argues there is no such thing as an objective self, that consciousness is the subjective appearance of a world.

Token 20985:
The inverted spectrum argument concerning qualia was introduced by John Locke (1690).

Token 20986:
Frank Jack-son (1982) designed an inﬂuential thought experiment involving Mary, a color scientist whohas been brought up in an entirely black-and-white world.

Token 20987:
There’s Something About Mary (Ludlow et al. , 2004) collects several papers on this topic.

Token 20988:
Functionalism has come under attack from authors who claim that they do not account for the qualia or “what it’s like” aspect of mental states (Nagel, 1974).

Token 20989:
Searle has focused instead on the alleged inability of functionalism to account for intentionality (Searle, 1980,1984, 1992).

Token 20990:
Churchland and Churchland (1982) rebut both these types of criticism.

Token 20991:
TheChinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).We’ll just mention here a related work: Terry Bisson’s (1990) science ﬁction story They’re Made out of Meat , in which alien robotic explorers who visit earth are incredulous to ﬁnd thinking human beings whose minds are made of meat.

Token 20992:
Presumably, the robotic alien equiv- alent of Searle believes that he can think due to the special causal powers of robotic circuits;causal powers that mere meat-brains do not possess.

Token 20993:
Ethical issues in AI predate the existence of the ﬁeld itself.

Token 20994:
I. J. Good’s (1965) ul- traintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).

Token 20995:
Written four years after the publication of Darwin’s On the Origins of Species and at a time when the most sophisticated machines were steam engines, Butler’s article on Darwin Among the Machines envisioned “the ultimate development of mechanical consciousness” by natural selection.

Token 20996:
The theme was reiterated by George Dyson (1998) in a book of the same title.

Token 20997:
The philosophical literature on minds, brains, and related topics is large and difﬁcult to read without training in the terminology and methods of argument employed.

Token 20998:
The Encyclo- pedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in this process.

Token 20999:
The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent articles and up-to-date references.

Token 21000:
The MIT Encyclopedia of Cognitive Science (Wilson and Keil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.There are several general introductions to the philosophical “AI question” (Boden, 1990;Haugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007).

Token 21001:
The Behavioral and Brain Sciences , abbreviated BBS, is a major journal devoted to philosophical and scientiﬁc debates about AI and neuroscience.

Token 21002:
Topics of ethics and responsibility in AI are covered inthe journals AI and Society andJournal of Artiﬁcial Intelligence and Law .

Token 21003:


Token 21004:
Exercises 1043 EXERCISES 26.1 Go through Turing’s list of alleged “disabilities” of machines, identifying which have been achieved, which are achievable in principle by a program, and which are still problem-atic because they require conscious mental states.

Token 21005:
26.2 Find and analyze an account in the popular media of one or more of the arguments to the effect that AI is impossible.

Token 21006:
26.3 In the brain replacement argument, it is important to be able to restore the subject’s brain to normal, such that its external behavior is as it would have been if the operation hadnot taken place.

Token 21007:
Can the skeptic reasonably object that this would require updating thoseneurophysiological properties of the neurons relating to conscious experience, as distinctfrom those involved in the functional behavior of the neurons?

Token 21008:
26.4 Suppose that a Prolog program containing many clauses about the rules of British citizenship is compiled and run on an ordinary computer.

Token 21009:
Analyze the “brain states” of thecomputer under wide and narrow content.

Token 21010:
26.5 Alan Perlis (1982) wrote, “A year spent in artiﬁcial intelligence is enough to make one believe in God”.

Token 21011:
He also wrote, in a letter to Philip Davis, that one of the central dreams ofcomputer science is that “through the performance of computers and their programs we will remove all doubt that there is only a chemical distinction between the living and nonliving world.” To what extent does the progress made so far in artiﬁcial intelligence shed light on these issues?

Token 21012:
Suppose that at some future date, the AI endeavor has been completely success- ful; that is, we have build intelligent agents capable of carrying out any human cognitive taskat human levels of ability.

Token 21013:
To what extent would that shed light on these issues?

Token 21014:
26.6 Compare the social impact of artiﬁcial intelligence in the last ﬁfty years with the social impact of the introduction of electric appliances and the internal combustion engine in theﬁfty years between 1890 and 1940.

Token 21015:
26.7 I. J. Good claims that intelligence is the most important quality, and that building ultraintelligent machines will change everything.

Token 21016:
A sentient cheetah counters that “Actuallyspeed is more important; if we could build ultrafast machines, that would change everything,”and a sentient elephant claims “You’re both wrong; what we need is ultrastrong machines.”What do you think of these arguments?

Token 21017:
26.8 Analyze the potential threats from AI technology to society. What threats are most se- rious, and how might they be combated?

Token 21018:
How do they compare to the potential beneﬁts?

Token 21019:
26.9 How do the potential threats from AI technology compare with those from other com- puter science technologies, and to bio-, nano-, and nuclear technologies?

Token 21020:
26.10 Some critics object that AI is impossible, while others object that it is toopossible and that ultraintelligent machines pose a threat.

Token 21021:
Which of these objections do you think ismore likely? Would it be a contradiction for someone to hold both positions?

Token 21022:
27AI: THE PRESENT AND FUTURE In which we take stock of where we are and where we are going, this being a good thing to do before continuing.

Token 21023:
In Chapter 2, we suggested that it would be helpful to view the AI task as that of designing rational agents—that is, agents whose actions maximize their expected utility given theirpercept histories.

Token 21024:
We showed that the design problem depends on the percepts and actionsavailable to the agent, the utility function that the agent’s behavior should satisfy, and thenature of the environment.

Token 21025:
A variety of different agent designs are possible, ranging fromreﬂex agents to fully deliberative, knowledge-based, decision-theoretic agents.

Token 21026:
Moreover,the components of these designs can have a number of different instantiations—for example, logical or probabilistic reasoning, and atomic, factored, or structured representations of states.

Token 21027:
The intervening chapters presented the principles by which these components operate.

Token 21028:
For all the agent designs and components, there has been tremendous progress both in our scientiﬁc understanding and in our technological capabilities.

Token 21029:
In this chapter, we stand back from the details and ask, “Will all this progress lead to a general-purpose intelligent agent that can perform well in a wide variety of environments?” Section 27.1 looks at the components of an intelligent agent to assess what’s known and what’s missing.

Token 21030:
Section 27.2does the same for the overall agent architecture. Section 27.3 asks whether designing rationalagents is the right goal in the ﬁrst place.

Token 21031:
(The answer is, “Not really, but it’s OK for now.”)Finally, Section 27.4 examines the consequences of success in our endeavors.

Token 21032:
27.1 A GENT COMPONENTS Chapter 2 presented several agent designs and their components.

Token 21033:
To focus our discussionhere, we will look at the utility-based agent, which we show again in Figure 27.1.

Token 21034:
When en-dowed with a learning component (Figure 2.15), this is the most general of our agent designs.

Token 21035:
Let’s see where the state of the art stands for each of the components.

Token 21036:
Interaction with the environment through sensors and actuators : For much of the history of AI, this has been a glaring weak point.

Token 21037:
With a few honorable exceptions, AI sys-tems were built in such a way that humans had to supply the inputs and interpret the outputs, 1044

Token 21038:
Section 27.1.

Token 21039:
Agent Components 1045 AgentEnvironmentSensors How happy I will be in such a stateState How the world evolves What my actions do Utility ActuatorsWhat action I should do nowWhat it will be like if I do action AWhat the world is like now Figure 27.1 A model-based, utility-based agent, as ﬁrst presented in Figure 2.14. while robotic systems focused on low-level tasks in which high-level reasoning and plan- ning were largely absent.

Token 21040:
This was due in part to the great expense and engineering effortrequired to get real robots to work at all.

Token 21041:
The situation has changed rapidly in recent yearswith the availability of ready-made programmable robots.

Token 21042:
These, in turn, have beneﬁtedfrom small, cheap, high-resolution CCD cameras and compact, reliable motor drives.

Token 21043:
MEMS(micro-electromechanical systems) technology has supplied miniaturized accelerometers, gy-roscopes, and actuators for an artiﬁcial ﬂying insect (Floreano et al.

Token 21044:
, 2009). It may also be possible to combine millions of MEMS devices to produce powerful macroscopic actuators.

Token 21045:
Thus, we see that AI systems are at the cusp of moving from primarily software-only systems to embedded robotic systems.

Token 21046:
The state of robotics today is roughly comparable tothe state of personal computers in about 1980: at that time researchers and hobbyists could experiment with PCs, but it would take another decade before they became commonplace.

Token 21047:
Keeping track of the state of the world : This is one of the core capabilities required for an intelligent agent.

Token 21048:
It requires both perception and updating of internal representations.Chapter 4 showed how to keep track of atomic state representations; Chapter 7 describedhow to do it for factored (propositional) state representations; Chapter 12 extended this toﬁrst-order logic; and Chapter 15 described ﬁltering algorithms for probabilistic reasoning in uncertain environments.

Token 21049:
Current ﬁltering and perception algorithms can be combined to do areasonable job of reporting low-level predicates such as “the cup is on the table.” Detectinghigher-level actions, such as “Dr.

Token 21050:
Russell is having a cup of tea with Dr. Norvig while dis-cussing plans for next week,” is more difﬁcult.

Token 21051:
Currently it can be done (see Figure 24.25 onpage 961) only with the help of annotated examples.

Token 21052:
Another problem is that, although the approximate ﬁltering algorithms from Chapter 15 can handle quite large environments, they are still dealing with a factored representation—they have random variables, but do not represent objects and relations explicitly.

Token 21053:
Section 14.6explained how probability and ﬁrst-order logic can be combined to solve this problem, and

Token 21054:
1046 Chapter 27. AI: The Present and Future Section 14.6.3 showed how we can handle uncertainty about the identity of objects.

Token 21055:
We expect that the application of these ideas for tracking complex environments will yield huge beneﬁts.However, we are still faced with a daunting task of deﬁning general, reusable representationschemes for complex domains.

Token 21056:
As discussed in Chapter 12, we don’t yet know how to do thatin general; only for isolated, simple domains.

Token 21057:
It is possible that a new focus on probabilistic rather than logical representation coupled with aggressive machine learning (rather than hand- encoding of knowledge) will allow for progress.

Token 21058:
Projecting, evaluating, and selecting future courses of action : The basic knowledge- representation requirements here are the same as for keeping track of the world; the primarydifﬁculty is coping with courses of action—such as having a conversation or a cup of tea—that consist eventually of thousands or millions of primitive steps for a real agent.

Token 21059:
It is onlyby imposing hierarchical structure on behavior that we humans cope at all.

Token 21060:
We saw in Section 11.2 how to use hierarchical representations to handle problems of this scale; fur-thermore, work in hierarchical reinforcement learning has succeeded in combining some of these ideas with the techniques for decision making under uncertainty described in Chap-ter 17.

Token 21061:
As yet, algorithms for the partially observable case (POMDPs) are using the sameatomic state representation we used for the search algorithms of Chapter 3.

Token 21062:
There is clearly a great deal of work to do here, but the technical foundations are largely in place.

Token 21063:
Section 27.2 discusses the question of how the search for effective long-range plans might be controlled.

Token 21064:
Utility as an expression of preferences : In principle, basing rational decisions on the maximization of expected utility is completely general and avoids many of the problems of purely goal-based approaches, such as conﬂicting goals and uncertain attainment.

Token 21065:
As yet, however, there has been very little work on constructing realistic utility functions—imagine, for example, the complex web of interacting preferences that must be understood by an agentoperating as an ofﬁce assistant for a human being.

Token 21066:
It has proven very difﬁcult to decomposepreferences over complex states in the same way that Bayes nets decompose beliefs overcomplex states.

Token 21067:
One reason may be that preferences over states are really compiled from preferences over state histories, which are described by reward functions (see Chapter 17).

Token 21068:
Even if the reward function is simple, the corresponding utility function may be very complex.

Token 21069:
This suggests that we take seriously the task of knowledge engineering for reward functionsas a way of conveying to our agents what it is that we want them to do.

Token 21070:
Learning : Chapters 18 to 21 described how learning in an agent can be formulated as inductive learning (supervised, unsupervised, or reinforcement-based) of the functions thatconstitute the various components of the agent.

Token 21071:
Very powerful logical and statistical tech-niques have been developed that can cope with quite large problems, reaching or exceedinghuman capabilities in many tasks—as long as we are dealing with a predeﬁned vocabularyof features and concepts.

Token 21072:
On the other hand, machine learning has made very little progresson the important problem of constructing new representations at levels of abstraction higherthan the input vocabulary.

Token 21073:
In computer vision, for example, learning complex concepts such asClassroom andCafeteria would be made unnecessarily difﬁcult if the agent were forced to work from pixels as the input representation; instead, the agent needs to be able to formintermediate concepts ﬁrst, such as Desk andTray , without explicit human supervision.

Token 21074:
Similar considerations apply to learning behavior: HavingACupOfTea is a very important

Token 21075:
Section 27.2.

Token 21076:
Agent Architectures 1047 high-level step in many plans, but how does it get into an action library that initially contains much simpler actions such as RaiseArm andSwallow ?

Token 21077:
Perhaps this will incorporate some of the ideas of deep belief networks —Bayesian networks that have multiple layers of hiddenDEEP BELIEF NETWORKS variables, as in the work of Hinton et al.

Token 21078:
(2006), Hawkins and Blakeslee (2004), and Bengio and LeCun (2007).

Token 21079:
The vast majority of machine learning research today assumes a factored representa- tion, learning a function h:Rn→Rfor regression and h:Rn→{0,1}for classiﬁcation.

Token 21080:
Learning researchers will need to adapt their very successful techniques for factored repre-sentations to structured representations, particularly hierarchical representations.

Token 21081:
The workon inductive logic programming in Chapter 19 is a ﬁrst step in this direction; the logical nextstep is to combine these ideas with the probabilistic languages of Section 14.6.

Token 21082:
Unless we understand such issues, we are faced with the daunting task of constructing large commonsense knowledge bases by hand, an approach that has not fared well to date.There is great promise in using the Web as a source of natural language text, images, andvideos to serve as a comprehensive knowledge base, but so far machine learning algorithmsare limited in the amount of organized knowledge they can extract from these sources.

Token 21083:
27.2 A GENT ARCHITECTURES It is natural to ask, “Which of the agent architectures in Chapter 2 should an agent use?”The answer is, “All of them!” We have seen that reﬂex responses are needed for situationsin which time is of the essence, whereas knowledge-based deliberation allows the agent toplan ahead.

Token 21084:
A complete agent must be able to do both, using a hybrid architecture .O n e HYBRID ARCHITECTURE important property of hybrid architectures is that the boundaries between different decision components are not ﬁxed.

Token 21085:
For example, compilation continually converts declarative in- formation at the deliberative level into more efﬁcient representations, eventually reaching the reﬂex level—see Figure 27.2.

Token 21086:
(This is the purpose of explanation-based learning, as discussed in Chapter 19.) Agent architectures such as S OAR (Laird et al.

Token 21087:
, 1987) and T HEO (Mitchell, 1990) have exactly this structure.

Token 21088:
Every time they solve a problem by explicit deliberation,they save away a generalized version of the solution for use by the reﬂex component.

Token 21089:
Aless studied problem is the reversal of this process: when the environment changes, learned reﬂexes may no longer be appropriate and the agent must return to the deliberative level toproduce new behaviors.

Token 21090:
Agents also need ways to control their own deliberations.

Token 21091:
They must be able to cease deliberating when action is demanded, and they must be able to use the time available for deliberation to execute the most proﬁtable computations.

Token 21092:
For example, a taxi-driving agent that sees an accident ahead must decide in a split second either to brake or to take evasive action.

Token 21093:
It should also spend that split second thinking about the most important questions, such as whether the lanes to the left and right are clear and whether there is a large truck close behind, rather than worrying about wear and tear on the tires or where to pick up the next passenger.

Token 21094:
These issues are usually studied under the heading of real-time AI .A sA I REAL-TIME AI

Token 21095:
1048 Chapter 27.

Token 21096:
AI: The Present and Future PerceptsCompilatio nKnowledge-based deliberation Reflex system Actions Figure 27.2 Compilation serves to convert deliberative decision making into more efﬁ- cient, reﬂexive mechanisms.

Token 21097:
systems move into more complex domains, all problems will become real-time, because the agent will never have long enough to solve the decision problem exactly.

Token 21098:
Clearly, there is a pressing need for general methods of controlling deliberation, rather than speciﬁc recipes for what to think about in each situation.

Token 21099:
The ﬁrst useful idea is to em-ploy anytime algorithms (Dean and Boddy, 1988; Horvitz, 1987).

Token 21100:
An anytime algorithm is ANYTIME ALGORITHM an algorithm whose output quality improves gradually over time, so that it has a reasonable decision ready whenever it is interrupted.

Token 21101:
Such algorithms are controlled by a metalevel de- cision procedure that assesses whether further computation is worthwhile.

Token 21102:
(See Section 3.5.4for a brief description of metalevel decision making.)

Token 21103:
Example of an anytime algorithmsinclude iterative deepening in game-tree search and MCMC in Bayesian networks.

Token 21104:
The second technique for controlling deliberation is decision-theoretic metareasoning DECISION- THEORETICMETAREASONING (Russell and Wefald, 1989, 1991; Horvitz, 1989; Horvitz and Breese, 1996).

Token 21105:
This method applies the theory of information value (Chapter 16) to the selection of individual computa-tions.

Token 21106:
The value of a computation depends on both its cost (in terms of delaying action) andits beneﬁts (in terms of improved decision quality).

Token 21107:
Metareasoning techniques can be used todesign better search algorithms and to guarantee that the algorithms have the anytime prop- erty.

Token 21108:
Metareasoning is expensive, of course, and compilation methods can be applied so that the overhead is small compared to the costs of the computations being controlled.

Token 21109:
Metalevelreinforcement learning may provide another way to acquire effective policies for controllingdeliberation: in essence, computations that lead to better decisions are reinforced, while thosethat turn out to have no effect are penalized.

Token 21110:
This approach avoids the myopia problems ofthe simple value-of-information calculation.

Token 21111:
Metareasoning is one speciﬁc example of a reﬂective architecture —that is, an archi- REFLECTIVE ARCHITECTURE tecture that enables deliberation about the computational entities and actions occurring within the architecture itself.

Token 21112:
A theoretical foundation for reﬂective architectures can be built bydeﬁning a joint state space composed from the environment state and the computational stateof the agent itself.

Token 21113:
Decision-making and learning algorithms can be designed that operate over this joint state space and thereby serve to implement and improve the agent’s compu- tational activities.

Token 21114:
Eventually, we expect task-speciﬁc algorithms such as alpha–beta searchand backward chaining to disappear from AI systems, to be replaced by general methods thatdirect the agent’s computations toward the efﬁcient generation of high-quality decisions.

Token 21115:
Section 27.3. Are We Going in the Right Direction? 1049 27.3 A REWEGOING IN THE RIGHT DIRECTION ?

Token 21116:
The preceding section listed many advances and many opportunities for further progress. But where is this all leading?

Token 21117:
Dreyfus (1992) gives the analogy of trying to get to the moon byclimbing a tree; one can report steady progress, all the way to the top of the tree.

Token 21118:
In thissection, we consider whether AI’s current path is more like a tree climb or a rocket trip.

Token 21119:
In Chapter 1, we said that our goal was to build agents that act rationally .H o w e v e r ,w e also said that ...achieving perfect rationality—always doing the right thing—is not feasible in compli- cated environments.

Token 21120:
The computational demands are just too high.

Token 21121:
For most of the book, however, we will adopt the working hypothesis that perfect rationality is a good starting point for analysis.

Token 21122:
Now it is time to consider again what exactly the goal of AI is. We want to build agents, but with what speciﬁcation in mind?

Token 21123:
Here are four possibilities: Perfect rationality .

Token 21124:
A perfectly rational agent acts at every instant in such a way as toPERFECT RATIONALITY maximize its expected utility, given the information it has acquired from the environment.

Token 21125:
We have seen that the calculations necessary to achieve perfect rationality in most environmentsare too time consuming, so perfect rationality is not a realistic goal.

Token 21126:
Calculative rationality .

Token 21127:
This is the notion of rationality that we have used implicitly in de- CALCULATIVE RATIONALITY signing logical and decision-theoretic agents, and most of theoretical AI research has focused on this property.

Token 21128:
A calculatively rational agent eventually returns what would have been the rational choice at the beginning of its deliberation.

Token 21129:
This is an interesting property for a systemto exhibit, but in most environments, the right answer at the wrong time is of no value.

Token 21130:
Inpractice, AI system designers are forced to compromise on decision quality to obtain reason-able overall performance; unfortunately, the theoretical basis of calculative rationality doesnot provide a well-founded way to make such compromises.

Token 21131:
Bounded rationality .

Token 21132:
Herbert Simon (1957) rejected the notion of perfect (or even approx- BOUNDED RATIONALITY imately perfect) rationality and replaced it with bounded rationality, a descriptive theory of decision making by real agents.

Token 21133:
He wrote, The capacity of the human mind for formulating and solving complex problems is very small compared with the size of the problems whose solution is required for objectivelyrational behavior in the real world—or even for a reasonable approximation to such ob- jective rationality.

Token 21134:
He suggested that bounded rationality works primarily by satisﬁcing —that is, deliberating only long enough to come up with an answer that is “good enough.” Simon won the NobelPrize in economics for this work and has written about it in depth (Simon, 1982).

Token 21135:
It appears to be a useful model of human behaviors in many cases.

Token 21136:
It is not a formal speciﬁcation for intelligent agents, however, because the deﬁnition of “good enough” is not given by thetheory.

Token 21137:
Furthermore, satisﬁcing seems to be just one of a large range of methods used to copewith bounded resources.

Token 21138:
1050 Chapter 27. AI: The Present and Future Bounded optimality (BO).

Token 21139:
A bounded optimal agent behaves as well as possible, given itsBOUNDED OPTIMALITY computational resources .

Token 21140:
That is, the expected utility of the agent program for a bounded optimal agent is at least as high as the expected utility of any other agent program running onthe same machine.

Token 21141:
Of these four possibilities, bounded optimality seems to offer the best hope for a strong theoretical foundation for AI.

Token 21142:
It has the advantage of being possible to achieve: there is always at least one best program—something that perfect rationality lacks.

Token 21143:
Bounded optimal agentsare actually useful in the real world, whereas calculatively rational agents usually are not, andsatisﬁcing agents might or might not be, depending on how ambitious they are.

Token 21144:
The traditional approach in AI has been to start with calculative rationality and then make compromises to meet resource constraints.

Token 21145:
If the problems imposed by the constraintsare minor, one would expect the ﬁnal design to be similar to a BO agent design.

Token 21146:
But as theresource constraints become more critical—for example, as the environment becomes morecomplex—one would expect the two designs to diverge.

Token 21147:
In the theory of bounded optimality,these constraints can be handled in a principled fashion. As yet, little is known about bounded optimality.

Token 21148:
It is possible to construct bounded optimal programs for very simple machines and for somewhat restricted kinds of environ- ments (Etzioni, 1989; Russell et al.

Token 21149:
, 1993), but as yet we have no idea what BO programs are like for large, general-purpose computers in complex environments.

Token 21150:
If there is to be a constructive theory of bounded optimality, we have to hope that the design of bounded op- timal programs does not depend too strongly on the details of the computer being used.

Token 21151:
It would make scientiﬁc research very difﬁcult if adding a few kilobytes of memory to a giga- byte machine made a signiﬁcant difference to the design of the BO program.

Token 21152:
One way tomake sure this cannot happen is to be slightly more relaxed about the criteria for boundedoptimality.

Token 21153:
By analogy with the notion of asymptotic complexity (Appendix A), we can de-ﬁneasymptotic bounded optimality (ABO) as follows (Russell and Subramanian, 1995).

Token 21154:
ASYMPTOTIC BOUNDEDOPTIMALITY Suppose a program Pis bounded optimal for a machine Min a class of environments E, where the complexity of environments in Eis unbounded.

Token 21155:
Then program P/primeis ABO for M inEif it can outperform Pby running on a machine kM that is ktimes faster (or larger) thanM.

Token 21156:
Unless kwere enormous, we would be happy with a program that was ABO for a nontrivial environment on a nontrivial architecture.

Token 21157:
There would be little point in puttingenormous effort into ﬁnding BO rather than ABO programs, because the size and speed ofavailable machines tends to increase by a constant factor in a ﬁxed amount of time anyway.

Token 21158:
We can hazard a guess that BO or ABO programs for powerful computers in complex environments will not necessarily have a simple, elegant structure.

Token 21159:
We have already seen thatgeneral-purpose intelligence requires some reﬂex capability and some deliberative capability;a variety of forms of knowledge and decision making; learning and compilation mechanismsfor all of those forms; methods for controlling reasoning; and a large store of domain-speciﬁc knowledge.

Token 21160:
A bounded optimal agent must adapt to the environment in which it ﬁnds itself, so that eventually its internal organization will reﬂect optimizations that are speciﬁc to theparticular environment.

Token 21161:
This is only to be expected, and it is similar to the way in whichracing cars restricted by engine capacity have evolved into extremely complex designs.

Token 21162:
We

Token 21163:
Section 27.4. What If AI Does Succeed?

Token 21164:
1051 suspect that a science of artiﬁcial intelligence based on bounded optimality will involve a good deal of study of the processes that allow an agent program to converge to boundedoptimality and perhaps less concentration on the details of the messy programs that result.

Token 21165:
In sum, the concept of bounded optimality is proposed as a formal task for AI research that is both well deﬁned and feasible.

Token 21166:
Bounded optimality speciﬁes optimal programs rather than optimal actions .

Token 21167:
Actions are, after all, generated by programs, and it is over programs that designers have control. 27.4 W HAT IFAI D OES SUCCEED ?

Token 21168:
In David Lodge’s Small World (1984), a novel about the academic world of literary criticism, the protagonist causes consternation by asking a panel of eminent but contradictory literarytheorists the following question: “ What if you were right?

Token 21169:
” None of the theorists seems to have considered this question before, perhaps because debating unfalsiﬁable theories is an endin itself.

Token 21170:
Similar confusion can be evoked by asking AI researchers, “ What if you succeed? ” As Section 26.3 relates, there are ethical issues to consider.

Token 21171:
Intelligent computers are more powerful than dumb ones, but will that power be used for good or ill?

Token 21172:
Those who striveto develop AI have a responsibility to see that the impact of their work is a positive one.

Token 21173:
Thescope of the impact will depend on the degree of success of AI.

Token 21174:
Even modest successes in AIhave already changed the ways in which computer science is taught (Stein, 2002) and softwaredevelopment is practiced.

Token 21175:
AI has made possible new applications such as speech recognitionsystems, inventory control systems, surveillance systems, robots, and search engines.

Token 21176:
We can expect that medium-level successes in AI would affect all kinds of people in their daily lives.

Token 21177:
So far, computerized communication networks, such as cell phones and the Internet, have had this kind of pervasive effect on society, but AI has not.

Token 21178:
AI has been at work behind the scenes—for example, in automatically approving or denying credit card transac- tions for every purchase made on the Web—but has not been visible to the average consumer.

Token 21179:
We can imagine that truly useful personal assistants for the ofﬁce or the home would have a large positive impact on people’s lives, although they might cause some economic disloca-tion in the short term.

Token 21180:
Automated assistants for driving could prevent accidents, saving tensof thousands of lives per year.

Token 21181:
A technological capability at this level might also be appliedto the development of autonomous weapons, which many view as undesirable.

Token 21182:
Some of thebiggest societal problems we face today—such as the harnessing of genomic information fortreating disease, the efﬁcient management of energy resources, and the veriﬁcation of treatiesconcerning nuclear weapons—are being addressed with the help of AI technologies.

Token 21183:
Finally, it seems likely that a large-scale success in AI—the creation of human-level in- telligence and beyond—would change the lives of a majority of humankind.

Token 21184:
The very nature of our work and play would be altered, as would our view of intelligence, consciousness, and the future destiny of the human race.

Token 21185:
AI systems at this level of capability could threaten hu-man autonomy, freedom, and even survival.

Token 21186:
For these reasons, we cannot divorce AI researchfrom its ethical consequences (see Section 26.3).

Token 21187:
1052 Chapter 27. AI: The Present and Future Which way will the future go?

Token 21188:
Science ﬁction authors seem to favor dystopian futures over utopian ones, probably because they make for more interesting plots.

Token 21189:
But so far, AIseems to ﬁt in with other revolutionary technologies (printing, plumbing, air travel, telephony)whose negative repercussions are outweighed by their positive aspects.

Token 21190:
In conclusion, we see that AI has made great progress in its short history, but the ﬁnal sentence of Alan Turing’s (1950) essay on Computing Machinery and Intelligence is still valid today: We can see only a short distance ahead, but we can see that much remains to be done.

Token 21191:


Token 21192:
AMATHEMATICAL BACKGROUND A.1 C OMPLEXITY ANALYSIS AND O() N OTATION Computer scientists are often faced with the task of comparing algorithms to see how fast they run or how much memory they require.

Token 21193:
There are two approaches to this task.

Token 21194:
The ﬁrstisbenchmarking —running the algorithms on a computer and measuring speed in seconds BENCHMARKING and memory consumption in bytes.

Token 21195:
Ultimately, this is what really matters, but a benchmark can be unsatisfactory because it is so speciﬁc: it measures the performance of a particularprogram written in a particular language, running on a particular computer, with a particularcompiler and particular input data.

Token 21196:
From the single result that the benchmark provides, itcan be difﬁcult to predict how well the algorithm would do on a different compiler, com-puter, or data set.

Token 21197:
The second approach relies on a mathematical analysis of algorithms , ANALYSIS OF ALGORITHMS independently of the particular implementation and input, as discussed below.

Token 21198:
A.1.1 Asymptotic analysis We will consider algorithm analysis through the following example, a program to compute the sum of a sequence of numbers: function SUMMATION (sequence )returns a number sum←0 fori=1toLENGTH (sequence )do sum←sum +sequence [i] return sum The ﬁrst step in the analysis is to abstract over the input, in order to ﬁnd some parameter or parameters that characterize the size of the input.

Token 21199:
In this example, the input can be charac-terized by the length of the sequence, which we will call n. The second step is to abstract over the implementation, to ﬁnd some measure that reﬂects the running time of the algorithm but is not tied to a particular compiler or computer.

Token 21200:
For the S UMMATION program, this could be just the number of lines of code executed, or it could be more detailed, measuring thenumber of additions, assignments, array references, and branches executed by the algorithm.

Token 21201:
1053

Token 21202:
1054 Appendix A.

Token 21203:
Mathematical background Either way gives us a characterization of the total number of steps taken by the algorithm as a function of the size of the input.

Token 21204:
We will call this characterization T(n). If we count lines of code, we have T(n)=2n+2for our example.

Token 21205:
If all programs were as simple as S UMMATION , the analysis of algorithms would be a trivial ﬁeld. But two problems make it more complicated.

Token 21206:
First, it is rare to ﬁnd a parameter likenthat completely characterizes the number of steps taken by an algorithm.

Token 21207:
Instead, the best we can usually do is compute the worst case Tworst(n)or the average case Tavg(n).

Token 21208:
Computing an average means that the analyst must assume some distribution of inputs.

Token 21209:
The second problem is that algorithms tend to resist exact analysis. In that case, it is necessary to fall back on an approximation.

Token 21210:
We say that the S UMMATION algorithm is O(n), meaning that its measure is at most a constant times n, with the possible exception of a few small values of n. More formally, T(n)isO(f(n))ifT(n)≤kf(n)for some k,for all n>n 0.

Token 21211:
TheO()notation gives us what is called an asymptotic analysis .

Token 21212:
We can say without ques-ASYMPTOTIC ANALYSIS tion that, as nasymptotically approaches inﬁnity, an O(n)algorithm is better than an O(n2) algorithm.

Token 21213:
A single benchmark ﬁgure could not substantiate such a claim.

Token 21214:
TheO()notation abstracts over constant factors, which makes it easier to use, but less precise, than the T()notation.

Token 21215:
For example, an O(n2)algorithm will always be worse than anO(n)in the long run, but if the two algorithms are T(n2+1 ) andT(100n+ 1000) ,t h e n theO(n2)algorithm is actually better for n<110.

Token 21216:
Despite this drawback, asymptotic analysis is the most widely used tool for analyzing algorithms.

Token 21217:
It is precisely because the analysis abstracts over both the exact number of oper- ations (by ignoring the constant factor k) and the exact content of the input (by considering only its size n) that the analysis becomes mathematically feasible.

Token 21218:
The O()notation is a good compromise between precision and ease of analysis.

Token 21219:
A.1.2 NP and inherently hard problems The analysis of algorithms and the O()notation allow us to talk about the efﬁciency of a particular algorithm.

Token 21220:
However, they have nothing to say about whether there could be a better algorithm for the problem at hand.

Token 21221:
The ﬁeld of complexity analysis analyzes problems ratherCOMPLEXITY ANALYSIS than algorithms.

Token 21222:
The ﬁrst gross division is between problems that can be solved in polynomial time and problems that cannot be solved in polynomial time, no matter what algorithm isused.

Token 21223:
The class of polynomial problems—those which can be solved in time O(n k)for some k—is called P. These are sometimes called “easy” problems, because the class contains those problems with running times like O(logn)andO(n).

Token 21224:
But it also contains those with time O(n1000), so the name “easy” should not be taken too literally.

Token 21225:
Another important class of problems is NP, the class of nondeterministic polynomial problems.

Token 21226:
A problem is in this class if there is some algorithm that can guess a solution and then verify whether the guess is correct in polynomial time.

Token 21227:
The idea is that if you have an arbitrarily large number of processors, so that you can try all the guesses at once, or you arevery lucky and always guess right the ﬁrst time, then the NP problems become P problems.One of the biggest open questions in computer science is whether the class NP is equivalent

Token 21228:
Section A.2.

Token 21229:
Vectors, Matrices, and Linear Algebra 1055 to the class P when one does not have the luxury of an inﬁnite number of processors or omniscient guessing.

Token 21230:
Most computer scientists are convinced that P /negationslash=NP; that NP problems are inherently hard and have no polynomial-time algorithms.

Token 21231:
But this has never been proven. Those who are interested in deciding whether P = NP look at a subclass of NP called the NP-complete problems.

Token 21232:
The word “complete” is used here in the sense of “most extreme” NP-COMPLETE and thus refers to the hardest problems in the class NP.

Token 21233:
It has been proven that either all the NP-complete problems are in P or none of them is.

Token 21234:
This makes the class theoreticallyinteresting, but the class is also of practical interest because many important problems areknown to be NP-complete.

Token 21235:
An example is the satisﬁability problem: given a sentence ofpropositional logic, is there an assignment of truth values to the proposition symbols of thesentence that makes it true?

Token 21236:
Unless a miracle occurs and P = NP, there can be no algorithmthat solves allsatisﬁability problems in polynomial time.

Token 21237:
However, AI is more interested in whether there are algorithms that perform efﬁciently on typical problems drawn from a pre- determined distribution; as we saw in Chapter 7, there are algorithms such as W ALKSAT that do quite well on many problems.

Token 21238:
The class co-NP is the complement of NP, in the sense that, for every decision problem CO-NP in NP, there is a corresponding problem in co-NP with the “yes” and “no” answers reversed.

Token 21239:
We know that P is a subset of both NP and co-NP, and it is believed that there are problems in co-NP that are not in P. The co-NP-complete problems are the hardest problems in co-NP.

Token 21240:
CO-NP-COMPLETE The class #P (pronounced “sharp P”) is the set of counting problems corresponding to the decision problems in NP.

Token 21241:
Decision problems have a yes-or-no answer: is there a solutionto this 3-SAT formula?

Token 21242:
Counting problems have an integer answer: how many solutions arethere to this 3-SAT formula?

Token 21243:
In some cases, the counting problem is much harder than thedecision problem.

Token 21244:
For example, deciding whether a bipartite graph has a perfect matchingcan be done in time O(VE)(where the graph has Vvertices and Eedges), but the counting problem “how many perfect matches does this bipartite graph have” is #P-complete, meaning that it is hard as any problem in #P and thus at least as hard as any NP problem.

Token 21245:
Another class is the class of PSPACE problems—those that require a polynomial amount of space, even on a nondeterministic machine.

Token 21246:
It is believed that PSPACE-hard problems are worse than NP-complete problems, although it could turn out that NP = PSPACE, just as it could turn out that P = NP.

Token 21247:
A.2 V ECTORS ,MATRICES ,AND LINEAR ALGEBRA Mathematicians deﬁne a vector as a member of a vector space, but we will use a more con- VECTOR crete deﬁnition: a vector is an ordered sequence of values.

Token 21248:
For example, in two-dimensional space, we have vectors such as x=/angbracketleft3,4/angbracketrightandy=/angbracketleft0,2/angbracketright.

Token 21249:
We follow the convention of bold- face characters for vector names, although some authors use arrows or bars over the names: /vectorxor¯y.

Token 21250:
The elements of a vector can be accessed using subscripts: z=/angbracketleftz1,z2,...,z n/angbracketright.O n e confusing point: this book is synthesizing work from many subﬁelds, which variously calltheir sequences vectors, lists, or tuples, and variously use the notations /angbracketleft1,2/angbracketright, [1, 2], or (1, 2).

Token 21251:
1056 Appendix A. Mathematical background The two fundamental operations on vectors are vector addition and scalar multiplica- tion.

Token 21252:
The vector addition x+yis the elementwise sum: x+y=/angbracketleft3+0,4+2/angbracketright=/angbracketleft3,6/angbracketright.

Token 21253:
Scalar multiplication multiplies each element by a constant: 5x=/angbracketleft5×3,5×4/angbracketright=/angbracketleft15,20/angbracketright.

Token 21254:
The length of a vector is denoted |x|and is computed by taking the square root of the sum of the squares of the elements: |x|=/radicalbig (32+42)=5 .

Token 21255:
The dot product x·y(also called scalar product) of two vectors is the sum of the products of corresponding elements, that is, x·y=/summationtext ixiyi, or in our particular case, x·y=3×0+4×2=8 .

Token 21256:
Vectors are often interpreted as directed line segments (arrows) in an n-dimensional Euclidean space.

Token 21257:
Vector addition is then equivalent to placing the tail of one vector at the head of the other, and the dot product x·yis equal to|x||y|cosθ,w h e r e θis the angle between xandy.

Token 21258:
Amatrix is a rectangular array of values arranged into rows and columns.

Token 21259:
Here is a MATRIX matrix Aof size 3×4:⎛ ⎝A1,1A1,2A1,3A1,4 A2,1A2,2A2,3A2,4 A3,1A3,2A3,3A3,4⎞ ⎠ The ﬁrst index of Ai,jspeciﬁes the row and the second the column.

Token 21260:
In programming lan- guages, Ai,jis often written A[i,j] orA[i][j] .

Token 21261:
The sum of two matrices is deﬁned by adding their corresponding elements; for example (A+B)i,j=Ai,j+Bi,j.

Token 21262:
(The sum is undeﬁned if AandBhave different sizes.) We can also deﬁne the multiplication of a matrix by a scalar: (cA)i,j=cAi,j.

Token 21263:
Matrix multiplication (the product of two matrices) is more complicated.

Token 21264:
The product ABis deﬁned only if Ais of size a×bandBis of size b×c(i.e., the second matrix has the same number of rows as the ﬁrst has columns); the result is a matrix of size a×c.

Token 21265:
If the matrices are of appropriate size, then the result is (AB)i,k=/summationdisplay jAi,jBj,k.

Token 21266:
Matrix multiplication is not commutative, even for square matrices: AB/negationslash=BAin general. It is, however, associative: (AB)C=A(BC).

Token 21267:
Note that the dot product can be expressed in terms of a transpose and a matrix multiplication: x·y=x/latticetopy.

Token 21268:
Theidentity matrix I has elements Ii,jequal to 1 when i=jand equal to 0 otherwise.

Token 21269:
IDENTITY MATRIX It has the property that AI=Afor all A.T h e transpose ofA, written A/latticetopis formed by TRANSPOSE turning rows into columns and vice versa, or, more formally, by A/latticetopi,j=Aj,i.T h e inverse of INVERSE a square matrix Ais another square matrix A−1such that A−1A=I.F o r a singular matrix, SINGULAR the inverse does not exist.

Token 21270:
For a nonsingular matrix, it can be computed in O(n3)time.

Token 21271:
Matrices are used to solve systems of linear equations in O(n3)time; the time is domi- nated by inverting a matrix of coefﬁcients.

Token 21272:
Consider the following set of equations, for whichwe want a solution in x,y,a n dz: +2x+y−z=8 −3x−y+2z=−11 −2x+y+2z=−3.

Token 21273:
Section A.3.

Token 21274:
Probability Distributions 1057 We can represent this system as the matrix equation Ax=b,w h e r e A=⎛ ⎝21−1 −3−12 −212⎞ ⎠, x=⎛ ⎝x yz⎞ ⎠, b=⎛ ⎝8 −11 −3⎞ ⎠.

Token 21275:
To solve Ax=bwe multiply both sides by A −1,yielding A−1Ax=A−1b, which simpliﬁes tox=A−1b.

Token 21276:
After inverting Aand multiplying by b,w eg e tt h ea n s w e r x=⎛ ⎝x yz⎞ ⎠=⎛ ⎝2 3 −1⎞ ⎠.

Token 21277:
A.3 P ROBABILITY DISTRIBUTIONS A probability is a measure over a set of events that satisﬁes three axioms: 1.

Token 21278:
The measure of each event is between 0 and 1.

Token 21279:
We write this as 0≤P(X=xi)≤1, where Xis a random variable representing an event and xiare the possible values of X.

Token 21280:
In general, random variables are denoted by uppercase letters and their values by lowercase letters. 2.

Token 21281:
The measure of the whole set is 1; that is,/summationtextn i=1P(X=xi)=1 . 3.

Token 21282:
The probability of a union of disjoint events is the sum of the probabilities of the indi- vidual events; that is, P(X=x1∨X=x2)=P(X=x1)+P(X=x2),w h e r e x1and x2are disjoint.

Token 21283:
Aprobabilistic model consists of a sample space of mutually exclusive possible outcomes, together with a probability measure for each outcome.

Token 21284:
For example, in a model of the weathertomorrow, the outcomes might be sunny, cloudy, rainy ,a n d snowy .

Token 21285:
A subset of these out- comes constitutes an event. For example, the event of precipitation is the subset consisting of{rainy, snowy}.

Token 21286:
We use P(X)to denote the vector of values /angbracketleftP(X=x 1),...,P (X=xn)/angbracketright.W e a l s o useP(xi)as an abbreviation for P(X=xi)and/summationtext xP(x)for/summationtextn i=1P(X=xi).

Token 21287:
The conditional probability P(B|A)is deﬁned as P(B∩A)/P(A).AandBare condi- tionally independent if P(B|A)=P(B)(or equivalently, P(A|B)=P(A)).

Token 21288:
For continuous variables, there are an inﬁnite number of values, and unless there are point spikes, the proba- bility of any one value is 0.

Token 21289:
Therefore, we deﬁne a probability density function ,w h i c hw ePROBABILITY DENSITY FUNCTION also denote as P(·), but which has a slightly different meaning from the discrete probability function.

Token 21290:
The density function P(x)for a random variable X, which might be thought of as P(X=x), is intuitively deﬁned as the ratio of the probability that Xfalls into an interval around x, divided by the width of the interval, as the interval width goes to zero: P(x)= lim dx→0P(x≤X≤x+dx)/dx .

Token 21291:
1058 Appendix A. Mathematical background The density function must be nonnegative for all xand must have/integraldisplay∞ −∞P(x)dx=1.

Token 21292:
We can also deﬁne a cumulative probability density function FX(x), which is the proba-CUMULATIVE PROBABILITYDENSITY FUNCTION bility of a random variable being less than x: FX(x)=P(X≤x)=/integraldisplayx −∞P(u)du .

Token 21293:
Note that the probability density function has units, whereas the discrete probability function is unitless.

Token 21294:
For example, if values of Xare measured in seconds, then the density is measured in Hz (i.e., 1/sec).

Token 21295:
If values of Xare points in three-dimensional space measured in meters, then density is measured in 1/m3.

Token 21296:
One of the most important probability distributions is the Gaussian distribution ,a l s oGAUSSIAN DISTRIBUTION known as the normal distribution .

Token 21297:
A Gaussian distribution with mean μand standard devi- ationσ(and therefore variance σ2)i sd e ﬁ n e da s P(x)=1 σ√ 2πe−(x−μ)2/(2σ2), where xis a continuous variable ranging from −∞ to+∞.

Token 21298:
With mean μ=0and variance σ2=1, we get the special case of the standard normal distribution .

Token 21299:
For a distribution overSTANDARD NORMAL DISTRIBUTION a vector xinndimensions, there is the multivariate Gaussian distribution:MULTIVARIATE GAUSSIAN P(x)=1 /radicalbig (2π)n|Σ|e−1 2“ (x−μ)/latticetopΣ−1(x−μ)” , where μis the mean vector and Σis the covariance matrix (see below).

Token 21300:
In one dimension, we can deﬁne the cumulative distribution function F(x)as theCUMULATIVE DISTRIBUTION probability that a random variable will be less than x.

Token 21301:
For the normal distribution, this is F(x)=x/integraldisplay −∞P(z)dz=1 2(1 + erf(z−μ σ√ 2)), where erf (x)is the so-called error function , which has no closed-form representation.

Token 21302:
The central limit theorem states that the distribution formed by sampling nindepen-CENTRAL LIMIT THEOREM dent random variables and taking their mean tends to a normal distribution as ntends to inﬁnity.

Token 21303:
This holds for almost any collection of random variables, even if they are not strictlyindependent, unless the variance of any ﬁnite subset of variables dominates the others.

Token 21304:
The expectation of a random variable, E(X), is the mean or average value, weighted EXPECTATION by the probability of each value.

Token 21305:
For a discrete variable it is: E(X)=/summationdisplay ixiP(X=xi).

Token 21306:
For a continuous variable, replace the summation with an integral over the probability density function, P(x): E(X)=∞/integraldisplay −∞xP(x)dx ,

Token 21307:


Token 21308:
Bibliographical and Historical Notes 1059 Theroot mean square , RMS, of a set of values (often samples of a random variable) is ROOT MEAN SQUARE the square root of the mean of the squares of the values, RMS(x1,...,x n)=/radicalbigg x2 1+...+x2n n. Thecovariance of two random variables is the expectation of the product of their differences COVARIANCE from their means: cov(X,Y)=E((X−μX)(Y−μY)).

Token 21309:
The covariance matrix , often denoted Σ, is a matrix of covariances between elements of a COVARIANCE MATRIX vector of random variables.

Token 21310:
Given X=/angbracketleftX1,...X n/angbracketright/latticetop, the entries of the covariance matrix are as follows: Σi,j=cov(Xi,Xj)=E((Xi−μi)(Xj−μj)).

Token 21311:
A few more miscellaneous points: we use log(x)for the natural logarithm, loge(x).W eu s e argmaxxf(x)for the value of xfor which f(x)is maximal.

Token 21312:
BIBLIOGRAPHICAL AND HISTORICAL NOTES TheO()notation so widely used in computer science today was ﬁrst introduced in the context of number theory by the German mathematician P. G. H. Bachmann (1894).

Token 21313:
The concept ofNP-completeness was invented by Cook (1971), and the modern method for establishing areduction from one problem to another is due to Karp (1972).

Token 21314:
Cook and Karp have both wonthe Turing award, the highest honor in computer science, for their work.

Token 21315:
Classic works on the analysis and design of algorithms include those by Knuth (1973) and Aho, Hopcroft, and Ullman (1974); more recent contributions are by Tarjan (1983) andCormen, Leiserson, and Rivest (1990).

Token 21316:
These books place an emphasis on designing andanalyzing algorithms to solve tractable problems.

Token 21317:
For the theory of NP-completeness and other forms of intractability, see Garey and Johnson (1979) or Papadimitriou (1994).

Token 21318:
Good texts on probability include Chung (1979), Ross (1988), and Bertsekas and Tsitsiklis (2008).

Token 21319:


Token 21320:
BNOTES ON LANGUAGES AND ALGORITHMS B.1 D EFINING LANGUAGES WITH BACKUS –NAUR FORM (BNF) In this book, we deﬁne several languages, including the languages of propositional logic (page 243), ﬁrst-order logic (page 293), and a subset of English (page 899).

Token 21321:
A formal lan-guage is deﬁned as a set of strings where each string is a sequence of symbols.

Token 21322:
The languages we are interested in consist of an inﬁnite set of strings, so we need a concise way to charac- terize the set.

Token 21323:
We do that with a grammar .

Token 21324:
The particular type of grammar we use is called a context-free grammar , because each expression has the same form in any context.

Token 21325:
We write CONTEXT-FREE GRAMMAR our grammars in a formalism called Backus–Naur form (BNF) .

Token 21326:
There are four componentsBACKUS–NAUR FORM (BNF) to a BNF grammar: •A set of terminal symbols .

Token 21327:
These are the symbols or words that make up the strings of TERMINAL SYMBOL the language.

Token 21328:
They could be letters ( A, B, C, ...) or words ( a, aardvark, abacus, ...), or whatever symbols are appropriate for the domain.

Token 21329:
•A set of nonterminal symbols that categorize subphrases of the language.

Token 21330:
For exam-NONTERMINAL SYMBOL ple, the nonterminal symbol NounPhrase in English denotes an inﬁnite set of strings including “you” and “the big slobbery dog.” •Astart symbol , which is the nonterminal symbol that denotes the complete set of START SYMBOL strings of the language.

Token 21331:
In English, this is Sentence ; for arithmetic, it might be Expr , and for programming languages it is Program .

Token 21332:
•A set of rewrite rules ,o ft h ef o r m LHS→RHS ,w h e r e LHS is a nonterminal symbol and RHS is a sequence of zero or more symbols.

Token 21333:
These can be either terminal or nonterminal symbols, or the symbol /epsilon1, which is used to denote the empty string.

Token 21334:
A rewrite rule of the form Sentence→NounPhrase VerbPhrase means that whenever we have two strings categorized as a NounPhrase and aVerbPhrase , we can append them together and categorize the result as a Sentence .

Token 21335:
As an abbreviation, the two rules ( S→A)a n d(S→B) can be written ( S→A|B). 1060

Token 21336:
Section B.2.

Token 21337:
Describing Algorithms with Pseudocode 1061 Here is a BNF grammar for simple arithmetic expressions: Expr→Expr Operator Expr |(Expr )|Number Number→Digit|Number Digit Digit→0|1|2|3|4|5|6|7|8|9 Operator→+|−|÷|× We cover languages and grammars in more detail in Chapter 22.

Token 21338:
Be aware that other books use slightly different notations for BNF; for example, you might see /angbracketleftDigit/angbracketrightinstead of Digit for a nonterminal, ‘word’ instead of word for a terminal, or ::= instead of→in a rule.

Token 21339:
B.2 D ESCRIBING ALGORITHMS WITH PSEUDOCODE The algorithms in this book are described in pseudocode.

Token 21340:
Most of the pseudocode should be familiar to users of languages like Java, C++, or Lisp.

Token 21341:
In some places we use mathematicalformulas or ordinary English to describe parts that would otherwise be more cumbersome.

Token 21342:
Afew idiosyncrasies should be noted.

Token 21343:
•Persistent variables :W eu s et h ek e y w o r d persistent to say that a variable is given an initial value the ﬁrst time a function is called and retains that value (or the value given toit by a subsequent assignment statement) on all subsequent calls to the function.

Token 21344:
Thus,persistent variables are like global variables in that they outlive a single call to theirfunction, but they are accessible only within the function.

Token 21345:
The agent programs in thebook use persistent variables for memory.

Token 21346:
Programs with persistent variables can be implemented as objects in object-oriented languages such as C++, Java, Python, and Smalltalk.

Token 21347:
In functional languages, they can be implemented by functional closures over an environment containing the required variables.

Token 21348:
•Functions as values : Functions and procedures have capitalized names, and variables have lowercase italic names.

Token 21349:
So most of the time, a function call looks like F N(x).

Token 21350:
However, we allow the value of a variable to be a function; for example, if the value of the variable fis the square root function, then f(9) returns 3.

Token 21351:
•for each : The notation “ for each xincdo” means that the loop is executed with the variable xbound to successive elements of the collection c. •Indentation is signiﬁcant : Indentation is used to mark the scope of a loop or condi- tional, as in the language Python, and unlike Java and C++ (which use braces) or Pascaland Visual Basic (which use end).

Token 21352:
•Destructuring assignment : The notation “ x,y←pair” means that the right-hand side must evaluate to a two-element tuple, and the ﬁrst element is assigned to xand the second to y.

Token 21353:
The same idea is used in “ for each x,yinpairs do” and can be used to swap two variables: “ x,y←y,x” •Generators andyield : the notation “ generator G(x)yields numbers” deﬁnes G as a generator function.

Token 21354:
This is best understood by an example. The code fragment shown in

Token 21355:
1062 Appendix B.

Token 21356:
Notes on Languages and Algorithms generator POWERS -OF-2()yields ints i←1 whiletrue do yieldi i←2×i forpinPOWERS -OF-2() do PRINT (p) Figure B.1 Example of a generator function and its invocation within a loop.

Token 21357:
Figure B.1 prints the numbers 1, 2, 4, . . . , and never stops.

Token 21358:
The call to P OWERS -OF-2 returns a generator, which in turn yields one value each time the loop code asks for the next element of the collection.

Token 21359:
Even though the collection is inﬁnite, it is enumerated one element at a time. •Lists :[x,y,z]denotes a list of three elements.

Token 21360:
[ﬁrst|rest]denotes a list formed by adding ﬁrst to the list rest. In Lisp, this is the cons function. •Sets:{x,y,z}denotes a set of three elements.

Token 21361:
{x:p(x)}denotes the set of all elements xfor which p(x)is true.

Token 21362:
•Arrays start at 1 : Unless stated otherwise, the ﬁrst index of an array is 1 as in usual mathematical notation, not 0, as in Java and C. B.3 O NLINE HELP Most of the algorithms in the book have been implemented in Java, Lisp, and Python at ouronline code repository: aima.cs.berkeley.edu The same Web site includes instructions for sending comments, corrections, or suggestions for improving the book, and for joining discussion lists.

Token 21363:


Token 21364:
Bibliography The following abbreviations are used for frequently cited conferences and journals: AAAI Proceedings of the AAAI Confer ence on Artiﬁcial Intelligence AAMAS Proceedings of the International Conference on Autonomous Agents and Multi-agent Systems ACL Proceedings of the Annual Meeting of the Asso ciation for Computational Linguistics AIJ Artiﬁcial Intelligence AIMag AI Magazine AIPS Proceedings of the International Conference on AI Planning Systems BBS Behavioral and Brain Sciences CACM Communications of the Association for Computing Machinery COGSCI Proceedings of the Annual Conference of the Cognitive Science Society COLING Proceedings of the International Confer ence on Computational Linguistics COLT Proceedings of the Annual ACM Workshop on Computational Learning Theory CP Proceedings of the International Conference on Principles and Practice of Constraint Programming CVPR Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition EC Proceedings of the ACM Conference on Electronic Commerce ECAI Proceedings of the European Conference on Artiﬁcial Intelligence ECCV Proceedings of the European Conference on Computer Vision ECML Proceedings of the The European Conference on Machine Learning ECP Proceedings of the European Conference on Planning FGCS Proceedings of the International Conference on Fifth Generation Computer Systems FOCS Proceedings of the Annual Symposium on Foundations of Computer Science ICAPS Proceedings of the International Conference on Automated Planning and Scheduling ICASSP Proceedings of the International Conference on Acoustics, Speech, and Signal Processing ICCV Proceedings of the International Conference on Computer Vision ICLP Proceedings of the International Conference on Logic Programming ICML Proceedings of the International Conference on Machine Learning ICPR Proceedings of the International Conference on Pattern Recognition ICRA Proceedings of the IEEE International Conference on Robotics and Automation ICSLP Proceedings of the International Conference on Speech and Language Processing IJAR International Journal of Approximate Reasoning IJCAI Proceedings of the International Joint Conference on Artiﬁcial Intelligence IJCNN Proceedings of the International Joint Conference on Neural Networks IJCV International Journal of Computer Vision ILP Proceedings of the International Workshop on Inductive Logic Programming ISMIS Proceedings of the International Symposiu m on Methodologies for Intelligent Systems ISRR Proceedings of the International Symposium on Robotics Research JACM Journal of the Association for Computing Machinery JAIR Journal of Artiﬁcial Intelligence Research JAR Journal of Automated Reasoning JASA Journal of the American Statistical Association JMLR Journal of Machine Learning Research JSL Journal of Symbolic Logic KDD Proceedings of the International Conference on Knowledge Discovery and Data Mining KR Proceedings of the International Conference on Prin ciples of Knowledge Representation and Reasoning LICS Proceedings of the IEEE Symposium on Logic in Computer Science NIPS Advances in Neural Information Processing Systems PAMI IEEE Transactions on Pattern Analysis and Machine Intelligence PNAS Proceedings of the National Academy of Sciences of the United States of America PODS Proceedings of the ACM International Symposium on Principles of Database Systems SIGIR Proceedings of the Special Interest Group on Information Retrieval SIGMOD Proceedings of the ACM SIGMOD International Conference on Management of Data SODA Proceedings of the Annual ACM–SIAM Symposium on Discrete Algorithms STOC Proceedings of the Annual ACM Symposium on Theory of Computing TARK Proceedings of the Conference on Theoretical Aspects of Reasoning about Knowledge UAI Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence 1063

Token 21365:
1064 Bibliography Aarup , M., Arentoft, M. M., Parrod, Y., Stader, J., and Stokes, I. (1994).

Token 21366:
OPTIMUM-AIV: A knowledge-based planning and scheduling systemfor spacecraft AIV. In Fox, M. and Zweben, M. (Eds. ), Knowledge Based Scheduling .

Token 21367:
Morgan Kauf- mann. Abney , S. (2007). Semisupervised Learning for Computational Linguistics . CRC Press. Abramson , B. and Yung, M. (1989).

Token 21368:
Divide and conquer under global constraints: A solution to the N-queens problem. J. Parallel and Distributed Com- puting ,6(3), 649–662.

Token 21369:
Achlioptas , D. (2009). Random satisﬁability. In Biere, A., Heule, M., van Maaren, H., and Walsh, T. (Eds. ), Handbook of Satisﬁability . IOS Press.

Token 21370:
Achlioptas , D., Beame, P., and Molloy, M. (2004). Exponential bounds for DPLL below the satisﬁabil-ity threshold. In SODA-04 .

Token 21371:
Achlioptas , D., Naor, A., and Peres, Y. (2007). On the maximum satisﬁability of random formulas.JACM ,54(2). Achlioptas , D. and Peres, Y. (2004).

Token 21372:
The threshold for random k-SAT is 2klog 2 −o(k).J. American Mathematical Society ,17(4), 947–973. Ackley , D. H. and Littman, M. L. (1991).

Token 21373:
Inter- actions between learning and evolution. In Lang- ton, C., Taylor, C., Farmer, J. D., and Ramussen, S. (Eds. ), Artiﬁcial Life II , pp. 487–509.

Token 21374:
Addison- Wesley. Adelson-Velsky , G. M., Arlazarov, V. L., Bitman, A. R., Zhivotovsky, A. A., and Uskov, A. V. (1970).

Token 21375:
Programming a computer to play chess. Russian Mathematical Surveys ,25, 221–262. Adida , B. and Birbeck, M. (2008). RDFa primer. Tech. rep., W3C.

Token 21376:
Agerbeck , C. and Hansen, M. O. (2008). A multi- agent approach to solving NP-complete problems. Master’s thesis, Technical Univ. of Denmark.

Token 21377:
Aggarwal , G., Goel, A., and Motwani, R. (2006). Truthful auctions for pricing search keywords. In EC-06 , pp. 1–7.

Token 21378:
Agichtein , E. and Gravano, L. (2003). Querying text databases for efﬁcient information extraction. In Proc. IEEE Conference on Data Engineering .

Token 21379:
Agmon , S. (1954). The relaxation method for lin- ear inequalities. Canadian Journal of Mathematics , 6 (3), 382–392.

Token 21380:
Agre , P. E. and Chapman, D. (1987). Pengi: an im- plementation of a theory of activity. In IJCAI-87 , pp. 268–272.

Token 21381:
Aho, A. V., Hopcroft, J., and Ullman, J. D. (1974). The Design and Analysis of Computer Algorithms . Addison-Wesley.

Token 21382:
Aizerman , M., Braverman, E., and Rozonoer, L. (1964). Theoretical foundations of the potential function method in pattern recognition learning.

Token 21383:
Au- tomation and Remote Control ,25, 821–837.

Token 21384:
Al-Chang , M., Bresina, J., Charest, L., Chase, A., Hsu, J., Jonsson, A., Kanefsky, B., Morris, P., Rajan, K., Yglesias, J., Chaﬁn, B., Dias, W., and Maldague, P. (2004).

Token 21385:
MAPGEN: Mixed-Initiative planning andscheduling for the Mars Exploration Rover mission.IEEE Intelligent Systems ,19(1), 8–12. Albus , J. S. (1975).

Token 21386:
A new approach to manipulator control: The cerebellar model articulation controller (CMAC). J. Dynamic Systems, Measurement, and Control ,97, 270–277.

Token 21387:
Aldous , D. and Vazirani, U. (1994). “Go with the winners” algorithms. In FOCS-94 , pp.

Token 21388:
492–501.Alekhnovich , M., Hirsch, E. A., and Itsykson, D. (2005).

Token 21389:
Exponential lower bounds for the running time of DPLL algorithms on satisﬁable formulas.JAR,35(1–3), 51–72. Allais , M. (1953).

Token 21390:
Le comportment de l’homme rationnel devant la risque: critique des postulats et axiomes de l’´ ecole Am´ ericaine. Econometrica ,21, 503–546.

Token 21391:
Allen , J. F. (1983). Maintaining knowledge about temporal intervals. CACM ,26(11), 832–843. Allen , J. F. (1984).

Token 21392:
Towards a general theory of ac- tion and time. AIJ,23, 123–154. Allen , J. F. (1991). Time and time again: The many ways to represent time. Int. J.

Token 21393:
Intelligent Systems ,6, 341–355. Allen , J. F., Hendler, J., and Tate, A. (Eds.). (1990). Readings in Planning . Morgan Kaufmann. Allis , L. (1988).

Token 21394:
A knowledge-based approach to connect four. The game is solved: White wins. Mas-ter’s thesis, Vrije Univ., Amsterdam.

Token 21395:
Almuallim , H. and Dietterich, T. (1991). Learning with many irrelevant features. In AAAI-91 ,V o l .2 , pp. 547–552. ALPAC ( 1966).

Token 21396:
Language and machines: Com- puters in translation and linguistics. Tech.

Token 21397:
rep.1416, The Automatic Language Processing Advi- sory Committee of the National Academy of Sci- ences. Alterman , R. (1988). Adaptive planning.

Token 21398:
Cognitive Science ,12, 393–422. Amarel , S. (1967). An approach to heuristic problem-solving and theorem proving in the propo- sitional calculus.

Token 21399:
In Hart, J. and Takasu, S.(Eds. ), Systems and Computer Science . University of Toronto Press. Amarel , S. (1968).

Token 21400:
On representations of prob- lems of reasoning about actions. In Michie, D.(Ed. ), Machine Intelligence 3 , Vol. 3, pp. 131–171.

Token 21401:
Elsevier/North-Holland. Amir , E. and Russell, S. J. (2003). Logical ﬁltering. InIJCAI-03 . Amit , D., Gutfreund, H., and Sompolinsky, H. (1985).

Token 21402:
Spin-glass models of neural networks. Phys- ical Review ,A3 2, 1007–1018. Andersen , S. K., Olesen, K. G., Jensen, F. V., and Jensen, F. (1989).

Token 21403:
HUGIN—A shell for building Bayesian belief universes for expert systems. InIJCAI-89 , Vol. 2, pp. 1080–1085. Anderson , J. R. (1980).

Token 21404:
Cognitive Psychology and Its Implications . W. H. Freeman. Anderson , J. R. (1983). The Architecture of Cogni- tion. Harvard University Press.

Token 21405:
Andoni , A. and Indyk, P. (2006). Near-optimal hash- ing algorithms for approximate nearest neighbor inhigh dimensions. In FOCS-06 .

Token 21406:
Andre , D. and Russell, S. J. (2002). State abstraction for programmable reinforcement learning agents. InAAAI-02 , pp. 119–125.

Token 21407:
Anthony , M. and Bartlett, P. (1999). Neural Net- work Learning: Theoretical Foundations .C a m - bridge University Press. Aoki , M. (1965).

Token 21408:
Optimal control of partially ob- servable Markov systems. J. Franklin Institute , 280(5), 367–386. Appel , K. and Haken, W. (1977).

Token 21409:
Every planar map is four colorable: Part I: Discharging. Illinois J. Math. ,21, 429–490.Appelt , D. (1999). Introduction to information ex- traction.

Token 21410:
CACM ,12(3), 161–172. Apt, K. R. (1999). The essence of constraint prop- agation. Theoretical Computer Science ,221(1–2), 179–210. Apt, K. R. (2003).

Token 21411:
Principles of Constraint Pro- gramming . Cambridge University Press. Apt´e , C., Damerau, F., and Weiss, S. (1994).

Token 21412:
Auto- mated learning of decision rules for text categoriza- tion. ACM Transactions on Information Systems ,12, 233–251. Arbuthnot , J. (1692).

Token 21413:
Of the Laws of Chance . Motte, London. Translation into English, with ad- ditions, of Huygens (1657). Archibald , C., Altman, A., and Shoham, Y.

Token 21414:
(2009). Analysis of a winning computational bil- liards player. In IJCAI-09 . Ariely , D. (2009). Predictably Irrational (Revised edition). Harper.

Token 21415:
Arkin , R. (1998). Behavior-Based Robotics .M I T Press. Armando , A., Carbone, R., Compagna, L., Cuel- lar, J., and Tobarra, L. (2008).

Token 21416:
Formal analysis of SAML 2.0 web browser single sign-on: Breaking the SAML-based single sign-on for google apps. In FMSE ’08: Proc.

Token 21417:
6th ACM workshop on Formal methods in security engineering , pp. 1–10. Arnauld , A. (1662). La logique, ou l’art de penser .

Token 21418:
Chez Charles Savreux, au pied de la Tour de NostreDame, Paris. Arora , S. (1998).

Token 21419:
Polynomial time approximation schemes for Euclidean traveling salesman and othergeometric problems. JACM ,45(5), 753–782.

Token 21420:
Arunachalam , R. and Sadeh, N. M. (2005). The supply chain trading agent competition. Electronic Commerce Research and Applications ,Spring , 66– 84.

Token 21421:
Ashby , W. R. (1940). Adaptiveness and equilibrium. J. Mental Science ,86, 478–483. Ashby , W. R. (1948). Design for a brain.

Token 21422:
Electronic Engineering ,December , 379–383. Ashby , W. R. (1952). Design for a Brain . Wiley. Asimov , I. (1942). Runaround.

Token 21423:
Astounding Science Fiction ,March . Asimov , I. (1950). I, Robot . Doubleday. Astrom , K. J. (1965).

Token 21424:
Optimal control of Markov decision processes with incomplete state estimation. J. Math. Anal. Applic. ,10, 174–205. Audi , R. (Ed.). (1999).

Token 21425:
The Cambridge Dictionary of Philosophy . Cambridge University Press. Axelrod , R. (1985). The Evolution of Cooperation . Basic Books.

Token 21426:
Baader , F., Calvanese, D., McGuinness, D., Nardi, D., and Patel-Schneider, P. (2007). The Description Logic Handbook (2nd edition).

Token 21427:
Cambridge Univer- sity Press. Baader , F. and Snyder, W. (2001). Uniﬁcation the- ory. In Robinson, J. and Voronkov, A. (Eds.

Token 21428:
), Hand- book of Automated Reasoning , pp. 447–533. Else- vier. Bacchus , F. (1990). Representing and Reasoning with Probabilistic Knowledge .

Token 21429:
MIT Press. Bacchus , F. and Grove, A. (1995). Graphical models for preference and utility. In UAI-95 , pp. 3–10. Bacchus , F. and Grove, A. (1996).

Token 21430:
Utility indepen- dence in a qualitative decision theory. In KR-96 , pp. 542–552.

Token 21431:
Bibliography 1065 Bacchus , F., Grove, A., Halpern, J. Y., and Koller, D. (1992). From statistics to beliefs. In AAAI-92 , pp. 602–608.

Token 21432:
Bacchus , F. and van Beek, P. (1998). On the conver- sion between non-binary and binary constraint satis-faction problems. In AAAI-98 , pp. 311–318.

Token 21433:
Bacchus , F. and van Run, P. (1995). Dynamic vari- able ordering in CSPs. In CP-95 , pp. 258–275. Bachmann , P. G. H. (1894).

Token 21434:
Die analytische Zahlen- theorie . B. G. Teubner, Leipzig. Backus , J. W. (1996). Transcript of question and an- swer session. In Wexelblat, R. L. (Ed.

Token 21435:
), History of Programming Languages , p. 162. Academic Press. Bagnell , J. A. and Schneider, J. (2001).

Token 21436:
Autonomous helicopter control using reinforcement learning pol-icy search methods. In ICRA-01 . Baker , J. (1975). The Dragon system—An overview.

Token 21437:
IEEE Transactions on Acoustics; Speech; and Signal Processing ,23, 24–29. Baker , J. (1979). Trainable grammars for speech recognition.

Token 21438:
In Speech Communication Papers for the 97th Meeting of the Acoustical Society of Amer- ica, pp. 547–550.

Token 21439:
Baldi , P., Chauvin, Y., Hunkapiller, T., and Mc- Clure, M. (1994). Hidden Markov models of bio- logical primary sequence information.

Token 21440:
PNAS ,91(3), 1059–1063. Baldwin , J. M. (1896). A new factor in evolution. American Naturalist ,30, 441–451. Continued on pages 536–553.

Token 21441:
Ballard , B. W. (1983). The *-minimax search pro- cedure for trees containing chance nodes. AIJ,21(3), 327–350. Baluja , S. (1997).

Token 21442:
Genetic algorithms and explicit search statistics. In Mozer, M. C., Jordan, M. I., and Petsche, T. (Eds. ), NIPS 9 , pp. 319–325. MIT Press.

Token 21443:
Bancilhon , F., Maier, D., Sagiv, Y., and Ullman, J. D. (1986). Magic sets and other strange ways to implement logic programs. In PODS-86 , pp. 1–16.

Token 21444:
Banko , M. and Brill, E. (2001). Scaling to very very large corpora for natural l anguage disambiguation. InACL-01 , pp. 26–33.

Token 21445:
Banko , M., Brill, E., Dumais, S. T., and Lin, J. (2002). Askmsr: Question answering using the worldwide web. In Proc.

Token 21446:
AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases , pp. 7–9.

Token 21447:
Banko , M., Cafarella, M. J., Soderland, S., Broad- head, M., and Etzioni, O. (2007). Open information extraction from the web. In IJCAI-07 .

Token 21448:
Banko , M. and Etzioni, O. (2008). The tradeoffs between open and traditional relation extraction. In ACL-08 , pp. 28–36. Bar-Hillel , Y. (1954).

Token 21449:
Indexical expressions. Mind , 63, 359–379. Bar-Hillel , Y. (1960). The present status of auto- matic translation of langua ges. In Alt, F. L. (Ed.

Token 21450:
), Advances in Computers , Vol. 1, pp. 91–163. Aca- demic Press. Bar-Shalom , Y. (Ed.). (1992).

Token 21451:
Multitarget- multisensor tracking: Advanced applications . Artech House. Bar-Shalom , Y. and Fortmann, T. E. (1988). Track- ing and Data Association .

Token 21452:
Academic Press. Bartak , R. (2001). Theory and practice of constraint propagation. In Proc.

Token 21453:
Third Workshop on Constraint Programming for Decision and Control (CPDC-01) , pp. 7–14.Barto , A. G., Bradtke, S. J., and Singh, S. P. (1995).

Token 21454:
Learning to act using real-time dynamic program- ming. AIJ,73(1), 81–138. Barto , A. G., Sutton, R. S., and Anderson, C. W. (1983).

Token 21455:
Neuron-like adaptive elements that can solvedifﬁcult learning control problems. IEEE Transac- tions on Systems, Man and Cybernetics ,13, 834– 846.

Token 21456:
Barto , A. G., Sutton, R. S., and Brouwer, P. S. (1981). Associative search network: A reinforce-ment learning associative memory.

Token 21457:
Biological Cy- bernetics ,40(3), 201–211. Barwise , J. and Etchemendy, J. (1993).

Token 21458:
The Lan- guage of First-Order Logic: Including the Macin-tosh Program Tarski’s World 4.0 (Third Revised and Expanded edition).

Token 21459:
Center for the Study of Language and Information (CSLI). Barwise , J. and Etchemendy, J. (2002). Language, Proof and Logic . CSLI (Univ.

Token 21460:
of Chicago Press). Baum , E., Boneh, D., and Garrett, C. (1995). On genetic algorithms. In COLT-95 , pp. 230–239. Baum , E. and Haussler, D. (1989).

Token 21461:
What size net gives valid generalization? Neural Computation , 1(1), 151–160. Baum , E. and Smith, W. D. (1997).

Token 21462:
A Bayesian ap- proach to relevance in game playing. AIJ,97(1–2), 195–242. Baum , E. and Wilczek, F. (1988).

Token 21463:
Supervised learn- ing of probability distributions by neural networks. In Anderson, D. Z. (Ed. ), Neural Information Pro- cessing Systems , pp. 52–61.

Token 21464:
American Institute of Physics. Baum , L. E. and Petrie, T. (1966). Statistical inference for probabilistic functions of ﬁnite state Markov chains.

Token 21465:
Annals of Mathematical Statistics , 41. Baxter , J. and Bartlett, P. (2000). Reinforcement learning in POMDP’s via direct gradient ascent.

Token 21466:
In ICML-00 , pp. 41–48. Bayardo , R. J. and Miranker, D. P. (1994).

Token 21467:
An optimal backtrack algorithm for tree-structured con-straint satisfaction problems. AIJ,71(1), 159–181. Bayardo , R. J. and Schrag, R. C. (1997).

Token 21468:
Using CSP look-back techniques to solve real-world SAT instances. In AAAI-97 , pp. 203–208. Bayes , T. (1763).

Token 21469:
An essay towards solving a prob- lem in the doctrine of chances. Philosophical Trans- actions of the Royal Society of London ,53, 370–418.

Token 21470:
Beal , D. F. (1980). An analysis of minimax. In C l a r k e ,M .R .B . ( E d . ) , Advances in Computer Chess 2 , pp. 103–109.

Token 21471:
Edinburgh University Press. Beal , J. and Winston, P. H. (2009). The new frontier of human-level artiﬁcial intelligence.

Token 21472:
IEEE Intelli- gent Systems ,24(4), 21–23. Beckert , B. and Posegga, J. (1995). Leantap: Lean, tableau-based deduction. JAR,15(3), 339–358.

Token 21473:
Beeri , C., Fagin, R., Maier, D., and Yannakakis, M. (1983). On the desirability of acyclic databaseschemes. JACM ,30(3), 479–513. Bekey , G. (2008).

Token 21474:
Robotics: State Of The Art And Future Challenges . Imperial College Press. Bell, C. and Tate, A. (1985).

Token 21475:
Using temporal con- straints to restrict search in a planner. In Proc. Third Alvey IKBS SIG Workshop . Bell, J. L. and Machover, M. (1977).

Token 21476:
A Course in Mathematical Logic . Elsevier/North-Holland. Bellman , R. E. (1952). On the theory of dynamic programming.

Token 21477:
PNAS ,38, 716–719.Bellman , R. E. (1961). Adaptive Control Processes: A Guided Tour . Princeton University Press. Bellman , R. E. (1965).

Token 21478:
On the application of dy- namic programming to the determination of optimalplay in chess and checkers. PNAS ,53, 244–246. Bellman , R. E. (1978).

Token 21479:
An Introduction to Artiﬁcial Intelligence: Can Computers Think? Boyd & Fraser Publishing Company. Bellman , R. E. (1984).

Token 21480:
Eye of the Hurricane .W o r l d Scientiﬁc. Bellman , R. E. and Dreyfus, S. E. (1962). Applied Dynamic Programming . Princeton University Press.

Token 21481:
Bellman , R. E. (1957). Dynamic Programming . Princeton University Press. Belongie , S., Malik, J., and Puzicha, J. (2002).

Token 21482:
Shape matching and object recognition using shape contexts. PAMI ,24(4), 509–522. Ben-Tal , A. and Nemirovski, A. (2001).

Token 21483:
Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engineering Applications . SIAM (Society for Industrial and App lied Mathematics).

Token 21484:
Bengio , Y. and LeCun, Y. (2007). Scaling learn- ing algorithms towards AI. In Bottou, L., Chapelle, O., DeCoste, D., and Weston, J. (Eds.

Token 21485:
), Large-Scale Kernel Machines . MIT Press. Bentham , J. (1823). Principles of Morals and Legis- lation . Oxford University Press, Oxford, UK.

Token 21486:
Orig- inal work published in 1789. Berger , J. O. (1985). Statistical Decision Theory and Bayesian Analysis . Springer Verlag. Berkson , J. (1944).

Token 21487:
Application of the logistic func- tion to bio-assay. JASA ,39, 357–365. Berlekamp , E. R., Conway, J. H., and Guy, R. K. (1982).

Token 21488:
Winning Ways, For Your Mathematical Plays . Academic Press. Berlekamp , E. R. and Wolfe, D. (1994). Mathemat- ical Go: Chilling Gets the Last Point .

Token 21489:
A.K. Peters. Berleur , J. and Brunnstein, K. (2001). Ethics of Computing: Codes, Spaces for Discussion and Law . Chapman and Hall. Berliner , H. J.

Token 21490:
(1979). The B* tree search algorithm: A best-ﬁrst proof procedure. AIJ,12(1), 23–40. Berliner , H. J. (1980a).

Token 21491:
Backgammon computer program beats world champion. AIJ,14, 205–220. Berliner , H. J. (1980b). Computer backgammon. Scientiﬁc American ,249(6), 64–72.

Token 21492:
Bernardo , J. M. and Smith, A. F. M. (1994). Bayesian Theory . Wiley. Berners-Lee , T., Hendler, J., and Lassila, O. (2001). The semantic web.

Token 21493:
Scientiﬁc American ,284(5), 34– 43. Bernoulli , D. (1738). Specimen theoriae novae de mensura sortis. Proc.

Token 21494:
St. Petersburg Imperial Academy of Sciences ,5, 175–192. Bernstein , A. and Roberts, M. (1958). Computer vs. chess player.

Token 21495:
Scientiﬁc American ,198(6), 96– 105. Bernstein , P. L. (1996). Against the Odds: The Re- markable Story of Risk . Wiley.

Token 21496:
Berrou , C., Glavieux, A., and Thitimajshima, P. (1993). Near Shannon limit error control-correcting coding and decoding: Turbo-codes. 1. In Proc.

Token 21497:
IEEE International Conference on Communications , pp. 1064–1070. Berry , D. A. and Fristedt, B. (1985).

Token 21498:
Bandit Prob- lems: Sequential Allocation of Experiments . Chap- man and Hall.

Token 21499:
1066 Bibliography Bertele , U. and Brioschi, F. (1972). Nonserial dy- namic programming . Academic Press.

Token 21500:
Bertoli , P., Cimatti, A., and Roveri, M. (2001a). Heuristic search + symbolic model checking = ef-ﬁcient conforma nt planning. In IJCAI-01 , pp.

Token 21501:
467– 472. Bertoli , P., Cimatti, A., Roveri, M., and Traverso, P. (2001b).

Token 21502:
Planning in nondeterministic domains un-der partial observability via symbolic model check-ing. In IJCAI-01 , pp. 473–478.

Token 21503:
Bertot , Y ., Casteran, P., Hu et, G., and Paulin- Mohring, C. (2004). Interactive Theorem Proving and Program Development . Springer.

Token 21504:
Bertsekas , D. (1987). Dynamic Programming: De- terministic and Stochastic Models . Prentice-Hall. Bertsekas , D. and Tsitsiklis, J. N. (1996).

Token 21505:
Neuro- dynamic programming . Athena Scientiﬁc. Bertsekas , D. and Tsitsiklis, J. N. (2008). Introduc- tion to Probability (2nd edition).

Token 21506:
Athena Scientiﬁc. Bertsekas , D. and Shreve, S. E. (2007). Stochastic Optimal Control: The Discrete-Time Case .A t h e n a Scientiﬁc.

Token 21507:
Bessi `ere, C. (2006). Constraint propagation. In Rossi, F., van Beek, P., and Walsh, T. (Eds. ), Hand- book of Constraint Programming . Elsevier.

Token 21508:
Bhar , R. and Hamori, S. (2004). Hidden Markov Models: Applications to Financial Economics . Springer. Bibel , W. (1993). Deduction: Automated Logic .

Token 21509:
Academic Press. Biere , A., Heule, M., van Maaren, H., and Walsh, T. (Eds.). (2009). Handbook of Satisﬁability .I O S Press.

Token 21510:
Billings , D., Burch, N., Davidson, A., Holte, R., Schaeffer, J., Schauenberg, T., and Szafron, D. (2003).

Token 21511:
Approximating game-theoretic optimalstrategies for full-scale poker. In IJCAI-03 . Binder , J., Koller, D., Russell, S. J., and Kanazawa, K. (1997a).

Token 21512:
Adaptive probabilistic networks with hidden variables. Machine Learning ,29, 213–244. Binder , J., Murphy, K., and Russell, S. J. (1997b).

Token 21513:
Space-efﬁcient inference in dynamic probabilisticnetworks. In IJCAI-97 , pp. 1292–1296. Binford , T. O. (1971). Visual perception by com- puter.

Token 21514:
Invited paper presented at the IEEE SystemsScience and Cybernetics Conference, Miami. Binmore , K. (1982). Essays on Foundations of Game Theory .

Token 21515:
Pitman. Bishop , C. M. (1995). Neural Networks for Pattern Recognition . Oxford University Press. Bishop , C. M. (2007).

Token 21516:
Pattern Recognition and Ma- chine Learning . Springer-Verlag. Bisson , T. (1990). They’re made out of meat. Omni Magazine .

Token 21517:
Bistarelli , S., Montanari, U., and Rossi, F. (1997). Semiring-based constraint satisfaction and optimiza-tion. JACM ,44(2), 201–236.

Token 21518:
Bitner , J. R. and Reingold, E. M. (1975). Backtrack programming techniques. CACM ,18(11), 651–656.

Token 21519:
Bizer , C., Auer, S., Kobilarov, G., Lehmann, J., and Cyganiak, R. (2007). DBPedia – querying wikipedialike a database.

Token 21520:
In Developers Track Presentation at the 16th International Conference on World Wide Web.Blazewicz , J., Ecker, K., Pesch, E., Schmidt, G., and Weglarz, J.

Token 21521:
(2007). Handbook on Schedul- ing: Models and Methods for Advanced Planning(International Handbooks on Information Systems) .

Token 21522:
Springer-Verlag New York, Inc. Blei, D. M., Ng, A. Y., and Jordan, M. I. (2001). Latent Dirichlet Allocation.

Token 21523:
In Neural Information Processing Systems , Vol. 14. Blinder , A. S. (1983). Issues in the coordination of monetary and ﬁscal policies.

Token 21524:
In Monetary Policy Issues in the 1980s . Federal Reserve Bank, Kansas City, Missouri. Bliss , C. I. (1934). The method of probits.

Token 21525:
Science , 79(2037), 38–39. Block , H. D., Knight, B., and Rosenblatt, F. (1962). Analysis of a four-layer series-coupled perceptron.Rev.

Token 21526:
Modern Physics ,34(1), 275–282. Blum , A. L. and Furst, M. (1995). Fast planning through planning graph analysis. In IJCAI-95 , pp. 1636–1642.

Token 21527:
Blum , A. L. and Furst, M. (1997). Fast planning through planning graph analysis. AIJ,90(1–2), 281– 300. Blum , A. L. (1996).

Token 21528:
On-line algorithms in machine learning. In Proc. Workshop on On-Line Algorithms, Dagstuhl , pp. 306–325. Blum , A. L. and Mitchell, T. M. (1998).

Token 21529:
Combin- ing labeled and unlabeled data with co-training. InCOLT-98 , pp. 92–100.

Token 21530:
Blumer , A., Ehrenfeucht, A., H aussler, D., and War- muth, M. (1989). Learnability and the Vapnik- Chervonenkis dimension. JACM ,36(4), 929–965.

Token 21531:
Bobrow , D. G. (1967). Natural language input for a computer problem solving system. In Minsky, M. L. (Ed. ), Semantic Information Processing , pp.

Token 21532:
133– 215. MIT Press. Bobrow , D. G., Kaplan, R., Kay, M., Norman, D. A., Thompson, H., and Winograd, T. (1977). GUS, a frame driven dialog system.

Token 21533:
AIJ,8, 155–173. Boden , M. A. (1977). Artiﬁcial Intelligence and Natural Man . Basic Books. Boden , M. A. (Ed.). (1990).

Token 21534:
The Philosophy of Ar- tiﬁcial Intelligence . Oxford University Press. Bolognesi , A. and Ciancarini, P. (2003).

Token 21535:
Computer programming of kriegspiel endings: The case of KR vs. k. In Advances in Computer Games 10 . Bonet , B. (2002).

Token 21536:
An epsilon-optimal grid-based algorithm for partially observable Markov decisionprocesses. In ICML-02 , pp. 51–58. Bonet , B. and Geffner, H. (1999).

Token 21537:
Planning as heuristic search: New results. In ECP-99 , pp. 360– 372. Bonet , B. and Geffner, H. (2000).

Token 21538:
Planning with incomplete information as heuristic search in beliefspace. In ICAPS-00 , pp. 52–61. Bonet , B. and Geffner, H. (2005).

Token 21539:
An algorithm bet- ter than AO ∗?I n AAAI-05 . Boole , G. (1847).

Token 21540:
The Mathematical Analysis of Logic: Being an Essay towards a Calculus of Deduc-tive Reasoning . Macmillan, Barclay, and Macmillan, Cambridge.

Token 21541:
Booth , T. L. (1969). Probabilistic representation of formal languages.

Token 21542:
In IEEE Conference Record of the 1969 Tenth Annual Symposium on Switching andAutomata Theory , pp. 74–81. Borel , E. (1921).

Token 21543:
La th´ eorie du jeu et les ´ equations int´egrales ` an o y a us y m ´ etrique.

Token 21544:
Comptes Rendus Hebdomadaires des S´ eances de l’Acad´ emie des Sci- ences ,173, 1304–1308.Borenstein , J., Everett, B., and Feng, L. (1996).

Token 21545:
Navigating Mobile Robots: Systems and Techniques . A. K. Peters, Ltd. Borenstein , J. and Koren., Y. (1991).

Token 21546:
The vector ﬁeld histogram—Fast obstacle avoidance for mobilerobots. IEEE Transactions on Robotics and Automa- tion,7(3), 278–288.

Token 21547:
Borgida , A., Brachman, R. J., McGuinness, D., and Alperin Resnick, L. (1989). CLASSIC: A structuraldata model for objects.

Token 21548:
SIGMOD Record ,18(2), 58– 67. Boroditsky , L. (2003). Linguistic relativity. In Nadel, L. (Ed. ), Encyclopedia of Cognitive Science , pp. 917–921.

Token 21549:
Macmillan. Boser , B., Guyon, I., and Vapnik, V. N. (1992). A training algorithm for optimal margin classiﬁers. InCOLT-92 .

Token 21550:
Bosse , M., Newman, P., Leonard, J., Soika, M., Feiten, W., and Teller, S. (2004).

Token 21551:
Simultaneous localization and map building in large-scale cyclic environments using the atlas framework. Int. J. Robotics Research ,23(12), 1113–1139.

Token 21552:
Bourzutschky , M. (2006). 7-man endgames with pawns. CCRL Discussion Board , kirill-kryukov.com/chess/ discussion-board/viewtopic.php?t= 805.

Token 21553:
Boutilier , C. and Brafman, R. I. (2001). Partial- order planning with concurre nt interacting actions. JAIR ,14, 105–136.

Token 21554:
Boutilier , C., Dearden, R., and Goldszmidt, M. (2000). Stochastic dynamic programming with fac-tored representations. AIJ,121, 49–107.

Token 21555:
Boutilier , C., Reiter, R., and Price, B. (2001). Sym- bolic dynamic programming for ﬁrst-order MDPs. In IJCAI-01 , pp. 467–472.

Token 21556:
Boutilier , C., Friedman, N., Goldszmidt, M., and Koller, D. (1996). Context- speciﬁc independence in Bayesian networks. In UAI-96 , pp. 115–123.

Token 21557:
Bouzy , B. and Cazenave, T. (2001). Computer go: An AI oriented survey. AIJ,132(1), 39–103. Bowerman , M. and Levinson, S. (2001).

Token 21558:
Language acquisition and conceptual development . Cambridge University Press. Bowling , M., Johanson, M., Burch, N., and Szafron, D. (2008).

Token 21559:
Strategy evaluation in extensive games with importance sampling. In ICML-08 . Box, G. E. P. (1957).

Token 21560:
Evolutionary operation: A method of increasing industrial productivity. Applied Statistics ,6, 81–101.

Token 21561:
Box, G. E. P., Jenkins, G., and Reinsel, G. (1994). Time Series Analysis: Forecasting and Control (3rd edition). Prentice Hall. Boyan , J. A. (2002).

Token 21562:
Technical update: Least- squares temporal difference learning. Machine Learning ,49(2–3), 233–246. Boyan , J. A. and Moore, A. W. (1998).

Token 21563:
Learn- ing evaluation functions for global optimization andBoolean satisﬁability. In AAAI-98 . Boyd , S. and Vandenberghe, L. (2004).

Token 21564:
Convex Op- timization . Cambridge University Press. Boyen , X., Friedman, N., and Koller, D. (1999).

Token 21565:
Dis- covering the hidden structure of complex dynamicsystems. In UAI-99 . Boyer , R. S. and Moore, J. S. (1979). A Computa- tional Logic .

Token 21566:
Academic Press. Boyer , R. S. and Moore, J. S. (1984). Proof checking the RSA public key encryption algorithm.

Token 21567:
American Mathematical Monthly ,91(3), 181–189.

Token 21568:
Bibliography 1067 Brachman , R. J. (1979). On the epistemologi- cal status of semantic networks. In Findler, N. V. (Ed.

Token 21569:
), Associative Networks: Representation and Use of Knowledge by Computers , pp. 3–50. Aca- demic Press. Brachman ,R .J . ,F i k e s ,R .E .

Token 21570:
,a n dL e v e s q u e ,H .J . (1983). Krypton: A functional approach to knowl- edge representation. Computer ,16(10), 67–73.

Token 21571:
Brachman , R. J. and Levesque, H. J. (Eds.). (1985). Readings in Knowledge Representation . Morgan Kaufmann. Bradtke , S. J. and Barto, A. G. (1996).

Token 21572:
Linear least- squares algorithms for temporal difference learning.Machine Learning ,22, 33–57. Brafman , O. and Brafman, R. (2009).

Token 21573:
Sway: The Irresistible Pull of Irrational Behavior .B r o a d w a y Business. Brafman , R. I. and Domshlak, C. (2008).

Token 21574:
From one to many: Planning for loosely coupled multi-agent systems. In ICAPS-08 , pp. 28–35. Brafman , R. I. and Tennenholtz, M. (2000).

Token 21575:
A near optimal polynomial time algorithm for learning in certain classes of stochastic games. AIJ,121, 31–47. Braitenberg , V. (1984).

Token 21576:
Vehicles: Experiments in Synthetic Psychology . MIT Press. Bransford , J. and Johnson, M. (1973). Considera- tion of some problems in comprehension.

Token 21577:
In Chase,W. G. (Ed. ), Visual Information Processing . Aca- demic Press. Brants , T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. (2007).

Token 21578:
Large language models in machine trans- lation. In EMNLP-CoNLL-2007: Proc.

Token 21579:
2007 Joint Conference on Empirical Methods in Natural Lan- guage Processing and Computational Natural Lan- guage Learning , pp. 858–867. Bratko , I.

Token 21580:
(1986). Prolog Programming for Artiﬁ- cial Intelligence (1st edition). Addison-Wesley. Bratko , I. (2001).

Token 21581:
Prolog Programming for Artiﬁ- cial Intelligence (Third edition). Addison-Wesley. Bratman , M. E. (1987). Intention, Plans, and Prac- tical Reason .

Token 21582:
Harvard University Press. Bratman , M. E. (1992). Planning and the stability of intention. Minds and Machines ,2(1), 1–16. Breese , J. S. (1992).

Token 21583:
Construction of belief and de- cision networks. Computational Intelligence ,8(4), 624–647. Breese , J. S. and Heckerman, D. (1996).

Token 21584:
Decision- theoretic troubleshooting: A framework for repair and experiment. In UAI-96 , pp. 124–132. Breiman , L. (1996). Bagging predictors.

Token 21585:
Machine Learning ,24(2), 123–140. Breiman , L., Friedman, J., Olshen, R. A., and Stone, C. J. (1984). Classiﬁcation and Regression Trees .

Token 21586:
Wadsworth International Group. Brelaz , D. (1979). New methods to color the vertices of a graph. CACM ,22(4), 251–256. Brent , R. P. (1973).

Token 21587:
Algorithms for minimization without derivatives . Prentice-Hall. Bresnan , J. (1982). The Mental Representation of Grammatical Relations . MIT Press.

Token 21588:
Brewka , G., Dix, J., and Konolige, K. (1997). Nononotonic Reasoning: An Overview . CSLI Publi- cations. Brickley , D. and Guha, R. V. (2004).

Token 21589:
RDF vocab- ulary description language 1.0: RDF schema. Tech. rep., W3C.Bridle , J. S. (1990).

Token 21590:
Probabilistic interpretation of feedforward classiﬁcation network outputs, with re- lationships to statistical pattern recognition.

Token 21591:
In Fo- gelman Souli´ e, F. and H´ erault, J. (Eds. ), Neurocom- puting: Algorithms, Architectures and Applications . Springer-Verlag.

Token 21592:
Briggs , R. (1985). Knowledge representation in Sanskrit and artiﬁcial intelligence. AIMag ,6(1), 32– 39. Brin , D. (1998).

Token 21593:
The Transparent Society .P e r s e u s . Brin , S. (1999). Extracting patterns and relations from the world wide web.

Token 21594:
Technical report 1999-65,Stanford InfoLab. Brin , S. and Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. In Proc.

Token 21595:
Seventh World Wide Web Conference . Bringsjord , S. (2008). If I were judge. In Epstein, R., Roberts, G., and Beber, G. (Eds.

Token 21596:
), Parsing the Turing Test . Springer. Broadbent , D. E. (1958). Perception and Commu- nication . Pergamon. Brooks , R. A. (1986).

Token 21597:
A robust layered control sys- tem for a mobile robot. IEEE Journal of Robotics and Automation ,2, 14–23. Brooks , R. A. (1989).

Token 21598:
Engineering approach to building complete, intelligent beings. Proc. SPIE— the International Society for Optical Engineering , 1002 , 618–625.

Token 21599:
Brooks , R. A. (1991). Intelligence without represen- tation. AIJ,47(1–3), 139–159. Brooks , R. A. and Lozano-Perez, T. (1985).

Token 21600:
A sub- division algorithm in conﬁguration space for ﬁnd- path with rotation. IEEE Transactions on Systems, Man and Cybernetics ,15(2), 224–233.

Token 21601:
Brown , C., Finkelstein, L., and Purdom, P. (1988). Backtrack searching in the presence of symmetry. In Mora, T. (Ed.

Token 21602:
), Applied Algebra, Algebraic Al- gorithms and Error-Correcting Codes , pp. 99–110. Springer-Verlag. Brown , K. C. (1974).

Token 21603:
A note on the apparent bias of net revenue estimates. J. Finance ,29, 1215–1216.

Token 21604:
Brown , P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L., and Roossin, P. (1988).

Token 21605:
A statistical approach to lan- guage translation. In COLING-88 , pp. 71–76.

Token 21606:
Brown , P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer, R. L. (1993).

Token 21607:
The mathematics of sta- tistical machine translation: Parameter estimation. Computational Linguistics ,19(2), 263–311.

Token 21608:
Brownston , L., Farrell, R., Kant, E., and Martin, N. (1985). Programming expert systems in OPS5: An introduction to rule-based programming .

Token 21609:
Addison- Wesley. Bruce , V., Georgeson, M., and Green, P. (2003). Vi- sual Perception: Physiology, Psychology and Ecol- ogy. Psychology Press.

Token 21610:
Bruner , J. S., Goodnow, J. J., and Austin, G. A. (1957). A Study of Thinking . Wiley. Bryant , B. D. and Miikkulainen, R. (2007).

Token 21611:
Acquir- ing visibly intelligent behavior with example-guided neuroevolution. In AAAI-07 . Bryce , D. and Kambhampati, S. (2007).

Token 21612:
A tuto- rial on planning graph-based reachability heuristics. AIMag ,Spring , 47–83. Bryce , D., Kambhampati, S., and Smith, D. E. (2006).

Token 21613:
Planning graph heuristics for belief space search. JAIR ,26, 35–99.Bryson , A. E. and Ho, Y.-C. (1969). Applied Opti- mal Control . Blaisdell.

Token 21614:
Buchanan , B. G. and Mitchell, T. M. (1978). Model-directed learning of production rules. In Wa- terman, D. A. and Hayes-Roth, F. (Eds.

Token 21615:
), Pattern- Directed Inference Systems , pp. 297–312. Academic Press. Buchanan , B. G., Mitchell, T. M., Smith, R. G., and Johnson, C. R. (1978).

Token 21616:
Models of learning systems.InEncyclopedia of Computer Science and Technol- ogy, Vol. 11. Dekker. Buchanan , B. G. and Shortliffe, E. H. (Eds.).

Token 21617:
(1984). Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic ProgrammingProject . Addison-Wesley.

Token 21618:
Buchanan , B. G., Sutherland, G. L., and Feigen- baum, E. A. (1969).

Token 21619:
Heuristic DENDRAL: A pro-gram for generating explanatory hypotheses in or-ganic chemistry. In Meltzer, B., Michie, D., and Swann, M. (Eds.

Token 21620:
), Machine Intelligence 4 , pp. 209– 254. Edinburgh University Press. Buehler , M., Iagnemma, K., and Singh, S. (Eds.). (2006).

Token 21621:
The 2005 DARPA Grand Challenge: The Great Robot Race . Springer-Verlag. Bunt , H. C. (1985).

Token 21622:
The formal representation of (quasi-) continuous concepts. In Hobbs, J. R. andMoore, R. C. (Eds. ), Formal Theories of the Com- monsense World , chap.

Token 21623:
2, pp. 37–70. Ablex. Burgard , W., Cremers, A. B., Fox, D., H¨ ahnel, D., Lakemeyer, G., Schulz, D., Steiner, W., and Thrun, S. (1999).

Token 21624:
Experiences with an interactive museum tour-guide robot. AIJ,114(1–2), 3–55. Buro , M. (1995).

Token 21625:
ProbCut: An effective selective extension of the alpha-beta algorithm. J. Interna- tional Computer Chess Association ,18(2), 71–76. Buro , M. (2002).

Token 21626:
Improving heuristic mini-max search by supervised learning. AIJ,134(1–2), 85– 99. Burstein , J., Leacock, C., and Swartz, R. (2001).

Token 21627:
Automated evaluation of essays and short answers.InFifth International Computer Assisted Assessment (CAA) Conference . Burton , R. (2009).

Token 21628:
On Being Certain: Believing You Are Right Even When You’re Not . St. Martin’s Grif- ﬁn. Buss , D. M. (2005). Handbook of evolutionary psy- chology .

Token 21629:
Wiley. Butler , S. (1863). Darwin among the machines. The Press (Christchurch, New Zealand) ,June 13 . Bylander , T. (1992).

Token 21630:
Complexity results for serial decomposability. In AAAI-92 , pp. 729–734. Bylander , T. (1994).

Token 21631:
The computational complexity of propositional STRIPS planning. AIJ,69, 165– 204. Byrd , R. H., Lu, P., Nocedal, J., and Zhu, C. (1995).

Token 21632:
A limited memory algorithm for bound constrainedoptimization. SIAM Journal on Scientiﬁc and Statis- tical Computing ,16(5), 1190–1208.

Token 21633:
Cabeza , R. and Nyberg, L. (2001). Imaging cogni- tion II: An empirical review of 275 PET and fMRI studies. J. Cognitive Neuroscience ,12, 1–47.

Token 21634:
Cafarella , M. J., Halevy, A., Zhang, Y., Wang, D. Z., and Wu, E. (2008). Webtables: Exploring the power of tables on the web. In VLDB-2008 .

Token 21635:
Calvanese , D., Lenzerini, M., and Nardi, D. (1999). Unifying class-based representation for- malisms. JAIR ,11, 199–240.

Token 21636:
Campbell , M. S., Hoane, A. J., and Hsu, F.-H. (2002). Deep Blue. AIJ,134(1–2), 57–83.

Token 21637:
1068 Bibliography Canny , J. and Reif, J. (1987). New lower bound techniques for robot motion planning problems. In FOCS-87 , pp. 39–48. Canny , J.

Token 21638:
(1986). A computational approach to edge detection. PAMI ,8, 679–698. Canny , J. (1988). The Complexity of Robot Motion Planning . MIT Press.

Token 21639:
Capen , E., Clapp, R., and Campbell, W. (1971). Competitive bidding in high-risk situations. J. Petroleum Technology ,23, 641–653.

Token 21640:
Caprara , A., Fischetti, M., and Toth, P. (1995). A heuristic method for the set covering problem. Op- erations Research ,47, 730–743.

Token 21641:
Carbonell , J. G. (1983). Derivational analogy and its role in problem solving. In AAAI-83 , pp. 64–69.

Token 21642:
Carbonell , J. G., Knoblock, C. A., and Minton, S. (1989). PRODIGY: An integrated architecture forplanning and learning.

Token 21643:
Technical report CMU-CS- 89-189, Computer Science Department, Carnegie- Mellon University. Carbonell , J. R. and Collins, A. M. (1973).

Token 21644:
Natural semantics in artiﬁcial intelligence. In IJCAI-73 , pp. 344–351. Cardano , G. (1663). Liber de ludo aleae . Lyons. Carnap , R. (1928).

Token 21645:
Der logische Aufbau der Welt . Weltkreis-verlag. Translated into English as (Car- nap, 1967). Carnap , R. (1948).

Token 21646:
On the application of inductive logic. Philosophy and Phenomenological Research , 8, 133–148. Carnap , R. (1950).

Token 21647:
Logical Foundations of Proba- bility . University of Chicago Press. Carroll , S. (2007).

Token 21648:
The Making of the Fittest: DNA and the Ultimate Forensic Record of Evolution .N o r - ton. Casati , R. and Varzi, A. (1999).

Token 21649:
Parts and places: the structures of spatial representation . MIT Press. Cassandra , A. R., Kaelbling, L. P., and Littman, M. L. (1994).

Token 21650:
Acting optimally in partially observ- able stochastic domains. In AAAI-94 , pp. 1023– 1028. Cassandras , C. G. and Lygeros, J. (2006).

Token 21651:
Stochas- tic Hybrid Systems . CRC Press. Castro , R., Coates, M., Liang, G., Nowak, R., and Yu, B. (2004). Network tomography: Recent devel- opments.

Token 21652:
Statistical Science ,19 (3), 499–517. Cesa-Bianchi , N. and Lugosi, G. (2006). Prediction, learning, and Games . Cambridge University Press.

Token 21653:
Cesta , A., Cortellessa, G., Denis, M., Donati, A., Fratini, S., Oddi, A., Poli cella, N., Rabenau, E., and Schulster, J. (2007).

Token 21654:
MEXAR2: AI solves missionplanner problems. IEEE Intelligent Systems ,22(4), 12–19.

Token 21655:
Chakrabarti , P. P., Ghose, S., Acharya, A., and de Sarkar, S. C. (1989). Heuristic search in restrictedmemory. AIJ,41(2), 197–222.

Token 21656:
Chandra , A. K. and Harel, D. (1980). Computable queries for relational data bases. J. Computer and System Sciences ,21(2), 156–178.

Token 21657:
Chang , C.-L. and Lee, R. C.-T. (1973). Symbolic Logic and Mechanical Theorem Proving . Academic Press. Chapman , D. (1987).

Token 21658:
Planning for conjunctive goals. AIJ,32(3), 333–377. Charniak , E. (1993). Statistical Language Learn- ing. MIT Press.Charniak , E. (1996).

Token 21659:
Tree-bank grammars. In AAAI-96 , pp. 1031–1036. Charniak , E. (1997). Statistical parsing with a context-free grammar and word statistics.

Token 21660:
In AAAI- 97, pp. 598–603. Charniak , E. and Goldman, R. (1992). A Bayesian model of plan recognition. AIJ,64(1), 53–79.

Token 21661:
Charniak , E. and McDermott, D. (1985). Introduc- tion to Artiﬁcial Intelligence . Addison-Wesley.

Token 21662:
Charniak , E., Riesbeck, C., McDermott, D., and Meehan, J. (1987). Artiﬁcial Intelligence Program- ming (2nd edition). Lawrence Erlbaum Associates.

Token 21663:
Charniak , E. (1991). Bayesian networks without tears. AIMag ,12(4), 50–63. Charniak , E. and Johnson, M. (2005).

Token 21664:
Coarse- to-ﬁne n-best parsing and maxent discriminative reranking. In ACL-05 . Chater , N. and Oaksford, M. (Eds.). (2008).

Token 21665:
The probabilistic mind: Prospects for Bayesian cognitivescience . Oxford University Press. Chatﬁeld , C. (1989).

Token 21666:
The Analysis of Time Series: An Introduction (4th edition). Chapman and Hall. Cheeseman , P. (1985). In defense of probability. In IJCAI-85 , pp.

Token 21667:
1002–1009. Cheeseman , P. (1988). An inquiry into computer un- derstanding. Computational Intelligence ,4(1), 58– 66. Cheeseman ,P .

Token 21668:
,K a n e f s k y ,B . ,a n dT a y l o r ,W . (1991). Where the really hard problems are. InIJCAI-91 , pp. 331–337.

Token 21669:
Cheeseman , P., Self, M., Kelly, J., and Stutz, J. (1988). Bayesian classiﬁcation. In AAAI-88 ,V o l .2 , pp. 607–611. Cheeseman , P. and Stutz, J.

Token 21670:
(1996). Bayesian classiﬁcation (AutoClass): Theory and results. In Fayyad, U., Piatesky-Shapiro, G., Smyth, P., andUthurusamy, R. (Eds.

Token 21671:
), Advances in Knowledge Dis- covery and Data Mining . AAAI Press/MIT Press. Chen , S. F. and Goodman, J. (1996).

Token 21672:
An empirical study of smoothing techni ques for language model- ing. In ACL-96 , pp. 310–318. Cheng , J. and Druzdzel, M. J. (2000).

Token 21673:
AIS-BN: An adaptive importance sampling algorithm for eviden-tial reasoning in large Bayesian networks. JAIR ,13, 155–188.

Token 21674:
Cheng , J., Greiner, R., Kelly, J., Bell, D. A., and Liu, W. (2002). Learning Bayesian networks fromdata: An information-theory based approach.

Token 21675:
AIJ, 137, 43–90. Chklovski , T. and Gil, Y. (2005).

Token 21676:
Improving the design of intelligent acquisition interfaces for col-lecting world knowledge from web contributors. InProc.

Token 21677:
Third International Conference on Knowledge Capture (K-CAP) . Chomsky , N. (1956). Three models for the descrip- tion of language.

Token 21678:
IRE Transactions on Information Theory ,2(3), 113–124. Chomsky , N. (1957). Syntactic Structures . Mouton. Choset , H. (1996).

Token 21679:
Sensor Based Motion Planning: The Hierarchical Generalized Voronoi Graph .P h . D . thesis, California Institute of Technology.

Token 21680:
Choset , H., Lynch, K., Hutchinson, S., Kantor, G., Burgard, W., Kavraki, L., and Thrun, S. (2004).

Token 21681:
Principles of Robotic Motion: Theory, Algorithms, and Implementation . MIT Press. Chung , K. L. (1979).

Token 21682:
Elementary Probability Theory with Stochastic Processes (3rd edition). Springer-Verlag.Church , A. (1936). A note on the Entschei- dungsproblem.

Token 21683:
JSL,1, 40–41 and 101–102. Church , A. (1956). Introduction to Mathematical Logic . Princeton University Press. Church , K. and Patil, R. (1982).

Token 21684:
Coping with syn- tactic ambiguity or how to put the block in the box on the table. Computational Linguistics ,8(3–4), 139– 149. Church , K. (2004).

Token 21685:
Speech and language process- ing: Can we use the past to predict the future. In Proc. Conference on Text, Speech, and Dialogue .

Token 21686:
Church , K. and Gale, W. A. (1991).

Token 21687:
A comparison of the enhanced Good–Turing and deleted estima- tion methods for estimating probabilities of Englishbigrams.

Token 21688:
Computer Speech and Language ,5, 19–54. Churchland , P. M. and Churchland, P. S. (1982). Functionalism, qualia, and intentionality. In Biro,J.

Token 21689:
I. and Shahan, R. W. (Eds. ), Mind, Brain and Function: Essays in the Philosophy of Mind , pp. 121–145. University of Oklahoma Press.

Token 21690:
Churchland , P. S. (1986). Neurophilosophy: Toward a Uniﬁed Science of the Mind–Brain . MIT Press. Ciancarini , P. and Wooldridge, M. (2001).

Token 21691:
Agent- Oriented Software Engineering . Springer-Verlag. Cimatti , A., Roveri, M., and Traverso, P. (1998).

Token 21692:
Automatic OBDD-based generation of universal plans in non-deterministic domains. In AAAI-98 , pp. 875–881. Clark , A. (1998).

Token 21693:
Being There: Putting Brain, Body, and World Together Again . MIT Press. Clark , A. (2008).

Token 21694:
Supersizing the Mind: Embodi- ment, Action, and Cognitive Extension . Oxford Uni- versity Press. Clark , K. L. (1978). Negation as failure.

Token 21695:
In Gallaire, H. and Minker, J. (Eds. ), Logic and Data Bases , pp. 293–322. Plenum. Clark , P. and Niblett, T. (1989). The CN2 induction algorithm.

Token 21696:
Machine Learning ,3, 261–283. Clark , S. and Curran, J. R. (2004). Parsing the WSJ using CCG and log-linear models. In ACL-04 , pp. 104–111.

Token 21697:
Clarke , A. C. (1968a). 2001: A Space Odyssey . Signet. Clarke , A. C. (1968b). The world of 2001. Vogue. Clarke , E. and Grumberg, O. (1987).

Token 21698:
Research on automatic veriﬁcation of ﬁnite-state concurrent sys-tems. Annual Review of Computer Science ,2, 269– 290. Clarke , M. R. B. (Ed.). (1977).

Token 21699:
Advances in Com- puter Chess 1 . Edinburgh University Press. Clearwater , S. H. (Ed.). (1996). Market-Based Con- trol. World Scientiﬁc.

Token 21700:
Clocksin , W. F. and Mellish, C. S. (2003). Program- ming in Prolog (5th edition). Springer-Verlag. Clocksin , W. F. (2003).

Token 21701:
Clause and Effect: Pro- log Programming for the Working Programmer . Springer.

Token 21702:
Coarfa , C., Demopoulos, D., Aguirre, A., Subrama- nian, D., and Yardi, M. (2003). Random 3-SAT: Theplot thickens. Constraints ,8(3), 243–261.

Token 21703:
Coates , A., Abbeel, P., and Ng, A. Y. (2009). Ap- prenticeship learning for helicopter control. JACM , 52(7), 97–105. Cobham , A. (1964).

Token 21704:
The intrinsic computational difﬁculty of functions. In Proc. 1964 International Congress for Logic, Methodology, and Philosophy of Science , pp.

Token 21705:
24–30.

Token 21706:
Bibliography 1069 Cohen , P. R. (1995). Empirical methods for artiﬁ- cial intelligence . MIT Press. Cohen , P. R. and Levesque, H. J. (1990).

Token 21707:
Intention is choice with commitment. AIJ,42(2–3), 213–261. Cohen , P. R., Morgan, J., and Pollack, M. E. (1990). Intentions in Communication .

Token 21708:
MIT Press. Cohen , W. W. and Page, C. D. (1995). Learnabil- ity in inductive logic programming: Methods and results.

Token 21709:
New Generation Computing ,13(3–4), 369– 409. Cohn , A. G., Bennett, B., Gooday, J. M., and Gotts, N. (1997).

Token 21710:
RCC: A calculus for region based qualita- tive spatial reasoning. GeoInformatica ,1, 275–316. Collin , Z., Dechter, R., and Katz, S. (1999).

Token 21711:
Self-stabilizing distributed constraint satisfaction.Chicago Journal of Theoretical Computer Science , 1999 (115).

Token 21712:
Collins , F. S., Morgan, M., and Patrinos, A. (2003). The human genome project: Lessons from large- scale biology. Science ,300(5617), 286–290.

Token 21713:
Collins , M. (1999). Head-driven Statistical Models for Natural Language Processing . Ph.D. thesis, Uni- versity of Pennsylvania.

Token 21714:
Collins , M. and Duffy, K. (2002). New ranking algo- rithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.

Token 21715:
In ACL-02 . Colmerauer , A. and Roussel, P. (1993). The birth of Prolog. SIGPLAN Notices ,28(3), 37–52. Colmerauer , A. (1975).

Token 21716:
Les grammaires de meta- morphose. Tech. rep., Groupe d’Intelligence Artiﬁ- cielle, Universit´ e de Marseille-Luminy.

Token 21717:
Colmerauer , A., Kanoui, H., Pasero, R., and Roussel, P. (1973). Un syst´ eme de communi- cation homme–machine en Franc ¸ais.

Token 21718:
Rapport, Groupe d’Intelligence Artiﬁcielle, Universit´ ed ’ A i x - Marseille II. Condon , J. H. and Thompson, K. (1982). Belle chess hardware.

Token 21719:
In Clarke, M. R. B. (Ed. ), Advances in Computer Chess 3 , pp. 45–54. Pergamon.

Token 21720:
Congdon , C. B., Huber, M., Kortenkamp, D., Bid- lack, C., Cohen, C., Huffman, S., Koss, F., Raschke, U., and Weymouth, T. (1992).

Token 21721:
CARMEL versusFlakey: A comparison of two robots. Tech.

Token 21722:
rep. Pa-pers from the AAAI Robot Competition, RC-92-01, American Association for Artiﬁcial Intelligence. Conlisk , J. (1989).

Token 21723:
Three variants on the Allais ex- ample. American Economic Review ,79(3), 392–407. Connell , J. (1989).

Token 21724:
A Colony Architecture for an Ar- tiﬁcial Creature . Ph.D. thesis, Artiﬁcial Intelligence Laboratory, MIT.

Token 21725:
Also available as AI Technical Re- port 1151. Consortium , T. G. O. (2008). The gene ontology project in 2008. Nucleic Acids Research ,36.

Token 21726:
Cook , S. A. (1971). The complexity of theorem- proving procedures. In STOC-71 , pp. 151–158. Cook , S. A. and Mitchell, D. (1997).

Token 21727:
Finding hard instances of the satisﬁability problem: A survey. In Du, D., Gu, J., and Pardalos, P. (Eds.

Token 21728:
), Satisﬁabil- ity problems: Theory and applications . American Mathematical Society. Cooper , G. (1990).

Token 21729:
The computational complexity of probabilistic inference using Bayesian belief net- works. AIJ,42, 393–405. Cooper , G. and Herskovits, E. (1992).

Token 21730:
A Bayesian method for the induction of probabilistic networks from data. Machine Learning ,9, 309–347. Copeland , J. (1993).

Token 21731:
Artiﬁcial Intelligence: A Philosophical Introduction . Blackwell.Copernicus ( 1543). De Revolutionibus Orbium Coelestium . Apud Ioh.

Token 21732:
Petreium, Nuremberg. Cormen , T. H., Leiserson, C. E., and Rivest, R. (1990). Introduction to Algorithms . MIT Press.

Token 21733:
Cortes , C. and Vapnik, V. N. (1995). Support vector networks. Machine Learning ,20, 273–297. Cournot , A. (Ed.). (1838).

Token 21734:
Recherches sur les principes math´ ematiques de la th´ eorie des richesses . L. Hachette, Paris. Cover , T. and Thomas, J. (2006).

Token 21735:
Elements of Infor- mation Theory (2nd edition). Wiley. Cowan , J. D. and Sharp, D. H. (1988a). Neural nets.

Token 21736:
Quarterly Reviews of Biophysics ,21, 365–427. Cowan , J. D. and Sharp, D. H. (1988b). Neural nets and artiﬁcial intelligence. Daedalus ,117, 85–121.

Token 21737:
Cowell , R., Dawid, A. P., Lauritzen, S., and Spiegel- halter, D. J. (2002). Probabilistic Networks and Ex- pert Systems . Springer. Cox, I. (1993).

Token 21738:
A review of statistical data associ- ation techniques for motion correspondence. IJCV , 10, 53–66. Cox, I. and Hingorani, S. L. (1994).

Token 21739:
An efﬁcient im- plementation and evaluation of Reid’s multiple hy-pothesis tracking algorithm for visual tracking. InICPR-94 , Vol. 1, pp. 437–442.

Token 21740:
Cox, I. and Wilfong, G. T. (Eds.). (1990). Au- tonomous Robot Vehicles . Springer Verlag. Cox, R. T. (1946).

Token 21741:
Probability, frequency, and rea- sonable expectation. American Journal of Physics , 14(1), 1–13. Craig , J. (1989).

Token 21742:
Introduction to Robotics: Mechan- ics and Control (2nd edition) . Addison-Wesley Pub- lishing, Inc. Craik , K. J. (1943). The Nature of Explanation .

Token 21743:
Cambridge University Press. Craswell , N., Zaragoza, H., and Robertson, S. E. (2005). Microsoft cambridge at trec-14: Enterprise track. In Proc.

Token 21744:
Fourteenth Text REtrieval Confer- ence. Crauser , A., Mehlhorn, K., Meyer, U., and Sanders, P. (1998).

Token 21745:
A parallelization of Dijkstra’s shortestpath algorithm. In Proc. 23rd International Sym- posium on Mathematical Foundations of Computer Science, , pp.

Token 21746:
722–731. Craven , M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T. M., Nigam, K., and Slattery, S. (2000).

Token 21747:
Learning to construct knowledge bases from the World Wide Web. AIJ,118(1/2), 69–113. Crawford , J. M. and Auton, L. D. (1993).

Token 21748:
Experi- mental results on the crossover point in satisﬁabilityproblems. In AAAI-93 , pp. 21–27. Cristianini , N. and Hahn, M. (2007).

Token 21749:
Introduction to Computational Genomics: A Case Studies Ap-proach . Cambridge University Press. Cristianini ,N .a n dS c h ¨ olkopf, B. (2002).

Token 21750:
Support vector machines and kernel methods: The new gen-eration of learning machines. AIMag ,23(3), 31–41. Cristianini , N. and Shawe-Taylor, J.

Token 21751:
(2000). An introduction to support vector machines and other kernel-based learning methods . Cambridge Univer- sity Press. Crockett , L. (1994).

Token 21752:
The Turing Test and the Frame Problem: AI’s Mistaken Understanding of Intelli- gence .A b l e x . Croft , B., Metzler, D., and Stroham, T. (2009).

Token 21753:
Search Engines: Information retrieval in Practice . Addison Wesley.Cross , S. E. and Walker, E. (1994).

Token 21754:
DART: Apply- ing knowledge based planning and scheduling to cri- sis action planning. In Zweben, M. and Fox, M. S.(Eds.

Token 21755:
), Intelligent Scheduling , pp. 711–729. Morgan Kaufmann. Cruse , D. A. (1986). Lexical Semantics . Cambridge University Press.

Token 21756:
Culberson , J. and Schaeffer, J. (1996). Searching with pattern databases.

Token 21757:
In Advances in Artiﬁcial Intelligence (Lecture Notes in Artiﬁcial Intelligence 1081) , pp. 402–416. Springer-Verlag. Culberson , J. and Schaeffer, J.

Token 21758:
(1998). Pattern databases. Computational Intelligence ,14(4), 318– 334. Cullingford , R. E. (1981).

Token 21759:
Integrating knowl- edge sources for computer “understanding” tasks. IEEE Transactions on Systems, Man and Cybernet- ics (SMC) ,11.

Token 21760:
Cummins , D. and Allen, C. (1998). The Evolution of Mind . Oxford University Press. Cushing , W., Kambhampati, S., Mausam, and Weld, D. S. (2007).

Token 21761:
When is temporal planning really tem- poral? In IJCAI-07 . Cybenko , G. (1988).

Token 21762:
Continuous valued neural net- works with two hidden layers are sufﬁcient. Techni-cal report, Department of Computer Science, TuftsUniversity.

Token 21763:
Cybenko , G. (1989). Approximation by superposi- tions of a sigmoidal function. Mathematics of Con- trols, Signals, and Systems ,2, 303–314.

Token 21764:
Daganzo , C. (1979). Multinomial probit: The theory and its application to demand forecasting . Academic Press. Dagum , P. and Luby, M. (1993).

Token 21765:
Approximating probabilistic inference in Bayesian belief networksis NP-hard. AIJ,60(1), 141–153. Dalal , N. and Triggs, B. (2005).

Token 21766:
Histograms of ori- ented gradients for human detection. In CVPR , pp. 886–893. Dantzig , G. B. (1949). Programming of interdepen- dent activities: II.

Token 21767:
Mathematical model. Economet- rica,17, 200–211. Darwiche , A. (2001). Recursive conditioning. AIJ, 126, 5–41.

Token 21768:
Darwiche , A. and Ginsberg, M. L. (1992). A sym- bolic generalization of probability theory. In AAAI- 92, pp. 622–627. Darwiche , A. (2009).

Token 21769:
Modeling and reasoning with Bayesian networks . Cambridge University Press. Darwin , C. (1859).

Token 21770:
On The Origin of Species by Means of Natural Selection . J. Murray, London. Darwin , C. (1871). D e s c e n to fM a n . J. Murray.

Token 21771:
Dasgupta , P., Chakrabarti, P. P., and de Sarkar, S. C. (1994). Agent searching in a tree and the optimality of iterative deepening. AIJ,71, 195–208.

Token 21772:
Davidson , D. (1980). Essays on Actions and Events . Oxford University Press. Davies , T. R. (1985). Analogy.

Token 21773:
Informal note IN- CSLI-85-4, Center for the Study of Language and Information (CSLI). Davies , T. R. and Russell, S. J. (1987).

Token 21774:
A logical ap- proach to reasoning by analogy. In IJCAI-87 ,V o l .1 , pp. 264–270. Davis , E. (1986).

Token 21775:
Representing and Acquiring Geo- graphic Knowledge . Pitman and Morgan Kaufmann. Davis , E. (1990). Representations of Commonsense Knowledge .

Token 21776:
Morgan Kaufmann.

Token 21777:
1070 Bibliography Davis , E. (2005). Knowledge and communication: A ﬁrst-order theory. AIJ,166, 81–140. Davis , E. (2006).

Token 21778:
The expressivity of quantifying over regions. J. Logic and Computation ,16, 891– 916. Davis , E. (2007). Physical reasoning.

Token 21779:
In van Harme- lan, F., Lifschitz, V., and Porter, B. (Eds. ), The Hand- book of Knowledge Representation , pp. 597–620. El- sevier. Davis , E. (2008).

Token 21780:
Pouring liquids: A study in com- monsense physical reasoning. AIJ,172(1540–1578). Davis , E. and Morgenstern, L. (2004).

Token 21781:
Introduction: Progress in formal commonsense reasoning. AIJ, 153, 1–12. Davis , E. and Morgenstern, L. (2005).

Token 21782:
A ﬁrst-order theory of communication and multi-agent plans. J. Logic and Computation ,15(5), 701–749.

Token 21783:
Davis , K. H., Biddulph, R., and Balashek, S. (1952). Automatic recognition of spoken digits. J. Acousti- cal Society of America ,24(6), 637–642.

Token 21784:
Davis , M. (1957). A computer program for Pres- burger’s algorithm. In Proving Theorems (as Done by Man, Logician, or Machine) , pp. 215–233. Proc.

Token 21785:
Summer Institute for Symbolic Logic. Second edi-tion; publication date is 1960. Davis , M., Logemann, G., and Loveland, D. (1962).

Token 21786:
A machine program for theorem-proving. CACM ,5, 394–397. Davis , M. and Putnam, H. (1960). A computing pro- cedure for quantiﬁcation theory.

Token 21787:
JACM ,7(3), 201– 215. Davis , R. and Lenat, D. B. (1982). Knowledge- Based Systems in Artiﬁcial Intelligence .M c G r a w - Hill. Dayan , P. (1992).

Token 21788:
The convergence of TD( λ)f o r general λ.Machine Learning ,8(3–4), 341–362. Dayan , P. and Abbott, L. F. (2001).

Token 21789:
Theoretical Neu- roscience: Computational and Mathematical Mod- eling of Neural Systems . MIT Press. Dayan , P. and Niv, Y. (2008).

Token 21790:
Reinforcement learn- ing and the brain: The good, the bad and the ugly. Current Opinion in Neurobiology ,18(2), 185–196.

Token 21791:
de Dombal , F. T., Leaper, D. J., Horrocks, J. C., and Staniland, J. R. (1974).

Token 21792:
Human and computer- aided diagnosis of abdominal pain: Further report with emphasis on performance of clinicians. British Medical Journal ,1, 376–380.

Token 21793:
de Dombal , F. T., Staniland, J. R., and Clamp, S. E. (1981). Geographical variation in disease presenta- tion. Medical Decision Making ,1, 59–69.

Token 21794:
de Finetti , B. (1937). Le pr´ evision: ses lois logiques, ses sources subjectives. Ann. Inst. Poincar ´ e,7, 1–68. de Finetti , B. (1993).

Token 21795:
On the subjective meaning of probability. In Monari, P. and Cocchi, D. (Eds. ), Probabilita e Induzione , pp. 291–321. Clueb.

Token 21796:
de Freitas , J. F. G., Niranjan, M., and Gee, A. H. (2000). Sequential Monte Carlo methods to train neural network models.

Token 21797:
Neural Computation ,12(4), 933–953. de Kleer , J. (1975). Qualitative and quantitative knowledge in classical mechanics. Tech.

Token 21798:
rep. AI- TR-352, MIT Artiﬁcial Intelligence Laboratory. de Kleer , J. (1989). A comparison of ATMS and CSP techniques. In IJCAI-89 , Vol. 1, pp.

Token 21799:
290–296. de Kleer , J. and Brown, J. S. (1985). A qualitative physics based on conﬂuences. In Hobbs, J. R. and Moore, R. C. (Eds.

Token 21800:
), Formal Theories of the Com- monsense World , chap. 4, pp. 109–183. Ablex.de Marcken , C. (1996). Unsupervised Language Acquisition .

Token 21801:
Ph.D. thesis, MIT. De Morgan , A. (1864). On the syllogism, No. IV, and on the logic of relations.

Token 21802:
Transaction of the Cambridge Philosophical Society ,X, 331–358. De Raedt , L. (1992).

Token 21803:
Interactive Theory Revision: An Inductive Logic Programming Approach . Aca- demic Press. de Salvo Braz , R., Amir, E., and Roth, D. (2007).

Token 21804:
Lifted ﬁrst-order probabilistic inference. In Getoor, L. and Taskar, B. (Eds. ), Introduction to Statistical Relational Learning . MIT Press.

Token 21805:
Deacon , T. W. (1997). The symbolic species: The co-evolution of language and the brain . W .W .N o r - ton.

Token 21806:
Deale , M., Yvanovich, M., Schnitzius, D., Kautz, D., Carpenter, M., Zweben, M., Davis, G., and Daun, B. (1994).

Token 21807:
The space shuttle ground processingscheduling system. In Zweben, M. and Fox, M.(Eds. ), Intelligent Scheduling , pp. 423–449. Morgan Kaufmann.

Token 21808:
Dean , T., Basye, K., Chekaluk, R., and Hyun, S. (1990). Coping with uncertainty in a control system for navigation and exploration.

Token 21809:
In AAAI-90 ,V o l .2 , pp. 1010–1015. Dean , T. and Boddy, M. (1988). An analysis of time- dependent planning. In AAAI-88 , pp. 49–54.

Token 21810:
Dean , T., Firby, R. J., and Miller, D. (1990). Hierar- chical planning involving deadlines, travel time, and resources.

Token 21811:
Computational Intelligence ,6(1), 381– 398. Dean , T., Kaelbling, L. P., Kirman, J., and Nichol- son, A. (1993).

Token 21812:
Planning with deadlines in stochastic domains. In AAAI-93 , pp. 574–579. Dean , T. and Kanazawa, K. (1989a). A model for projection and action.

Token 21813:
In IJCAI-89 , pp. 985–990. Dean , T. and Kanazawa, K. (1989b). A model for reasoning about persistence and causation.

Token 21814:
Compu- tational Intelligence ,5(3), 142–150. Dean , T., Kanazawa, K., and Shewchuk, J. (1990).

Token 21815:
Prediction, observation and estimation in planning and control. In 5th IEEE International Symposium on Intelligent Control , Vol. 2, pp. 645–650.

Token 21816:
Dean , T. and Wellman, M. P. (1991). Planning and Control . Morgan Kaufmann. Dearden , R., Friedman, N., and Andre, D. (1999).

Token 21817:
Model-based Bayesian exploration. In UAI-99 . Dearden , R., Friedman, N., and Russell, S. J. (1998). Bayesian q-learning. In AAAI-98 .

Token 21818:
Debevec , P., Taylor, C., and Malik, J. (1996). Mod- eling and rendering architecture from photographs:A hybrid geometry- and image-based approach.

Token 21819:
In Proc. 23rd Annual Conference on Computer Graph- ics (SIGGRAPH) , pp. 11–20. Debreu , G. (1960). Topological methods in cardinal utility theory.

Token 21820:
In Arrow, K. J., Karlin, S., and Sup- pes, P. (Eds. ), Mathematical Methods in the Social Sciences, 1959 . Stanford University Press.

Token 21821:
Dechter , R. (1990a). Enhancement schemes for con- straint processing: Backjumping, learning and cutset decomposition. AIJ,41, 273–312.

Token 21822:
Dechter , R. (1990b). On the expressiveness of net- works with hidden variables. In AAAI-90 , pp. 379– 385. Dechter , R. (1992). Constraint networks.

Token 21823:
In Shapiro, S. (Ed. ), Encyclopedia of Artiﬁcial Intelli- gence (2nd edition)., pp. 276–285. Wiley and Sons.Dechter , R. (1999).

Token 21824:
Bucket elimination: A unifying framework for reasoning. AIJ,113 , 41–85. Dechter , R. and Pearl, J. (1985).

Token 21825:
Generalized best-ﬁrst search strategies and the optimality of A*. JACM ,32(3), 505–536. Dechter , R. and Pearl, J. (1987).

Token 21826:
Network-based heuristics for constraint -satisfaction problems. AIJ, 34(1), 1–38. Dechter , R. and Pearl, J. (1989).

Token 21827:
Tree clustering for constraint networks. AIJ,38(3), 353–366. Dechter , R. (2003). Constraint Processing . Morgan Kaufmann.

Token 21828:
Dechter , R. and Frost, D. (2002). Backjump-based backtracking for constraint satisfaction problems.AIJ,136(2), 147–188.

Token 21829:
Dechter , R. and Mateescu, R. (2007). AND/OR search spaces for graphical models. AIJ,171(2–3), 73–106. DeCoste ,D .a n dS c h ¨ olkopf, B. (2002).

Token 21830:
Training in- variant support vector machines. Machine Learning , 46(1), 161–190. Dedekind , R. (1888). Was sind und was sollen die Zahlen .

Token 21831:
Braunschweig, Germany. Deerwester , S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., and Harshman, R. A. (1990).

Token 21832:
Index- ing by latent semantic analysis. J. American Society for Information Science ,41(6), 391–407. DeGroot , M. H. (1970).

Token 21833:
Optimal Statistical Deci- sions . McGraw-Hill. DeGroot , M. H. and Schervish, M. J. (2001). Prob- ability and Statistics (3rd edition).

Token 21834:
Addison Wesley. DeJong , G. (1981). Generalizations based on expla- nations. In IJCAI-81 , pp. 67–69. DeJong , G. (1982).

Token 21835:
An overview of the FRUMP system. In Lehnert, W. and Ringle, M. (Eds. ), Strate- gies for Natural Language Processing , pp. 149–176. Lawrence Erlbaum.

Token 21836:
DeJong , G. and Mooney, R. (1986). Explanation- based learning: An alternative view. Machine Learn- ing,1, 145–176.

Token 21837:
Del Moral , P., Doucet, A., and Jasra, A. (2006). Se- quential Monte Carlo samplers. J. Royal Statistical Society, Series B ,68 (3), 411–436.

Token 21838:
Del Moral , P. (2004). Feynman–Kac Formulae, Ge- nealogical and Interacting Particle Systems with Ap- plications . Springer-Verlag.

Token 21839:
Delgrande , J. and Schaub, T. (2003). On the relation between Reiter’s default logic and its (major) vari- ants.

Token 21840:
In Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Un-certainty , pp. 452–463. Dempster , A. P. (1968).

Token 21841:
A generalization of Bayesian inference. J. Royal Statistical Society , 30 (Series B) , 205–247. Dempster , A. P., Laird, N., and Rubin, D. (1977).

Token 21842:
Maximum likelihood from incomplete data via the EM algorithm. J. Royal Statistical Society ,39 (Se- ries B) , 1–38.

Token 21843:
Deng , X. and Papadimitriou, C. H. (1990). Explor- ing an unknown graph. In FOCS-90 , pp. 355–361. Denis , F. (2001).

Token 21844:
Learning regular languages from simple positive examples. Machine Learning , 44(1/2), 37–66. Dennett , D. C. (1984).

Token 21845:
Cognitive wheels: the frame problem of AI. In Hookway, C. (Ed. ), Minds, Ma- chines, and Evolution: Philosophical Studies , pp. 129–151.

Token 21846:
Cambridge University Press. Dennett , D. C. (1991). Consciousness Explained . Penguin Press.

Token 21847:
Bibliography 1071 Denney , E., Fischer, B., and Schumann, J. (2006). An empirical evaluation of automated theorem provers in software certiﬁcation.

Token 21848:
Int. J. AI Tools , 15(1), 81–107. Descartes , R. (1637). Discourse on method. In Cot- tingham, J., Stoothoff, R., and Murdoch, D. (Eds.

Token 21849:
), The Philosophical Writings of Descartes ,V o l .I . Cambridge University Press, Cambridge, UK. Descartes , R. (1641).

Token 21850:
Meditations on ﬁrst philoso- phy. In Cottingham, J., Stoothoff, R., and Murdoch,D. (Eds. ), The Philosophical Writings of Descartes , Vol. II.

Token 21851:
Cambridge University Press, Cambridge, UK. Descotte , Y. and Latombe, J.-C. (1985). Mak- ing compromises among antagonist constraints in aplanner.

Token 21852:
AIJ,27, 183–217. Detwarasiti , A. and Shachter, R. D. (2005). Inﬂu- ence diagrams for team decision analysis. Decision Analysis ,2(4), 207–228.

Token 21853:
Devroye , L. (1987). A course in density estimation . Birkhauser. Dickmanns , E. D. and Zapp, A. (1987).

Token 21854:
Au- tonomous high speed road vehicle guidance bycomputer vision.

Token 21855:
In Automatic Control—World Congress, 1987: Selected Papers from the 10th Tri- ennial World Congress of the International Federa- tion of Automatic Control , pp.

Token 21856:
221–226. Dietterich , T. (1990). Machine learning. Annual Review of Computer Science ,4, 255–306. Dietterich , T. (2000).

Token 21857:
Hierarchical reinforcement learning with the MAXQ value function decompo- sition. JAIR ,13, 227–303. Dijkstra , E. W. (1959).

Token 21858:
A note on two problems in connexion with graphs. Numerische Mathematik ,1, 269–271. Dijkstra , E. W. (1984). The threats to computing science.

Token 21859:
In ACM South Central Regional Confer- ence. Dillenburg , J. F. and Nelson, P. C. (1994). Perimeter search. AIJ,65(1), 165–178.

Token 21860:
Dinh , H., Russell, A., and Su, Y. (2007). On the value of good advice: The complexity of A* with accurate heuristics. In AAAI-07 .

Token 21861:
Dissanayake , G., Newman, P., Clark, S., Durrant- Whyte, H., and Csorba, M. (2001).

Token 21862:
A solution to thesimultaneous localisation and map building (SLAM)problem. IEEE Transactions on Robotics and Au- tomation ,17(3), 229–241.

Token 21863:
Do, M. B. and Kambhampati, S. (2001). Sapa: A domain-independent heuris tic metric temporal plan- ner. In ECP-01 .

Token 21864:
Do, M. B. and Kambhampati, S. (2003). Planning as constraint satisfaction: solving the planning graphby compiling it into CSP. AIJ,132(2), 151–182.

Token 21865:
Doctorow , C. (2001). Metacrap: Putting the torch to seven straw-men of the meta-utopia. www.well. com/˜doctorow/metacrap.htm .

Token 21866:
Domingos , P. and Pazzani, M. (1997). On the opti- mality of the simple Bayesian classiﬁer under zero–one loss. Machine Learning ,29, 103–30.

Token 21867:
Domingos , P. and Richardson, M. (2004). Markov logic: A unifying framework for statistical relational learning. In Proc.

Token 21868:
ICML-04 Workshop on Statistical Relational Learning . Donninger , C. and Lorenz, U. (2004). The chess monster hydra. In Proc.

Token 21869:
14th International Con- ference on Field-Programmable Logic and Applica- tions , pp. 927–932.Doorenbos , R. (1994).

Token 21870:
Combining left and right un- linking for matching a large number of learned rules. InAAAI-94 . Doran , J. and Michie, D. (1966).

Token 21871:
Experiments with the graph traverser program. Proc. Royal Society of London ,294, Series A , 235–259. Dorf , R. C. and Bishop, R. H. (2004).

Token 21872:
Modern Con- trol Systems (10th edition). Prentice-Hall. Doucet , A. (1997).

Token 21873:
Monte Carlo methods for Bayesian estimation of hidden Markov models: Ap- plication to radiation signals . Ph.D. thesis, Univer- sit´e de Paris-Sud.

Token 21874:
Doucet , A., de Freitas, N., and Gordon, N. (2001). Sequential Monte Carlo Methods in Prac- tice. Springer-Verlag.

Token 21875:
Doucet , A., de Freitas, N., Murphy, K., and Russell, S. J. (2000). Rao-blackwellised particle ﬁltering for dynamic bayesian networks. In UAI-00 .

Token 21876:
Dowling , W. F. and Gallier, J. H. (1984). Linear- time algorithms for testing the satisﬁability of propo- sitional Horn formulas. J.

Token 21877:
Logic Programming ,1, 267–284. Dowty , D., Wall, R., and Peters, S. (1991). Intro- duction to Montague Semantics .D .R e i d e l . Doyle , J. (1979).

Token 21878:
A truth maintenance system. AIJ, 12(3), 231–272. Doyle , J. (1983). What is rational psychology? To- ward a modern mental philosophy.

Token 21879:
AIMag ,4(3), 50– 53. Doyle , J. and Patil, R. (1991).

Token 21880:
Two theses of knowl- edge representation: Language restrictions, taxo- nomic classiﬁcation, and the utility of representation services.

Token 21881:
AIJ,48(3), 261–297. Drabble , B. (1990). Mission scheduling for space- craft: Diaries of T-SCHED .I n Expert Planning Sys- tems, pp. 76–81.

Token 21882:
Institute of Electrical Engineers. Dredze , M., Crammer, K., and Pereira, F. (2008). Conﬁdence-weighted linear classiﬁcation. In ICML- 08, pp.

Token 21883:
264–271. Dreyfus , H. L. (1972). What Computers Can’t Do: A Critique of Artiﬁcial Reason . Harper and Row. Dreyfus , H. L. (1992).

Token 21884:
What Computers Still Can’t Do: A Critique of Artiﬁcial Reason . MIT Press. Dreyfus , H. L. and Dreyfus, S. E. (1986).

Token 21885:
Mind over Machine: The Power of Human Intuition and Exper-tise in the Era of the Computer . Blackwell. Dreyfus , S. E. (1969).

Token 21886:
An appraisal of some shortest-paths algorithms. Operations Research ,17, 395–412. Dubois , D. and Prade, H. (1994).

Token 21887:
A survey of belief revision and updating rules in various uncertainty models. Int. J. Intelligent Systems ,9(1), 61–100.

Token 21888:
Duda , R. O., Gaschnig, J., and Hart, P. E. (1979). Model design in the Prospector consultant systemfor mineral exploration. In Michie, D. (Ed.

Token 21889:
), Ex- pert Systems in the Microelectronic Age , pp. 153– 167. Edinburgh University Press. Duda , R. O. and Hart, P. E. (1973).

Token 21890:
Pattern classiﬁ- cation and scene analysis . Wiley. Duda , R. O., Hart, P. E., and Stork, D. G. (2001). Pattern Classiﬁcation (2nd edition). Wiley.

Token 21891:
Dudek , G. and Jenkin, M. (2000). Computational Principles of Mobile Robotics . Cambridge Univer- sity Press. Duffy , D. (1991).

Token 21892:
Principles of Automated Theorem Proving . John Wiley & Sons.Dunn , H. L. (1946). Record linkage”. A m .J .P u b l i c Health ,36(12), 1412–1416.

Token 21893:
Durfee , E. H. and Lesser, V. R. (1989). Negotiat- ing task decomposition and allocation using partialglobal planning. In Huhns, M. and Gasser, L.

Token 21894:
(Eds. ), Distributed AI , Vol. 2. Morgan Kaufmann. Durme , B. V. and Pasca, M. (2008).

Token 21895:
Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In AAAI-08 , pp.

Token 21896:
1243–1248. Dyer , M. (1983). In-Depth Understanding . MIT Press. Dyson , G. (1998). Darwin among the machines : the evolution of global intelligence .

Token 21897:
Perseus Books. Duzeroski , S., Muggleton, S. H., and Russell, S. J. (1992). PAC-learnability of determinate logic pro- grams. In COLT-92 , pp.

Token 21898:
128–135. Earley , J. (1970). An efﬁcient context-free parsing algorithm. CACM ,13(2), 94–102. Edelkamp , S. (2009).

Token 21899:
Scaling search with symbolic pattern databases. In Model Checking and Artiﬁcial Intelligence (MOCHART) , pp. 49–65. Edmonds , J. (1965).

Token 21900:
Paths, trees, and ﬂowers. Canadian Journal of Mathematics ,17, 449–467. Edwards , P. (Ed.). (1967). The Encyclopedia of Phi- losophy . Macmillan.

Token 21901:
Een,N .a n dS ¨ orensson, N. (2003). An extensi- ble SAT-solver. In Giunchiglia, E. and Tacchella,A. (Eds.

Token 21902:
), Theory and Applications of Satisﬁability Testing: 6th International Conference (SAT 2003) . Springer-Verlag.

Token 21903:
Eiter , T., Leone, N., Mateis, C., Pfeifer, G., and Scarcello, F. (1998). The KR system dlv: Progressreport, comparisons and benchmarks.

Token 21904:
In KR-98 , pp. 406–417. Elio, R. (Ed.). (2002). Common Sense, Reasoning, and Rationality . Oxford University Press. Elkan , C. (1993).

Token 21905:
The paradoxical success of fuzzy logic. In AAAI-93 , pp. 698–703. Elkan , C. (1997). Boosting and naive Bayesian learning. Tech.

Token 21906:
rep., Department of Computer Sci-ence and Engineering, University of California, SanDiego. Ellsberg , D. (1962). Risk, Ambiguity, and Decision .

Token 21907:
Ph.D. thesis, Harvard University. Elman , J., Bates, E., Johnson, M., Karmiloff-Smith, A., Parisi, D., and Plunkett, K. (1997).

Token 21908:
Rethinking Innateness . MIT Press. Empson , W. (1953). Seven Types of Ambiguity .N e w Directions. Enderton , H. B. (1972).

Token 21909:
A Mathematical Introduc- tion to Logic . Academic Press. Epstein , R., Roberts, G., and Beber, G. (Eds.). (2008). Parsing the Turing Test . Springer.

Token 21910:
Erdmann , M. A. and Mason, M. (1988). An explo- ration of sensorless manipulation. IEEE Journal of Robotics and Automation ,4(4), 369–379.

Token 21911:
Ernst , H. A. (1961). MH-1, a Computer-Operated Mechanical Hand . Ph.D. thesis, Massachusetts In- stitute of Technology.

Token 21912:
Ernst , M., Millstein, T., and Weld, D. S. (1997). Au- tomatic SAT-compilation of planning problems. InIJCAI-97 , pp. 1169–1176.

Token 21913:
Erol , K., Hendler, J., and Nau, D. S. (1994). HTN planning: Complexity and expressivity. In AAAI-94 , pp. 1123–1128.

Token 21914:
1072 Bibliography Erol , K., Hendler, J., and Nau, D. S. (1996). Com- plexity results for HTN planning. AIJ,18(1), 69–93. Etzioni , A. (2004).

Token 21915:
From Empire to Community: A New Approach to International Relation .P a l g r a v e Macmillan. Etzioni , O. (1989).

Token 21916:
Tractable decision-analytic con- trol. In Proc. First International Conference on Knowledge Representation and Reasoning , pp. 114– 125.

Token 21917:
Etzioni , O., Banko, M., Soderland, S., and Weld, D. S. (2008). Open information extraction from the web. CACM ,51(12).

Token 21918:
Etzioni , O., Hanks, S., Weld, D. S., Draper, D., Lesh, N., and Williamson, M. (1992). An approach to planning with incomplete information. In KR-92 .

Token 21919:
Etzioni , O. and Weld, D. S. (1994). A softbot-based interface to the Internet. CACM ,37(7), 72–76. Etzioni , O., Banko, M., and Cafarella, M. J.

Token 21920:
(2006). Machine reading. In AAAI-06 . Etzioni , O., Cafarella, M. J., Downey, D., Popescu, A.-M., Shaked, T., Soderland, S., Weld, D. S., andYates, A.

Token 21921:
(2005). Unsupervised named-entity ex- traction from the web: An experimental study. AIJ, 165(1), 91–134. Evans , T. G. (1968).

Token 21922:
A program for the solution of a class of geometric-analogy intelligence-test ques- tions. In Minsky, M. L. (Ed.

Token 21923:
), Semantic Information Processing , pp. 271–353. MIT Press. Fagin , R., Halpern, J. Y., Moses, Y., and Vardi, M. Y. (1995).

Token 21924:
Reasoning about Knowledge . MIT Press. Fahlman , S. E. (1974). A planning system for robot construction tasks. AIJ,5(1), 1–49. Faugeras , O. (1993).

Token 21925:
Three-Dimensional Computer Vision: A Geometric Viewpoint . MIT Press. Faugeras , O., Luong, Q.-T., and Papadopoulo, T. (2001).

Token 21926:
The Geometry of Multiple Images .M I T Press. Fearing , R. S. and Hollerbach, J. M. (1985). Basic solid mechanics for tactile sensing. Int. J.

Token 21927:
Robotics Research ,4(3), 40–54. Featherstone , R. (1987). Robot Dynamics Algo- rithms . Kluwer Academic Publishers. Feigenbaum , E. A. (1961).

Token 21928:
The simulation of ver- bal learning behavior. Proc. Western Joint Computer Conference ,19, 121–131. Feigenbaum , E. A., Buchanan, B .

Token 21929:
G., and Leder- berg, J. (1971). On generality and problem solv-ing: A case study using the DENDRAL program. In Meltzer, B. and Michie, D. (Eds.

Token 21930:
), Machine Intel- ligence 6 , pp. 165–190. Edinburgh University Press. Feldman , J. and Sproull, R. F. (1977).

Token 21931:
Decision the- ory and artiﬁcial intelligence II: The hungry mon-key. Technical report, Computer Science Depart-ment, University of Rochester.

Token 21932:
Feldman , J. and Yakimovsky, Y. (1974). Decision theory and artiﬁcial intelligence I: Semantics-based region analyzer. AIJ,5(4), 349–371.

Token 21933:
Fellbaum , C. (2001). Wordnet: An Electronic Lexi- cal Database . MIT Press. Fellegi , I. and Sunter, A. (1969). A theory for record linkage”.

Token 21934:
JASA ,64, 1183–1210. Felner , A., Korf, R. E., and Hanan, S. (2004). Addi- tive pattern database heuristics. JAIR ,22, 279–318.

Token 21935:
Felner , A., Korf, R. E., Meshulam, R., and Holte, R. (2007). Compressed p attern databases.

Token 21936:
JAIR ,30, 213–247.Felzenszwalb , P. and Huttenlocher, D. (2000). Efﬁ- cient matching of pictorial structures. In CVPR .

Token 21937:
Felzenszwalb , P. and McAllester, D. A. (2007). The generalized A* architecture. JAIR . Ferguson , T. (1992).

Token 21938:
Mate with knight and bishop in kriegspiel. Theoretical Computer Science ,96(2), 389–403. Ferguson , T. (1995).

Token 21939:
Mate with the two bishops in kriegspiel. www.math.ucla.edu/˜tom/papers. Ferguson , T. (1973). Bayesian analysis of some nonparametric problems.

Token 21940:
Annals of Statistics ,1(2), 209–230. Ferraris , P. and Giunchiglia, E. (2000). Planning as satisability in nondeterministic domains. In AAAI- 00, pp.

Token 21941:
748–753. Ferriss , T. (2007). The 4-Hour Workweek .C r o w n . Fikes , R. E., Hart, P. E., and Nilsson, N. J. (1972).

Token 21942:
Learning and executing generalized robot plans. AIJ, 3(4), 251–288. Fikes , R. E. and Nilsson, N. J. (1971).

Token 21943:
STRIPS: A new approach to the application of theorem proving to problem solving. AIJ,2(3–4), 189–208. Fikes , R. E. and Nilsson, N. J. (1993).

Token 21944:
STRIPS, a retrospective. AIJ,59(1–2), 227–232. Fine , S., Singer, Y., and Tishby, N. (1998).

Token 21945:
The hier- archical hidden markov model: Analysis and appli- cations. Machine Learning ,32(41–62). Finney , D. J. (1947).

Token 21946:
Probit analysis: A statistical treatment of the sigmoid response curve . Cambridge University Press. Firth , J. (1957). Papers in Linguistics .

Token 21947:
Oxford Uni- versity Press. Fisher , R. A. (1922). On the mathematical founda- tions of theoretical statistics.

Token 21948:
Philosophical Transac- tions of the Royal Society of London ,Series A 222 , 309–368. Fix, E. and Hodges, J. L. (1951).

Token 21949:
Discrimina- tory analysis—Nonparametric discrimination: Con-sistency properties. Tech. rep. 21-49-004, USAF School of Aviation Medicine. Floreano ,D .

Token 21950:
,Z u f f e r e y ,J .C . ,S r i n i v a s a n ,M .V . ,a n d Ellington, C. (2009). Flying Insects and Robots . Springer. Fogel , D. B. (2000).

Token 21951:
Evolutionary Computation: Toward a New Philosophy of Machine Intelligence . IEEE Press. Fogel , L. J., Owens, A. J., and Walsh, M. J. (1966).

Token 21952:
Artiﬁcial Intelligence through Simulated Evolution . Wiley. Foo, N. (2001). Why engineering models do not have a frame problem.

Token 21953:
In Discrete event modeling and simulation technologies: a tapestry of systems and AI-based theories and methodologies . Springer. Forbes , J. (2002).

Token 21954:
Learning Optimal Control for Au- tonomous Vehicles . Ph.D. thesis, University of Cali- fornia. Forbus , K. D. (1985). Qualitative process theory.

Token 21955:
In Bobrow, D. (Ed. ), Qualitative Reasoning About Physical Systems , pp. 85–186. MIT Press. Forbus , K. D. and de Kleer, J. (1993).

Token 21956:
Building Problem Solvers . MIT Press. Ford , K. M. and Hayes, P. J. (1995). Turing Test considered harmful. In IJCAI-95 , pp. 972–977.

Token 21957:
Forestier , J.-P. and Varaiya, P. (1978). Multilayer control of large Markov chains.

Token 21958:
IEEE Transactions on Automatic Control ,23(2), 298–304.Forgy , C. (1981). OPS5 user’s manual.

Token 21959:
Technical report CMU-CS-81-135, Computer Science Depart- ment, Carnegie-Mellon University. Forgy , C. (1982).

Token 21960:
A fast algorithm for the many patterns/many objects match problem. AIJ,19(1), 17–37. Forsyth , D. and Ponce, J. (2002).

Token 21961:
Computer Vision: A Modern Approach . Prentice Hall. Fourier , J. (1827).

Token 21962:
Analyse des travaux de l’Acad´ emie Royale des Sciences, pendant l’ann´ ee 1824; partie math´ ematique.

Token 21963:
Histoire de l’Acad´ emie Royale des Sciences de France ,7, xlvii–lv. Fox, C. and Tversky, A. (1995). Ambiguity aver- sion and comparative ignorance.

Token 21964:
Quarterly Journal of Economics ,110(3), 585–603. Fox, D., Burgard, W., Dellaert, F., and Thrun, S. (1999).

Token 21965:
Monte carlo localization: Efﬁcient positionestimation for mobile robots. In AAAI-99 . Fox, M. S. (1990).

Token 21966:
Constraint-guided scheduling: A short history of research at CMU. Computers in Industry ,14(1–3), 79–88. Fox, M. S., Allen, B., and Strohm, G. (1982).

Token 21967:
Job shop scheduling: An investigation in constraint- directed reasoning. In AAAI-82 , pp. 155–158. Fox, M. S. and Long, D. (1998).

Token 21968:
The automatic in- ference of state invariants in TIM. JAIR ,9, 367–421. Franco , J. and Paull, M. (1983).

Token 21969:
Probabilistic anal- ysis of the Davis Putnam procedure for solving thesatisﬁability problem. Discrete Applied Mathemat- ics,5, 77–87.

Token 21970:
Frank , I., Basin, D. A., and Matsubara, H. (1998). Finding optimal strategies for imperfect information games. In AAAI-98 , pp. 500–507.

Token 21971:
Frank , R. H. and Cook, P. J. (1996). The Winner- Take-All Society . Penguin. Franz , A. (1996).

Token 21972:
Automatic Ambiguity resolution in Natural Language Processing: An Empirical Ap-proach . Springer. Franz , A. and Brants, T. (2006).

Token 21973:
All our n-gram are belong to you. Blog posting. Frege , G. (1879).

Token 21974:
Begriffsschrift, eine der arith- metischen nachgebildete Formelsprache des reinenDenkens . Halle, Berlin.

Token 21975:
English translation appears in van Heijenoort (1967). Freitag , D. and McCallum, A. (2000).

Token 21976:
Information extraction with hmm structures learned by stochasticoptimization. In AAAI-00 . Freuder , E. C. (1978).

Token 21977:
Synthesizing constraint ex- pressions. CACM ,21(11), 958–966. Freuder , E. C. (1982). A sufﬁcient condition for backtrack-free search.

Token 21978:
JACM ,29(1), 24–32. Freuder , E. C. (1985). A sufﬁcient condition for backtrack-bounded search. JACM ,32(4), 755–761.

Token 21979:
Freuder , E. C. and Mackworth, A. K. (Eds.). (1994). Constraint-based reasoning . MIT Press. Freund , Y. and Schapire, R. E. (1996).

Token 21980:
Experiments with a new boosting algorithm. In ICML-96 . Freund , Y. and Schapire, R. E. (1999).

Token 21981:
Large margin classiﬁcation using the perceptron algorithm. Ma- chine Learning ,37(3), 277–296. Friedberg , R. M. (1958).

Token 21982:
A learning machine: Part I. IBM Journal of Research and Development , 2, 2–13. Friedberg , R. M., Dunham, B., and North, T. (1959).

Token 21983:
A learning machine: Part II. IBM Journal of Research and Development ,3(3), 282–287.

Token 21984:
Bibliography 1073 Friedgut , E. (1999). Necessary and sufﬁcient con- ditions for sharp thresholds of graph properties, and the k-SAT problem.

Token 21985:
J. American Mathematical So- ciety ,12, 1017–1054. Friedman , G. J. (1959). Digital simulation of an evolutionary process.

Token 21986:
General Systems Yearbook ,4, 171–184. Friedman , J., Hastie, T., and Tibshirani, R. (2000).

Token 21987:
Additive logistic regression: A statistical view ofboosting. Annals of Statistics ,28(2), 337–374. Friedman , N. (1998).

Token 21988:
The Bayesian structural EM algorithm. In UAI-98 . Friedman , N. and Goldszmidt, M. (1996). Learning Bayesian networks with local structure.

Token 21989:
In UAI-96 , pp. 252–262. Friedman , N. and Koller, D. (2003).

Token 21990:
Be- ing Bayesian about Bayesian network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning ,50, 95–125.

Token 21991:
Friedman , N., Murphy, K., and Russell, S. J. (1998). Learning the structure of dynamic proba-bilistic networks. In UAI-98 . Friedman , N. (2004).

Token 21992:
Inferring cellular networks using probabilistic graphical models. Science , 303(5659), 799–805. Fruhwirth , T. and Abdennadher, S. (2003).

Token 21993:
Essen- tials of constraint programming . Cambridge Univer- sity Press. Fuchs , J. J., Gasquet, A., Olalainty, B., and Currie, K. W. (1990).

Token 21994:
PlanERS-1: An expert planning sys-tem for generating spacecraft mission plans. In First International Conference on Expert Planning Sys-tems, pp.

Token 21995:
70–75. Institute of Electrical Engineers. Fudenberg , D. and Tirole, J. (1991). Game theory . MIT Press.

Token 21996:
Fukunaga , A. S., Rabideau, G., Chien, S., and Yan, D. (1997).

Token 21997:
ASPEN: A framework for automated planning and scheduling of spacecraft control and operations. In Proc.

Token 21998:
International Symposium on AI, Robotics and Automation in Space , pp. 181–187. Fung , R. and Chang, K. C. (1989).

Token 21999:
Weighting and integrating evidence for stochastic simulation in Bayesian networks. In UAI-98 , pp. 209–220. Gaddum , J. H. (1933).

Token 22000:
Reports on biological stan- dard III: Methods of biological assay depending on a quantal response.

Token 22001:
Special report series of the medi- cal research council 183, Medical Research Council. Gaifman , H. (1964). Concerning measures in ﬁrst order calculi.

Token 22002:
Israel Journal of Mathematics ,2,1 – 18. Gallaire , H. and Minker, J. (Eds.). (1978). Logic and Databases . Plenum. Gallier , J. H. (1986).

Token 22003:
Logic for Computer Science: Foundations of Automatic Theorem Proving .H a r p e r and Row.

Token 22004:
Gamba , A., Gamberini, L., Palmieri, G., and Sanna, R. (1961). Further experiments with PAPA. Nuovo Cimento Supplemento ,20(2), 221–231. Garding , J.

Token 22005:
(1992). Shape from texture for smooth curved surfaces in perspective projection. J. Mathe- matical Imaging and Vision ,2(4), 327–350.

Token 22006:
Gardner , M. (1968). Logic Machines, Diagrams and Boolean Algebra . Dover. Garey , M. R. and Johnson, D. S. (1979). Computers and Intractability .

Token 22007:
W. H. Freeman. Gaschnig , J. (1977). A general backtrack algorithm that eliminates most redundant tests. In IJCAI-77 ,p . 457.Gaschnig , J. (1979).

Token 22008:
Performance measurement and analysis of certain search algorithms.

Token 22009:
Technical re- port CMU-CS-79-124, Computer Science Depart- ment, Carnegie-Mellon University. Gasser , R. (1995).

Token 22010:
Efﬁciently harnessing computa- tional resources for exhaustive search . Ph.D. thesis, ETH Z¨ urich. Gasser , R. (1998). Solving nine men’s morris.

Token 22011:
In Nowakowski, R. (Ed. ), Games of No Chance .C a m - bridge University Press. Gat, E. (1998). Three-layered architectures.

Token 22012:
In Ko- rtenkamp, D., Bonasso, R. P., and Murphy, R. (Eds. ), AI-based Mobile Robots: Case Studies of SuccessfulRobot Systems , pp. 195–210. MIT Press.

Token 22013:
Gauss , C. F. (1809). Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambien- tium. Sumtibus F. Perthes et I. H. Besser, Hamburg.

Token 22014:
Gauss , C. F. (1829). Beitr¨ age zur theorie der algebraischen gleichungen. Collected in Werke, Vol. 3 , pages 71–102.

Token 22015:
K. Gesellschaft Wissenschaft, G¨ottingen, Germany, 1876. Gawande , A. (2002). Complications: A Surgeon’s Notes on an Imperfect Science .

Token 22016:
Metropolitan Books. Geiger , D., Verma, T., and Pearl, J. (1990). Identify- ing independence in Bayesian networks. Networks , 20(5), 507–534.

Token 22017:
Geisel , T. (1955). On Beyond Zebra . Random House. Gelb , A. (1974). Applied Optimal Estimation . MIT Press. Gelernter , H. (1959).

Token 22018:
Realization of a geometry- theorem proving machine. In Proc. an Interna- tional Conference on Information Processing , pp. 273–282. UNESCO House.

Token 22019:
Gelfond , M. and Lifschitz, V. (1988). Compiling cir- cumscriptive theories into logic programs.

Token 22020:
In Non- Monotonic Reasoning: 2nd International WorkshopProceedings , pp. 74–99. Gelfond , M. (2008). Answer sets.

Token 22021:
In van Harmelan, F., Lifschitz, V., and Porter, B. (Eds. ), Handbook of Knowledge Representation , pp. 285–316. Elsevier.

Token 22022:
Gelly , S. and Silver, D. (2008). Achieving master level play in 9 x 9 computer go. In AAAI-08 , pp. 1537–1540. Gelman , A., Carlin, J.

Token 22023:
B., Stern, H. S., and Rubin, D. (1995). Bayesian Data Analysis . Chapman & Hall. Geman , S. and Geman, D. (1984).

Token 22024:
Stochastic relax- ation, Gibbs distributions, and Bayesian restoration of images. PAMI ,6(6), 721–741. Genesereth , M. R. (1984).

Token 22025:
The use of design de- scriptions in automated diagnosis. AIJ,24(1–3), 411–436. Genesereth , M. R. and Nilsson, N. J. (1987).

Token 22026:
Log- ical Foundations of Artiﬁcial Intelligence . Morgan Kaufmann. Genesereth , M. R. and Nourbakhsh, I. (1993).

Token 22027:
Time-saving tips for problem solving with incom- plete information. In AAAI-93 , pp. 724–730. Genesereth , M. R. and Smith, D. E. (1981).

Token 22028:
Meta- level architecture. Memo HPP-81-6, Computer Sci- ence Department, Stanford University. Gent , I., Petrie, K., and Puget, J.-F. (2006).

Token 22029:
Sym- metry in constraint programming. In Rossi, F., van Beek, P., and Walsh, T. (Eds. ), Handbook of Con- straint Programming .

Token 22030:
Elsevier.Gentner , D. (1983). Structure mapping: A theoret- ical framework for analogy. Cognitive Science ,7, 155–170.

Token 22031:
Gentner , D. and Goldin-Meadow, S. (Eds.). (2003). Language in mind: Advances in the study of lan- guage and though . MIT Press.

Token 22032:
Gerevini , A. and Long, D. (2005). Plan constraints and preferences in PDDL3. Tech. rep., Dept.

Token 22033:
of Elec- tronics for Automation, University of Brescia, Italy. Gerevini , A. and Serina, I. (2002).

Token 22034:
LPG: A plan- ner based on planning graphs with action costs. In ICAPS-02 , pp. 281–290. Gerevini , A. and Serina, I. (2003).

Token 22035:
Planning as propositional CSP: from walksat to local search for action graphs. Constraints ,8 , 389–413. Gershwin , G. (1937).

Token 22036:
Let’s call the whole thing off. Song. Getoor , L. and Taskar, B. (Eds.). (2007). Introduc- tion to Statistical Relational Learning . MIT Press.

Token 22037:
Ghahramani , Z. and Jordan, M. I. (1997). Facto- rial hidden Markov models. Machine Learning ,29, 245–274. Ghahramani , Z. (1998).

Token 22038:
Learning dynamic bayesian networks. In Adaptive Processing of Se- quences and Data Structures , pp. 168–197. Ghahramani , Z. (2005).

Token 22039:
Tutorial on nonparametric Bayesian methods. Tutorial presentation at the UAI Conference.

Token 22040:
Ghallab , M., Howe, A., Knoblock, C. A., and Mc- Dermott, D. (1998). PDDL—The planning domaindeﬁnition language. Tech.

Token 22041:
rep. DCS TR-1165, Yale Center for Computational Vision and Control. Ghallab , M. and Laruelle, H. (1994).

Token 22042:
Representa- tion and control in IxTeT, a temporal planner. In AIPS-94 , pp. 61–67. Ghallab , M., Nau, D. S., and Traverso, P. (2004).

Token 22043:
Automated Planning: Theory and practice . Morgan Kaufmann. Gibbs , R. W. (2006). Metaphor interpretation as em- bodied simulation.

Token 22044:
Mind ,21(3), 434–458. Gibson , J. J. (1950). The Perception of the Visual World . Houghton Mifﬂin. Gibson , J. J. (1979).

Token 22045:
The Ecological Approach to Visual Perception . Houghton Mifﬂin. Gilks , W. R., Richardson, S., and Spiegelhalter, D. J. (Eds.). (1996).

Token 22046:
Markov chain Monte Carlo in prac- tice. Chapman and Hall. Gilks , W. R., Thomas, A., and Spiegelhalter, D. J. (1994).

Token 22047:
A language and program for complex Bayesian modelling. The Statistician ,43, 169–178. Gilmore , P. C. (1960).

Token 22048:
A proof method for quantiﬁ- cation theory: Its justiﬁcation and realization. IBM Journal of Research and Development ,4, 28–35.

Token 22049:
Ginsberg , M. L. (1993). Essentials of Artiﬁcial In- telligence . Morgan Kaufmann. Ginsberg , M. L. (1999).

Token 22050:
GIB: Steps toward an expert-level bridge-playing program. In IJCAI-99 , pp. 584–589.

Token 22051:
Ginsberg , M. L., Frank, M., Halpin, M. P., and Tor- rance, M. C. (1990). Search lessons learned fromcrossword puzzles. In AAAI-90 , Vol. 1, pp.

Token 22052:
210–215. Ginsberg , M. L. (2001). GIB: Imperfect infoorma- tion in a computationa lly challenging game. JAIR , 14, 303–358.

Token 22053:
Gionis , A., Indyk, P., and Motwani, R. (1999). Simi- larity search in high dimensions vis hashing. In Proc.

Token 22054:
25th Very Large Database (VLDB) Conference .

Token 22055:
1074 Bibliography Gittins , J. C. (1989). Multi-Armed Bandit Allocation Indices . Wiley. Glanc , A. (1978). On the etymology of the word “robot”.

Token 22056:
SIGART Newsletter ,67, 12. Glover , F. and Laguna, M. (Eds.). (1997). Tabu search .K l u w e r . G¨odel, K. (1930).

Token 22057:
¨Uber die Vollst¨ andigkeit des Logikkalk¨ uls. Ph.D. thesis, University of Vienna. G¨odel, K. (1931).

Token 22058:
¨Uber formal unentscheidbare S¨atze der Principia mathematica und verwandter Systeme I. Monatshefte f¨ ur Mathematik und Physik , 38, 173–198.

Token 22059:
Goebel , J., Volk, K., Walker, H., and Gerbault, F. (1989). Automatic classiﬁcation of spectra from the infrared astronomical satellite (IRAS).

Token 22060:
Astronomy and Astrophysics ,222, L5–L8. Goertzel , B. and Pennachin, C. (2007). Artiﬁcial General Intelligence . Springer.

Token 22061:
Gold , B. and Morgan, N. (2000). Speech and Audio Signal Processing . Wiley. Gold , E. M. (1967). Language identiﬁcation in the limit.

Token 22062:
Information and Control ,10, 447–474. Goldberg , A. V., Kaplan, H., and Werneck, R. F. (2006).

Token 22063:
Reach for a*: Efﬁcient point-to-point short-est path algorithms. In Workshop on algorithm engi- neering and experiments , pp. 129–143.

Token 22064:
Goldman , R. and Boddy, M. (1996). Expressive planning and explicit knowledge. In AIPS-96 , pp. 110–117. Goldszmidt , M. and Pearl, J. (1996).

Token 22065:
Qualitative probabilities for default reasoning, belief revision, and causal modeling. AIJ,84(1–2), 57–112. Golomb , S. and Baumert, L. (1965).

Token 22066:
Backtrack pro- ramming. JACM ,14, 516–524. Golub , G., Heath, M., and Wahba, G. (1979).

Token 22067:
Gen- eralized cross-validation as a method for choosing a good ridge parameter. Technometrics ,21 (2).

Token 22068:
Gomes , C., Selman, B., Crato, N., and Kautz, H. (2000). Heavy-tailed phenomena in satisﬁability and constrain processing. JAR,24, 67–100.

Token 22069:
Gomes , C., Kautz, H., Sabharwal, A., and Selman, B. (2008). Satisﬁability solvers. In van Harmelen,F., Lifschitz, V., and Porter, B. (Eds.

Token 22070:
), Handbook of Knowledge Representation . Elsevier. Gomes , C. and Selman, B. (2001). Algorithm port- folios. AIJ,126, 43–62.

Token 22071:
Gomes , C., Selman, B., and Kautz, H. (1998). Boosting combinatorial search through randomiza- tion. In AAAI-98 , pp. 431–437. Gonthier , G. (2008).

Token 22072:
Formal proof–The four-color theorem. Notices of the AMS ,55(11), 1382–1393. Good , I. J. (1961). A causal calculus.

Token 22073:
British Jour- nal of the Philosophy of Science ,11, 305–318. Good , I. J. (1965). Speculations concerning the ﬁrst ultraintelligent machine.

Token 22074:
In Alt, F. L. and Rubinoff,M. (Eds. ), Advances in Computers , Vol. 6, pp. 31– 88. Academic Press. Good , I. J. (1983).

Token 22075:
Good Thinking: The Founda- tions of Probability and Its Applications . University of Minnesota Press. Goodman , D. and Keene, R. (1997).

Token 22076:
Man versus Machine: Kasparov versus Deep Blue . H3 Publica- tions. Goodman , J. (2001). A bit of progress in language modeling. Tech.

Token 22077:
rep. MSR-TR-2001-72, Microsoft Research.Goodman , J. and Heckerman, D. (2004). Fighting spam with statistics.

Token 22078:
Signiﬁcance, the Magazine of the Royal Statistical Society ,1, 69–72. Goodman , N. (1954). Fact, Fiction and Forecast . University of London Press.

Token 22079:
Goodman , N. (1977). The Structure of Appearance (3rd edition). D. Reidel. Gopnik , A. and Glymour, C. (2002).

Token 22080:
Causal maps and bayes nets: A cognitive and computational ac-count of theory-formation. In Caruthers, P., Stich,S., and Siegal, M. (Eds.

Token 22081:
), The Cognitive Basis of Sci- ence. Cambridge University Press. Gordon , D. M. (2000). Ants at Work . Norton. Gordon , D. M. (2007).

Token 22082:
Control without hierarchy. Nature ,446(8), 143. Gordon , M. J., Milner, A. J., and Wadsworth, C. P. (1979). Edinburgh LCF . Springer-Verlag.

Token 22083:
Gordon , N. (1994). Bayesian methods for tracking . Ph.D. thesis, Imperial College. Gordon , N., Salmond, D. J., and Smith, A. F. M. (1993).

Token 22084:
Novel approach to nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings F (Radar and Signal Processing) ,140(2), 107–113. Gorry , G. A.

Token 22085:
(1968). Strategies for computer-aided diagnosis. Mathematical Biosciences ,2(3–4), 293– 318.

Token 22086:
Gorry , G. A., Kassirer, J. P., Essig, A., and Schwartz, W. B. (1973).

Token 22087:
Decision analysis as the basis for computer-aided management of acute renal failure. American Journal of Medicine ,55, 473–484.

Token 22088:
Gottlob , G., Leone, N., and Scarcello, F. (1999a). A comparison of structural CSP decomposition meth- ods. In IJCAI-99 , pp. 394–399.

Token 22089:
Gottlob , G., Leone, N., and Scarcello, F. (1999b). Hypertree decompositions and tractable queries. In PODS-99 , pp. 21–32. Graham ,S .L .

Token 22090:
,H a r r i s o n ,M .A . ,a n dR u z z o ,W .L . (1980). An improved context-free recognizer.

Token 22091:
ACM Transactions on Programming Languages and Sys- tems,2(3), 415–462. Grama , A. and Kumar, V. (1995).

Token 22092:
A survey of paral- lel search algorithms for discrete optimization prob- lems. ORSA Journal of Computing ,7(4), 365–385. Grassmann , H. (1861).

Token 22093:
Lehrbuch der Arithmetik . T h .C h r .F r .E n s l i n ,B e r l i n . Grayson , C. J. (1960).

Token 22094:
Decisions under uncer- tainty: Drilling decisions by oil and gas operators.Tech. rep., Division of Research, Harvard BusinessSchool.

Token 22095:
Green , B., Wolf, A., Chomsky, C., and Laugherty, K. (1961). BASEBALL: An automatic question an-swerer. In Proc.

Token 22096:
Western Joint Computer Confer- ence, pp. 219–224. Green , C. (1969a). Application of theorem proving to problem solving. In IJCAI-69 , pp. 219–239.

Token 22097:
Green , C. (1969b). Theorem-proving by resolu- tion as a basis for question-answering systems. In Meltzer, B., Michie, D., and Swann, M. (Eds.

Token 22098:
), Ma- chine Intelligence 4 , pp. 183–205. Edinburgh Uni- versity Press. Green , C. and Raphael, B. (1968).

Token 22099:
The use of theorem-proving techniques in question-answeringsystems. In Proc. 23rd ACM National Conference .

Token 22100:
Greenblatt , R. D., Eastlake, D. E., and Crocker, S. D. (1967). The Greenblatt chess program. In Proc. Fall Joint Computer Conference , pp. 801–810.

Token 22101:
Greiner , R. (1989). Towards a formal analysis of EBL. In ICML-89 , pp. 450–453.Grinstead , C. and Snell, J. (1997).

Token 22102:
Introduction to Probability .A M S . Grove , W. and Meehl, P. (1996).

Token 22103:
Comparative efﬁ- ciency of informal (subjective, impressionistic) andformal (mechanical, algorithmic) prediction proce- dures: The clinical statistical controversy.

Token 22104:
Psychol- ogy, Public Policy, and Law ,2, 293–323. Gruber , T. (2004). Interview of Tom Gruber. AIS SIGSEMIS Bulletin ,1(3). Gu, J. (1989).

Token 22105:
Parallel Algorithms and Architectures for Very Fast AI Search . Ph.D. thesis, University of Utah.

Token 22106:
Guard , J., Oglesby, F., Bennett, J., and Settle, L. (1969). Semi-automated mathematics. JACM ,16, 49–62.

Token 22107:
Guestrin , C., Koller, D., Gearhart, C., and Kanodia, N. (2003a). Generalizing plans to new environmentsin relational MDPs. In IJCAI-03 .

Token 22108:
Guestrin ,C . ,K o l l e r ,D . ,P a r r ,R . ,a n dV e n k a t a r a - man, S. (2003b). Efﬁcient solution algorithms forfactored MDPs.

Token 22109:
JAIR ,19, 399–468. Guestrin , C., Lagoudakis, M. G., and Parr, R. (2002). Coordinated reinforcement learning. In ICML-02 , pp. 227–234.

Token 22110:
Guibas , L. J., Knuth, D. E., and Sharir, M. (1992). Randomized incremental construction of Delaunay and Voronoi diagrams. Algorithmica ,7, 381–413.

Token 22111:
See also 17th Int. Coll. on Automata, Languages and Programming , 1990, pp. 414–431. Gumperz , J. and Levinson, S. (1996).

Token 22112:
Rethinking Linguistic Relativity . Cambridge University Press. Guyon , I. and Elisseeff, A. (2003). An introduction to variable and feature selection.

Token 22113:
JMLR , pp. 1157– 1182. Hacking , I. (1975). The Emergence of Probability . Cambridge University Press. Haghighi , A. and Klein, D. (2006).

Token 22114:
Prototype- driven grammar induction. In COLING-06 . Hald , A. (1990). A History of Probability and Statis- tics and Their Applications before 1750 .

Token 22115:
Wiley. Halevy , A. (2007). Dataspaces: A new paradigm for data integration. In Brazilian Symposium on Databases .

Token 22116:
Halevy , A., Norvig, P., and Pereira, F. (2009). The unreasonable effectiveness of data. IEEE Intelligent Systems ,March/April , 8–12. Halpern , J. Y.

Token 22117:
(1990). An analysis of ﬁrst-order log- ics of probability. AIJ,46(3), 311–350. Halpern , J. Y. (1999). Technical addendum, Cox’s theorem revisited.

Token 22118:
JAIR ,11, 429–435. Halpern , J. Y. and Weissman, V. (2008). Using ﬁrst- order logic to reason about policies.

Token 22119:
ACM Transac- tions on Information and System Security ,11(4). Hamming , R. W. (1991). The Art of Probability for Scientists and Engineers .

Token 22120:
Addison-Wesley. Hammond , K. (1989). Case-Based Planning: View- ing Planning as a Memory Task . Academic Press.

Token 22121:
Hamscher , W., Console, L., and Kleer, J. D. (1992). Readings in Model-based Diagnosis . Morgan Kauf- mann. Han, X. and Boyden, E. (2007).

Token 22122:
Multiple-color op- tical activation, silencing, and desynchronization ofneural activity, with single-spike temporal resolu-tion. PLoS One ,e299 .

Token 22123:
Hand , D., Mannila, H., and Smyth, P. (2001). Prin- ciples of Data Mining . MIT Press.

Token 22124:
Bibliography 1075 Handschin , J. E. and Mayne, D. Q. (1969).

Token 22125:
Monte Carlo techniques to estimate the conditional expecta- tion in multi-stage nonlinear ﬁltering. Int. J. Control , 9(5), 547–559.

Token 22126:
Hansen , E. (1998). Solving POMDPs by searching in policy space. In UAI-98 , pp. 211–219. Hansen , E. and Zilberstein, S. (2001).

Token 22127:
LAO*: a heuristic search algorithm that ﬁnds solutions withloops. AIJ,129(1–2), 35–62. Hansen , P. and Jaumard, B. (1990).

Token 22128:
Algorithms for the maximum satisﬁability problem. Computing , 44(4), 279–303. Hanski , I. and Cambefort, Y. (Eds.). (1991). Dung Beetle Ecology .

Token 22129:
Princeton University Press. Hansson , O. and Mayer, A. (1989). Heuristic search as evidential reasoning. In UAI 5 .

Token 22130:
Hansson , O., Mayer, A., and Yung, M. (1992). Crit- icizing solutions to rela xed models yields powerful admissible heuristics.

Token 22131:
Information Sciences ,63(3), 207–227. Haralick , R. M. and Elliot, G. L. (1980).

Token 22132:
Increas- ing tree search efﬁciency for constraint satisfactionproblems. AIJ,14(3), 263–313. Hardin , G. (1968). The tragedy of the commons.

Token 22133:
Science ,162, 1243–1248. Hardy , G. H. (1940). A Mathematician’s Apology . Cambridge University Press. Harman , G. H. (1983).

Token 22134:
Change in View: Principles of Reasoning . MIT Press. Harris , Z. (1954). Distributional structure. Word , 10(2/3).

Token 22135:
Harrison , J. R. and March, J. G. (1984). Decision making and postdecision surprises. Administrative Science Quarterly ,29, 26–42. Harsanyi , J.

Token 22136:
(1967). Games with incomplete infor- mation played by Bayesian players. Management Science ,14, 159–182. Hart , P. E., Nilsson, N. J., and Raphael, B.

Token 22137:
(1968). A formal basis for the heuristic determination of mini-mum cost paths.

Token 22138:
IEEE Transactions on Systems Sci- ence and Cybernetics ,SSC-4(2) , 100–107. Hart , P. E., Nilsson, N. J., and Raphael, B. (1972).

Token 22139:
Correction to “A formal basis for the heuristic deter- mination of minimum cost paths”. SIGART Newslet- ter,37, 28–29. Hart , T. P. and Edwards, D. J.

Token 22140:
(1961). The tree prune (TP) algorithm. Artiﬁcial intelligence projectmemo 30, Massachusetts Institute of Technology. Hartley , H. (1958).

Token 22141:
Maximum likelihood estimation from incomplete data. Biometrics ,14, 174–194. Hartley , R. and Zisserman, A. (2000).

Token 22142:
Multiple view geometry in computer vision . Cambridge University Press. Haslum , P., Botea, A., Helmert, M., Bonet, B., and Koenig, S. (2007).

Token 22143:
Domain-independent construc- tion of pattern database heuristics for cost-optimalplanning. In AAAI-07 , pp. 1007–1012.

Token 22144:
Haslum , P. and Geffner, H. (2001). Heuristic plan- ning with time and resources. In Proc. IJCAI-01 Workshop on Planning with Resources .

Token 22145:
Haslum , P. (2006). Improving heuristics through re- laxed search – An analysis of TP4 and HSP*a in the 2004 planning competition. JAIR ,25, 233–267.

Token 22146:
Haslum , P., Bonet, B., and Geffner, H. (2005). New admissible heuristics for domain-independent plan- ning.

Token 22147:
In AAAI-05 .Hastie , T. and Tibshirani, R. (1996). Discriminant adaptive nearest neighbor classiﬁcation and regres- sion.

Token 22148:
In Touretzky, D. S., Mozer, M. C., and Has- selmo, M. E. (Eds. ), NIPS 8 , pp. 409–15. MIT Press. Hastie , T., Tibshirani, R., and Friedman, J.

Token 22149:
(2001). The Elements of Statistical Learning: Data Mining, Inference and Prediction (2nd edition). Springer- Verlag.

Token 22150:
Hastie , T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data Mining,Inference and Prediction (2nd edition).

Token 22151:
Springer- Verlag. Haugeland , J. (Ed.). (1985). Artiﬁcial Intelligence: The Very Idea . MIT Press. Hauk , T. (2004).

Token 22152:
Search in Trees with Chance Nodes . Ph.D. thesis, Univ. of Alberta. Haussler , D. (1989). Learning conjunctive concepts in structural domains.

Token 22153:
Machine Learning ,4(1), 7– 40. Havelund , K., Lowry, M., Park, S., Pecheur, C., Penix, J., Visser, W., and White, J. L. (2000).

Token 22154:
Formalanalysis of the remote agent before and after ﬂight. InProc. 5th NASA Langley Formal Methods Work- shop . Havenstein , H. (2005).

Token 22155:
Spring comes to AI winter. Computer World . Hawkins , J. and Blakeslee, S. (2004). On Intelli- gence . Henry Holt and Co. Hayes , P. J. (1978).

Token 22156:
The naive physics manifesto. In Michie, D. (Ed. ), Expert Systems in the Microelec- tronic Age . Edinburgh University Press. Hayes , P. J. (1979).

Token 22157:
The logic of frames. In Metzing, D. (Ed. ), Frame Conceptions and Text Understand- ing, pp. 46–61. de Gruyter. Hayes , P. J. (1985a).

Token 22158:
Naive physics I: Ontology for liquids. In Hobbs, J. R. and Moore, R. C. (Eds. ), For- mal Theories of the Commonsense World , chap. 3, pp. 71–107.

Token 22159:
Ablex. Hayes , P. J. (1985b). The second naive physics man- ifesto. In Hobbs, J. R. and Moore, R. C. (Eds.

Token 22160:
), For- mal Theories of the Commonsense World , chap. 1, pp. 1–36. Ablex. Haykin , S. (2008). Neural Networks: A Compre- hensive Foundation .

Token 22161:
Prentice Hall. Hays , J. and Efros, A. A. (2007). Scene completion Using millions of photographs. ACM Transactions on Graphics (SIGGRAPH) ,26(3).

Token 22162:
Hearst , M. A. (1992). Automatic acquisition of hy- ponyms from large text corpora. In COLING-92 . Hearst , M. A. (2009).

Token 22163:
Search User Interfaces .C a m - bridge University Press. Hebb , D. O. (1949). The Organization of Behavior . Wiley. Heckerman , D. (1986).

Token 22164:
Probabilistic interpretation for MYCIN’s certainty factors. In Kanal, L. N. and Lemmer, J. F. (Eds. ), UAI 2 , pp. 167–196. Elsevier/North-Holland.

Token 22165:
Heckerman , D. (1991). Probabilistic Similarity Networks . MIT Press. Heckerman , D. (1998). A tutorial on learning with Bayesian networks.

Token 22166:
In Jordan, M. I. (Ed. ), Learning in graphical models .K l u w e r . Heckerman , D., Geiger, D., and Chickering, D. M. (1994).

Token 22167:
Learning Bayesian networks: The combi-nation of knowledge and statistical data. Technicalreport MSR-TR-94-09, Microsoft Research.

Token 22168:
Heidegger , M. (1927). Being and Time . SCM Press.Heinz , E. A. (2000). Scalable search in computer chess .V i e w e g .

Token 22169:
Held , M. and Karp, R. M. (1970). The traveling salesman problem and minimum spanning trees. Op- erations Research ,18, 1138–1162.

Token 22170:
Helmert , M. (2001). On the complexity of planning in transportation domains. In ECP-01 . Helmert , M. (2003).

Token 22171:
Complexity results for stan- dard benchmark domains in planning. AIJ,143(2), 219–262. Helmert , M. (2006). The fast downward planning system.

Token 22172:
JAIR ,26, 191–246. Helmert , M. and Richter, S. (2004). Fast downward – Making use of causal dependencies in the prob- lem representation. In Proc.

Token 22173:
International Planning Competition at ICAPS , pp. 41–43. Helmert ,M .a n dR ¨ oger, G. (2008). How good is almost perfect? In AAAI-08 .

Token 22174:
Hendler , J., Carbonell, J. G., Lenat, D. B., Mi- zoguchi, R., and Rosenbloom, P. S. (1995).

Token 22175:
VERY large knowledge bases – Architecture vs engineer-ing. In IJCAI-95 , pp. 2033–2036. Henrion , M. (1988).

Token 22176:
Propagation of uncertainty in Bayesian networks by probabilistic logic sampling. In Lemmer, J. F. and Kanal, L. N. (Eds. ), UAI 2 , pp. 149–163.

Token 22177:
Elsevier/North-Holland. Henzinger , T. A. and Sastry, S. (Eds.). (1998). Hy- brid systems: Computation and control . Springer- Verlag. Herbrand , J.

Token 22178:
(1930). Recherches sur la Th´ eorie de la D´ emonstration . Ph.D. thesis, University of Paris. Hewitt , C. (1969).

Token 22179:
PLANNER: a language for prov- ing theorems in robots. In IJCAI-69 , pp. 295–301. Hierholzer , C. (1873).

Token 22180:
¨Uber die M¨ oglichkeit, einen Linienzug ohne Wiederholung und ohne Un- terbrechung zu umfahren. Mathematische Annalen , 6, 30–32.

Token 22181:
Hilgard , E. R. and Bower, G. H. (1975). Theories of Learning (4th edition). Prentice-Hall. Hintikka , J. (1962). Knowledge and Belief .

Token 22182:
Cornell University Press. Hinton , G. E. and Anderson, J. A. (1981). Parallel Models of Associative Memory . Lawrence Erlbaum Associates.

Token 22183:
Hinton , G. E. and Nowlan, S. J. (1987). How learn- ing can guide evolution. Complex Systems ,1(3), 495–502.

Token 22184:
Hinton , G. E., Osindero, S., and Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural Computation ,18, 1527–15554.

Token 22185:
Hinton , G. E. and Sejnowski, T. (1983). Optimal perceptual inference. In CVPR , pp. 448–453. Hinton , G. E. and Sejnowski, T. (1986).

Token 22186:
Learning and relearning in Boltzmann machines. In Rumel-hart, D. E. and McClelland, J. L. (Eds. ), Paral- lel Distributed Processing , chap. 7, pp.

Token 22187:
282–317. MIT Press. Hirsh , H. (1987). Explanation-based generalization in a logic programming environment. In IJCAI-87 . Hobbs , J. R. (1990).

Token 22188:
Literature and Cognition .C S L I Press. Hobbs , J. R., Appelt, D., Bear, J., Israel, D., Kameyama, M., Stickel, M. E., and Tyson, M. (1997).

Token 22189:
FASTUS: A cascaded ﬁnite-state transducerfor extracting information from natural-languagetext. In Roche, E. and Schabes, Y. (Eds.

Token 22190:
), Finite- State Devices for Natural Language Processing , pp. 383–406. MIT Press.

Token 22191:
1076 Bibliography Hobbs , J. R. and Moore, R. C. (Eds.). (1985). For- mal Theories of the Commonsense World .A b l e x .

Token 22192:
Hobbs , J. R., Stickel, M. E., Appelt, D., and Martin, P. (1993). Interpretation as abduction. AIJ,63(1–2), 69–142. Hoffmann , J. (2001).

Token 22193:
FF: The fast-forward planning system. AIMag ,22(3), 57–62. Hoffmann , J. and Brafman, R. I. (2006).

Token 22194:
Confor- mant planning via heuristic forward search: A newapproach. AIJ,170(6–7), 507–541. Hoffmann , J. and Brafman, R. I. (2005).

Token 22195:
Contingent planning via heuristic forward search with implicit belief states. In ICAPS-05 . Hoffmann , J. (2005).

Token 22196:
Where “ignoring delete lists” works: Local search topology in planning bench- marks. JAIR ,24, 685–758. Hoffmann , J. and Nebel, B. (2001).

Token 22197:
The FF plan- ning system: Fast plan generation through heuristic search. JAIR ,14, 253–302. Hoffmann , J., Sabharwal, A., and Domshlak, C. (2006).

Token 22198:
Friends or foes? An AI planning perspective on abstraction and search. In ICAPS-06 , pp. 294– 303. Hogan , N. (1985).

Token 22199:
Impedance control: An approach to manipulation. Parts I, II, and III. J. Dynamic Sys- tems, Measurement, and Control ,107(3), 1–24.

Token 22200:
Hoiem , D., Efros, A. A., and Hebert, M. (2008). Putting objects in perspective. IJCV ,80(1). Holland , J. H. (1975).

Token 22201:
Adaption in Natural and Ar- tiﬁcial Systems . University of Michigan Press. Holland , J. H. (1995). Hidden Order: How Adapta- tion Builds Complexity .

Token 22202:
Addison-Wesley. Holte , R. and Hernadvolgyi, I. (2001). Steps towards the automatic creation of search heuristics. Tech.rep. TR04-02, CS Dept., Univ.

Token 22203:
of Alberta. Holzmann , G. J. (1997). The Spin model checker. IEEE Transactions on Software Engineering ,23(5), 279–295. Hood , A. (1824).

Token 22204:
Case 4th—28 July 1824 (Mr. Hood’s cases of injuries of the brain). Phrenologi- cal Journal and Miscellany ,2, 82–94. Hooker , J. (1995).

Token 22205:
Testing heuristics: We have it all wrong. J. Heuristics ,1, 33–42. Hoos , H. and Tsang, E. (2006). Local search meth- ods.

Token 22206:
In Rossi, F., van Beek, P ., and Walsh, T. (Eds. ), Handbook of Constraint Processing , pp. 135–168. Elsevier. Hope , J. (1994).

Token 22207:
The Authorship of Shakespeare’s Plays . Cambridge University Press. Hopﬁeld , J. J. (1982).

Token 22208:
Neurons with graded response have collective computational properties like thoseof two-state neurons. PNAS ,79, 2554–2558. Horn , A. (1951).

Token 22209:
On sentences which are true of direct unions of algebras. JSL,16, 14–21. Horn , B. K. P. (1970).

Token 22210:
Shape from shading: A method for obtaining the shape of a smooth opaqueobject from one view.

Token 22211:
Technical report 232, MIT Artiﬁcial Intelligence Laboratory. Horn , B. K. P. (1986). Robot Vision . MIT Press. Horn , B. K. P. and Brooks, M. J.

Token 22212:
(1989). Shape from Shading . MIT Press. Horn , K. V. (2003). Constructing a logic of plausi- ble inference: A guide to cox’s theorem. IJAR ,34, 3–24.

Token 22213:
Horning , J. J. (1969). A study of grammatical infer- ence. Ph.D. thesis, Stanford University.Horowitz , E. and Sahni, S. (1978).

Token 22214:
Fundamentals of Computer Algorithms . Computer Science Press. Horswill , I. (2000). Functional programming of behavior-based systems.

Token 22215:
Autonomous Robots ,9, 83– 93. Horvitz , E. J. (1987). Problem-solving design: Rea- soning about computational value, trade-offs, and re- sources.

Token 22216:
In Proc. Second Annual NASA Research Fo- rum, pp. 26–43. Horvitz , E. J. (1989).

Token 22217:
Rational metareasoning and compilation for optimizing decisions under boundedresources. In Proc. Computational Intelligence 89 .

Token 22218:
Association for Computing Machinery. Horvitz , E. J. and Barry, M. (1995). Display of in- formation for time-critical decision making. In UAI- 95, pp.

Token 22219:
296–305. Horvitz , E. J., Breese, J. S., Heckerman, D., and Hovel, D. (1998).

Token 22220:
The Lumiere project: Bayesian user modeling for inferring the goals and needs ofsoftware users. In UAI-98 , pp. 256–265.

Token 22221:
Horvitz , E. J., Breese, J. S., and Henrion, M. (1988). Decision theory in expert systems and artiﬁcial intel-ligence. IJAR ,2 , 247–302.

Token 22222:
Horvitz , E. J. and Breese, J. S. (1996). Ideal parti- tion of resources for metareasoning. In AAAI-96 , pp. 1229–1234.

Token 22223:
Horvitz , E. J. and Heckerman, D. (1986). The incon- sistent use of measures of certainty in artiﬁcial intel- ligence research.

Token 22224:
In Kanal, L. N. and Lemmer, J. F. (Eds. ), UAI 2 , pp. 137–151. Elsevier/North-Holland. Horvitz , E. J., Heckerman, D., and Langlotz, C. P. (1986).

Token 22225:
A framework for comparing alternative for-malisms for plausible reasoning. In AAAI-86 ,V o l .1 , pp. 210–214. Howard , R. A. (1960).

Token 22226:
Dynamic Programming and Markov Processes . MIT Press. Howard , R. A. (1966). Information value theory.

Token 22227:
IEEE Transactions on Systems Science and Cyber-netics ,SSC-2 , 22–26. Howard , R. A. (1977). Risk preference. In Howard, R. A. and Matheson, J. E.

Token 22228:
(Eds. ), Readings in De- cision Analysis , pp. 429–465. Decision Analysis Group, SRI International. Howard , R. A. (1989).

Token 22229:
Microrisks for medical de- cision analysis. Int. J. Technology Assessment in Health Care ,5, 357–370. Howard , R. A. and Matheson, J. E. (1984).

Token 22230:
Inﬂu- ence diagrams. In Howard, R. A. and Matheson,J. E. (Eds. ), Readings on the Principles and Appli- cations of Decision Analysis , pp. 721–762.

Token 22231:
Strategic Decisions Group. Howe , D. (1987). The computational behaviour of girard’s paradox. In LICS-87 , pp. 205–214. Hsu, F.-H. (2004).

Token 22232:
Behind Deep Blue: Building the Computer that Defeated the World Chess Champion . Princeton University Press.

Token 22233:
Hsu, F.-H., Anantharaman, T. S., Campbell, M. S., and Nowatzyk, A. (1990). A grandmaster chess ma-chine. Scientiﬁc American ,263(4), 44–50.

Token 22234:
Hu, J. and Wellman, M. P. (1998). Multiagent re- inforcement learning: Theoretical framework and analgorithm. In ICML-98 , pp. 242–250.

Token 22235:
Hu, J. and Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. JMLR ,4, 1039– 1069.

Token 22236:
Huang , T., Koller, D., Malik, J., Ogasawara, G., Rao, B., Russell, S. J., and Weber, J. (1994).

Token 22237:
Au- tomatic symbolic trafﬁc scene analysis using belief networks. In AAAI-94 , pp. 966–972.Huang , T. and Russell, S. J. (1998).

Token 22238:
Object iden- tiﬁcation: A Bayesian analysis with application to trafﬁc surveillance. AIJ,103, 1–17. Huang , X. D., Acero, A., and Hon, H. (2001).

Token 22239:
Spo- ken Language Processing . Prentice Hall. Hubel , D. H. (1988). Eye, Brain, and Vision .W .H . Freeman.

Token 22240:
Huddleston , R. D. and Pullum, G. K. (2002). The Cambridge Grammar of the English Language . Cambridge University Press. Huffman , D. A. (1971).

Token 22241:
Impossible objects as non- sense sentences. In Meltzer, B. and Michie, D.(Eds. ), Machine Intelligence 6 , pp. 295–324. Edin- burgh University Press.

Token 22242:
Hughes , B. D. (1995). Random Walks and Random Environments, Vol. 1: Random Walks . Oxford Uni- versity Press. Hughes , G. E. and Cresswell, M. J.

Token 22243:
(1996). AN e w Introduction to Modal Logic . Routledge. Huhns , M. N. and Singh, M. P. (Eds.). (1998). Read- ings in Agents . Morgan Kaufmann.

Token 22244:
Hume , D. (1739). A Treatise of Human Nature (2nd edition). Republished by Oxford University Press,1978, Oxford, UK. Humphrys , M. (2008).

Token 22245:
How my program passed the turing test. In Epstein, R., Roberts, G., and Beber, G.(Eds. ), Parsing the Turing Test . Springer.

Token 22246:
Hunsberger , L. and Grosz, B. J. (2000). A com- binatorial auction for collaborative planning. In Int.

Token 22247:
Conference on Multi-Agent Systems (ICMAS-2000) . Hunt , W. and Brock, B. (1992). A formal HDL and its use in the FM9001 veriﬁcation.

Token 22248:
Philosophical Transactions of the Royal Society of London ,339. Hunter , L. and States, D. J. (1992). Bayesian clas- siﬁcation of protein structure.

Token 22249:
IEEE Expert ,7(4), 67–75. Hurst , M. (2000). The Interpretation of Text in Ta- bles. Ph.D. thesis, Edinburgh. Hurwicz , L. (1973).

Token 22250:
The design of mechanisms for resource allocation. American Economic Review Pa- pers and Proceedings ,63(1), 1–30. Husmeier , D. (2003).

Token 22251:
Sensitivity and speciﬁcity of inferring genetic regulatory interactions from mi- croarray experiments with dynamic bayesian net- works.

Token 22252:
Bioinformatics ,19(17), 2271–2282. Huth , M. and Ryan, M. (2004). Logic in com- puter science: modelling and reasoning about sys- tems (2nd edition).

Token 22253:
Cambridge University Press. Huttenlocher , D. and Ullman, S. (1990). Recogniz- ing solid objects by alignment with an image. IJCV , 5 (2), 195–212.

Token 22254:
Huygens , C. (1657). De ratiociniis in ludo aleae. In van Schooten, F. (Ed. ), Exercitionum Mathematico- rum. Elsevirii, Amsterdam.

Token 22255:
Translated into English by John Arbuthnot (1692). Huyn , N., Dechter, R., and Pearl, J. (1980). Proba- bilistic analysis of the complexity of A*.

Token 22256:
AIJ,15(3), 241–254. Hwa , R. (1998). An empirical evaluation of proba- bilistic lexicalized tree insertion grammars. In ACL- 98, pp. 557–563.

Token 22257:
Hwang , C. H. and Schubert, L. K. (1993). EL: A for- mal, yet natural, comprehensive knowledge repre- sentation. In AAAI-93 , pp. 676–682.

Token 22258:
Ingerman , P. Z. (1967). Panini–Backus form sug- gested. CACM ,10(3), 137. Inoue , K. (2001). Inverse entailment for full clausal theories.

Token 22259:
In LICS-2001 Workshop on Logic and Learning .

Token 22260:
Bibliography 1077 Intille , S. and Bobick, A. (1999). A framework for recognizing multi-agent action from visual evidence. InAAAI-99 , pp. 518–525.

Token 22261:
Isard , M. and Blake, A. (1996). Contour tracking by stochastic propagation o f conditional density. In ECCV , pp. 343–356.

Token 22262:
Iwama , K. and Tamaki, S. (2004). Improved upper bounds for 3-SAT. In SODA-04 . Jaakkola , T. and Jordan, M. I. (1996).

Token 22263:
Computing upper and lower bounds on likelihoods in intractable networks. In UAI-96 , pp. 340–348. Morgan Kauf- mann.

Token 22264:
Jaakkola , T., Singh, S. P., and Jordan, M. I. (1995). Reinforcement learning algorithm for partially ob- servable Markov decision problems.

Token 22265:
In NIPS 7 , pp. 345–352. Jackson , F. (1982). Epiphenomenal qualia. Philo- sophical Quarterly ,32, 127–136. Jaffar , J. and Lassez, J.-L. (1987).

Token 22266:
Constraint logic programming. In Proc. Fourteenth ACM Conference on Principles of Programming Languages , pp. 111– 119.

Token 22267:
Association for Computing Machinery. Jaffar , J., Michaylov, S., Stuckey, P. J., and Yap, R. H. C. (1992).

Token 22268:
The CLP(R) language and system.ACM Transactions on Programming Languages andSystems ,14(3), 339–395. Jaynes , E. T. (2003).

Token 22269:
Probability Theory: The Logic of Science . Cambridge Univ. Press. Jefferson , G. (1949).

Token 22270:
The mind of mechanical man: The Lister Oration delivered at the Royal Collegeof Surgeons in England. British Medical Journal , 1(25), 1105–1121.

Token 22271:
Jeffrey , R. C. (1983). The Logic of Decision (2nd edition). University of Chicago Press. Jeffreys , H. (1948). Theory of Probability . Oxford.

Token 22272:
Jelinek , F. (1976). Continuous speech recognition by statistical methods. Proc. IEEE ,64(4), 532–556. Jelinek , F. (1997).

Token 22273:
Statistical Methods for Speech Recognition . MIT Press. Jelinek , F. and Mercer, R. L. (1980).

Token 22274:
Interpolated estimation of Markov source parameters from sparse data. In Proc. Workshop on Pattern Recognition in Practice , pp. 381–397.

Token 22275:
Jennings , H. S. (1906). Behavior of the Lower Or- ganisms . Columbia University Press.

Token 22276:
Jenniskens , P., Betlem, H., Betlem, J., and Barifaijo, E. (1994). The Mbale meteorite shower. Meteoritics , 29(2), 246–254. Jensen , F. V. (2001).

Token 22277:
Bayesian Networks and Deci- sion Graphs . Springer-Verlag. Jensen , F. V. (2007). Bayesian Networks and Deci- sion Graphs . Springer-Verlag.

Token 22278:
Jevons , W. S. (1874). The Principles of Science . Routledge/Thoemmes Press, London. Ji, S., Parr, R., Li, H., Liao, X., and Carin, L. (2007).

Token 22279:
Point-based policy iteration. In AAAI-07 . Jimenez , P. and Torras, C. (2000).

Token 22280:
An efﬁcient al- gorithm for searching implicit AND/OR graphs with cycles. AIJ,124(1), 1–30. Joachims , T. (2001).

Token 22281:
A statistical learning model of text classiﬁcation with support vector machines. In SIGIR-01 , pp. 128–136. Johnson , W. W. and Story, W. E. (1879).

Token 22282:
Notes on the “15” puzzle. American Journal of Mathematics , 2, 397–404.Johnston , M. D. and Adorf, H.-M. (1992).

Token 22283:
Schedul- ing with neural networks: The case of the Hubble space telescope. Computers and Operations Re- search ,19(3–4), 209–240.

Token 22284:
Jones , N. D., Gomard, C. K., and Sestoft, P. (1993). Partial Evaluation and Automatic Program Genera-tion. Prentice-Hall.

Token 22285:
Jones , R., Laird, J., and Nielsen, P. E. (1998). Auto- mated intelligent pilots for combat ﬂight simulation. InAAAI-98 , pp. 1047–54.

Token 22286:
Jones , R., McCallum, A., Nigam, K., and Riloff, E. (1999). Bootstrapping for text learning tasks. In Proc.

Token 22287:
IJCAI-99 Workshop on Text Mining: Founda- tions, Techniques, and Applications , pp. 52–63. Jones , T. (2007).

Token 22288:
Artiﬁcial Intelligence: A Systems Approach . Inﬁnity Science Press. Jonsson , A., Morris, P., Muscettola, N., Rajan, K., and Smith, B. (2000).

Token 22289:
Planni ng in interplanetary space: Theory and practice. In AIPS-00 , pp. 177– 186. Jordan , M. I. (1995). Why the logistic function?

Token 22290:
a tutorial discussion on probabilities and neural net- works.

Token 22291:
Computational cognitive science technicalreport 9503, Massachusetts Institute of Technology. Jordan , M. I. (2005).

Token 22292:
Dirichlet processes, Chinese restaurant processes and all that. Tutorial presenta-tion at the NIPS Conference.

Token 22293:
Jordan , M. I., Ghahramani, Z., Jaakkola, T., and Saul, L. K. (1998). An introduction to variational methods for graphical models. In Jordan, M. I.

Token 22294:
(Ed. ), Learning in Graphical Models .K l u w e r . Jouannaud , J.-P. and Kirchner, C. (1991).

Token 22295:
Solving equations in abstract algebras: A rule-based surveyof uniﬁcation. In Lassez, J.-L. and Plotkin, G. (Eds. ),Computational Logic , pp. 257–321.

Token 22296:
MIT Press. Judd , J. S. (1990). Neural Network Design and the Complexity of Learning . MIT Press. Juels , A. and Wattenberg, M. (1996).

Token 22297:
Stochastic hillclimbing as a baseline method for evaluating ge- netic algorithms. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E. (Eds.

Token 22298:
), NIPS 8 , pp. 430–6. MIT Press. Junker , U. (2003). The logic of ilog (j)conﬁgurator: Combining constraint programming with a descrip-tion logic.

Token 22299:
In Proc. IJCAI-03 Conﬁguration Work- shop , pp. 13–20. Jurafsky , D. and Martin, J. H. (2000).

Token 22300:
Speech and Language Processing: An Introduction to Nat- ural Language Processing, Computational Linguis-tics, and Speech Recognition . Prentice-Hall.

Token 22301:
Jurafsky , D. and Martin, J. H. (2008).

Token 22302:
Speech and Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis- tics, and Speech Recognition (2nd edition).

Token 22303:
Prentice- Hall. Kadane , J. B. and Simon, H. A. (1977). Optimal strategies for a class of constrained sequential prob-lems.

Token 22304:
Annals of Statistics ,5, 237–255. Kadane , J. B. and Larkey, P. D. (1982). Subjective probability and the theory of games.

Token 22305:
Management Science ,28(2), 113–120. Kaelbling , L. P., Littman, M. L., and Cassandra, A. R. (1998).

Token 22306:
Planning and actiong in partially ob- servable stochastic domains. AIJ,101, 99–134. Kaelbling , L. P., Littman, M. L., and Moore, A. W. (1996).

Token 22307:
Reinforcement learning: A survey. JAIR ,4, 237–285.Kaelbling , L. P. and Rosenschein, S. J. (1990). Ac- tion and planning in embedded agents.

Token 22308:
Robotics and Autonomous Systems ,6(1–2), 35–48. Kager , R. (1999). Optimality Theory . Cambridge University Press.

Token 22309:
Kahn , H. and Marshall, A. W. (1953). Methods of reducing sample size in Monte Carlo computations.Operations Research ,1(5), 263–278.

Token 22310:
Kahneman , D., Slovic, P., and Tversky, A. (Eds.). (1982). Judgment under Uncertainty: Heuristics and Biases . Cambridge University Press.

Token 22311:
Kahneman , D. and Tversky, A. (1979). Prospect theory: An analysis of decision under risk. Econo- metrica , pp. 263–291. Kaindl , H. and Khorsand, A.

Token 22312:
(1994). Memory- bounded bidirectional search. In AAAI-94 , pp. 1359– 1364. Kalman , R. (1960).

Token 22313:
A new approach to linear ﬁlter- ing and prediction problems. J. Basic Engineering , 82, 35–46. Kambhampati , S. (1994).

Token 22314:
Exploiting causal struc- ture to control retrieval and reﬁtting during plan reuse. Computational Intelligence ,10, 213–244.

Token 22315:
Kambhampati , S., Mali, A. D., and Srivastava, B. (1998). Hybrid planning for partially hierarchicaldomains. In AAAI-98 , pp. 882–888.

Token 22316:
Kanal , L. N. and Kumar, V. (1988). Search in Arti- ﬁcial Intelligence . Springer-Verlag. Kanazawa , K., Koller, D., and Russell, S. J. (1995).

Token 22317:
Stochastic simulation al gorithms for dynamic prob- abilistic networks. In UAI-95 , pp. 346–351. Kantorovich , L. V. (1939).

Token 22318:
Mathematical methods of organizing and planning production. Publishd intranslation in Management Science ,6(4), 366–422, July 1960.

Token 22319:
Kaplan , D. and Montague, R. (1960). A paradox re- gained. Notre Dame Journal of Formal Logic ,1(3), 79–90. Karmarkar , N. (1984).

Token 22320:
A new polynomial-time al- gorithm for linear programming. Combinatorica ,4, 373–395. Karp , R. M. (1972). Reducibility among combina- torial problems.

Token 22321:
In Miller, R. E. and Thatcher, J. W. (Eds. ), Complexity of Computer Computations , pp. 85–103. Plenum. Kartam , N. A. and Levitt, R. E. (1990).

Token 22322:
A constraint-based approach to construction planning of multi-story buildings. In Expert Planning Sys- tems, pp. 245–250.

Token 22323:
Institute of Electrical Engineers. Kasami , T. (1965). An efﬁcient recognition and syn- tax analysis algorithm for context-free languages. Tech.

Token 22324:
rep. AFCRL-65-758, Air Force Cambridge Research Laboratory. Kasparov , G. (1997). IBM owes me a rematch. Time ,149(21), 66–67.

Token 22325:
Kaufmann , M., Manolios, P., and Moore, J. S. (2000). Computer-Aided Reasoning: An Approach . Kluwer. Kautz , H. (2006).

Token 22326:
Deconstructing planning as satis- ﬁability. In AAAI-06 . Kautz , H., McAllester, D. A., and Selman, B. (1996). Encoding plans in propositional logic.

Token 22327:
InKR-96 , pp. 374–384. Kautz , H. and Selman, B. (1992). Planning as satis- ﬁability. In ECAI-92 , pp. 359–363.

Token 22328:
1078 Bibliography Kautz , H. and Selman, B. (1998). BLACKBOX: A new approach to the application of theorem proving to problem solving.

Token 22329:
Working Notes of the AIPS-98Workshop on Planning as Combinatorial Search. Kavraki , L., Svestka, P., Latombe, J.-C., and Over- mars, M. (1996).

Token 22330:
Probabilistic roadmaps for path planning in high-dimensi onal conﬁguration spaces. IEEE Transactions on Robotics and Automation , 12(4), 566–580.

Token 22331:
Kay, M., Gawron, J. M., and Norvig, P. (1994). Verbmobil: A Translation System for Face-To-Face Dialog . CSLI Press. Kearns , M. (1990).

Token 22332:
The Computational Complexity of Machine Learning . MIT Press. Kearns , M., Mansour, Y., and Ng, A. Y. (2000).

Token 22333:
Ap- proximate planning in large POMDPs via reusable trajectories. In Solla, S. A., Leen, T. K., and M¨ uller, K.-R. (Eds. ), NIPS 12 . MIT Press.

Token 22334:
Kearns , M. and Singh, S. P. (1998). Near-optimal reinforcement learning in polynomial time. In ICML-98 , pp. 260–268. Kearns , M. and Vazirani, U.

Token 22335:
(1994). An Introduction to Computational Learning Theory . MIT Press. Kearns , M. and Mansour, Y. (1998).

Token 22336:
A fast, bottom- up decision tree pruning algorithm with near-optimalgeneralization. In ICML-98 , pp. 269–277. Kebeasy ,R .M . ,H u s s e i n ,A .I .

Token 22337:
,a n dD a h y ,S .A . (1998). Discrimination between natural earthquakes and nuclear explosions using the Aswan Seismic Network.

Token 22338:
Annali di Geoﬁsica ,41(2), 127–140. Keeney , R. L. (1974). Multiplicative utility func- tions. Operations Research ,22, 22–34.

Token 22339:
Keeney , R. L. and Raiffa, H. (1976). Decisions with Multiple Objectives: Preferences and Value Trade- offs. Wiley. Kemp , M. (Ed.). (1989).

Token 22340:
Leonardo on Painting: An Anthology of Writings . Yale University Press. Kephart , J. O. and Chess, D. M. (2003). The vision of autonomic computing.

Token 22341:
IEEE Computer ,36(1), 41–50. Kersting , K., Raedt, L. D., and Kramer, S. (2000). Interpreting bayesian logic programs. In Proc.

Token 22342:
AAAI- 2000 Workshop on Learning Statistical Models fromRelational Data . Kessler , B., Nunberg, G., and Sch¨ utze, H. (1997).

Token 22343:
Automatic detection of text genre. CoRR ,cmp- lg/9707002 . Keynes , J. M. (1921). A Treatise on Probability . Macmillan. Khare , R. (2006).

Token 22344:
Microformats: The next (small) thing on the semantic web. IEEE Internet Comput- ing,10(1), 68–75. Khatib , O. (1986).

Token 22345:
Real-time obstacle avoidance for robot manipulator and mobile robots. Int. J. Robotics Research ,5(1), 90–98. Khmelev , D. V. and Tweedie, F. J.

Token 22346:
(2001). Using Markov chains for identiﬁcation of writer. Literary and Linguistic Computing ,16(3), 299–307. Kietz ,J .

Token 22347:
- U .a n dD uzeroski, S. (1994). Inductive logic programming and learnability. SIGART Bul- letin,5(1), 22–32.

Token 22348:
Kilgarriff , A. and Grefenstette, G. (2006). Intro- duction to the special issue on the web as corpus.Computational Linguistics ,29(3), 333–347.

Token 22349:
Kim , J. H. (1983). CONVINCE: A Conversational Inference Consolidation Engine .

Token 22350:
Ph.D. thesis, De- partment of Computer Science, University of Cali- fornia at Los Angeles.Kim , J. H. and Pearl, J. (1983).

Token 22351:
A computational model for combined causal and diagnostic reasoning in inference systems. In IJCAI-83 , pp. 190–193.

Token 22352:
Kim , J.-H., Lee, C.-H., Lee, K.-H., and Kup- puswamy, N. (2007). Evolving personality of a ge- netic robot in ubiquitous environment.

Token 22353:
In The 16th IEEE International Symposium on Robot and Hu- man interactive Communication , pp. 848–853.

Token 22354:
King , R. D., Rowland, J., Oliver, S. G., and Young, M. (2009). The automation of science. Science , 324(5923), 85–89. Kirk , D. E. (2004).

Token 22355:
Optimal Control Theory: An Introduction . Dover. Kirkpatrick , S., Gelatt, C. D., and Vecchi, M. P. (1983). Optimization by simulated annealing.

Token 22356:
Sci- ence,220, 671–680. Kister , J., Stein, P., Ulam, S., Walden, W., and Wells, M. (1957). Experiments in chess. JACM ,4, 174–177.

Token 22357:
Kisynski , J. and Poole, D. (2009). Lifted aggrega- tion in directed ﬁrst-order probabilistic models. InIJCAI-09 .

Token 22358:
Kitano , H., Asada, M., Kuniyoshi, Y., Noda, I., and Osawa, E. (1997a). RoboCup: The robot world cup initiative. In Proc.

Token 22359:
First International Conference on Autonomous Agents , pp. 340–347.

Token 22360:
Kitano , H., Asada, M., Kuniyoshi, Y., Noda, I., Os- awa, E., and Matsubara, H. (1997b). RoboCup: A challenge problem for AI. AIMag ,18(1), 73–85.

Token 22361:
Kjaerulff , U. (1992). A computational scheme for reasoning in dynamic probabilistic networks. InUAI-92 , pp. 121–129.

Token 22362:
Klein , D. and Manning, C. (2001). Parsing with tree- bank grammars: Empirical bounds, theoretical mod- els, and the structure of the Penn treebank.

Token 22363:
In ACL- 01. Klein , D. and Manning, C. (2003). A* parsing: Fast exact Viterbi parse selection. In HLT-NAACL-03 , pp. 119–126.

Token 22364:
Klein , D., Smarr, J., Nguyen, H., and Manning, C. (2003). Named entity recognition with character- level models.

Token 22365:
In Conference on Natural Language Learning (CoNLL) . Kleinberg , J. M. (1999). Authoritative sources in a hyperlinked environment.

Token 22366:
JACM ,46(5), 604–632. Klemperer , P. (2002). What really matters in auc- tion design. J. Economic Perspectives ,16(1). Kneser , R. and Ney, H. (1995).

Token 22367:
Improved backing- off for M-gram language modeling. In ICASSP-95 , pp. 181–184. Knight , K. (1999). A statistical MT tutorial work- book.

Token 22368:
Prepared in connection with the Johns Hop- kins University summer workshop. Knuth , D. E. (1964). Representing numbers using only one 4.

Token 22369:
Mathematics Magazine ,37(Nov/Dec), 308–310. Knuth , D. E. (1968). Semantics for context-free lan- guages. Mathematical Systems Theory ,2(2), 127– 145.

Token 22370:
Knuth , D. E. (1973). The Art of Computer Program- ming (second edition)., Vol. 2: Fundamental Algo- rithms. Addison-Wesley. Knuth , D. E. (1975).

Token 22371:
An analysis of alpha–beta pruning. AIJ,6(4), 293–326. Knuth , D. E. and Bendix, P. B. (1970). Simple word problems in universal algebras. In Leech, J.

Token 22372:
(Ed. ), Computational Problems in Abstract Algebra , pp. 263–267. Pergamon.Kocsis , L. and Szepesvari, C. (2006). Bandit-based Monte-Carlo planning.

Token 22373:
In ECML-06 . Koditschek , D. (1987). Exact robot navigation by means of potential functions: some topological con- siderations. In ICRA-87 , Vol.

Token 22374:
1, pp. 1–6. Koehler , J., Nebel, B., Hoffmann, J., and Dimopou- los, Y. (1997). Extending planning graphs to an ADL subset. In ECP-97 , pp. 273–285.

Token 22375:
Koehn , P. (2009). Statistical Machine Translation . Cambridge University Press. Koenderink , J. J. (1990). Solid Shape . MIT Press.

Token 22376:
Koenig , S. (1991). Optimal probabilistic and decision-theoretic planning using Markovian deci-sion theory.

Token 22377:
Master’s report, Computer Science Di- vision, University of California. Koenig , S. (2000).

Token 22378:
Exploring unknown environ- ments with real-time search or reinforcement learn-ing. In Solla, S. A., Leen, T. K., and M¨ uller, K.-R. (Eds.

Token 22379:
), NIPS 12 . MIT Press. Koenig , S. (2001). Agent-centered search. AIMag , 22(4), 109–131. Koller , D., Meggido, N., and von Stengel, B. (1996).

Token 22380:
Efﬁcient computation of equilibria for ex- tensive two-person games. Games and Economic Behaviour ,14(2), 247–259. Koller , D. and Pfeffer, A. (1997).

Token 22381:
Representations and solutions for game-theoretic problems. AIJ, 94(1–2), 167–215. Koller , D. and Pfeffer, A. (1998).

Token 22382:
Probabilistic frame-based systems. In AAAI-98 , pp. 580–587. Koller , D. and Friedman, N. (2009).

Token 22383:
Probabilis- tic Graphical Models: Principles and Techniques . MIT Press. Koller , D. and Milch, B. (2003).

Token 22384:
Multi-agent inﬂu- ence diagrams for representing and solving games. Games and Economic Behavior ,45, 181–221. Koller , D. and Parr, R. (2000).

Token 22385:
Policy iteration for factored MDPs. In UAI-00 , pp. 326–334. Koller , D. and Sahami, M. (1997).

Token 22386:
Hierarchically classifying documents using very few words. In ICML-97 , pp. 170–178. Kolmogorov , A. N. (1941).

Token 22387:
Interpolation und ex- trapolation von stationaren zufalligen folgen. Bul- letin of the Academy of Sciences of the USSR ,Ser. Math. 5 , 3–14.

Token 22388:
Kolmogorov , A. N. (1950). Foundations of the The- ory of Probability . Chelsea. Kolmogorov , A. N. (1963). On tables of random numbers.

Token 22389:
Sankhya, the Indian Journal of Statistics , Series A 25 . Kolmogorov , A. N. (1965). Three approaches to the quantitative deﬁnition of information.

Token 22390:
Problems in Information Transmission ,1(1), 1–7. Kolodner , J. (1983). Reconstructive memory: A computer model. Cognitive Science ,7, 281–328.

Token 22391:
Kolodner , J. (1993). Case-Based Reasoning .M o r - gan Kaufmann. Kondrak , G. and van Beek, P. (1997).

Token 22392:
A theoretical evaluation of selected backtracking algorithms. AIJ, 89, 365–387. Konolige , K. (1997).

Token 22393:
COLBERT: A language for re- active control in Saphira. In K¨unstliche Intelligenz: Advances in Artiﬁcial Intelligence , LNAI, pp. 31– 52.

Token 22394:
Konolige , K. (2004). Large-scale map-making. In AAAI-04 , pp. 457–463.

Token 22395:
Bibliography 1079 Konolige , K. (1982). A ﬁrst order formalization of knowledge and action for a multi-agent planning system.

Token 22396:
In Hayes, J. E., Mi chie, D., and Pao, Y.-H. (Eds. ), Machine Intelligence 10 . Ellis Horwood. Konolige , K. (1994).

Token 22397:
Easy to be hard: Difﬁcult prob- lems for greedy algorithms. In KR-94 , pp. 374–378. Koo, T., Carreras, X., and Collins, M. (2008).

Token 22398:
Sim- ple semi-supervised dependency parsing. In ACL-08 . Koopmans , T. C. (1972). Representation of pref- erence orderings over time.

Token 22399:
In McGuire, C. B. and Radner, R. (Eds. ), Decision and Organization . Elsevier/North-Holland. Korb , K. B. and Nicholson, A. (2003).

Token 22400:
Bayesian Artiﬁcial Intelligence . Chapman and Hall. Korb , K. B., Nicholson, A., and Jitnah, N. (1999). Bayesian poker. In UAI-99 .

Token 22401:
Korf , R. E. (1985a). Depth-ﬁrst iterative-deepening: an optimal admissible tree search. AIJ,27(1) , 97– 109. Korf , R. E. (1985b).

Token 22402:
Iterative-deepening A*: An op- timal admissible tree search. In IJCAI-85 , pp. 1034– 1036. Korf , R. E. (1987).

Token 22403:
Planning as search: A quantita- tive approach. AIJ,33(1) , 65–88. Korf , R. E. (1990). Real-time heuristic search. AIJ, 42(3), 189–212.

Token 22404:
Korf , R. E. (1993). Linear-space best-ﬁrst search. AIJ,62(1), 41–78. Korf , R. E. (1995). Space-efﬁcient search algo- rithms.

Token 22405:
ACM Computing Surveys ,27(3), 337–339. Korf , R. E. and Chickering, D. M. (1996). Best-ﬁrst minimax search. AIJ,84(1–2), 299–337.

Token 22406:
Korf , R. E. and Felner, A. (2002). Disjoint pattern database heuristics. AIJ,134(1–2), 9–22. Korf , R. E., Reid, M., and Edelkamp, S. (2001).

Token 22407:
Time complexity of iterative-deepening-A*. AIJ, 129, 199–218. Korf , R. E. and Zhang, W. (2000).

Token 22408:
Divide-and- conquer frontier search app lied to optimal sequence alignment. In American Association for Artiﬁcial In- telligence , pp. 910–916.

Token 22409:
Korf , R. E. (2008). Linear-time disk-based implicit graph search. JACM ,55(6). Korf , R. E. and Schultze, P. (2005).

Token 22410:
Large-scale parallel breadth-ﬁrst search. In AAAI-05 , pp. 1380– 1385. Kotok , A. (1962). A chess playing program for the IBM 7090.

Token 22411:
AI project memo 41, MIT ComputationCenter. Koutsoupias , E. and Papadimitriou, C. H. (1992). On the greedy algorithm for satisﬁability.

Token 22412:
Informa- tion Processing Letters ,43(1), 53–55. Kowalski , R. (1974). Predicate logic as a program- ming language. In Proc. IFIP Congress , pp.

Token 22413:
569– 574. Kowalski , R. (1979). Logic for Problem Solving . Elsevier/North-Holland. Kowalski , R. (1988). The early years of logic pro- gramming.

Token 22414:
CACM ,31, 38–43. Kowalski , R. and Sergot, M. (1986). A logic-based calculus of events. New Generation Computing , 4(1), 67–95. Koza , J. R. (1992).

Token 22415:
Genetic Programming: On the Programming of Computers by Means of Natural Se- lection . MIT Press.Koza , J. R. (1994).

Token 22416:
Genetic Programming II: Auto- matic discovery of reusable programs . MIT Press. Koza , J. R., Bennett, F. H., Andre, D., and Keane, M. A. (1999).

Token 22417:
Genetic Programming III: Darwinian invention and problem solving . Morgan Kaufmann. Kraus , S., Ephrati, E., and Lehmann, D. (1991).

Token 22418:
Negotiation in a non-cooperative environment. AIJ, 3(4), 255–281. Krause , A. and Guestrin, C. (2009).

Token 22419:
Optimal value of information in graphical models. JAIR ,35, 557– 591. Krause , A., McMahan, B., Guestrin, C., and Gupta, A. (2008).

Token 22420:
Robust submodular observation selection. JMLR ,9, 2761–2801. Kripke , S. A. (1963). Semantical considerations on modal logic.

Token 22421:
Acta Philosophica Fennica ,16, 83–94. Krogh , A., Brown, M., Mian, I. S., Sjolander, K., and Haussler, D. (1994).

Token 22422:
Hidden Markov models in computational biology: Applications to protein modeling. J. Molecular Biology ,235, 1501–1531.

Token 22423:
K¨ubler , S., McDonald, R., and Nivre, J. (2009). De- pendency Parsing . Morgan Claypool. Kuhn , H. W. (1953).

Token 22424:
Extensive games and the prob- lem of information. In Kuhn, H. W. and Tucker, A. W. (Eds. ), Contributions to the Theory of Games II.

Token 22425:
Princeton University Press. Kuhn , H. W. (1955). The Hungarian method for the assignment problem. Naval Research Logistics Quarterly ,2, 83–97.

Token 22426:
Kuipers , B. J. (1985). Qualitative simulation. In Bo- b r o w ,D . ( E d . ) , Qualitative Reasoning About Physi- cal Systems , pp. 169–203.

Token 22427:
MIT Press. Kuipers , B. J. and Levitt, T. S. (1988). Navigation and mapping in large-scale space. AIMag ,9(2), 25– 43. Kuipers , B. J. (2001).

Token 22428:
Qualitative simulation. In Meyers, R. A. (Ed. ), Encyclopeida of Physical Sci- ence and Technology . Academic Press.

Token 22429:
Kumar , P. R. and Varaiya, P. (1986). Stochastic Sys- tems: Estimation, Identiﬁcation, and Adaptive Con-trol. Prentice-Hall. Kumar , V. (1992).

Token 22430:
Algorithms for constraint satis- faction problems: A survey. AIMag ,13(1), 32–44. Kumar , V. and Kanal, L. N. (1983).

Token 22431:
A general branch and bound formulation for understanding andsynthesizing and/or tree search procedures. AIJ,21, 179–198.

Token 22432:
Kumar , V. and Kanal, L. N. (1988). The CDP: A unifying formulation for heuristic search, dynamicprogramming, and branch-and-bound.

Token 22433:
In Kanal, L. N. and Kumar, V. (Eds. ), Search in Artiﬁcial In- telligence , chap. 1, pp. 1–27. Springer-Verlag.

Token 22434:
Kumar , V., Nau, D. S., and Kanal, L. N. (1988). A general branch-and-bound formulation for AND/ORgraph and game tree search.

Token 22435:
In Kanal, L. N. and Kumar, V . (Eds. ), Search in Artiﬁcial Intelligence , chap. 3, pp. 91–130. Springer-Verlag.

Token 22436:
Kurien , J., Nayak, P., and Smith, D. E. (2002). Fragment-based conformant planning. In AIPS-02 . Kurzweil , R. (1990).

Token 22437:
The Age of Intelligent Ma- chines . MIT Press. Kurzweil , R. (2005). The Singularity is Near . Viking. Kwok , C., Etzioni, O., and Weld, D. S. (2001).

Token 22438:
Scal- ing question answering to the web. In Proc. 10th International Conference on the World Wide Web .Kyburg , H. E. and Teng, C.-M. (2006).

Token 22439:
Nonmono- tonic logic and statistical inference. Computational Intelligence ,22(1), 26–51. Kyburg , H. E. (1977).

Token 22440:
Randomness and the right reference class. J. Philosophy ,74(9), 501–521. Kyburg , H. E. (1983). The reference class.

Token 22441:
Philos- ophy of Science ,50, 374–397. La Mettrie , J. O. (1748). L’homme machine . E. Luzac, Leyde, France. La Mura , P. and Shoham, Y. (1999).

Token 22442:
Expected util- ity networks. In UAI-99 , pp. 366–373. Laborie , P. (2003).

Token 22443:
Algorithms for propagating re- source constraints in AI planning and scheduling.AIJ,143(2), 151–188. Ladkin , P. (1986a).

Token 22444:
Primitives and units for time speciﬁcation. In AAAI-86 , Vol. 1, pp. 354–359. Ladkin , P. (1986b).

Token 22445:
Time representation: a taxon- omy of interval relations. In AAAI-86 , Vol. 1, pp. 360–366. Lafferty , J., McCallum, A., and Pereira, F. (2001).

Token 22446:
Conditional random ﬁelds: Probabilistic models forsegmenting and labeling sequence data. In ICML-01 . Lafferty , J. and Zhai, C. (2001).

Token 22447:
Probabilistic rele- vance models based on doc ument and query genera- tion. In Proc. Workshop on Language Modeling and Information Retrieval .

Token 22448:
Lagoudakis , M. G. and Parr, R. (2003). Least- squares policy iteration. JMLR ,4, 1107–1149. Laird , J., Newell, A., and Rosenbloom, P. S. (1987).

Token 22449:
SOAR: An architecture for general intelligence. AIJ, 33(1), 1–64. Laird , J., Rosenbloom, P. S., and Newell, A. (1986).

Token 22450:
Chunking in Soar: The anatomy of a general learn-ing mechanism. Machine Learning ,1, 11–46. Laird , J. (2008).

Token 22451:
Extending the Soar cognitive ar- chitecture. In Artiﬁcial General Intelligence Confer- ence. Lakoff , G. (1987).

Token 22452:
Women, Fire, and Dangerous Things: What Categories Reveal About the Mind . University of Chicago Press. Lakoff , G. and Johnson, M. (1980).

Token 22453:
Metaphors We Live By . University of Chicago Press. Lakoff , G. and Johnson, M. (1999).

Token 22454:
Philosophy in the Flesh : The Embodied Mind and Its Challenge to Western Thought . Basic Books. Lam , J. and Greenspan, M. (2008).

Token 22455:
Eye-in-hand vi- sual servoing for accurate shooting in pool robotics. In5th Canadian Conference on Computer and Robot Vision . Lamarck , J. B. (1809).

Token 22456:
Philosophie zoologique . Chez Dentu et L’Auteur, Paris. Landhuis , E. (2004).

Token 22457:
Lifelong debunker takes on arbiter of neutral choices: Magician-turned- mathematician uncovers bias in a ﬂip of a coin. Stan- ford Report .

Token 22458:
Langdon , W. and Poli, R. (2002). Foundations of Genetic Programming . Springer.

Token 22459:
Langley , P., Simon, H. A., Bradshaw, G. L., and Zytkow, J. M. (1987). Scientiﬁc Discovery: Com- putational Explorations of the Creative Processes .

Token 22460:
MIT Press. Langton , C. (Ed.). (1995). Artiﬁcial Life . MIT Press. Laplace , P. (1816). Essai philosophique sur les probabilit´ es(3rd edition).

Token 22461:
Courcier Imprimeur, Paris.

Token 22462:
1080 Bibliography Laptev , I. and Perez, P. (2007). Retrieving actions in movies. In ICCV , pp. 1–8. Lari , K. and Young, S. J. (1990).

Token 22463:
The estimation of stochastic context-free grammars using the inside- outside algorithm. Computer Speech and Language , 4, 35–56. Larra ˜naga ,P .

Token 22464:
,K u i j p e r s ,C . ,M u r g a ,R . ,I n z a ,I . ,a n d Dizdarevic, S. (1999).

Token 22465:
Genetic algorithms for the travelling salesman problem: A review of represen-tations and operators. Artiﬁcial Intelligence Review , 13, 129–170.

Token 22466:
Larson , S. C. (1931). The shrinkage of the coef- ﬁcient of multiple correlation. J. Educational Psy- chology ,22, 45–55. Laskey , K. B. (2008).

Token 22467:
MEBN: A language for ﬁrst- order bayesian knowledge bases. AIJ,172, 140–178. Latombe , J.-C. (1991). Robot Motion Planning . Kluwer.

Token 22468:
Lauritzen , S. (1995). The EM algorithm for graphi- cal association models with missing data.

Token 22469:
Computa- tional Statistics and Data Analysis ,19, 191–201. Lauritzen , S. (1996). Graphical models . Oxford University Press.

Token 22470:
Lauritzen , S., Dawid, A. P., Larsen, B., and Leimer, H. (1990). Independence properties of directed Markov ﬁelds. Networks ,20(5), 491–505.

Token 22471:
Lauritzen , S. and Spiegelhalter, D. J. (1988).

Token 22472:
Local computations with probabilities on graphical struc- tures and their application to expert systems.

Token 22473:
J. Royal Statistical Society ,B5 0(2), 157–224. Lauritzen , S. and Wermuth, N. (1989).

Token 22474:
Graphical models for associations between variables, some of which are qualitative and some quantitative. Annals of Statistics ,17, 31–57.

Token 22475:
LaValle , S. (2006). Planning Algorithms .C a m - bridge University Press. Lavrauc ,N .a n dD uzeroski, S. (1994).

Token 22476:
Inductive Logic Programming: Techniques and Applications . Ellis Horwood. Lawler , E. L., Lenstra, J. K., Kan, A., and Shmoys, D. B. (1992).

Token 22477:
The Travelling Salesman Problem .W i - ley Interscience. Lawler , E. L., Lenstra, J. K., Kan, A., and Shmoys, D. B. (1993).

Token 22478:
Sequencing and scheduling: Algo- rithms and complexity. In Graves, S. C., Zipkin, P. H., and Kan, A. H. G. R. (Eds.

Token 22479:
), Logistics of Pro- duction and Inventory: Handbooks in Operations Research and Management Science, Volume 4 , pp. 445–522. North-Holland.

Token 22480:
Lawler , E. L. and Wood, D. E. (1966). Branch-and- bound methods: A survey. Operations Research , 14(4) , 699–719.

Token 22481:
Lazanas , A. and Latombe, J.-C. (1992). Landmark- based robot navigation. In AAAI-92 , pp. 816–822. LeCun , Y., Jackel, L., Boser, B., and Denker, J.

Token 22482:
(1989). Handwritten digit recognition: Applica- tions of neural network chips and automatic learn- ing. IEEE Communications Magazine ,27(11), 41– 46.

Token 22483:
LeCun , Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., Drucker, H., Guyon, I., Muller, U., Sackinger, E., Simard, P., and Vapnik, V. N. (1995).

Token 22484:
Comparison of learning algorithms for handwritten digit recognition. In Int. Conference on Artiﬁcial Neural Networks , pp. 53–60.

Token 22485:
Leech , G., Rayson, P., and Wilson, A. (2001). Word Frequencies in Written and Spoken English: Based on the British National Corpus .

Token 22486:
Longman.Legendre , A. M. (1805). Nouvelles m´ ethodes pour la d´etermination des orbites des com` etes.. Lehrer , J. (2009). How We Decide .

Token 22487:
Houghton Mif- ﬂin. Lenat , D. B. (1983).

Token 22488:
EURISKO: A program that learns new heuristics and domain concepts: The na-ture of heuristics, III: Program design and results.AIJ,21(1–2), 61–98.

Token 22489:
Lenat , D. B. and Brown, J. S. (1984). Why AM and EURISKO appear to work. AIJ,23(3), 269–294. Lenat , D. B. and Guha, R. V. (1990).

Token 22490:
Building Large Knowledge-Based Systems: Representation and In- ference in the CYC Project . Addison-Wesley. Leonard , H. S. and Goodman, N. (1940).

Token 22491:
The cal- culus of individuals and its uses. JSL,5(2), 45–55. Leonard , J. and Durrant-Whyte, H. (1992).

Token 22492:
Directed sonar sensing for mobile robot navigation .K l u w e r . Le´sniewski , S. (1916). Podstawy og´ olnej teorii mnogo´ sci. Moscow.

Token 22493:
Lettvin , J. Y ., Maturana, H. R., McCulloch, W. S., and Pitts, W. (1959). What the frog’s eye tells the frog’s brain. Proc. IRE ,47(11), 1940–1951.

Token 22494:
Letz , R., Schumann, J., Bayerl, S., and Bibel, W. (1992). SETHEO: A high-performance theorem prover. JAR,8(2), 183–212.

Token 22495:
Levesque , H. J. and Brachman, R. J. (1987). Ex- pressiveness and tractability in knowledge represen- tation and reasoning.

Token 22496:
Computational Intelligence , 3(2), 78–93. Levin , D. A., Peres, Y., and Wilmer, E. L. (2008). Markov Chains and Mixing Times .

Token 22497:
American Math- ematical Society. Levitt , G. M. (2000). The Turk, Chess Automaton . McFarland and Company. Levy , D. (Ed.). (1988a).

Token 22498:
Computer Chess Com- pendium . Springer-Verlag. Levy , D. (Ed.). (1988b). Computer Games . Springer-Verlag. Levy , D. (1989).

Token 22499:
The million pound bridge program. In Levy, D. and Beal, D. (Eds. ), Heuristic Program- ming in Artiﬁcial Intelligence . Ellis Horwood.

Token 22500:
Levy , D. (2007). Love and Sex with Robots .H a r p e r . Lewis , D. D. (1998).

Token 22501:
Naive Bayes at forty: The in- dependence assumption in information retrieval. InECML-98 , pp. 4–15. Lewis , D. K. (1966).

Token 22502:
An argument for the identity theory. J. Philosophy ,63(1), 17–25. Lewis , D. K. (1980). Mad pain and Martian pain. In Block, N. (Ed.

Token 22503:
), Readings in Philosophy of Psychol- ogy, Vol. 1, pp. 216–222. Harvard University Press. Leyton-Brown , K. and Shoham, Y. (2008).

Token 22504:
Essen- tials of Game Theory: A Concise, MultidisciplinaryIntroduction . Morgan Claypool. Li, C. M. and Anbulagan (1997).

Token 22505:
Heuristics based on unit propagation for satisﬁability problems. InIJCAI-97 , pp. 366–371. Li, M. and Vitanyi, P. M. B. (1993).

Token 22506:
An Introduc- tion to Kolmogorov Complexity and Its Applications . Springer-Verlag. Liberatore , P. (1997).

Token 22507:
The complexity of the lan- guage A.Electronic Transactions on Artiﬁcial Intel- ligence ,1, 13–38. Lifschitz , V. (2001).

Token 22508:
Answer set programming and plan generation. AIJ,138(1–2), 39–54.Lighthill , J. (1973). Artiﬁcial intelligence: A gen- eral survey.

Token 22509:
In Lighthill, J., Sutherland, N. S., Need- ham, R. M., Longuet-Higgins, H. C., and Michie, D. (Eds. ), Artiﬁcial Intelligence: A Paper Symposium .

Token 22510:
Science Research Council of Great Britain. Lin, S. (1965). Computer solutions of the travelling salesman problem.

Token 22511:
Bell Systems Technical Journal , 44(10) , 2245–2269. Lin, S. and Kernighan, B. W. (1973).

Token 22512:
An effective heuristic algorithm for the travelling-salesman prob- lem. Operations Research ,21(2), 498–516. Lindley , D. V. (1956).

Token 22513:
On a measure of the infor- mation provided by an experiment. Annals of Math- ematical Statistics ,27(4), 986–1005.

Token 22514:
Lindsay , R. K., Buchanan, B. G., Feigenbaum, E. A., and Lederberg, J. (1980).

Token 22515:
Applications of Arti- ﬁcial Intelligence for Organic Chemistry: The DEN-DRAL Project . McGraw-Hill. Littman , M. L. (1994).

Token 22516:
Markov games as a frame- work for multi-agent reinforcement learning. In ICML-94 , pp. 157–163.

Token 22517:
Littman , M. L., Keim, G. A., and Shazeer, N. M. (1999). Solving crosswords with PROVERB. In AAAI-99 , pp. 914–915. Liu, J. S. and Chen, R. (1998).

Token 22518:
Sequential Monte Carlo methods for dynamic systems. JASA ,93, 1022–1031. Livescu , K., Glass, J., and Bilmes, J. (2003).

Token 22519:
Hidden feature modeling for speech recognition using dy-namic Bayesian networks. In EUROSPEECH-2003 , pp. 2529–2532.

Token 22520:
Livnat , A. and Pippenger, N. (2006). An optimal brain can be composed of conﬂicting agents. PNAS , 103(9), 3198–3202. Locke , J. (1690).

Token 22521:
An Essay Concerning Human Un- derstanding . William Tegg. Lodge , D. (1984). Small World . Penguin Books. Loftus , E. and Palmer, J. (1974).

Token 22522:
Reconstruction of automobile destruction: An example of the interac- tion between language and memory.

Token 22523:
J. Verbal Learn- ing and Verbal Behavior ,13, 585–589. Lohn , J. D., Kraus, W. F., and Colombano, S. P. (2001).

Token 22524:
Evolutionary optimization of yagi-uda an-tennas. In Proc. Fourth International Conference on Evolvable Systems , pp. 236–243.

Token 22525:
Longley , N. and Sankaran, S. (2005). The NHL’s overtime-loss rule: Empirically analyzing the unin- tended effects. Atlantic Economic Journal .

Token 22526:
Longuet-Higgins , H. C. (1981). A computer algo- rithm for reconstructing a scene from two projec- tions. Nature ,293, 133–135.

Token 22527:
Loo, B. T., Condie, T., Garofalakis, M., Gay, D. E., Hellerstein, J. M., Maniatis, P., Ramakrishnan, R., Roscoe, T., and Stoica, I. (2006).

Token 22528:
Declarative net-working: Language, execution and optimization. In SIGMOD-06 . Love , N., Hinrichs, T., and Genesereth, M. R. (2006).

Token 22529:
General game playing: Game descrip- tion language speciﬁcation. Tech. rep. LG-2006-01,Stanford University Computer Science Dept.

Token 22530:
Lovejoy , W. S. (1991). A survey of algorithmic methods for partially observed Markov decision pro-cesses.

Token 22531:
Annals of Operations Research ,28(1–4), 47– 66. Loveland , D. (1970). A linear format for resolution. InProc.

Token 22532:
IRIA Symposium on Automatic Demonstra- tion, pp. 147–162.

Token 22533:
Bibliography 1081 Lowe , D. (1987). Three-dimensional object recog- nition from single two-dimensional images. AIJ,31, 355–395. Lowe , D. (1999).

Token 22534:
Object recognition using local scale invariant feature. In ICCV . Lowe , D. (2004). Distinctive image features from scale-invariant keypoints.

Token 22535:
IJCV ,60(2), 91–110. L¨owenheim , L. (1915). ¨Uber m¨ oglichkeiten im Rel- ativkalk¨ ul.Mathematische Annalen ,76, 447–470. Lowerre , B. T. (1976).

Token 22536:
TheHARPY Speech Recog- nition System . Ph.D. thesis, Computer Science De- partment, Carnegie-Mellon University. Lowerre , B. T. and Reddy, R. (1980).

Token 22537:
The HARPY speech recognition system. In Lea, W. A. (Ed. ), Trends in Speech Recognition , chap. 15. Prentice- Hall. Lowry , M. (2008).

Token 22538:
Intelligent software engineering tools for NASA’s crew exploration vehicle. In Proc. ISMIS . Loyd , S. (1959).

Token 22539:
Mathematical Puzzles of Sam Loyd: Selected and Edited by Martin Gardner . Dover. Lozano-Perez , T. (1983).

Token 22540:
Spatial planning: A con- ﬁguration space approach. IEEE Transactions on Computers ,C-32 (2), 108–120.

Token 22541:
Lozano-Perez , T., Mason, M., and Taylor, R. (1984). Automatic synthesis of ﬁne-motion strate- gies for robots. Int. J. Robotics Research ,3(1), 3–24.

Token 22542:
Lu, F. and Milios, E. (1997). Globally consistent range scan alignment for environment mapping. Au- tonomous Robots ,4, 333–349.

Token 22543:
Luby , M., Sinclair, A., and Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms. Informa- tion Processing Letters ,47, 173–180.

Token 22544:
Lucas , J. R. (1961). Minds, machines, and G¨ odel. Philosophy ,36. Lucas , J. R. (1976). This G¨ odel is killing me: A rejoinder.

Token 22545:
Philosophia ,6(1), 145–148. Lucas , P. (1996). Knowledge acquisition for decision-theoretic expert systems. AISB Quarterly , 94, 23–33.

Token 22546:
Lucas , P., van der Gaag, L., and Abu-Hanna, A. (2004). Bayesian networks in biomedicine and health-care. Artiﬁcial Intelligence in Medicine .

Token 22547:
Luce , D. R. and Raiffa, H. (1957). Games and De- cisions . Wiley. Ludlow , P., Nagasawa, Y., and Stoljar, D. (2004). There’s Something About Mary .

Token 22548:
MIT Press. Luger , G. F. (Ed.). (1995). Computation and intelli- gence: Collected readings . AAAI Press. Lyman , P. and Varian, H. R. (2003).

Token 22549:
How much information? www.sims.berkeley. edu/how-much-info-2003 . Machina , M. (2005). Choice under uncertainty.

Token 22550:
InEncyclopedia of Cognitive Science , pp. 505–514. Wiley. MacKay , D. J. C. (1992). A practical Bayesian framework for back-propagation networks.

Token 22551:
Neural Computation ,4(3), 448–472. MacKay , D. J. C. (2002). Information Theory, In- ference and Learning Algorithms . Cambridge Uni- versity Press.

Token 22552:
MacKenzie , D. (2004). Mechanizing Proof .M I T Press. Mackworth , A. K. (1977). Consistency in networks of relations.

Token 22553:
AIJ,8(1), 99–118.Mackworth , A. K. (1992). Constraint satisfaction. In Shapiro, S. (Ed.

Token 22554:
), Encyclopedia of Artiﬁcial Intel- ligence (second edition)., Vol. 1, pp. 285–293. Wi- ley. Mahanti , A. and Daniels, C. J. (1993).

Token 22555:
A SIMD ap- proach to parallel heuristic search. AIJ,60(2), 243– 282. Mailath , G. and Samuelson, L. (2006).

Token 22556:
Repeated Games and Reputations: Long-Run Relationships . Oxford University Press. Majercik , S. M. and Littman, M. L. (2003).

Token 22557:
Contin- gent planning under uncertainty via stochastic satis- ﬁability. AIJ, pp. 119–162. Malik , J. and Perona, P. (1990).

Token 22558:
Preattentive texture discrimination with early vision mechanisms. J. Opt. Soc. Am. A ,7(5), 923–932. Malik , J. and Rosenholtz, R. (1994).

Token 22559:
Recovering surface curvature and orientation from texture distor- tion: A least squares algorithm and sensitivity anal- ysis. In ECCV , pp. 353–364.

Token 22560:
Malik , J. and Rosenholtz, R. (1997). Computing local surface orientation and shape from texture for curved surfaces. IJCV ,23(2), 149–168.

Token 22561:
Maneva , E., Mossel, E., and Wainwright, M. J. (2007). A new look at survey propagation and its generalizations. JACM ,54(4).

Token 22562:
Manna , Z. and Waldinger, R. (1971). Toward auto- matic program synthesis. CACM ,14(3), 151–165. Manna , Z. and Waldinger, R. (1985).

Token 22563:
The Logical Basis for Computer Programming: Volume 1: De- ductive Reasoning . Addison-Wesley. Manning ,C .a n dS c h ¨ utze, H. (1999).

Token 22564:
Foundations of Statistical Natural Language Processing .M I T Press. Manning , C., Raghavan, P., and Sch¨ utze, H. (2008).

Token 22565:
Introduction to Information Retrieval . Cambridge University Press. Mannion , M. (2002). Using ﬁrst-order logic for product line model validation.

Token 22566:
In Software Product Lines: Second International Conference . Springer. Manzini , G. (1995). BIDA*: An improved perime- ter search algorithm.

Token 22567:
AIJ,72(2), 347–360. Marbach , P. and Tsitsiklis, J. N. (1998).

Token 22568:
Simulation- based optimization of Markov reward processes.Technical report LIDS-P-2411, Laboratory for Infor- mation and Decision Systems, Massachusetts Insti- tute of Technology.

Token 22569:
Marcus , G. (2009). Kluge: The Haphazard Evolu- tion of the Human Mind . Mariner Books. Marcus , M. P., Santorini, B., and Marcinkiewicz, M. A.

Token 22570:
(1993). Building a large annotated corpus of english: The penn treebank. Computational Linguis- tics,19(2), 313–330. Markov , A. A. (1913).

Token 22571:
An example of statistical investigation in the text of “Eugene Onegin” illus- trating coupling of “tests” in chains. Proc.

Token 22572:
Academy of Sciences of St. Petersburg ,7. Maron , M. E. (1961). Automatic indexing: An ex- perimental inquiry. JACM ,8(3), 404–417.

Token 22573:
Maron , M. E. and Kuhns, J.-L. (1960). On rel- evance, probabilistic indexing and information re- trieval. CACM ,7, 219–244. Marr , D. (1982).

Token 22574:
Vision: A Computational Investi- gation into the Human Representation and Process-ing of Visual Information . W. H. Freeman.

Token 22575:
Marriott , K. and Stuckey, P. J. (1998). Program- ming with Constraints: An Introduction . MIT Press.Marsland , A. T. and Schaeffer, J. (Eds.).

Token 22576:
(1990). Computers, Chess, and Cognition . Springer-Verlag. Marsland , S. (2009). Machine Learning: An Algo- rithmic Perspective . CRC Press.

Token 22577:
Martelli , A. and Montanari, U. (1973). Additive AND/OR graphs. In IJCAI-73 , pp. 1–11. Martelli , A. and Montanari, U. (1978).

Token 22578:
Optimizing decision trees through heuristically guided search.CACM ,21, 1025–1039. Martelli , A. (1977).

Token 22579:
On the complexity of admissi- ble search algorithms. AIJ, 8(1), 1–13. Marthi , B., Pasula, H., Russell, S. J., and Peres, Y . (2002).

Token 22580:
Decayed MCMC ﬁltering. In UAI-02 , pp. 319–326. Marthi , B., Russell, S. J., Latham, D., and Guestrin, C. (2005).

Token 22581:
Concurrent hierarchical reinforcement learning. In IJCAI-05 . Marthi , B., Russell, S. J., and Wolfe, J. (2007).

Token 22582:
An- gelic semantics for high-level actions. In ICAPS-07 . Marthi , B., Russell, S. J., and Wolfe, J. (2008).

Token 22583:
An- gelic hierarchical planning: Optimal and online al- gorithms. In ICAPS-08 . Martin , D., Fowlkes, C., and Malik, J. (2004).

Token 22584:
Learning to detect natural image boundaries usinglocal brightness, color, and texture cues. PAMI , 26(5), 530–549. Martin , J. H. (1990).

Token 22585:
A Computational Model of Metaphor Interpretation . Academic Press. Mason , M. (1993). Kicking the sensing habit. AIMag ,14(1), 58–59.

Token 22586:
Mason , M. (2001). Mechanics of Robotic Manipu- lation . MIT Press. Mason , M. and Salisbury, J. (1985).

Token 22587:
Robot hands and the mechanics of manipulation . MIT Press. Mataric , M. J. (1997). Reinforcement learning in the multi-robot domain.

Token 22588:
Autonomous Robots ,4(1), 73–83. Mates , B. (1953). Stoic Logic . University of Cali- fornia Press.

Token 22589:
Matuszek , C., Cabral, J., Witbrock, M., and DeO- liveira, J. (2006). An introduction to the syntax and semantics of cyc. In Proc.

Token 22590:
AAAI Spring Symposium on Formalizing and Compiling Background Knowl-edge and Its Applications to Knowledge Representa-tion and Question Answering .

Token 22591:
Maxwell , J. and Kaplan, R. (1993). The interface between phrasal and functional constraints. Compu- tational Linguistics ,19(4), 571–590.

Token 22592:
McAllester , D. A. (1980). An outlook on truth main- tenance. Ai memo 551, MIT AI Laboratory. McAllester , D. A. (1988).

Token 22593:
Conspiracy numbers for min-max search. AIJ,35(3), 287–310. McAllester , D. A. (1998). What is the most press- ing issue facing AI and the AAAI today?

Token 22594:
Candidatestatement, election for Councilor of the American Association for Artiﬁcial Intelligence. McAllester , D. A. and Rosenblitt, D. (1991).

Token 22595:
Sys- tematic nonlinear planning. In AAAI-91 ,V o l .2 ,p p . 634–639. McCallum , A. (2003). Efﬁciently inducing features of conditional random ﬁelds.

Token 22596:
In UAI-03 . McCarthy , J. (1958). Programs with common sense. In Proc. Symposium on Mechanisation of Thought Processes , Vol. 1, pp. 77–84.

Token 22597:
McCarthy , J. (1963). Situations, actions, and causal laws. Memo 2, Stanford University Artiﬁcial Intelli- gence Project.

Token 22598:
1082 Bibliography McCarthy , J. (1968). Programs with common sense. In Minsky, M. L. (Ed. ), Semantic Informa- tion Processing , pp. 403–418.

Token 22599:
MIT Press. McCarthy , J. (1980). Circumscription: A form of non-monotonic reasoning. AIJ,13(1–2), 27–39. McCarthy , J. (2007).

Token 22600:
From here to human-level AI. AIJ,171(18), 1174–1182. McCarthy , J. and Hayes, P. J. (1969).

Token 22601:
Some philo- sophical problems from the standpoint of artiﬁcialintelligence. In Meltzer, B., Michie, D., and Swann, M. (Eds.

Token 22602:
), Machine Intelligence 4 , pp. 463–502. Ed- inburgh University Press. McCarthy ,J . ,M i n s k y ,M .L . ,R o c h e s t e r ,N .

Token 22603:
,a n d Shannon, C. E. (1955). Proposal for the Dartmouth summer research project on artiﬁcial intelligence.Tech. rep., Dartmouth College.

Token 22604:
McCawley , J. D. (1988). The Syntactic Phenomena of English , Vol. 2 volumes. University of Chicago Press. McCorduck , P. (2004).

Token 22605:
Machines who think: a per- sonal inquiry into the history and prospects of artiﬁ-cial intelligence (Revised edition). A K Peters.

Token 22606:
McCulloch , W. S. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.Bulletin of Mathematical Biophysics ,5, 115–137.

Token 22607:
McCune , W. (1992). Automated discovery of new axiomatizations of the left group and right group cal-culi. JAR,9(1), 1–24. McCune , W. (1997).

Token 22608:
Solution of the Robbins prob- lem. JAR,19(3), 263–276. McDermott , D. (1976). Artiﬁcial intelligence meets natural stupidity.

Token 22609:
SIGART Newsletter ,57, 4–9. McDermott , D. (1978a). Planning and acting. Cog- nitive Science ,2(2), 71–109. McDermott , D. (1978b).

Token 22610:
Tarskian semantics, or, no notation without denotation! Cognitive Science , 2(3). McDermott , D. (1985). Reasoning about plans.

Token 22611:
In Hobbs, J. and Moore, R. (Eds. ), Formal theories of the commonsense world . Intellect Books. McDermott , D. (1987). A critique of pure reason.

Token 22612:
Computational Intelligence ,3(3), 151–237. McDermott , D. (1996). A heuristic estimator for means-ends analysis in planning. In ICAPS-96 , pp.

Token 22613:
142–149. McDermott , D. and Doyle, J. (1980). Non- monotonic logic: i. AIJ,13(1–2), 41–72. McDermott , J. (1982).

Token 22614:
R1: A rule-based conﬁgurer of computer systems. AIJ,19(1), 39–88. McEliece , R. J., MacKay, D. J. C., and Cheng, J.- F. (1998).

Token 22615:
Turbo decoding as an instance of Pearl’s“belief propagation” algorithm. IEEE Journal on Se- lected Areas in Communications ,16(2), 140–152.

Token 22616:
McGregor , J. J. (1979). Relational consistency al- gorithms and their application in ﬁnding subgraphand graph isomorphisms.

Token 22617:
Information Sciences , 19(3), 229–250. McIlraith , S. and Zeng, H. (2001). Semantic web services. IEEE Intelligent Systems ,16(2), 46–53.

Token 22618:
McLachlan , G. J. and Krishnan, T. (1997). The EM Algorithm and Extensions . Wiley. McMillan , K. L. (1993). Symbolic Model Checking . Kluwer.

Token 22619:
Meehl , P. (1955). Clinical vs. Statistical Prediction . University of Minnesota Press.Mendel , G. (1866). Versuche ¨ uber pﬂanzen- hybriden.

Token 22620:
Verhandlungen des Naturforschenden Vereins, Abhandlungen, Br¨ unn,4, 3–47. Translated into English by C. T. Druery, published by Bateson(1902).

Token 22621:
Mercer , J. (1909). Functions of positive and nega- tive type and their connection with the theory of in- tegral equations. Philos. Trans. Roy. Soc.

Token 22622:
London, A , 209, 415–446. Merleau-Ponty , M. (1945). Phenomenology of Per- ception . Routledge.

Token 22623:
Metropolis , N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, E. (1953). Equations of statecalculations by fast computing machines.

Token 22624:
J. Chemi- cal Physics ,21, 1087–1091. Metzinger , T. (2009). The Ego Tunnel: The Science of the Mind and the Myth of the Self . Basic Books.

Token 22625:
M´ezard , M. and Nadal, J.-P. (1989). Learning in feedforward layered networks: The tiling algorithm. J. Physics ,22, 2191–2204.

Token 22626:
Michalski , R. S. (1969). On the quasi-minimal so- lution of the general covering problem. In Proc.

Token 22627:
First International Symposium on Information Pro- cessing , pp. 125–128. Michalski , R. S., Mozetic, I., Hong, J., and Lavra uc, N. (1986).

Token 22628:
The multi-purpose incremental learn-ing system AQ15 and its testing application to threemedical domains. In AAAI-86 , pp. 1041–1045.

Token 22629:
Michie , D. (1966). Game-playing and game- learning automata. In Fox, L. (Ed. ), Advances in Programming and Non-Numerical Computation , pp. 183–200.

Token 22630:
Pergamon. Michie , D. (1972). Machine intelligence at Edin- burgh. Management Informatics ,2(1), 7–12. Michie , D. (1974).

Token 22631:
Machine intelligence at Edin- burgh. In On Intelligence , pp. 143–155. Edinburgh University Press. Michie , D. and Chambers, R. A. (1968).

Token 22632:
BOXES: An experiment in adaptive control. In Dale, E. andM i c h i e ,D . ( E d s . ) , Machine Intelligence 2 , pp. 125– 133. Elsevier/North-Holland.

Token 22633:
Michie , D., Spiegelhalter, D. J., and Taylor, C. (Eds.). (1994). Machine Learning, Neural and Sta- tistical Classiﬁcation . Ellis Horwood.

Token 22634:
Milch , B., Marthi, B., Sontag, D., Russell, S. J., Ong, D., and Kolobov, A. (2005). BLOG: Proba-bilistic models with unknown objects. In IJCAI-05 .

Token 22635:
Milch , B., Zettlemoyer, L. S., Kersting, K., Haimes, M., and Kaelbling, L. P. (2008). Lifted probabilisticinference with counting formulas.

Token 22636:
In AAAI-08 , pp. 1062–1068. Milgrom , P. (1997). Putting auction theory to work: The simultaneous ascending auction. Tech.

Token 22637:
rep. Technical Report 98-0002, Stanford University De- partment of Economics. Mill, J. S. (1843).

Token 22638:
A System of Logic, Ratiocinative and Inductive: Being a Connected View of the Prin- ciples of Evidence, and Methods of Scientiﬁc Inves- tigation .

Token 22639:
J. W. Parker, London. Mill, J. S. (1863). Utilitarianism .P a r k e r , S o n a n d Bourn, London.

Token 22640:
Miller , A. C., Merkhofer, M. M., Howard, R. A., Matheson, J. E., and Rice, T. R. (1976). Develop- ment of automated aids for decision analysis.

Token 22641:
Tech- nical report, SRI International. Minker , J. (2001). Logic-Based Artiﬁcial Intelli- gence .K l u w e r .Minsky , M. L. (1975).

Token 22642:
A framework for represent- ing knowledge. In Winston, P. H. (Ed. ), The Psychol- ogy of Computer Vision , pp. 211–277. McGraw-Hill.

Token 22643:
Originally an MIT AI Laboratory memo; the 1975version is abridged, but is the most widely cited. Minsky , M. L. (1986).

Token 22644:
The society of mind .S i m o n and Schuster. Minsky , M. L. (2007).

Token 22645:
The Emotion Machine: Com- monsense Thinking, Artiﬁcial Intelligence, and theFuture of the Human Mind . Simon and Schuster.

Token 22646:
Minsky , M. L. and Papert, S. (1969). Perceptrons: An Introduction to Computational Geometry (ﬁrst edition). MIT Press.

Token 22647:
Minsky , M. L. and Papert, S. (1988). Perceptrons: An Introduction to Computational Geometry (Ex- panded edition). MIT Press.

Token 22648:
Minsky , M. L., Singh, P., and Sloman, A. (2004).

Token 22649:
The st. thomas common sense symposium: De- signing architectures for human-level intelligence.AIMag ,25(2), 113–124. Minton , S. (1984).

Token 22650:
Constraint-based generalization: Learning game-playing plans from single examples. InAAAI-84 , pp. 251–254. Minton , S. (1988).

Token 22651:
Quantitative results concerning the utility of explanation-based learning. In AAAI- 88, pp. 564–569. Minton , S., Johnston, M. D., Philips, A.

Token 22652:
B., and Laird, P. (1992). Minimizing conﬂicts: A heuris- tic repair method for constraint satisfaction and scheduling problems. AIJ,58(1–3), 161–205.

Token 22653:
Misak , C. (2004). The Cambridge Companion to Peirce . Cambridge University Press. Mitchell , M. (1996). An Introduction to Genetic Al- gorithms .

Token 22654:
MIT Press. Mitchell , M., Holland, J. H., and Forrest, S. (1996). When will a genetic algorithm outperform hillclimbing?

Token 22655:
In Cowan, J., Tesauro, G., and Alspec-tor, J. (Eds. ), NIPS 6 . MIT Press. Mitchell , T. M. (1977).

Token 22656:
Version spaces: A candidate elimination approach to rule learning. In IJCAI-77 , pp. 305–310. Mitchell , T. M. (1982). Generalization as search.

Token 22657:
AIJ,18(2), 203–226. Mitchell , T. M. (1990). Becoming increasingly reac- tive (mobile robots). In AAAI-90 , Vol. 2, pp. 1051– 1058.

Token 22658:
Mitchell , T. M. (1997). Machine Learning . McGraw-Hill. Mitchell , T. M., Keller, R., and Kedar-Cabelli, S. (1986).

Token 22659:
Explanation-based generalization: A unify- ing view. Machine Learning ,1, 47–80. Mitchell , T. M., Utgoff, P. E., and Banerji, R. (1983).

Token 22660:
Learning by experimentation: Acquiring and reﬁning problem-solving heuristics. In Michalski, R. S., Carbonell, J. G., and Mitchell, T. M. (Eds.

Token 22661:
), Machine Learning: An Artiﬁcial Intelligence Ap- proach , pp. 163–190. Morgan Kaufmann. Mitchell , T. M. (2005).

Token 22662:
Reading the web: A break- through goal for AI. AIMag ,26(3), 12–16. Mitchell , T. M. (2007). Learning, information ex- traction and the web.

Token 22663:
In ECML/PKDD ,p .1 . Mitchell , T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A., andJust, M. A. (2008).

Token 22664:
Predicting human brain activ-ity associated with the meanings of nouns. Science , 320, 1191–1195. Mohr , R. and Henderson, T. C. (1986).

Token 22665:
Arc and path consistency revisited. AIJ,28(2), 225–233.

Token 22666:
Bibliography 1083 Mohri , M., Pereira, F., and Riley, M. (2002). Weighted ﬁnite-state transducers in speech recogni- tion.

Token 22667:
Computer Speech and Language ,16(1), 69–88. Montague , P. R., Dayan, P., Person, C., and Se- jnowski, T. (1995).

Token 22668:
Bee foraging in uncertain envi- ronments using predictive Hebbian learning. Nature , 377, 725–728. Montague , R. (1970). English as a formal language.

Token 22669:
InLinguaggi nella Societ` a e nella Tecnica , pp. 189– 224. Edizioni di Comunit` a. Montague , R. (1973).

Token 22670:
The proper treatment of quan- tiﬁcation in ordinary English. In Hintikka, K. J. J., Moravcsik, J. M. E., and Suppes, P. (Eds.

Token 22671:
), Ap- proaches to Natural Language .D .R e i d e l . Montanari , U. (1974).

Token 22672:
Networks of constraints: Fundamental properties and applications to picture processing. Information Sciences ,7(2), 95–132.

Token 22673:
Montemerlo , M. and Thrun, S. (2004). Large-scale robotic 3-D mapping of urban structures. In Proc. International Symposium on Experimental Robotics .

Token 22674:
Springer Tracts in Advanced Robotics (STAR). Montemerlo , M., Thrun, S., Koller, D., and Weg- breit, B. (2002).

Token 22675:
FastSLAM: A factored solution tothe simultaneous localiza tion and mapping problem. InAAAI-02 . Mooney , R. (1999).

Token 22676:
Learning for semantic interpre- tation: Scaling up without dumbing down. In Proc. 1st Workshop on Learning Language in Logic , pp. 7–15.

Token 22677:
Moore , A. and Wong, W.-K. (2003).

Token 22678:
Optimal rein- sertion: A new search operator for accelerated and more accurate Bayesian network structure learning.InICML-03 .

Token 22679:
Moore , A. W. and Atkeson, C. G. (1993). Prior- itized sweeping—Reinforcement learning with less data and less time. Machine Learning ,13, 103–130.

Token 22680:
Moore , A. W. and Lee, M. S. (1997). Cached suf- ﬁcient statistics for efﬁcient machine learning with large datasets. JAIR ,8, 67–91.

Token 22681:
Moore , E. F. (1959). The shortest path through a maze. In Proc. an International Symposium on the Theory of Switching, Part II , pp. 285–292.

Token 22682:
Harvard University Press. Moore , R. C. (1980). Reasoning about knowledge and action.

Token 22683:
Artiﬁcial intelligence center technical note 191, SRI International. Moore , R. C. (1985). A formal theory of knowl- edge and action.

Token 22684:
In Hobbs, J. R. and Moore, R. C. (Eds. ), Formal Theories of the Commonsense World , pp. 319–358. Ablex. Moore , R. C. (2005).

Token 22685:
Association-based bilingual word alignment. In Proc. ACL-05 Workshop on Building and Using Parallel Texts , pp. 1–8. Moravec , H. P. (1983).

Token 22686:
The stanford cart and the cmu rover. Proc. IEEE ,71(7), 872–884. Moravec , H. P. and Elfes, A. (1985). High resolu- tion maps from wide angle sonar.

Token 22687:
In ICRA-85 , pp. 116–121. Moravec , H. P. (1988). Mind Children: The Future of Robot and Human Intelligence . Harvard Univer- sity Press.

Token 22688:
Moravec , H. P. (2000). Robot: Mere Machine to Transcendent Mind . Oxford University Press. Morgenstern , L. (1998).

Token 22689:
Inheritance comes of age: Applying nonmonotonic techniques to problems in industry. AIJ,103, 237–271.Morjaria ,M .A . ,R i n k ,F .J .

Token 22690:
,S m i t h ,W .D . ,K l e m p - ner, G., Burns, C., and Stein, J. (1995).

Token 22691:
Elicitation of probabilities for belief networks: Combining quali-tative and quantitative information. In UAI-95 , pp. 141–148.

Token 22692:
Morrison , P. and Morrison, E. (Eds.). (1961). Charles Babbage and His Calculating Engines: Se- lected Writings by Charles Babbage and Others . Dover.

Token 22693:
Moskewicz , M. W., Madigan, C. F., Zhao, Y., Zhang, L., and Malik, S. (2001). Chaff: Engineer-ing an efﬁcient SAT solver. In Proc.

Token 22694:
38th Design Automation Conference (DAC 2001) , pp. 530–535. Mosteller , F. and Wallace, D. L. (1964).

Token 22695:
Inference and Disputed Authorship: The Federalist . Addison- Wesley. Mostow , J. and Prieditis, A. E. (1989).

Token 22696:
Discovering admissible heuristics by abstracting and optimizing: A transformational approach. In IJCAI-89 ,V o l .1 , pp. 701–707.

Token 22697:
Motzkin , T. S. and Schoenberg, I. J. (1954). The relaxation method for linear inequalities. Canadian Journal of Mathematics ,6(3), 393–404.

Token 22698:
Moutarlier , P. and Chatila, R. (1989). Stochastic multisensory data fusion for mobile robot location and environment modeling. In ISRR-89 .

Token 22699:
Mueller , E. T. (2006). Commonsense Reasoning . Morgan Kaufmann. Muggleton , S. H. (1991). Inductive logic program- ming.

Token 22700:
New Generation Computing ,8, 295–318. Muggleton , S. H. (1992). Inductive Logic Program- ming . Academic Press. Muggleton , S. H. (1995).

Token 22701:
Inverse entailment and Progol. New Generation Computing ,13(3-4), 245– 286. Muggleton , S. H. (2000). Learning stochastic logic programs. Proc.

Token 22702:
AAAI 2000 Workshop on Learning Statistical Models from Relational Data. Muggleton , S. H. and Buntine, W. (1988).

Token 22703:
Machine invention of ﬁrst-order predicates by inverting reso- lution. In ICML-88 , pp. 339–352. Muggleton , S. H. and De Raedt, L. (1994).

Token 22704:
Induc- tive logic programming: Theory and methods. J. Logic Programming ,19/20 , 629–679. Muggleton , S. H. and Feng, C. (1990).

Token 22705:
Efﬁcient in- duction of logic programs. In Proc. Workshop on Algorithmic Learning Theory , pp. 368–381. M¨uller , M. (2002). Computer Go.

Token 22706:
AIJ,134(1–2), 145–179. M¨uller , M. (2003). Conditional combinatorial games, and their applica tion to analyzing capturing races in go.

Token 22707:
Information Sciences ,154(3–4), 189– 202. Mumford , D. and Shah, J. (1989).

Token 22708:
Optimal approx- imations by piece-wise smooth functions and asso- ciated variational problems. Commun. Pure Appl. Math. ,42, 577–685.

Token 22709:
Murphy , K., Weiss, Y., and Jordan, M. I. (1999). Loopy belief propagation f or approximate inference: An empirical study. In UAI-99 , pp. 467–475.

Token 22710:
Murphy , K. (2001). The Bayes net toolbox for MATLAB. Computing Science and Statistics ,33. Murphy , K. (2002).

Token 22711:
Dynamic Bayesian Networks: Representation, Inference and Learning .P h . D . t h e - sis, UC Berkeley. Murphy , K. and Mian, I. S. (1999).

Token 22712:
Modelling gene expression data using Bayesian networks. people.cs.ubc.ca/˜murphyk/Papers/ ismb99.pdf .Murphy , K. and Russell, S. J. (2001).

Token 22713:
Rao- blackwellised particle ﬁltering for dynamic Bayesian networks. In Doucet, A., de Freitas, N., and Gordon, N. J. (Eds.

Token 22714:
), Sequential Monte Carlo Methods in Practice . Springer-Verlag. Murphy , K. and Weiss, Y. (2001).

Token 22715:
The fac- tored frontier algorithm for approximate inference in DBNs. In UAI-01 , pp. 378–385. Murphy , R. (2000). Introduction to AI Robotics .

Token 22716:
MIT Press. Murray-Rust , P., Rzepa, H. S., Williamson, J., and Willighagen, E. L. (2003). Chemical markup, XMLand the world–wide web. 4. CML schema.

Token 22717:
J. Chem. Inf. Comput. Sci. ,43, 752–772. Murthy , C. and Russell, J. R. (1990). A constructive proof of Higman’s lemma. In LICS-90 , pp. 257–269.

Token 22718:
Muscettola , N. (2002). Computing the envelope for stepwise-constant resource allocations. In CP-02 , pp. 139–154.

Token 22719:
Muscettola , N., Nayak, P., Pell, B., and Williams, B. (1998). Remote agent: To boldly go where no AIsystem has gone before. AIJ,103, 5–48.

Token 22720:
Muslea , I. (1999). Extraction patterns for informa- tion extraction tasks: A survey. In Proc.

Token 22721:
AAAI-99 Workshop on Machine Learning for Information Ex-traction . Myerson , R. (1981). Optimal auction design.

Token 22722:
Math- ematics of Operations Research ,6, 58–73. Myerson , R. (1986). Multistage games with com- munication. Econometrica ,54, 323–358.

Token 22723:
Myerson , R. (1991). Game Theory: Analysis of Conﬂict . Harvard University Press. Nagel , T. (1974). What is it like to be a bat?

Token 22724:
Philo- sophical Review ,83, 435–450. Nalwa , V. S. (1993). A Guided Tour of Computer Vision . Addison-Wesley. Nash , J. (1950).

Token 22725:
Equilibrium points in N-person games. PNAS ,36, 48–49. Nau, D. S. (1980). Pathology on game trees: A sum- mary of results. In AAAI-80 , pp. 102–104.

Token 22726:
Nau, D. S. (1983). Pathology on game trees revis- ited, and an alternative to minimaxing. AIJ,21(1–2), 221–244.

Token 22727:
Nau, D. S., Kumar, V., and Kanal, L. N. (1984). General branch and bound, and its relation to A* andAO*. AIJ,23, 29–58. Nayak , P. and Williams, B.

Token 22728:
(1997). Fast context switching in real-time propositional reasoning. InAAAI-97 , pp. 50–56. Neal , R. (1996).

Token 22729:
Bayesian Learning for Neural Net- works . Springer-Verlag. Nebel , B. (2000).

Token 22730:
On the compilability and expres- sive power of propositional planning formalisms.JAIR ,12, 271–315.

Token 22731:
Neﬁan , A., Liang, L., Pi, X., Liu, X., and Murphy, K. (2002). Dynamic bayesian networks for audio- visual speech recognition.

Token 22732:
EURASIP , Journal of Ap- plied Signal Processing ,11, 1–15. Nesterov , Y. and Nemirovski, A. (1994).

Token 22733:
Interior- Point Polynomial Methods in Convex Programming . SIAM (Society for Industr ial and Applied Mathe- matics). Netto , E. (1901).

Token 22734:
Lehrbuch der Combinatorik .B . G. Teubner. Nevill-Manning , C. G. and Witten, I. H. (1997).

Token 22735:
Identifying hierarchical structures in sequences: A linear-time algorithm. JAIR ,7, 67–82.

Token 22736:
1084 Bibliography Newell , A. (1982). The knowledge level. AIJ,18(1), 82–127. Newell , A. (1990). Uniﬁed Theories of Cognition .

Token 22737:
Harvard University Press. Newell , A. and Ernst, G. (1965). The search for gen- erality. In Proc. IFIP Congress , Vol. 1, pp. 17–24.

Token 22738:
Newell , A., Shaw, J. C., and Simon, H. A. (1957). Empirical explorations with the logic theory ma- chine. Proc.

Token 22739:
Western Joint Computer Conference , 15, 218–239. Reprinted in Feigenbaum and Feld- man (1963). Newell , A., Shaw, J. C., and Simon, H. A. (1958).

Token 22740:
Chess playing programs and the problem of com- plexity. IBM Journal of Research and Development , 4(2), 320–335. Newell , A. and Simon, H. A. (1961).

Token 22741:
GPS, a pro- gram that simulates human thought. In Billing, H. (Ed. ), Lernende Automaten , pp. 109–124. R. Olden- bourg. Newell , A. and Simon, H. A.

Token 22742:
(1972). Human Prob- lem Solving . Prentice-Hall. Newell , A. and Simon, H. A. (1976). Computer science as empirical inquiry: Symbols and search.

Token 22743:
CACM ,19, 113–126. Newton , I. (1664–1671). Methodus ﬂuxionum et se- rierum inﬁnitarum. Unpublished notes. Ng, A. Y. (2004).

Token 22744:
Feature selection, l1vs.l2regu- larization, and rotational invariance. In ICML-04 . Ng, A. Y., Harada, D., and Russell, S. J. (1999).

Token 22745:
Pol- icy invariance under reward transformations: Theory and application to reward shaping. In ICML-99 . Ng, A. Y. and Jordan, M. I. (2000).

Token 22746:
PEGASUS: A policy search method for large MDPs and POMDPs.InUAI-00 , pp. 406–415. Ng, A. Y., Kim, H. J., Jordan, M. I., and Sastry, S. (2004).

Token 22747:
Autonomous helicopter ﬂight via reinforce- ment learning. In NIPS 16 . Nguyen , X. and Kambhampati, S. (2001). Reviving partial order planning.

Token 22748:
In IJCAI-01 , pp. 459–466. Nguyen , X., Kambhampati, S., and Nigenda, R. S. (2001).

Token 22749:
Planning graph as the basis for deriving heuristics for plan synthesis by state space and CSPsearch. Tech.

Token 22750:
rep., Computer Science and Engineer- ing Department, Arizona State University. Nicholson , A. and Brady, J. M. (1992).

Token 22751:
The data as- sociation problem when monitoring robot vehicles using dynamic belief networks. In ECAI-92 , pp. 689–693.

Token 22752:
Niemel ¨a, I., Simons, P., and Syrj¨ anen, T. (2000). Smodels: A system for answer set program- ming. In Proc.

Token 22753:
8th International Workshop on Non- Monotonic Reasoning . Nigam , K., McCallum, A., Thrun, S., and Mitchell, T. M. (2000).

Token 22754:
Text classiﬁcation from labeled andunlabeled documents using EM. Machine Learning , 39(2–3), 103–134. Niles , I. and Pease, A. (2001).

Token 22755:
Towards a standard upper ontology. In FOIS ’01: Proc. international conference on Formal Ontology in Information Sys-tems, pp. 2–9.

Token 22756:
Nilsson , D. and Lauritzen, S. (2000). Evaluating inﬂuence diagrams using LIMIDs. In UAI-00 , pp. 436–445. Nilsson , N. J. (1965).

Token 22757:
Learning Machines: Foun- dations of Trainable Pattern-Classifying Systems . McGraw-Hill. Republished in 1990. Nilsson , N. J. (1971).

Token 22758:
Problem-Solving Methods in Artiﬁcial Intelligence . McGraw-Hill.Nilsson , N. J. (1984). Shakey the robot. Technical note 323, SRI International.

Token 22759:
Nilsson , N. J. (1986). Probabilistic logic. AIJ,28(1), 71–87. Nilsson , N. J. (1991). Logic and artiﬁcial intelli- gence. AIJ,47(1–3), 31–56.

Token 22760:
Nilsson , N. J. (1995). Eye on the prize. AIMag , 16(2), 9–17. Nilsson , N. J. (1998). Artiﬁcial Intelligence: A New Synthesis . Morgan Kaufmann.

Token 22761:
Nilsson , N. J. (2005). Human-level artiﬁcial intelli- gence? be serious! AIMag ,26(4), 68–75. Nilsson , N. J. (2009).

Token 22762:
The Quest for Artiﬁcial Intel- ligence: A History of Ideas and Achievements .C a m - bridge University Press.

Token 22763:
Nisan , N., Roughgarden, T., Tardos, E., and Vazi- rani, V. (Eds.). (2007). Algorithmic Game Theory . Cambridge University Press. Noe, A. (2009).

Token 22764:
Out of Our Heads: Why You Are Not Your Brain, and Other Lessons from the Biology of Consciousness . Hill and Wang. Norvig , P. (1988).

Token 22765:
Multiple simultaneous interpreta- tions of ambiguous sentences. In COGSCI-88 . Norvig , P. (1992).

Token 22766:
Paradigms of Artiﬁcial Intelli- gence Programming: Case Studies in Common Lisp . Morgan Kaufmann. Norvig , P. (2009). Natural language corpus data.

Token 22767:
In Segaran, T. and Hammerbacher, J. (Eds. ), Beautiful Data . O’Reilly. Nowick , S. M., Dean, M. E., Dill, D. L., and Horowitz, M. (1993).

Token 22768:
The design of a high- performance cache controller: A case study in asyn- chronous synthesis. Integration: The VLSI Journal , 15(3), 241–262.

Token 22769:
Nunberg , G. (1979). The non-uniqueness of seman- tic solutions: Polysemy. Language and Philosophy , 3(2), 143–184. Nussbaum , M. C. (1978).

Token 22770:
Aristotle’s De Motu Ani- malium. Princeton University Press. Oaksford , M. and Chater, N. (Eds.). (1998). Ra- tional models of cognition .

Token 22771:
Oxford University Press. Och, F. J. and Ney, H. (2003). A systematic compar- ison of various statistical alignment model.

Token 22772:
Compu- tational Linguistics ,29(1), 19–51. Och, F. J. and Ney, H. (2004). The alignment template approach to statistical machine translation.

Token 22773:
Computational Linguistics ,30, 417–449. Ogawa , S., Lee, T.-M., Kay, A. R., and Tank, D. W. (1990).

Token 22774:
Brain magnetic res onance imaging with con- trast dependent on blood oxygenation. PNAS ,87, 9868–9872. Oh, S., Russell, S. J., and Sastry, S. (2009).

Token 22775:
Markov chain Monte Carlo data association for multi-target tracking. IEEE Transactions on Automatic Control , 54(3), 481–497. Olesen , K. G. (1993).

Token 22776:
Causal probabilistic networks with both discrete and continuous variables. PAMI , 15(3), 275–279. Oliver , N., Garg, A., and Horvitz, E. J. (2004).

Token 22777:
Lay- ered representations for learning and inferring ofﬁce activity from multiple sensory channels.

Token 22778:
Computer Vision and Image Understanding ,96, 163–180. Oliver , R. M. and Smith, J. Q. (Eds.). (1990).

Token 22779:
Inﬂu- ence Diagrams, Belief Nets and Decision Analysis . Wiley. Omohundro , S. (2008). The basic AI drives.

Token 22780:
In AGI-08 Workshop on the Sociocultural, Ethical and Futurological Implications of Artiﬁcial Intelligence .O’Reilly , U.-M. and Oppacher, F. (1994).

Token 22781:
Program search with a hierarchical variable length represen- tation: Genetic programmi ng, simulated annealing and hill climbing. In Proc.

Token 22782:
Third Conference on Par- allel Problem Solving from Nature , pp. 397–406. Ormoneit , D. and Sen, S. (2002). Kernel-based re- inforcement learning.

Token 22783:
Machine Learning ,49(2–3), 161–178. Osborne , M. J. (2004). An Introduction to Game Theory . Oxford University Pres.

Token 22784:
Osborne , M. J. and Rubinstein, A. (1994). A Course in Game Theory . MIT Press. Osherson , D. N., Stob, M., and Weinstein, S. (1986).

Token 22785:
Systems That Learn: An Introduction to Learning Theory for Cognitive and Computer Sci- entists . MIT Press. Padgham , L. and Winikoff, M. (2004).

Token 22786:
Developing Intelligent Agent Systems: A Practical Guide . Wiley. Page , C. D. and Srinivasan, A. (2002).

Token 22787:
ILP: A short look back and a longer look forward. Submitted to Journal of Machine Learning Research. Palacios , H. and Geffner, H. (2007).

Token 22788:
From confor- mant into classical planning: Efﬁcient translations that may be complete too. In ICAPS-07 . Palay , A. J. (1985).

Token 22789:
Searching with Probabilities . Pitman. Palmer , D. A. and Hearst, M. A. (1994). Adaptive sentence boundary disambiguation. In Proc.

Token 22790:
Confer- ence on Applied Natural Language Processing , pp. 78–83. Palmer , S. (1999). Vision Science: Photons to Phe- nomenology . MIT Press.

Token 22791:
Papadimitriou , C. H. (1994). Computational Com- plexity . Addison Wesley. Papadimitriou , C. H., Tamaki, H., Raghavan, P., and Vempala, S. (1998).

Token 22792:
Latent semantic indexing: A probabilistic analysis. In PODS-98 , pp. 159–168. Papadimitriou , C. H. and Tsitsiklis, J. N. (1987).

Token 22793:
The complexity of Markov decision processes. Mathematics of Operations Research ,12(3), 441– 450. Papadimitriou , C. H. and Yannakakis, M. (1991).

Token 22794:
Shortest paths without a map. Theoretical Computer Science ,84(1), 127–150. Papavassiliou , V. and Russell, S. J. (1999).

Token 22795:
Conver- gence of reinforcement learning with general func- tion approximators. In IJCAI-99 , pp. 748–757. Parekh , R. and Honavar, V. (2001).

Token 22796:
DFA learning from simple examples. Machine Learning ,44,9 – 35. Parisi , G. (1988). Statistical ﬁeld theory . Addison- Wesley.

Token 22797:
Parisi , M. M. G. and Zecchina, R. (2002). Ana- lytic and algorithmic solution of random satisﬁabil- ity problems. Science ,297, 812–815.

Token 22798:
Parker , A., Nau, D. S., and Subrahmanian, V. S. (2005). Game-tree search with combinatorially largebelief states. In IJCAI-05 , pp. 254–259.

Token 22799:
Parker , D. B. (1985). Learning logic.

Token 22800:
Tech- nical report TR-47, Center for Computational Re-search in Economics and Management Science,Massachusetts Institute of Technology.

Token 22801:
Parker , L. E. (1996). On the design of behavior- based multi-robot teams. J. Advanced Robotics , 10(6). Parr , R. and Russell, S. J. (1998).

Token 22802:
Reinforcement learning with hierarchies of machines. In Jordan, M .I . ,K e a r n s ,M . ,a n dS o l l a ,S .A . ( E d s . ) , NIPS 10 . MIT Press.

Token 22803:
Bibliography 1085 Parzen , E. (1962). On estimation of a probability density function and mode. Annals of Mathematical Statistics ,33, 1065–1076.

Token 22804:
Pasca , M. and Harabagiu, S. M. (2001). High perfor- mance question/answering. In SIGIR-01 , pp. 366– 374. Pasca ,M . ,L i n ,D . ,B i g h a m ,J .

Token 22805:
,L i f c h i t s ,A . ,a n d Jain, A. (2006). Organizing and searching the world wide web of facts—Step one: The one-million factextraction challenge.

Token 22806:
In AAAI-06 . Paskin , M. (2001). Grammatical bigrams. In NIPS . Pasula , H., Marthi, B., Milch, B., Russell, S. J., and Shpitser, I. (2003).

Token 22807:
Identity uncertainty and citation matching. In NIPS 15 . MIT Press. Pasula , H. and Russell, S. J. (2001).

Token 22808:
Approximate inference for ﬁrst-order probabilistic languages. In IJCAI-01 . Pasula , H., Russell, S. J., Ostland, M., and Ritov, Y. (1999).

Token 22809:
Tracking many objects with many sensors. InIJCAI-99 . Patashnik , O. (1980). Qubic: 4x4x4 tic-tac-toe. Mathematics Magazine ,53(4), 202–216.

Token 22810:
Patrick , B. G., Almulla, M., and Newborn, M. (1992). An upper bound on the time complexity of iterative-deepening-A*. AIJ,5(2–4), 265–278.

Token 22811:
Paul , R. P. (1981). Robot Manipulators: Mathemat- ics, Programming, and Control . MIT Press. Pauls , A. and Klein, D. (2009). K-best A* parsing.

Token 22812:
InACL-09 . Peano , G. (1889). Arithmetices principia, nova methodo exposita . Fratres Bocca, Turin. Pearce , J., Tambe, M., and Maheswaran, R. (2008).

Token 22813:
Solving multiagent networks using distributed con-straint optimization. AIMag ,29(3), 47–62. Pearl , J. (1982a).

Token 22814:
Reverend Bayes on inference en- gines: A distributed hierarchical approach. In AAAI- 82, pp. 133–136. Pearl , J. (1982b).

Token 22815:
The solution for the branching factor of the alpha–beta pruning algorithm and itsoptimality. CACM ,25(8), 559–564. Pearl , J. (1984).

Token 22816:
Heuristics: Intelligent Search Strategies for Computer Problem Solving . Addison- Wesley. Pearl , J. (1986).

Token 22817:
Fusion, propagation, and structur- ing in belief networks. AIJ, 29, 241–288. Pearl , J. (1987).

Token 22818:
Evidential reasoning using stochas- tic simulation of causal models. AIJ,32, 247–257. Pearl , J. (1988).

Token 22819:
Probabilistic Reasoning in Intelli- gent Systems: Networks of Plausible Inference .M o r - gan Kaufmann. Pearl , J. (2000).

Token 22820:
Causality: Models, Reasoning, and Inference . Cambridge University Press. Pearl , J. and Verma, T. (1991). A theory of inferred causation.

Token 22821:
In KR-91 , pp. 441–452. Pearson , J. and Jeavons, P. (1997). A survey of tractable constraint satisfaction problems.

Token 22822:
Techni- cal report CSD-TR-97-15, Royal Holloway College, U. of London. Pease , A. and Niles, I. (2002).

Token 22823:
IEEE standard upper ontology: A progress report. Knowledge Engineer- ing Review ,17(1), 65–70. Pednault , E. P. D. (1986).

Token 22824:
Formulating multiagent, dynamic-world problems in the classical planning framework. In Reasoning about Actions and Plans: Proc. 1986 Workshop , pp.

Token 22825:
47–82.Peirce , C. S. (1870).

Token 22826:
Description of a notation for the logic of relatives, resulting from an ampliﬁcation of the conceptions of Boole’s calculus of logic.

Token 22827:
Mem- oirs of the American Academy of Arts and Sciences , 9, 317–378. Peirce , C. S. (1883). A theory of probable inference. Note B.

Token 22828:
The logic of relatives. In Studies in Logic by Members of the Johns Hopkins University , pp. 187– 203, Boston. Peirce , C. S. (1902).

Token 22829:
Logic as semiotic: The the- ory of signs. Unpublished manuscript; reprinted in (Buchler 1955). Peirce , C. S. (1909). Existential graphs.

Token 22830:
Unpub- lished manuscript; reprinted in (Buchler 1955). Pelikan , M., Goldberg, D. E., and Cantu-Paz, E. (1999).

Token 22831:
BOA: The Bayesian optimization algo- rithm. In GECCO-99: Proc. Genetic and Evolution- ary Computation Conference , pp. 525–532.

Token 22832:
Pemberton , J. C. and Korf, R. E. (1992). Incremen- tal planning on graphs with cycles. In AIPS-92 , pp. 525–532.

Token 22833:
Penberthy , J. S. and Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner for ADL. InKR-92 , pp. 103–114.

Token 22834:
Peng , J. and Williams, R. J. (1993). Efﬁcient learn- ing and planning within the Dyna framework. Adap- tive Behavior ,2, 437–454.

Token 22835:
Penrose , R. (1989). The Emperor’s New Mind .O x - ford University Press. Penrose , R. (1994). Shadows of the Mind . Oxford University Press.

Token 22836:
Peot , M. and Smith, D. E. (1992). Conditional non- linear planning. In ICAPS-92 , pp. 189–197. Pereira , F. and Shieber, S. (1987).

Token 22837:
Prolog and Natural-Language Analysis . Center for the Study of Language and Information (CSLI). Pereira , F. and Warren, D. H. D. (1980).

Token 22838:
Deﬁnite clause grammars for language analysis: A survey of the formalism and a comparison with augmented transition networks. AIJ,13, 231–278.

Token 22839:
Pereira , F. and Wright, R. N. (1991). Finite-state ap- proximation of phrase structure grammars. In ACL- 91, pp. 246–255. Perlis , A. (1982).

Token 22840:
Epigrams in programming. SIG- PLAN Notices ,17(9), 7–13. Perrin , B. E., Ralaivola, L., and Mazurie, A. (2003).

Token 22841:
Gene networks inference using dynamic Bayesian networks. Bioinformatics ,19, II 138–II 148. Peterson , C. and Anderson, J. R. (1987).

Token 22842:
A mean ﬁeld theory learning algorithm for neural networks. Complex Systems ,1(5), 995–1019. Petrik , M. and Zilberstein, S. (2009).

Token 22843:
Bilinear pro- gramming approach for multiagent planning. JAIR , 35, 235–274. Petrov , S. and Klein, D. (2007a).

Token 22844:
Discriminative log-linear grammars with latent variables. In NIPS . Petrov , S. and Klein, D. (2007b). Improved infer- ence for unlexicalized parsing.

Token 22845:
In ACL-07 . Petrov , S. and Klein, D. (2007c). Learning and in- ference for hierarchically split pcfgs. In AAAI-07 .

Token 22846:
Pfeffer , A., Koller, D., Milch, B., and Takusagawa, K. T. (1999). SPOOK: A system for probabilistic object-oriented knowledge representation.

Token 22847:
In UAI- 99. Pfeffer , A. (2000). Probabilistic Reasoning for Complex Systems . Ph.D. thesis, Stanford University.Pfeffer , A. (2007).

Token 22848:
The design and implementation of IBAL: A general-purpose probabilistic language. In Getoor, L. and Taskar, B. (Eds.

Token 22849:
), Introduction to Statistical Relational Learning .M I TP r e s s . Pfeifer , R., Bongard, J., Brooks, R. A., and Iwa- sawa, S. (2006).

Token 22850:
How the Body Shapes the Way We Think: A New View of Intelligence . Bradford. Pineau , J., Gordon, G., and Thrun, S. (2003).

Token 22851:
Point- based value iteration: An anytime algorithm for POMDPs. In IJCAI-03 . Pinedo , M. (2008). Scheduling: Theory, Algorithms, and Systems .

Token 22852:
Springer Verlag. Pinkas , G. and Dechter, R. (1995). Improving con- nectionist energy minimization. JAIR ,3, 223–248. Pinker , S. (1995).

Token 22853:
Language acquisition. In Gleit- man, L. R., Liberman, M., and Osherson, D. N. (Eds. ), An Invitation to Cognitive Science (second edition)., Vol. 1.

Token 22854:
MIT Press. Pinker , S. (2003). The Blank Slate: The Modern Denial of Human Nature . Penguin. Pinto ,D . ,M c C a l l u m ,A . ,W e i ,X .

Token 22855:
,a n dC r o f t ,W .B . (2003). Table extraction using conditional randomﬁelds. In SIGIR-03 . Pipatsrisawat , K. and Darwiche, A. (2007).

Token 22856:
RSat 2.0: SAT solver description. Tech. rep. D–153, Au- tomated Reasoning Group, Computer Science De- partment, University of California, Los Angeles.

Token 22857:
Plaat , A., Schaeffer, J., Pijls, W., and de Bruin, A. (1996). Best-ﬁrst ﬁxed-depth minimax algorithms.AIJ,87(1–2), 255–293. Place , U. T. (1956).

Token 22858:
Is consciousness a brain pro- cess? British Journal of Psychology ,47, 44–50. Platt , J. (1999).

Token 22859:
Fast training of support vector ma- chines using sequential minimal optimization. In Ad- vances in Kernel Methods: Support Vector Learning , pp.

Token 22860:
185–208. MIT Press. Plotkin , G. (1971). Automatic Methods of Inductive Inference . Ph.D. thesis, Edinburgh University. Plotkin , G. (1972).

Token 22861:
Building-in equational theories. I nM e l t z e r ,B .a n dM i c h i e ,D . ( E d s . ) , Machine Intel- ligence 7 , pp. 73–90.

Token 22862:
Edinburgh University Press. Pohl , I. (1971). Bi-directional search. In Meltzer, B. and Michie, D. (Eds. ), Machine Intelligence 6 , pp. 127–140.

Token 22863:
Edinburgh University Press. Pohl , I. (1973).

Token 22864:
The avoidance of (relative) catastro- phe, heuristic competence, genuine dynamic weight-ing and computational issues in heuristic problem solving.

Token 22865:
In IJCAI-73 , pp. 20–23. Pohl , I. (1977). Practical and theoretical considera- tions in heuristic search algorithms. In Elcock, E. W. and Michie, D.

Token 22866:
(Eds. ), Machine Intelligence 8 , pp. 55–72. Ellis Horwood. Poli, R., Langdon, W., and McPhee, N. (2008). A Field Guide to Genetic Programming .

Token 22867:
Lulu.com. Pomerleau , D. A. (1993). Neural Network Percep- tion for Mobile Robot Guidance .K l u w e r . Ponte , J. and Croft, W. B. (1998).

Token 22868:
A language mod- eling approach to information retrieval. In SIGIR-98 , pp. 275–281. Poole , D. (1993).

Token 22869:
Probabilistic Horn abduction and Bayesian networks. AIJ,64, 81–129. Poole , D. (2003). First-order probabilistic inference. InIJCAI-03 , pp. 985–991.

Token 22870:
Poole , D., Mackworth, A. K., and Goebel, R. (1998). Computational intelligence: A logical approach . Oxford University Press.

Token 22871:
1086 Bibliography Popper , K. R. (1959). The Logic of Scientiﬁc Dis- covery . Basic Books. Popper , K. R. (1962).

Token 22872:
Conjectures and Refutations: The Growth of Scientiﬁc Knowledge . Basic Books. Portner , P. and Partee, B. H. (2002).

Token 22873:
Formal Seman- tics: The Essential Readings . Wiley-Blackwell. Post, E. L. (1921). Introduction to a general the- ory of elementary propositions.

Token 22874:
American Journal of Mathematics ,43, 163–185. Poundstone , W. (1993). Prisoner’s Dilemma .A n - chor. Pourret ,O . ,N a ¨ ım, P., and Marcot, B.

Token 22875:
(2008). Bayesian Networks: A practical guide to applica-tions . Wiley. Prades , J. L. P., Loomes, G., and Brey, R. (2008).

Token 22876:
Trying to estmate a monetary value for the QALY.Tech. rep. WP Econ 08.09, Univ. Pablo Olavide.

Token 22877:
Pradhan , M., Provan, G. M., Middleton, B., and Henrion, M. (1994). Knowledge engineering forlarge belief networks. In UAI-94 , pp. 484–490.

Token 22878:
Prawitz , D. (1960). An improved proof procedure. Theoria ,26, 102–139.

Token 22879:
Press , W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (2007). Numerical Recipes: The Art of Scientiﬁc Computing (third edition).

Token 22880:
Cambridge University Press. Preston , J. and Bishop, M. (2002). Views into the Chinese Room: New Essays on Searle and Artiﬁcial Intelligence .

Token 22881:
Oxford University Press. Prieditis , A. E. (1993). Machine discovery of effec- tive admissible heuristics. Machine Learning ,12(1– 3), 117–141.

Token 22882:
Prinz , D. G. (1952). Robot chess. Research ,5, 261– 266. Prosser , P. (1993). Hybrid algorithms for constraint satisfaction problems.

Token 22883:
Computational Intelligence , 9, 268–299. Pullum , G. K. (1991).

Token 22884:
The Great Eskimo Vocabu- lary Hoax (and Other Irreverent Essays on the Study of Language) . University of Chicago Press. Pullum , G. K. (1996).

Token 22885:
Learnability, hyperlearning, and the poverty of the stimulus. In 22nd Annual Meeting of the Berkeley Linguistics Society . Puterman , M. L. (1994).

Token 22886:
Markov Decision Pro- cesses: Discrete Stochastic Dynamic Programming . Wiley. Puterman , M. L. and Shin, M. C. (1978).

Token 22887:
Modiﬁed policy iteration algorithms for discounted Markov decision problems. Management Science ,24(11), 1127–1137. Putnam , H. (1960).

Token 22888:
Minds and machines. In Hook, S. (Ed. ), Dimensions of Mind , pp. 138–164. Macmil- lan. Putnam , H. (1963).

Token 22889:
‘Degree of conﬁrmation’ and inductive logic. In Schilpp, P. A. (Ed. ), The Philoso- phy of Rudolf Carnap , pp. 270–292. Open Court.

Token 22890:
Putnam , H. (1967). The nature of mental states. In Capitan, W. H. and Merrill, D. D. (Eds. ), Art, Mind, and Religion , pp. 37–48.

Token 22891:
University of Pitts- burgh Press. Putnam , H. (1975). The meaning of “meaning”. In Gunderson, K. (Ed.

Token 22892:
), Language, Mind and Knowl- edge: Minnesota Studies in the Philosophy of Sci- ence. University of Minnesota Press. Pylyshyn , Z. W. (1974).

Token 22893:
Minds, machines and phe- nomenology: Some reﬂections on Dreyfus’ “What Computers Can’t Do”. Int.

Token 22894:
J. Cognitive Psychology , 3(1), 57–77.Pylyshyn , Z. W. (1984). Computation and Cogni- tion: Toward a Foundation for Cognitive Science . MIT Press.

Token 22895:
Quillian , M. R. (1961). A design for an understand- ing machine.

Token 22896:
Paper presented at a colloquium: Se- mantic Problems in Natural Language, King’s Col- lege, Cambridge, England. Quine , W. V. (1953).

Token 22897:
Two dogmas of empiricism. InFrom a Logical Point of View , pp. 20–46. Harper and Row. Quine , W. V. (1960). Word and Object . MIT Press.

Token 22898:
Quine , W. V. (1982). Methods of Logic (fourth edi- tion). Harvard University Press. Quinlan , J. R. (1979).

Token 22899:
Discovering rules from large collections of examples: A case study. In Michie,D. (Ed. ), Expert Systems in the Microelectronic Age .

Token 22900:
Edinburgh University Press. Quinlan , J. R. (1986). Induction of decision trees. Machine Learning ,1, 81–106. Quinlan , J. R. (1990).

Token 22901:
Learning logical deﬁnitions from relations. Machine Learning ,5(3), 239–266. Quinlan , J. R. (1993). C4.5: Programs for machine learning .

Token 22902:
Morgan Kaufmann. Quinlan , J. R. and Cameron-Jones, R. M. (1993). FOIL: A midterm report. In ECML-93 , pp. 3–20.

Token 22903:
Quirk , R., Greenbaum, S., Leech, G., and Svartvik, J. (1985). A Comprehensive Grammar of the English Language . Longman.

Token 22904:
Rabani , Y., Rabinovich, Y., and Sinclair, A. (1998). A computational view o f population genetics.

Token 22905:
Ran- dom Structures and Algorithms ,12(4), 313–334. Rabiner , L. R. and Juang, B.-H. (1993). Fundamen- tals of Speech Recognition . Prentice-Hall.

Token 22906:
Ralphs , T. K., Ladanyi, L., and Saltzman, M. J. (2004). A library hierarchy for implementing scal- able parallel search algorithms.

Token 22907:
J. Supercomputing , 28(2), 215–234. Ramanan , D., Forsyth, D., and Zisserman, A. (2007). Tracking people by learning their appear- ance.

Token 22908:
IEEE Pattern Analysis and Machine Intelli- gence . Ramsey , F. P. (1931). Truth and probability. In Braithwaite, R. B. (Ed.

Token 22909:
), The Foundations of Math- ematics and Other Logical Essays . Harcourt Brace Jovanovich. Ranzato , M., Poultney, C., Chopra, S., and LeCun, Y.

Token 22910:
(2007). Efﬁcient learning of sparse representa- tions with an energy-based model. In NIPS 19 , pp. 1137–1144. Raphson , J. (1690).

Token 22911:
Analysis aequationum univer- salis. Apud Abelem Swalle, London. Rashevsky , N. (1936).

Token 22912:
Physico-mathematical as- pects of excitation and conduction in nerves.

Token 22913:
In Cold Springs Harbor Symposia on Quantitative Biology.IV: Excitation Phenomena , pp. 90–97. Rashevsky , N. (1938).

Token 22914:
Mathematical Biophysics: Physico-Mathematical Foundations of Biology .U n i - versity of Chicago Press. Rasmussen , C. E. and Williams, C. K. I.

Token 22915:
(2006). Gaussian Processes for Machine Learning . MIT Press. Rassenti , S., Smith, V., and Bulﬁn, R. (1982).

Token 22916:
A combinatorial auction mechanism for airport time slot allocation. Bell Journal of Economics ,13, 402– 417.Ratner , D. and Warmuth, M. (1986).

Token 22917:
Finding a shortest solution for the n×nextension of the 15-puzzle is intractable. In AAAI-86 , Vol. 1, pp. 168– 172.

Token 22918:
Rauch , H. E., Tung, F., and Striebel, C. T. (1965). Maximum likelihood estimates of linear dynamic systems. AIAA Journal ,3(8), 1445–1450.

Token 22919:
Rayward-Smith , V., Osman, I., Reeves, C., and Smith, G. (Eds.). (1996). Modern Heuristic Search Methods . Wiley. Rechenberg , I. (1965).

Token 22920:
Cybernetic solution path of an experimental problem. Library translation 1122, Royal Aircraft Establishment.

Token 22921:
Reeson , C. G., Huang, K.-C., Bayer, K. M., and Choueiry, B. Y. (2007). An interactive constraint- based approach to sudoku. In AAAI-07 , pp.

Token 22922:
1976– 1977. Regin , J. (1994). A ﬁltering algorithm for con- straints of difference in CSPs. In AAAI-94 , pp. 362– 367. Reichenbach , H. (1949).

Token 22923:
The Theory of Probabil- ity: An Inquiry into the Logical and Mathematical Foundations of the Calculus of Probability (second edition).

Token 22924:
University of California Press. Reid , D. B. (1979). An algorithm for tracking mul- tiple targets. IEEE Trans. Automatic Control ,24(6), 843–854.

Token 22925:
Reif, J. (1979). Complexity of the mover’s prob- lem and generalizations. In FOCS-79 , pp. 421–427. IEEE. Reiter , R. (1980).

Token 22926:
A logic for default reasoning. AIJ, 13(1–2), 81–132. Reiter , R. (1991).

Token 22927:
The frame problem in the situ- ation calculus: A simple solution (sometimes) and a completeness result for goal regression. In Lif-schitz, V. (Ed.

Token 22928:
), Artiﬁcial Intelligence and Mathe- matical Theory of Computation: Papers in Honor ofJohn McCarthy , pp. 359–380. Academic Press. Reiter , R. (2001).

Token 22929:
Knowledge in Action: Logical Foundations for Specifying and Implementing Dy-namical Systems . MIT Press. Renner , G. and Ekart, A. (2003).

Token 22930:
Genetic algo- rithms in computer aided design. Computer Aided Design ,35(8), 709–726. R´enyi, A. (1970). Probability Theory . Elsevier/North-Holland.

Token 22931:
Reynolds , C. W. (1987). Flocks, herds, and schools: A distributed behavioral model. Computer Graph- ics,21, 25–34.

Token 22932:
SIGGRAPH ’87 Conference Pro- ceedings. Riazanov , A. and Voronkov, A. (2002). The design and implementation of V AMPIRE.

Token 22933:
AI Communica- tions ,15(2–3), 91–110. Rich , E. and Knight, K. (1991). Artiﬁcial Intelli- gence (second edition). McGraw-Hill.

Token 22934:
Richards , M. and Amir, E. (2007). Opponent mod- eling in Scrabble. In IJCAI-07 . Richardson , M., Bilmes, J., and Diorio, C. (2000).

Token 22935:
Hidden-articulator Markov models: Performance improvements and robustness to noise. In ICASSP- 00. Richter , S. and Westphal, M. (2008).

Token 22936:
The LAMA planner. In Proc. International Planning Competi- tion at ICAPS . Ridley , M. (2004). Evolution . Oxford Reader. Rieger , C. (1976).

Token 22937:
An organization of knowledge for problem solving and language comprehension. AIJ, 7, 89–127.

Token 22938:
Bibliography 1087 Riley , J. and Samuelson, W. (1981). Optimal auc- tions. American Economic Review ,71, 381–392. Riloff , E. (1993).

Token 22939:
Automatically constructing a dic- tionary for information extraction tasks. In AAAI-93 , pp. 811–816. Rintanen , J. (1999).

Token 22940:
Improvements to the evalua- tion of quantiﬁed Boolean formulae. In IJCAI-99 , pp. 1192–1197. Rintanen , J. (2007).

Token 22941:
Asymptotically optimal encod- ings of conformant planning in QBF. In AAAI-07 , pp. 1045–1050. Ripley , B. D. (1996).

Token 22942:
Pattern Recognition and Neu- ral Networks . Cambridge University Press. Rissanen , J. (1984).

Token 22943:
Universal coding, information, prediction, and estimation. IEEE Transactions on Information Theory ,IT-30 (4), 629–636. Rissanen , J. (2007).

Token 22944:
Information and Complexity in Statistical Modeling . Springer. Ritchie , G. D. and Hanna, F. K. (1984). AM: A case study in AI methodology.

Token 22945:
AIJ,23(3), 249–268. Rivest , R. (1987). Learning decision lists. Machine Learning ,2(3), 229–246. Roberts , L. G. (1963).

Token 22946:
Machine perception of three- dimensional solids. Technical report 315, MIT Lin- coln Laboratory. Robertson , N. and Seymour, P. D. (1986).

Token 22947:
Graph minors. II. Algorithmic aspects of tree-width. J. Al- gorithms ,7(3), 309–322. Robertson , S. E. (1977).

Token 22948:
The probability ranking principle in IR. J. Documentation ,33, 294–304. Robertson , S. E. and Sparck Jones, K. (1976).

Token 22949:
Rel- evance weighting of search terms. J. American Soci- ety for Information Science ,27, 129–146. Robinson , A. and Voronkov, A. (2001).

Token 22950:
Handbook of Automated Reasoning . Elsevier. Robinson , J. A. (1965). A machine-oriented logic based on the resolution principle. JACM ,12, 23–41.

Token 22951:
Roche , E. and Schabes, Y. (1997). Finite-State Lan- guage Processing (Language, Speech and Commu-nication) . Bradford Books. Rock , I. (1984).

Token 22952:
Perception . W. H. Freeman. Rosenblatt , F. (1957). The perceptron: A perceiv- ing and recognizing automaton.

Token 22953:
Report 85-460-1,Project PARA, Cornell Aeronautical Laboratory. Rosenblatt , F. (1960).

Token 22954:
On the convergence of rein- forcement procedures in simple perceptrons. Report VG-1196-G-4, Cornell Aeronautical Laboratory. Rosenblatt , F. (1962).

Token 22955:
Principles of Neurodynam- ics: Perceptrons and the Theory of Brain Mecha- nisms .S p a r t a n . Rosenblatt , M. (1956).

Token 22956:
Remarks on some nonpara- metric estimates of a density function. Annals of Mathematical Statistics ,27, 832–837.

Token 22957:
Rosenblueth , A., Wiener, N., and Bigelow, J. (1943). Behavior, purpose, and teleology. Philos- ophy of Science ,10, 18–24.

Token 22958:
Rosenschein , J. S. and Zlotkin, G. (1994). Rules of Encounter . MIT Press. Rosenschein , S. J. (1985).

Token 22959:
Formal theories of knowledge in AI and robotics. New Generation Computing ,3(4), 345–357. Ross , P. E. (2004). Psyching out computer chess players.

Token 22960:
IEEE Spectrum ,41(2), 14–15. Ross , S. M. (1988). A First Course in Probability (third edition).

Token 22961:
Macmillan.Rossi , F., van Beek, P., and Walsh, T. (2006). Hand- book of Constraint Processing . Elsevier. Roussel , P. (1975).

Token 22962:
Prolog: Manual de reference et d’utilization. Tech. rep., Groupe d’Intelligence Arti-ﬁcielle, Universit´ e d’Aix-Marseille.

Token 22963:
Rouveirol , C. and Puget, J.-F. (1989). A simple and general solution for inverting resolution. In Proc. European Working Session on Learning , pp.

Token 22964:
201– 210. Rowat , P. F. (1979). Representing the Spatial Ex- perience and Solving Spatial problems in a Simu- lated Robot Environment .

Token 22965:
Ph.D. thesis, University of British Columbia. Roweis , S. T. and Ghahramani, Z. (1999). A unify- ing review of Linear Gaussian Models.

Token 22966:
Neural Com- putation ,11(2), 305–345. Rowley , H., Baluja, S., and Kanade, T. (1996). Neu- ral network-based face detection. In CVPR , pp. 203– 208.

Token 22967:
Roy, N., Gordon, G., and Thrun, S. (2005). Finding approximate POMDP solutions through belief com- pression. JAIR ,23 , 1–40. Rubin , D. (1988).

Token 22968:
Using the SIR algorithm to sim- ulate posterior distributions. In Bernardo, J. M.,d eG r o o t ,M .H . ,L i n d l e y ,D .V .

Token 22969:
,a n dS m i t h ,A .F .M . (Eds. ), Bayesian Statistics 3 , pp. 395–402. Oxford University Press.

Token 22970:
Rumelhart , D. E., Hinton, G. E., and Williams, R. J. (1986a). Learning internal representations by errorpropagation.

Token 22971:
In Rumelhart, D. E. and McClelland, J. L. (Eds. ), Parallel Distributed Processing ,V o l .1 , chap. 8, pp. 318–362. MIT Press.

Token 22972:
Rumelhart , D. E., Hinton, G. E., and Williams, R. J. (1986b). Learning representations by back-propagating errors. Nature ,323, 533–536.

Token 22973:
Rumelhart , D. E. and McClelland, J. L. (Eds.). (1986). Parallel Distributed Processing . MIT Press. Rummery , G. A. and Niranjan, M. (1994).

Token 22974:
On- lineQ-learning using connectionist systems. Tech. rep. CUED/F-INFENG/TR 166, Cambridge Univer- sity Engineering Department.

Token 22975:
Ruspini , E. H., Lowrance, J. D., and Strat, T. M. (1992). Understanding evidential reasoning. IJAR , 6(3), 401–424. Russell , J. G. B. (1990).

Token 22976:
Is screening for abdom- inal aortic aneurysm worthwhile? Clinical Radiol- ogy,41, 182–184. Russell , S. J. (1985).

Token 22977:
The compleat guide to MRS. Report STAN-CS-85-1080, Computer Science De-partment, Stanford University. Russell , S. J. (1986).

Token 22978:
A quantitative analysis of anal- ogy by similarity. In AAAI-86 , pp. 284–288. Russell , S. J. (1988). Tree-structured bias. In AAAI- 88, Vol. 2, pp.

Token 22979:
641–645. Russell , S. J. (1992). Efﬁcient memory-bounded search methods. In ECAI-92 , pp. 1–5. Russell , S. J. (1998).

Token 22980:
Learning agents for uncertain environments (extended abstract). In COLT-98 , pp. 101–103.

Token 22981:
Russell , S. J., Binder, J., Koller, D., and Kanazawa, K. (1995). Local learning in probabilistic networks with hidden variables. In IJCAI-95 , pp.

Token 22982:
1146–52. Russell , S. J. and Grosof, B. (1987). A declarative approach to bias in concept learning. In AAAI-87 .

Token 22983:
Russell , S. J. and Norvig, P. (2003). Artiﬁcial Intelli- gence: A Modern Approach (2nd edition).

Token 22984:
Prentice- Hall.Russell , S. J. and Subramanian, D. (1995). Provably bounded-optimal agents. JAIR ,3, 575–609.

Token 22985:
Russell , S. J., Subramanian, D., and Parr, R. (1993). Provably bounded optimal agents. In IJCAI-93 , pp. 338–345.

Token 22986:
Russell , S. J. and Wefald, E. H. (1989). On optimal game-tree search using rational meta-reasoning. In IJCAI-89 , pp. 334–340.

Token 22987:
Russell , S. J. and Wefald, E. H. (1991). Do the Right Thing: Studies in Limited Rationality .M I TP r e s s . Russell , S. J. and Wolfe, J. (2005).

Token 22988:
Efﬁcient belief-state AND-OR search, with applications to Kriegspiel. In IJCAI-05 , pp. 278–285. Russell , S. J. and Zimdars, A. (2003).

Token 22989:
Q- decomposition of reinforcement learning agents. In ICML-03 . Rustagi , J. S. (1976). Variational Methods in Statis- tics. Academic Press.

Token 22990:
Sabin , D. and Freuder, E. C. (1994). Contradicting conventional wisdom in constraint satisfaction. In ECAI-94 , pp. 125–129.

Token 22991:
Sacerdoti , E. D. (1974). Planning in a hierarchy of abstraction spaces. AIJ,5(2), 115–135. Sacerdoti , E. D. (1975). The nonlinear nature of plans.

Token 22992:
In IJCAI-75 , pp. 206–214. Sacerdoti , E. D. (1977). A Structure for Plans and Behavior . Elsevier/North-Holland. Sadri , F. and Kowalski, R. (1995).

Token 22993:
Variants of the event calculus. In ICLP-95 , pp. 67–81. Sahami , M., Dumais, S. T., Heckerman, D., and Horvitz, E. J. (1998).

Token 22994:
A Bayesian approach to ﬁl-tering junk E-mail. In Learning for Text Categoriza- tion: Papers from the 1998 Workshop .

Token 22995:
Sahami , M., Hearst, M. A., and Saund, E. (1996). Applying the multiple cause mixture model to text categorization. In ICML-96 , pp. 435–443.

Token 22996:
Sahin , N. T., Pinker, S., Cash, S. S., Schomer, D., and Halgren, E. (2009).

Token 22997:
Sequential processing of lexical, grammatical, and phonological information within Broca’s area. Science ,326(5291), 445–449.

Token 22998:
Sakuta , M. and Iida, H. (2002). AND/OR-tree search for solving problems with uncertainty: A case study using screen-shogi problems.

Token 22999:
IPSJ Journal , 43(01). Salomaa , A. (1969). Probabilistic and weighted grammars. Information and Control ,15, 529–544.

Token 23000:
Salton , G., Wong, A., and Yang, C. S. (1975). A vector space model for automatic indexing. CACM , 18(11), 613–620. Samuel , A. L. (1959).

Token 23001:
Some studies in machine learning using the game of checkers. IBM Journal of Research and Development ,3(3), 210–229. Samuel , A. L. (1967).

Token 23002:
Some studies in machine learning using the game of checkers II—Recentprogress. IBM Journal of Research and Develop- ment ,11(6), 601–617.

Token 23003:
Samuelsson , C. and Rayner, M. (1991).

Token 23004:
Quantita- tive evaluation of explanation-based learning as an optimization tool for a large-scale natural language system. In IJCAI-91 , pp. 609–615.

Token 23005:
Sarawagi , S. (2007). Information extraction. Foun- dations and Trends in Databases ,1(3), 261–377. Satia , J. K. and Lave, R. E. (1973).

Token 23006:
Markovian decision processes with probabilistic observation of states. Management Science ,20(1), 1–13. Sato , T. and Kameya, Y. (1997).

Token 23007:
PRISM: A symbolic-statistical modeling language. In IJCAI- 97, pp. 1330–1335.

Token 23008:
1088 Bibliography Saul , L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ﬁeld theory for sigmoid belief networks. JAIR , 4, 61–76. Savage , L. J.

Token 23009:
(1954). The Foundations of Statistics . Wiley. Sayre , K. (1993). Three more ﬂaws in the compu- tational model.

Token 23010:
Paper presented at the APA (Central Division) Annual Conference, Chicago, Illinois. Schaeffer , J. (2008).

Token 23011:
One Jump Ahead: Computer Perfection at Checkers . Springer-Verlag.

Token 23012:
Schaeffer , J., Burch, N., Bjornsson, Y., Kishimoto, A., M¨ uller, M., Lake, R., Lu, P., and Sutphen, S. (2007). Checkers is solved.

Token 23013:
Science ,317, 1518– 1522. Schank , R. C. and Abelson, R. P. (1977). Scripts, Plans, Goals, and Understanding . Lawrence Erl- baum Associates.

Token 23014:
Schank , R. C. and Riesbeck, C. (1981). Inside Com- puter Understanding: Five Programs Plus Minia-tures . Lawrence Erlbaum Associates.

Token 23015:
Schapire , R. E. and Singer, Y. (2000). Boostexter: A boosting-based system for text categorization. Ma- chine Learning ,39(2/3), 135–168.

Token 23016:
Schapire , R. E. (1990). The strength of weak learn- ability. Machine Learning ,5(2), 197–227. Schapire , R. E. (2003).

Token 23017:
The boosting approach to machine learning: An overview. In Denison, D. D., Hansen, M. H., Holmes, C., Mallick, B., and Yu,B. (Eds.

Token 23018:
), Nonlinear Estimation and Classiﬁcation . Springer. Schmid , C. and Mohr, R. (1996).

Token 23019:
Combining grey- value invariants with local constraints for objectrecognition. In CVPR . Schmolze , J. G. and Lipkis, T. A. (1983).

Token 23020:
Classi- ﬁcation in the KL-ONE representation system. In IJCAI-83 , pp. 330–332. Sch¨olkopf , B. and Smola, A. J. (2002). Learning with Kernels .

Token 23021:
MIT Press. Sch¨oning , T. (1999). A probabilistic algorithm for k- SAT and constraint satisfaction problems. In FOCS- 99, pp. 410–414.

Token 23022:
Schoppers , M. J. (1987). Universal plans for reac- tive robots in unpredictable environments. In IJCAI- 87, pp. 1039–1046. Schoppers , M. J. (1989).

Token 23023:
In defense of reaction plans as caches. AIMag ,10 (4), 51–60. Schr ¨oder , E. (1877). Der Operationskreis des Logikkalk¨ uls. B. G. Teubner, Leipzig.

Token 23024:
Schultz , W., Dayan, P., and Montague, P. R. (1997). A neural substrate of prediction and reward. Science , 275, 1593.

Token 23025:
Schulz , D., Burgard, W., Fox, D., and Cremers, A. B. (2003).

Token 23026:
People tracking with mobile robots using sample-based joint probabilistic data associa- tion ﬁlters. Int. J. Robotics Research ,22(2), 99–116.

Token 23027:
Schulz , S. (2004). System Description: E 0.81. In Proc. International Joint Conference on Automated Reasoning , Vol. 3097 of LNAI , pp. 223–228.

Token 23028:
Sch¨utze, H. (1995). Ambiguity in Language Learn- ing: Computational and Cognitive Models .P h . D . thesis, Stanford University.

Token 23029:
Also published by CSLI Press, 1997. Schwartz , J. T., Scharir, M., and Hopcroft, J. (1987). Planning, Geometry and Complexity of Robot Mo-tion.

Token 23030:
Ablex Publishing Corporation. Schwartz , S. P. (Ed.). (1977). Naming, Necessity, and Natural Kinds .

Token 23031:
Cornell University Press.Scott , D. and Krauss, P. (1966). Assigning probabil- ities to logical formulas. In Hintikka, J. and Suppes, P. (Eds.

Token 23032:
), Aspects of Inductive Logic . North-Holland. Searle , J. R. (1980). Minds, brains, and programs. BBS,3, 417–457. Searle , J. R. (1984).

Token 23033:
Minds, Brains and Science . Harvard University Press. Searle , J. R. (1990). Is the brain’s mind a computer program? Scientiﬁc American ,262, 26–31.

Token 23034:
Searle , J. R. (1992). The Rediscovery of the Mind . MIT Press. Sebastiani , F. (2002). Machine learning in auto- mated text categorization.

Token 23035:
ACM Computing Surveys , 34(1), 1–47. Segaran , T. (2007). Programming Collective In- telligence: Building Smart Web 2.0 Applications . O’Reilly.

Token 23036:
Selman , B., Kautz, H., and Cohen, B. (1996). Lo- cal search strategies for satisﬁability testing.

Token 23037:
In DI- MACS Series in Discrete Mathematics and Theo- retical Computer Science, Volume 26 , pp. 521–532. American Mathematical Society.

Token 23038:
Selman , B. and Levesque, H. J. (1993). The com- plexity of path-based defeasible inheritance. AIJ, 62(2), 303–339.

Token 23039:
Selman , B., Levesque, H. J., and Mitchell, D. (1992). A new method for solving hard satisﬁability problems. In AAAI-92 , pp. 440–446.

Token 23040:
Sha, F. and Pereira, F. (2003). Shallow parsing with conditional random ﬁelds. Technical report CIS TR MS-CIS-02-35, Univ. of Penn.

Token 23041:
Shachter , R. D. (1986). Evaluating inﬂuence dia- grams. Operations Research ,34, 871–882. Shachter , R. D. (1998).

Token 23042:
Bayes-ball: The rational pastime (for determining irrelevance and requisiteinformation in belief networks and inﬂuence dia-grams). In UAI-98 , pp.

Token 23043:
480–487. Shachter , R. D., D’Ambrosio, B., and Del Favero, B. A. (1990). Symbolic probabilistic inference in belief networks. In AAAI-90 , pp.

Token 23044:
126–131. Shachter , R. D. and Kenley, C. R. (1989). Gaussian inﬂuence diagrams. Management Science ,35(5), 527–550.

Token 23045:
Shachter , R. D. and Peot, M. (1989). Simulation ap- proaches to general probabilistic inference on beliefnetworks. In UAI-98 .

Token 23046:
Shachter , R. D. and Heckerman, D. (1987). Think- ing backward for knowledge acquisition. AIMag , 3(Fall). Shafer , G. (1976).

Token 23047:
A Mathematical Theory of Evi- dence . Princeton University Press. Shahookar , K. and Mazumder, P. (1991). VLSI cell placement techniques.

Token 23048:
Computing Surveys ,23(2), 143–220. Shanahan , M. (1997). Solving the Frame Problem . MIT Press. Shanahan , M. (1999). The event calculus explained.

Token 23049:
In Wooldridge, M. J. and Veloso, M. (Eds. ), Ar- tiﬁcial Intelligence Today , pp. 409–430. Springer- Verlag. Shankar , N. (1986).

Token 23050:
Proof-Checking Metamathe- matics . Ph.D. thesis, Computer Science Department, University of Texas at Austin. Shannon , C. E. and Weaver, W. (1949).

Token 23051:
The Math- ematical Theory of Communication . University of Illinois Press.Shannon , C. E. (1948). A mathematical theory of communication.

Token 23052:
Bell Systems Technical Journal ,27, 379–423, 623–656. Shannon , C. E. (1950). Programming a computer for playing chess.

Token 23053:
Philosophical Magazine ,41(4), 256–275. Shaparau , D., Pistore, M., and Traverso, P. (2008).

Token 23054:
Fusing procedural and declarative planning goals fornondeterministic domains. In AAAI-08 . Shapiro , E. (1981).

Token 23055:
An algorithm that infers theories from facts. In IJCAI-81 , p. 1064. Shapiro , S. C. (Ed.). (1992).

Token 23056:
Encyclopedia of Artiﬁ- cial Intelligence (second edition). Wiley. Shapley , S. (1953). Stochastic games. In PNAS , Vol. 39, pp. 1095–1100.

Token 23057:
Shatkay , H. and Kaelbling, L. P. (1997). Learning topological maps with weak local odometric infor- mation. In IJCAI-97 . Shelley , M. (1818).

Token 23058:
Frankenstein: Or, the Modern Prometheus . Pickering and Chatto. Sheppard , B. (2002). World-championship-caliber scrabble. AIJ,134(1–2), 241–275.

Token 23059:
Shi, J. and Malik, J. (2000). Normalized cuts and image segmentation. PAMI ,22(8), 888–905. Shieber , S. (1994).

Token 23060:
Lessons from a restricted Turing Test. CACM ,37, 70–78. Shieber , S. (Ed.). (2004). The Turing Test .M I T Press. Shoham , Y. (1993).

Token 23061:
Agent-oriented programming. AIJ,60(1), 51–92. Shoham , Y. (1994). Artiﬁcial Intelligence Tech- niques in Prolog . Morgan Kaufmann.

Token 23062:
Shoham , Y. and Leyton-Brown, K. (2009). Mul- tiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations . Cambridge Univ. Press.

Token 23063:
Shoham , Y., Powers, R., and Grenager, T. (2004). If multi-agent learning is the answer, what is the ques- tion? In Proc.

Token 23064:
AAAI Fall Symposium on Artiﬁcial Multi-Agent Learning . Shortliffe , E. H. (1976). Computer-Based Medical Consultations: MYCIN .

Token 23065:
Elsevier/North-Holland. Sietsma , J. and Dow, R. J. F. (1988). Neural net pruning—Why and how.

Token 23066:
In IEEE International Con- ference on Neural Networks , pp. 325–333. Siklossy , L. and Dreussi, J. (1973).

Token 23067:
An efﬁcient robot planner which generates its own procedures. In IJCAI-73 , pp. 423–430.

Token 23068:
Silverstein , C., Henzinger, M., Marais, H., and Moricz, M. (1998). Analysis of a very large altavista query log. Tech.

Token 23069:
rep. 1998-014, Digital Systems Re- search Center. Simmons , R. and Koenig, S. (1995).

Token 23070:
Probabilis- tic robot navigation in partially observable environ- ments. In IJCAI-95 , pp. 1080–1087. IJCAI, Inc. Simon , D. (2006).

Token 23071:
Optimal State Estimation: Kalman, H Inﬁnity, and Nonlinear Approaches .W i - ley. Simon , H. A. (1947). Administrative behavior . Macmillan.

Token 23072:
Simon , H. A. (1957). Models of Man: Social and Rational . John Wiley. Simon , H. A. (1963). Experiments with a heuristic compiler. JACM ,10, 493–506.

Token 23073:
Simon , H. A. (1981). The Sciences of the Artiﬁcial (second edition). MIT Press.

Token 23074:
Bibliography 1089 Simon , H. A. (1982). Models of Bounded Rational- ity, Volume 1 . The MIT Press. Simon , H. A. and Newell, A. (1958).

Token 23075:
Heuristic problem solving: The next advance in operations re-search. Operations Research ,6, 1–10. Simon , H. A. and Newell, A. (1961).

Token 23076:
Computer simulation of human thinking and problem solving.Datamation ,June/July , 35–37. Simon , J. C. and Dubois, O. (1989).

Token 23077:
Number of solutions to satisﬁability instances—Applications to knowledge bases. AIJ,3, 53–65. Simonis , H. (2005). Sudoku as a constraint prob- lem.

Token 23078:
In CP Workshop on Modeling and Reformulat- ing Constraint Satisfaction Problems , pp. 13–27. Singer , P. W. (2009). Wired for War . Penguin Press.

Token 23079:
Singh , P., Lin, T., Mueller, E. T., Lim, G., Perkins, T., and Zhu, W. L. (2002).

Token 23080:
Open mind common sense: Knowledge acquisition from the general pub- lic. In Proc.

Token 23081:
First International Conference on On- tologies, Databases, and Applications of Semantics for Large Scale Information Systems .

Token 23082:
Singhal , A., Buckley, C., and Mitra, M. (1996). Piv- oted document length normalization. In SIGIR-96 , pp. 21–29. Sittler , R. W. (1964).

Token 23083:
An optimal data association problem in surveillance theory. IEEE Transactions on Military Electronics ,8(2), 125–139. Skinner , B. F. (1953).

Token 23084:
Science and Human Behav- ior. Macmillan. Skolem , T. (1920).

Token 23085:
Logisch-kombinatorische Unter- suchungen ¨ uber die Erf¨ ullbarkeit oder Beweisbarkeit mathematischer S¨ atze nebst einem Theoreme ¨ uber die dichte Mengen.

Token 23086:
Videnskapsselskapets skrifter, I. Matematisk-naturvidenskabelig klasse ,4. Skolem , T. (1928). ¨Uber die mathematische Logik.

Token 23087:
Norsk matematisk tidsskrift ,10, 125–142. Slagle , J. R. (1963). A heuristic program that solves symbolic integration problems in freshman calculus.

Token 23088:
JACM ,10(4). Slate , D. J. and Atkin, L. R. (1977). CHESS 4.5— Northwestern University chess program. In Frey, P. W. (Ed.

Token 23089:
), Chess Skill in Man and Machine , pp. 82–118. Springer-Verlag. Slater , E. (1950). Statistics for the chess computer and the factor of mobility.

Token 23090:
In Symposium on Infor- mation Theory , pp. 150–152. Ministry of Supply. Sleator , D. and Temperley, D. (1993). Parsing En- glish with a link grammar.

Token 23091:
In Third Annual Work- shop on Parsing technologies . Slocum , J. and Sonneveld, D. (2006). The 15 Puzzle . Slocum Puzzle Foundation. Sloman , A.

Token 23092:
(1978). The Computer Revolution in Philosophy . Harvester Press. Smallwood , R. D. and Sondik, E. J. (1973).

Token 23093:
The optimal control of partially observable Markov pro-cesses over a ﬁnite horizon. Operations Research , 21, 1071–1088. Smart , J. J. C. (1959).

Token 23094:
Sensations and brain pro- cesses. Philosophical Review ,68, 141–156. Smith , B. (2004). Ontology. In Floridi, L. (Ed.

Token 23095:
), The Blackwell Guide to the Philosophy of Comput-ing and Information , pp. 155–166. Wiley-Blackwell.

Token 23096:
Smith , D. E., Genesereth, M. R., and Ginsberg, M. L. (1986). Controlling recursive inference. AIJ, 30(3), 343–389. Smith , D. A. and Eisner, J.

Token 23097:
(2008). Dependency parsing by belief propagation. In EMNLP , pp. 145– 156.Smith , D. E. and Weld, D. S. (1998). Conformant Graphplan. In AAAI-98 , pp.

Token 23098:
889–896. Smith , J. Q. (1988). Decision Analysis . Chapman and Hall. Smith , J. E. and Winkler, R. L. (2006).

Token 23099:
The opti- mizer’s curse: Skepticism and postdecision surprisein decision analysis. Management Science ,52(3), 311–322. Smith , J. M. (1982).

Token 23100:
Evolution and the Theory of Games . Cambridge University Press. Smith , J. M. and Szathm´ ary, E. (1999).

Token 23101:
The Ori- gins of Life: From the Birth of Life to the Origin of Language . Oxford University Press.

Token 23102:
Smith , M. K., Welty, C., and McGuinness, D. (2004). OWL web ontology language guide. Tech. rep., W3C. Smith , R. C. and Cheeseman, P. (1986).

Token 23103:
On the rep- resentation and estimation of spatial uncertainty. Int. J. Robotics Research ,5(4), 56–68. Smith , S. J. J., Nau, D. S., and Throop, T. A.

Token 23104:
(1998). Success in spades: Using AI planning techniques towin the world championship of computer bridge. In AAAI-98 , pp. 1079–1086.

Token 23105:
Smolensky , P. (1988). On the proper treatment of connectionism. BBS,2, 1–74. Smullyan , R. M. (1995). First-Order Logic . Dover.

Token 23106:
Smyth , P., Heckerman, D., and Jordan, M. I. (1997). Probabilistic independence networks for hid- den Markov probability models.

Token 23107:
Neural Computa- tion,9(2), 227–269. Snell , M. B. (2008). Do you have free will?

Token 23108:
John Searle reﬂects on various philosophical questions in light of new research on the brain. California Alumni Magazine ,March/April .

Token 23109:
Soderland , S. and Weld, D. S. (1991). Evaluating nonlinear planning.

Token 23110:
Technical report TR-91-02-03, University of Washington Department of Computer Science and Engineering. Solomonoff , R. J. (1964).

Token 23111:
A formal theory of in- ductive inference. Information and Control ,7, 1–22, 224–254. Solomonoff , R. J. (2009).

Token 23112:
Algorithmic probability– theory and applications. In Emmert-Streib, F. and Dehmer, M. (Eds. ), Information Theory and Statiti- cal Learning .

Token 23113:
Springer. Sondik , E. J. (1971). The Optimal Control of Par- tially Observable Markov Decision Processes .P h . D . thesis, Stanford University.

Token 23114:
Sosic , R. and Gu, J. (1994). Efﬁcient local search with conﬂict minimization: A case study of the n- queens problem.

Token 23115:
IEEE Transactions on Knowledge and Data Engineering ,6(5), 661–668. Sowa , J. (1999).

Token 23116:
Knowledge Representation: Logi- cal, Philosophical, and Computational Foundations . Blackwell. Spaan , M. T. J. and Vlassis, N. (2005).

Token 23117:
Perseus: Randomized point-based value iteration for POMDPs. JAIR ,24, 195–220.

Token 23118:
Spiegelhalter , D. J., Dawid, A. P., Lauritzen, S., and Cowell, R. (1993). Bayesian analysis in expert sys- tems. Statistical Science ,8, 219–282.

Token 23119:
Spielberg , S. (2001). AI. Movie. Spirtes , P., Glymour, C., and Scheines, R. (1993). Causation, prediction, and search . Springer-Verlag.

Token 23120:
Srinivasan , A., Muggleton, S. H., King, R. D., and Sternberg, M. J. E. (1994). Mutagenesis: ILP exper- iments in a non-determinate biological domain.

Token 23121:
In ILP-94 , Vol. 237, pp. 217–232.Srivas , M. and Bickford, M. (1990). Formal veri- ﬁcation of a pipelined microprocessor.

Token 23122:
IEEE Soft- ware ,7(5), 52–64. Staab , S. (2004). Handbook on Ontologies . Springer. Stallman , R. M. and Sussman, G. J. (1977).

Token 23123:
Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis. AIJ, 9(2), 135–196.

Token 23124:
Stanﬁll , C. and Waltz, D. (1986). Toward memory- based reasoning. CACM ,29(12), 1213–1228. Steﬁk , M. (1995). Introduction to Knowledge Sys- tems.

Token 23125:
Morgan Kaufmann. Stein , L. A. (2002). Interactive Programming in Java (pre-publication draft) . Morgan Kaufmann.

Token 23126:
Stephenson , T., Bourlard, H., Bengio, S., and Mor- ris, A. (2000).

Token 23127:
Automatic speech recognition using dynamic bayesian networks with both acoustic and articulatory features. In ICSLP-00 , pp. 951–954.

Token 23128:
Stergiou , K. and Walsh, T. (1999). The difference all-difference makes. In IJCAI-99 , pp. 414–419. Stickel , M. E. (1992).

Token 23129:
A prolog technology theorem prover: a new exposition and implementation in pro- log. Theoretical Computer Science ,104, 109–128. Stiller , L. (1992).

Token 23130:
KQNKRR. J. International Com- puter Chess Association ,15(1), 16–18. Stiller , L. (1996). Multilinear algebra and chess endgames. In Nowakowski, R. J.

Token 23131:
(Ed. ), Games of No Chance, MSRI, 29, 1996. Mathematical Sciences Research Institute. Stockman , G. (1979).

Token 23132:
A minimax algorithm better than alpha–beta? AIJ,12(2), 179–196. Stoffel , K., Taylor, M., and Hendler, J. (1997).

Token 23133:
Efﬁ- cient management of very large ontologies. In Proc. AAAI-97 , pp. 442–447. Stolcke , A. and Omohundro, S. (1994).

Token 23134:
Inducing probabilistic grammars by Bayesian model merging. InProc.

Token 23135:
Second International Colloquium on Gram- matical Inference and Applications (ICGI-94) , pp. 106–118. Stone , M. (1974).

Token 23136:
Cross-validatory choice and as- sessment of statostical predictions. J. Royal Statisti- cal Society ,36(111–133). Stone , P. (2000).

Token 23137:
Layered Learning in Multi-Agent Systems: A Winning Approach to Robotic Soccer . MIT Press. Stone , P. (2003).

Token 23138:
Multiagent competitions and re- search: Lessons from RoboCup and TAC. In Lima,P. U. and Rojas, P. (Eds.

Token 23139:
), RoboCup-2002: Robot Soccer World Cup VI , pp. 224–237. Springer Verlag. Stone , P., Kaminka, G., and Rosenschein, J. S. (2009).

Token 23140:
Leading a best-response teammate in an adhoc team. In AAMAS Workshop in Agent Mediated Electronic Commerce . Stork , D. G. (2004).

Token 23141:
Optics and realism in rennais- sance art. Scientiﬁc American , pp. 77–83. Strachey , C. (1952). Logical or non-mathematical programmes. In Proc.

Token 23142:
1952 ACM national meeting (Toronto) , pp. 46–49. Stratonovich , R. L. (1959).

Token 23143:
Optimum nonlinear sys- tems which bring about a separation of a signal withconstant parameters from noise. Radioﬁzika ,2(6), 892–901.

Token 23144:
Stratonovich , R. L. (1965). On value of informa- tion. Izvestiya of USSR Academy of Sciences, Tech- nical Cybernetics ,5, 3–12.

Token 23145:
Subramanian , D. and Feldman, R. (1990). The util- ity of EBL in recursive domain theories. In AAAI-90 , Vol. 2, pp. 942–949.

Token 23146:
1090 Bibliography Subramanian , D. and Wang, E. (1994). Constraint- based kinematic synthesis. In Proc.

Token 23147:
International Conference on Qualitative Reasoning , pp. 228–239. Sussman , G. J. (1975). A Computer Model of Skill Acquisition .

Token 23148:
Elsevier/North-Holland. Sutcliffe , G. and Suttner, C. (1998). The TPTP Prob- lem Library: CNF Release v1.2.1. JAR,21(2), 177– 203.

Token 23149:
Sutcliffe , G., Schulz, S., Claessen, K., and Gelder, A. V. (2006). Using the TPTP language for writing derivations and ﬁnite interpretations.

Token 23150:
In Proc. Inter- national Joint Conference on Automated Reasoning , pp. 67–81. Sutherland , I. (1963).

Token 23151:
Sketchpad: A man-machine graphical communication system. In Proc. Spring Joint Computer Conference , pp. 329–346. Sutton , C. and McCallum, A. (2007).

Token 23152:
An introduc- tion to conditional random ﬁelds for relational learn- ing. In Getoor, L. and Taskar, B. (Eds.

Token 23153:
), Introduction to Statistical Relational Learning . MIT Press. Sutton , R. S. (1988). Learning to predict by the methods of temporal differences.

Token 23154:
Machine Learn- ing,3, 9–44. Sutton , R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000).

Token 23155:
Policy gradient methods for re- inforcement learning with function approximation. In Solla, S. A., Leen, T. K., and M¨ uller, K.-R. (Eds.

Token 23156:
), NIPS 12 , pp. 1057–1063. MIT Press. Sutton , R. S. (1990).

Token 23157:
Integrated architectures for learning, planning, and reacting based on approx-imating dynamic programming. In ICML-90 , pp. 216–224.

Token 23158:
Sutton , R. S. and Barto, A. G. (1998). Reinforce- ment Learning: An Introduction . MIT Press. Svore , K. and Burges, C. (2009).

Token 23159:
A machine learning approach for improved bm25 retrieval. In Proc. Conference on Information Knowledge Man- agement . Swade , D. (2000).

Token 23160:
Difference Engine: Charles Bab- bage And The Quest To Build The First Computer . Diane Publishing Co. Swerling , P. (1959).

Token 23161:
First order error propagation in a stagewise smoothing procedure for satellite obser-vations. J. Astronautical Sciences ,6, 46–52.

Token 23162:
Swift , T. and Warren, D. S. (1994). Analysis of SLG- WAM evaluation of deﬁnite programs. In Logic Pro- gramming. Proc.

Token 23163:
1994 International Symposium onLogic programming , pp. 219–235. Syrj ¨anen , T. (2000). Lparse 1.0 user’s manual. saturn.tcs.hut.fi/Software/smodels .

Token 23164:
Tadepalli , P. (1993). Learning from queries and ex- amples with tree-structured bias. In ICML-93 , pp. 322–329.

Token 23165:
Tadepalli , P., Givan, R., and Driessens, K. (2004). Relational reinforcement learning: An overview. InICML-04 . Tait, P. G. (1880).

Token 23166:
Note on the theory of the “15 puzzle”. Proc. Royal Society of Edinburgh ,10, 664– 665. Tamaki , H. and Sato, T. (1986).

Token 23167:
OLD resolution with tabulation. In ICLP-86 , pp. 84–98. Tarjan , R. E. (1983). Data Structures and Network Algorithms .

Token 23168:
CBMS-NSF Regional Conference Se- ries in Applied Mathematics. SIAM (Society for In- dustrial and Applied Mathematics). Tarski , A. (1935).

Token 23169:
Die Wahrheitsbegriff in den for- malisierten Sprachen. Studia Philosophica ,1, 261– 405.Tarski , A. (1941).

Token 23170:
Introduction to Logic and to the Methodology of Deductive Sciences . Dover. Tarski , A. (1956).

Token 23171:
Logic, Semantics, Metamathe- matics: Papers from 1923 to 1938 . Oxford Univer- sity Press. Tash , J. K. and Russell, S. J. (1994).

Token 23172:
Control strate- gies for a stochastic planner. In AAAI-94 , pp. 1079– 1085. Taskar , B., Abbeel, P., and Koller, D. (2002).

Token 23173:
Dis- criminative probabilistic models for relational data. InUAI-02 . Tate, A. (1975a). Interacting goals and their use. In IJCAI-75 , pp. 215–218.

Token 23174:
Tate, A. (1975b). Using Goal Structure to Direct Search in a Problem Solver . Ph.D. thesis, University of Edinburgh. Tate, A. (1977).

Token 23175:
Generating project networks. In IJCAI-77 , pp. 888–893. Tate, A. and Whiter, A. M. (1984).

Token 23176:
Planning with multiple resource constraints and an application to anaval planning problem. In Proc. First Conference on AI Applications , pp. 410–416.

Token 23177:
Tatman , J. A. and Shachter, R. D. (1990). Dynamic programming and inﬂuence diagrams.

Token 23178:
IEEE Trans- actions on Systems, Man and Cybernetics ,20(2), 365–379. Tattersall , C. (1911).

Token 23179:
A Thousand End-Games: A Collection of Chess Positions That Can be Won orDrawn by the Best Play . British Chess Magazine.

Token 23180:
Taylor , G., Stensrud, B., Eitelman, S., and Dunham, C. (2007). Towards automating airspace manage-ment. In Proc.

Token 23181:
Computational Intelligence for Secu- rity and Defense Applications (CISDA) Conference , pp. 1–5. Tenenbaum , J., Grifﬁths, T., and Niyogi, S. (2007).

Token 23182:
Intuitive theories as grammars for causal inference. In Gopnik, A. and Schulz, L. (Eds.

Token 23183:
), Causal learn- ing: Psychology, Philosophy, and Computation .O x - ford University Press. Tesauro , G. (1992).

Token 23184:
Practical issues in temporal dif- ference learning. Machine Learning ,8(3–4), 257– 277. Tesauro , G. (1995).

Token 23185:
Temporal difference learning and TD-Gammon. CACM ,38(3), 58–68. Tesauro , G. and Sejnowski, T. (1989).

Token 23186:
A parallel network that learns to play backgammon. AIJ,39(3), 357–390. Teyssier , M. and Koller, D. (2005).

Token 23187:
Ordering-based search: A simple and effective algorithm for learningBayesian networks. In UAI-05 , pp. 584–590. Thaler , R. (1992).

Token 23188:
The Winner’s Curse: Paradoxes and Anomalies of Economic Life . Princeton Univer- sity Press. Thaler , R. and Sunstein, C. (2009).

Token 23189:
Nudge: Improv- ing Decisions About Health, Wealth, and Happiness . Penguin. Theocharous , G., Murphy, K., and Kaelbling, L. P. (2004).

Token 23190:
Representing hierarchical POMDPs as DBNs for multi-scale robot localization. In ICRA- 04. Thiele , T. (1880).

Token 23191:
Om anvendelse af mindste kvadraters methode i nogle tilfælde, hvor en kom-plikation af visse slags uensartede tilfældige fejlk- ilder giver fejlene en ‘systematisk’ karakter.

Token 23192:
Vidensk. Selsk. Skr. 5. Rk., naturvid. og mat. Afd. ,12, 381– 408.Thielscher , M. (1999).

Token 23193:
From situation calculus to ﬂuent calculus: State update axioms as a solution to the inferential frame problem. AIJ,111(1–2), 277– 299.

Token 23194:
Thompson , K. (1986). Retrograde analysis of cer- tain endgames. J. International Computer Chess As- sociation ,May, 131–139. Thompson , K. (1996).

Token 23195:
6-piece endgames. J. Inter- national Computer Chess Association ,19(4), 215– 226. Thrun , S., Burgard, W., and Fox, D. (2005).

Token 23196:
Proba- bilistic Robotics .M I TP r e s s . Thrun , S., Fox, D., and Burgard, W. (1998).

Token 23197:
A prob- abilistic approach to concurrent mapping and local-ization for mobile robots. Machine Learning ,31, 29–53. Thrun , S. (2006).

Token 23198:
Stanley, the robot that won the DARPA Grand Challenge. J. Field Robotics ,23(9), 661–692. Tikhonov , A. N. (1963).

Token 23199:
Solution of incorrectly formulated problems and the regularization method. Soviet Math. Dokl. ,5, 1035–1038. Titterington ,D .M .

Token 23200:
,S m i t h ,A .F .M . ,a n dM a k o v , U. E. (1985). Statistical analysis of ﬁnite mixture distributions . Wiley. Tofﬂer , A. (1970).

Token 23201:
Future Shock .B a n t a m . Tomasi , C. and Kanade, T. (1992). Shape and mo- tion from image streams under orthography: A fac- torization method.

Token 23202:
IJCV ,9, 137–154. Torralba , A., Fergus, R., and Weiss, Y. (2008). Small codes and large image databases for recogni- tion. In CVPR , pp. 1–8.

Token 23203:
Trucco , E. and Verri, A. (1998). Introductory Tech- niques for 3-D Computer Vision . Prentice Hall. Tsitsiklis , J. N. and Van Roy, B. (1997).

Token 23204:
An analy- sis of temporal-difference learning with function ap- proximation. IEEE Transactions on Automatic Con- trol,42(5), 674–690.

Token 23205:
Tumer , K. and Wolpert, D. (2000). Collective intel- ligence and braess’ paradox. In AAAI-00 , pp. 104– 109.

Token 23206:
Turcotte , M., Muggleton, S. H., and Sternberg, M. J. E. (2001). Automated discovery of structural sig-natures of protein fold and function.

Token 23207:
J. Molecular Biology ,306, 591–605. Turing , A. (1936). On computable numbers, with an application to the Entscheidungsproblem. Proc.

Token 23208:
London Mathematical Society, 2nd series ,42, 230– 265. Turing , A. (1948). Intelligent machinery. Tech. rep., National Physical Laboratory.

Token 23209:
reprinted in (Ince, 1992). Turing , A. (1950). Computing machinery and intel- ligence. Mind ,59, 433–460.

Token 23210:
Turing , A., Strachey, C., Bates, M. A., and Bowden, B. V. (1953). Digital computers applied to games. In Bowden, B. V . (Ed.

Token 23211:
), Faster than Thought , pp. 286– 310. Pitman. Tversky , A. and Kahneman, D. (1982). Causal schemata in judgements unde r uncertainty.

Token 23212:
In Kah- neman, D., Slovic, P., and Tversky, A. (Eds. ), Judge- ment Under Uncertainty: Heuristics and Biases . Cambridge University Press.

Token 23213:
Ullman , J. D. (1985). Implementation of logical query languages f or databases. ACM Transactions on Database Systems ,10(3), 289–321.

Token 23214:
Ullman , S. (1979). The Interpretation of Visual Mo- tion. MIT Press.

Token 23215:
Bibliography 1091 Urmson , C. and Whittaker, W. (2008). Self-driving cars and the Urban Challenge. IEEE Intelligent Sys- tems,23(2), 66–68.

Token 23216:
Valiant , L. (1984). A theory of the learnable. CACM ,27, 1134–1142. van Beek , P. (2006). Backtracking search algo- rithms.

Token 23217:
In Rossi, F., va n Beek, P., and Walsh, T. (Eds. ), Handbook of Constraint Programming .E l s e - vier. van Beek , P. and Chen, X. (1999).

Token 23218:
CPlan: A con- straint programming approach to planning. In AAAI- 99, pp. 585–590. van Beek , P. and Manchak, D. (1996).

Token 23219:
The design and experimental analysis of algorithms for temporal reasoning. JAIR ,4, 1–18. van Bentham , J. and ter Meulen, A. (1997).

Token 23220:
Hand- book of Logic and Language . MIT Press. Van Emden , M. H. and Kowalski, R. (1976). The semantics of predicate logic as a programming lan- guage.

Token 23221:
JACM ,23(4), 733–742. van Harmelen , F. and Bundy, A. (1988). Explanation-based generalisation = partial evalu- ation. AIJ,36(3), 401–412.

Token 23222:
van Harmelen , F., Lifschitz, V., and Porter, B. (2007). The Handbook of Knowledge Representa- tion. Elsevier. van Heijenoort , J. (Ed.). (1967).

Token 23223:
From Frege to G¨ odel: A Source Book in Mathematical Logic, 1879–1931 . Harvard University Press. Van Hentenryck , P., Saraswat, V., and Deville, Y.

Token 23224:
(1998). Design, implementation, and evaluation of the constraint language cc(FD). J. Logic Program- ming ,37(1–3), 139–164. van Hoeve , W.-J. (2001).

Token 23225:
The alldifferent constraint: as u r v e y . I n 6th Annual Workshop of the ERCIM Working Group on Constraints . van Hoeve , W.-J. and Katriel, I.

Token 23226:
(2006). Global constraints. In Rossi, F., van Beek, P., and Walsh, T. (Eds. ), Handbook of Constraint Processing , pp. 169–208. Elsevier.

Token 23227:
van Lambalgen , M. and Hamm, F. (2005). The Proper Treatment of Events . Wiley-Blackwell. van Nunen , J. A. E. E. (1976).

Token 23228:
A set of succes- sive approximation methods for discounted Marko- vian decision problems.

Token 23229:
Zeitschrift fur Operations Research, Serie A ,20(5), 203–208. Van Roy , B. (1998).

Token 23230:
Learning and value function approximation in complex decision processes .P h . D . thesis, Laboratory for Information and Decision Sys-tems, MIT.

Token 23231:
Van Roy , P. L. (1990). Can logic programming execute as fast as imperative programming?

Token 23232:
Re-port UCB/CSD 90/600, Computer Science Division, University of California, Berkeley, California. Vapnik , V. N. (1998).

Token 23233:
Statistical Learning Theory . Wiley. Vapnik , V. N. and Chervonenkis, A. Y. (1971).

Token 23234:
On the uniform convergence of relative frequencies ofevents to their probabilities. Theory of Probability and Its Applications ,16, 264–280.

Token 23235:
Varian , H. R. (1995). Economic mechanism design for computerized agents. In USENIX Workshop on Electronic Commerce , pp. 13–21. Vauquois , B. (1968).

Token 23236:
A survey of formal grammars and algorithms for recognition and transformation in mechanical translation. In Proc. IFIP Congress , pp.

Token 23237:
1114–1122.Veloso , M. and Carbonell, J. G. (1993). Derivational analogy in PRODIGY : Automating case acquisition, storage, and utilization.

Token 23238:
Machine Learning ,10, 249– 278. Vere , S. A. (1983). Planning in time: Windows and durations for activities and goals. PAMI ,5, 246–267.

Token 23239:
Verma , V., Gordon, G., Simmons, R., and Thrun, S. (2004). Particle ﬁlters for rover fault diagnosis.IEEE Robotics and Automation Magazine ,June.

Token 23240:
Vinge , V. (1993). The coming technological sin- gularity: How to survive in the post-human era. InVISION-21 Symposium .

Token 23241:
NASA Lewis Research Center and the Ohio Aerospace Institute. Viola , P. and Jones, M. (2002a).

Token 23242:
Fast and robust clas- siﬁcation using asymmetric adaboost and a detectorcascade. In NIPS 14 . Viola , P. and Jones, M. (2002b).

Token 23243:
Robust real-time object detection. ICCV . Visser , U. and Burkhard, H.-D. (2007). RoboCup 2006: achievements and goals for the future.

Token 23244:
AIMag , 28(2), 115–130. Visser , U., Ribeiro, F., Ohashi, T., and Dellaert, F. (Eds.). (2008). RoboCup 2007: Robot Soccer World Cup XI . Springer.

Token 23245:
Viterbi , A. J. (1967). Error bounds for convolutional codes and an asymptotically optimum decoding al- gorithm.

Token 23246:
IEEE Transactions on Information Theory , 13(2), 260–269. Vlassis , N. (2008).

Token 23247:
A Concise Introduction to Multi- agent Systems and Distributed Artiﬁcial Intelligence . Morgan and Claypool. von Mises , R. (1928).

Token 23248:
Wahrscheinlichkeit, Statistik und Wahrheit . J. Springer. von Neumann , J. (1928). Zur Theorie der Gesellschaftsspiele.

Token 23249:
Mathematische Annalen , 100(295–320). von Neumann , J. and Morgenstern, O. (1944). The- ory of Games and Economic Behavior (ﬁrst edition).

Token 23250:
Princeton University Press. von Winterfeldt , D. and Edwards, W. (1986). Deci- sion Analysis and Behavioral Research . Cambridge University Press.

Token 23251:
Vossen , T., Ball, M., Lotem, A., and Nau, D. S. (2001). Applying integer programming to AI plan-ning. Knowledge Engineering Review ,16, 85–100.

Token 23252:
Wainwright , M. J. and Jordan, M. I. (2008). Graph- ical models, exponential families, and variational in- ference. Machine Learning , 1(1–2), 1–305.

Token 23253:
Waldinger , R. (1975). Achieving several goals si- multaneously. In Elcock, E. W. and Michie, D. (Eds. ), Machine Intelligence 8 , pp. 94–138.

Token 23254:
Ellis Horwood. Wallace , A. R. (1858). On the tendency of varieties to depart indeﬁnitely fr om the original type. Proc.

Token 23255:
Linnean Society of London ,3, 53–62. Waltz , D. (1975). Understanding line drawings of scenes with shadows. In Winston, P. H. (Ed.

Token 23256:
), The Psychology of Computer Vision . McGraw-Hill. Wang , Y. and Gelly, S. (2007).

Token 23257:
Modiﬁcations of UCT and sequence-like simulations for Monte-Carlo Go. In IEEE Symposium on Computational Intelli- gence and Games , pp. 175–182.

Token 23258:
Wanner , E. (1974). On remembering, forgetting and understanding sentences . Mouton. Warren , D. H. D. (1974). WARPLAN: A System for Generating Plans.

Token 23259:
Department of Computational Logic Memo 76, University of Edinburgh.Warren , D. H. D. (1983). An abstract Prolog in- struction set.

Token 23260:
Technical note 309, SRI International. Warren ,D .H .D . ,P e r e i r a ,L .M . ,a n dP e r e i r a ,F . (1977).

Token 23261:
PROLOG: The language and its imple-mentation compared with LISP. SIGPLAN Notices , 12(8), 109–115. Wasserman , L. (2004). All of Statistics .

Token 23262:
Springer. Watkins , C. J. (1989). Models of Delayed Rein- forcement Learning . Ph.D. thesis, Psychology De- partment, Cambridge University.

Token 23263:
Watson , J. D. and Crick, F. H. C. (1953). A structure for deoxyribose nucleic acid. Nature ,171, 737.

Token 23264:
Waugh , K., Schnizlein, D., Bowling, M., and Szafron, D. (2009). Abstraction pathologies in ex-tensive games. In AAMAS-09 . Weaver , W. (1949).

Token 23265:
Translation. In Locke, W. N. and Booth, D. (Eds. ), Machine translation of lan- guages: fourteen essays , pp. 15–23. Wiley.

Token 23266:
Webber , B. L. and Nilsson, N. J. (Eds.). (1981). Readings in Artiﬁcial Intelligence . Morgan Kauf- mann. Weibull , J. (1995).

Token 23267:
Evolutionary Game Theory .M I T Press. Weidenbach , C. (2001). SPASS: Combining super- position, sorts and splitting. In Robinson, A. andVoronkov, A.

Token 23268:
(Eds. ), Handbook of Automated Rea- soning . MIT Press. Weiss , G. (2000a). Multiagent systems . MIT Press. Weiss , Y. (2000b).

Token 23269:
Correctness of local probability propagation in graphical models with loops. Neural Computation ,12(1), 1–41. Weiss , Y. and Freeman, W. (2001).

Token 23270:
Correctness of belief propagation in Gaussian graphical modelsof arbitrary topology. Neural Computation ,13(10), 2173–2200. Weizenbaum , J. (1976).

Token 23271:
Computer Power and Hu- man Reason . W. H. Freeman. Weld , D. S. (1994). An introduction to least com- mitment planning. AIMag ,15(4), 27–61.

Token 23272:
Weld , D. S. (1999). Recent advances in AI planning. AIMag ,20(2), 93–122. Weld , D. S., Anderson, C. R., and Smith, D. E. (1998).

Token 23273:
Extending graphplan to handle uncertaintyand sensing actions. In AAAI-98 , pp. 897–904. Weld , D. S. and de Kleer, J. (1990).

Token 23274:
Readings in Qualitative Reasoning about Physical Systems .M o r - gan Kaufmann. Weld , D. S. and Etzioni, O. (1994).

Token 23275:
The ﬁrst law of robotics: A call to arms. In AAAI-94 . Wellman , M. P. (1985). Reasoning about preference models.

Token 23276:
Technical report MIT/LCS/TR-340, Labo- ratory for Computer Science, MIT. Wellman , M. P. (1988).

Token 23277:
Formulation of Tradeoffs in Planning under Uncertainty . Ph.D. thesis, Mas- sachusetts Institute of Technology. Wellman , M. P. (1990a).

Token 23278:
Fundamental concepts of qualitative probabilistic networks. AIJ,44(3), 257– 303. Wellman , M. P. (1990b).

Token 23279:
The STRIPS assumption for planning under uncertainty. In AAAI-90 , pp. 198– 203. Wellman , M. P. (1995).

Token 23280:
The economic approach to artiﬁcial intelligence. ACM Computing Surveys , 27(3), 360–362. Wellman , M. P., Breese, J. S., and Goldman, R. (1992).

Token 23281:
From knowledge bases to decision models. Knowledge Engineering Review ,7(1), 35–53.

Token 23282:
1092 Bibliography Wellman , M. P. and Doyle, J. (1992). Modular util- ity representation for decision-theoretic planning. In ICAPS-92 , pp. 236–242.

Token 23283:
Wellman ,M . P . ,W u r m a n ,P . ,O ’ M a l l e y ,K . , Bangera, R., Lin, S., Reeves, D., and Walsh, W.(2001). A trading agent competition.

Token 23284:
IEEE Inter- net Computing . Wells , H. G. (1898). The War of the Worlds . William Heinemann. Werbos , P. (1974).

Token 23285:
Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sci- ences . Ph.D. thesis, Harvard University. Werbos , P. (1977).

Token 23286:
Advanced forecasting methods for global crisis warning and models of intelligence.General Systems Yearbook ,22, 25–38.

Token 23287:
Wesley , M. A. and Lozano-Perez, T. (1979). An algorithm for planning collision-free paths amongpolyhedral objects. CACM ,22(10), 560–570.

Token 23288:
Wexler , Y. and Meek, C. (2009). MAS: A multi- plicative approximation scheme for probabilistic in- ference. In NIPS 21 . Whitehead , A. N. (1911).

Token 23289:
An Introduction to Math- ematics . Williams and Northgate. Whitehead , A. N. and Russell, B. (1910). Principia Mathematica .

Token 23290:
Cambridge University Press. Whorf , B. (1956). Language, Thought, and Reality . MIT Press. Widrow , B. (1962).

Token 23291:
Generalization and information storage in networks of adaline “neurons”. In Self- Organizing Systems 1962 , pp. 435–461.

Token 23292:
Widrow , B. and Hoff, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON Conven- tion Record , pp. 96–104. Wiedijk , F. (2003).

Token 23293:
Comparing mathematical provers. In Mathematical Knowledge Management , pp. 188–202. Wiegley , J., Goldberg, K., Peshkin, M., and Brokowski, M. (1996).

Token 23294:
A complete algorithm fordesigning passive fences to orient parts. In ICRA- 96. Wiener , N. (1942).

Token 23295:
The extrapolation, interpolation, and smoothing of stationary time series. Osrd 370,Report to the Services 19, Research Project DIC- 6037, MIT.

Token 23296:
Wiener , N. (1948). Cybernetics . Wiley. Wilensky , R. (1978). Understanding goal-based stories . Ph.D. thesis, Yale University. Wilensky , R. (1983).

Token 23297:
Planning and Understanding . Addison-Wesley. Wilkins , D. E. (1980). Using patterns and plans in chess. AIJ,14(2), 165–203. Wilkins , D. E. (1988).

Token 23298:
Practical Planning: Extend- ing the AI Planning Paradigm . Morgan Kaufmann. Wilkins , D. E. (1990). Can AI planners solve prac- tical problems?

Token 23299:
Computational Intelligence ,6(4), 232–246. Williams , B., Ingham, M., Chung, S., and Elliott, P. (2003).

Token 23300:
Model-based programming of intelligent embedded systems and robotic space explorers. In Proc.

Token 23301:
IEEE: Special Issue on Modeling and Designof Embedded Software , pp. 212–237. Williams , R. J. (1992).

Token 23302:
Simple statistical gradient- following algorithms for connectionist reinforce- ment learning.

Token 23303:
Machine Learning ,8, 229–256.Williams , R. J. and Baird, L. C. I. (1993).

Token 23304:
Tight performance bounds on greedy policies based on im- perfect value functions. Tech.

Token 23305:
rep. NU-CCS-93-14, College of Computer Science, Northeastern Univer-sity. Wilson , R. A. and Keil, F. C. (Eds.). (1999).

Token 23306:
The MIT Encyclopedia of the Cognitive Sciences . MIT Press. Wilson , R. (2004). Four Colors Sufﬁce . Princeton University Press.

Token 23307:
Winograd , S. and Cowan, J. D. (1963). Reliable Computation in the Presence of Noise . MIT Press. Winograd , T. (1972).

Token 23308:
Understanding natural lan- guage. Cognitive Psychology ,3(1), 1–191. Winston , P. H. (1970). Learning structural descrip- tions from examples.

Token 23309:
Technical report MAC-TR-76,Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology. Winston , P. H. (1992).

Token 23310:
Artiﬁcial Intelligence (Third edition). Addison-Wesley. Wintermute , S., Xu, J., and Laird, J. (2007).

Token 23311:
SORTS: A human-level approach to real-time strat- egy AI. In Proc.

Token 23312:
Third Artiﬁcial Intelligence and In- teractive Digital Entertainment Conference (AIIDE-07). Witten , I. H. and Bell, T. C. (1991).

Token 23313:
The zero- frequency problem: Estimating the probabilities of novel events in adaptive text compression.

Token 23314:
IEEE Transactions on Information Theory ,37(4), 1085– 1094. Witten , I. H. and Frank, E. (2005).

Token 23315:
Data Mining: Practical Machine Learning Tools and Techniques(2nd edition). Morgan Kaufmann. Witten , I. H., Moffat, A., and Bell, T. C. (1999).

Token 23316:
Managing Gigabytes: Compressing and Indexing Documents and Images (second edition). Morgan Kaufmann. Wittgenstein , L. (1922).

Token 23317:
Tractatus Logico- Philosophicus (second edition). Routledge and Kegan Paul. Reprinted 1971, edited by D. F. Pearsand B. F. McGuinness.

Token 23318:
This edition of the Englishtranslation also contains Wittgenstein’s original Ger- man text on facing pages, as well as Bertrand Rus- sell’s introduction to the 1922 edition.

Token 23319:
Wittgenstein , L. (1953). Philosophical Investiga- tions . Macmillan. Wojciechowski , W. S. and Wojcik, A. S. (1983).

Token 23320:
Au- tomated design of multiple-valued logic circuits by automated theorem proving techniques. IEEE Trans- actions on Computers ,C-32 (9), 785–798.

Token 23321:
Wolfe , J. and Russell, S. J. (2007). Exploiting belief state structure in graph search. In ICAPS Workshop on Planning in Games . Woods , W. A.

Token 23322:
(1973). Progress in natural language understanding: An application to lunar geology. In AFIPS Conference Proceedings , Vol. 42, pp. 441– 450.

Token 23323:
Woods , W. A. (1975). What’s in a link? Founda- tions for semantic networks. In Bobrow, D. G. and Collins, A. M. (Eds.

Token 23324:
), Representation and Under- standing: Studies in Cognitive Science , pp. 35–82. Academic Press. Wooldridge , M. (2002).

Token 23325:
An Introduction to MultiA- gent Systems . Wiley. Wooldridge , M. and Rao, A. (Eds.). (1999). Foun- dations of rational agency .K l u w e r .

Token 23326:
Wos, L., Carson, D., and Robinson, G. (1964). The unit preference strategy in theorem proving. In Proc. Fall Joint Computer Conference , pp.

Token 23327:
615–621.Wos, L., Carson, D., and Robinson, G. (1965). Efﬁ- ciency and completeness of the set-of-support strat- egy in theorem proving.

Token 23328:
JACM ,12, 536–541. Wos, L., Overbeek, R., Lu sk, E., and Boyle, J. (1992). Automated Reasoning: Introduction and Ap- plications (second edition).

Token 23329:
McGraw-Hill. Wos, L. and Robinson, G. (1968). Paramodulation and set of support. In Proc. IRIA Symposium on Au- tomatic Demonstration , pp. 276–310.

Token 23330:
Wos, L., Robinson, G., Carson, D., and Shalla, L. (1967). The concept of demodulation in theoremproving. JACM ,14, 698–704.

Token 23331:
Wos, L. and Winker, S. (1983). Open questions solved with the assistance of AURA. In Automated Theorem Proving: After 25 Years: Proc.

Token 23332:
Special Ses- sion of the 89th Annual Meeting of the American Mathematical Society , pp. 71–88. American Math- ematical Society.

Token 23333:
Wos, L. and Pieper, G. (2003). Automated Rea- soning and the Discovery of Missing and Elegant Proofs . Rinton Press.

Token 23334:
Wray , R. E. and Jones, R. M. (2005). An intro- duction to Soar as an agent architecture. In Sun, R. (Ed.

Token 23335:
), Cognition and Multi-agent Interaction: From Cognitive Modeling to Social Simulation , pp. 53–78. Cambridge University Press. Wright , S. (1921).

Token 23336:
Correlation and causation. J. Agricultural Research ,20, 557–585. Wright , S. (1931). Evolution in Mendelian popula- tions. Genetics ,16, 97–159.

Token 23337:
Wright , S. (1934). The method of path coefﬁcients. Annals of Mathematical Statistics ,5, 161–215. Wu, D. (1993).

Token 23338:
Estimating probability distributions over hypotheses with variable uniﬁcation. In IJCAI- 93, pp. 790–795. Wu, F. and Weld, D. S. (2008).

Token 23339:
Automatically reﬁn- ing the wikipedia infobox ontology. In 17th World Wide Web Conference (WWW2008) .

Token 23340:
Yang , F., Culberson, J., Holte, R., Zahavi, U., and Felner, A. (2008). A general theory of additive state space abstractions. JAIR ,32, 631–662.

Token 23341:
Yang , Q. (1990). Formalizing planning knowledge for hierarchical planning. Computational Intelli- gence ,6, 12–24. Yarowsky , D. (1995).

Token 23342:
Unsupervised word sense dis- ambiguation rivaling supervised methods. In ACL- 95, pp. 189–196. Yedidia , J., Freeman, W., and Weiss, Y. (2005).

Token 23343:
Con- structing free-energy approximations and general-ized belief propagation algorithms. IEEE Transac- tions on Information Theory ,51(7), 2282–2312.

Token 23344:
Yip, K. M.-K. (1991). KAM: A System for Intelli- gently Guiding Numerical Experimentation by Com- puter . MIT Press. Yngve , V. (1955).

Token 23345:
A model and an hypothesis for language structure. In Locke, W. N. and Booth, A. D. (Eds. ), Machine Translation of Languages , pp. 208– 226.

Token 23346:
MIT Press. Yob, G. (1975). Hunt the wumpus! Creative Com- puting ,Sep/Oct . Yoshikawa , T. (1990). Foundations of Robotics: Analysis and Control .

Token 23347:
MIT Press. Young , H. P. (2004). Strategic Learning and Its Lim- its. Oxford University Press. Younger , D. H. (1967).

Token 23348:
Recognition and parsing of context-free languages in time n3.Information and Control ,10(2), 189–208.

Token 23349:
Bibliography 1093 Yudkowsky , E. (2008). Artiﬁcial intelligence as a positive and negative factor in global risk. In Bostrom, N. and Cirkovic, M.

Token 23350:
(Eds. ), Global Catas- trophic Risk . Oxford University Press. Zadeh , L. A. (1965). Fuzzy sets. Information and Control ,8, 338–353. Zadeh , L. A.

Token 23351:
(1978). Fuzzy sets as a basis for a the- ory of possibility. Fuzzy Sets and Systems ,1, 3–28.

Token 23352:
Zaritskii , V. S., Svetnik, V. B., and Shimelevich, L. I. (1975). Monte-Carlo technique in problems ofoptimal information processing.

Token 23353:
Automation and Re- mote Control ,36, 2015–22. Zelle , J. and Mooney, R. (1996). Learning to parse database queries using induc tive logic programming.

Token 23354:
InAAAI-96 , pp. 1050–1055. Zermelo , E. (1913). Uber Eine Anwendung der Mengenlehre auf die Theorie des Schachspiels. InProc.

Token 23355:
Fifth International Congress of Mathemati- cians , Vol. 2, pp. 501–504. Zermelo , E. (1976).

Token 23356:
An application of set theory to the theory of chess-playing. Firbush News ,6, 37–42.

Token 23357:
English translation of (Zermelo 1913).Zettlemoyer , L. S. and Collins, M. (2005).

Token 23358:
Learning to map sentences to logical form: Structured classi- ﬁcation with probabilistic categorial grammars. In UAI-05 .

Token 23359:
Zhang , H. and Stickel, M. E. (1996). An efﬁcient algorithm for unit-propagation. In Proc.

Token 23360:
Fourth In- ternational Symposium on Artiﬁcial Intelligence andMathematics . Zhang , L., Pavlovic, V., Cantor, C. R., and Kasif, S. (2003).

Token 23361:
Human-mouse gene identiﬁcation by com- parative evidence integration and evolutionary anal- ysis. Genome Research , pp. 1–13.

Token 23362:
Zhang , N. L. and Poole, D. (1994). A simple ap- proach to Bayesian network computations. In Proc.

Token 23363:
10th Canadian Conference on Artiﬁcial Intelligence , pp. 171–178. Zhang , N. L., Qi, R., and Poole, D. (1994).

Token 23364:
A com- putational theory of decision networks. IJAR ,11, 83–158. Zhou , R. and Hansen, E. (2002). Memory-bounded A* graph search. In Proc.

Token 23365:
15th International Flairs Conference .Zhou , R. and Hansen, E. (2006). Breadth-ﬁrst heuristic search. AIJ,170(4–5), 385–408.

Token 23366:
Zhu, D. J. and Latombe, J.-C. (1991). New heuris- tic algorithms for efﬁcient hierarchical path plan- ning.

Token 23367:
IEEE Transactions on Robotics and Automa- tion,7(1), 9–20. Zimmermann , H.-J. (Ed.). (1999).

Token 23368:
Practical appli- cations of fuzzy technologies .K l u w e r . Zimmermann , H.-J. (2001). Fuzzy Set Theory— And Its Applications (Fourth edition).

Token 23369:
Kluwer. Zinkevich , M., Johanson, M., Bowling, M., and Pic- cione, C. (2008). Regret minimization in games with incomplete information.

Token 23370:
In NIPS 20 , pp. 1729–1736. Zollmann , A., Venugopal, A., Och, F. J., and Ponte, J. (2008).

Token 23371:
A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. InCOLING-08 . Zweig , G. and Russell, S. J. (1998).

Token 23372:
Speech recogni- tion with dynamic Bayesian networks. In AAAI-98 , pp. 173–180.

Token 23373:
This page intentionally left blank

Token 23374:
Index Page numbers in bold refer to deﬁnitions of terms and algorithms; page numbers in italics refer to items in the bibliography.

Token 23375:
Symbols ∧(and), 244 χ2(chi squared), 706 |(cons list cell), 305 /turnstileleft(derives), 242 /follows(determination), 784 |=(entailment), 240 /epsilon1-ball, 714 ∃(there exists), 297 ∀(for all), 295 |(given), 485 ⇔(if and only if), 244 ⇒(implies), 244 ∼(indifferent), 612 λ(lambda)-expression, 294 ¬(not), 244 ∨(or), 244 /follows(preferred), 612 /mapsto→(uncertain rule), 548 A A(s)(actions in a state), 645 A* search, 93–99AAAI (American Association for AI), 31 Aarup, M., 432, 1064 Abbeel, P., 556, 857, 1068 ,1090 Abbott, L. F., 763, 854, 1070 ABC computer, 14Abdennadher, S., 230, 1073 Abelson, R. P., 23, 921, 1088 Abney, S., 921, 1064 ABO (Asymptotic Bounded Optimality), 1050 Abramson, B., 110, 1064 absolute error, 98 abstraction, 69, 677 abstraction hierarchy, 432 A BSTRIPS , 432 Abu-Hanna, A., 505, 1081 AC-3, 209 Academy Award, 435accessibility relations, 451 accusative case, 899Acero, A., 922, 1076 Acharya, A., 112, 1068 Achlioptas, D., 277, 278, 1064Ackley, D. H., 155, 1064 acoustic model, 913 in disambiguation, 906 ACT, 336 ACT*, 799 acting rationally, 4 action, 34, 67, 108, 367 high-level, 406 joint, 427 monitoring, 423, 424 primitive, 406 rational, 7, 30 action-utility function, 627, 831 action exclusion axiom, 273, 428 action monitoring, 423, 424 action schema, 367 activation function, 728 active learning, 831 active sensing, 928 active vision, 1025 actor, 426 actuator, 34,4 1 hydraulic, 977 pneumatic, 977 AD-tree, 826 A DABOOST ,751 adalines, 20 Adams, J., 450 Ada programming language, 14 adaptive control theory, 833, 854 adaptive dynamic programming, 834, 834–835, 853, 858 adaptive perception, 985 add-one smoothing, 863 add list, 368 Adelson-Velsky, G. M., 192, 1064 Adida, B., 469, 1064 ADL (Action Description Language), 394 admissible heuristic, 94, 376 Adorf, H.-M., 432, 1077 ADP (Adaptive Dynamic Programming), 834 adversarial search, 161 adversarial task, 866 adversary argument, 149 Advice Taker, 19, 23 AFSM, 1003 agent, 4, 34, 59active, 839 architecture of, 26, 1047 autonomous, 236 components, 1044–1047 decision-theoretic, 483, 610, 664–666 goal-based, 52–53, 59, 60 greedy, 839 hybrid, 268 intelligent, 30, 1036, 1044 knowledge-based, 13, 234–236, 285, 1044 learning, 54–57, 61 logical, 265–274, 314 model-based, 50, 50–52 online planning, 431 passive, 832 passive ADP, 858 passive learning, 858 problem-solving, 64, 64–69 rational, 4, 4–5, 34, 36–38, 59, 60, 636, 1044 reﬂex, 48, 48–50, 59, 647, 831 situated, 1025 software agent, 41 taxi-driving, 56, 1047 utility-based, 53–54, 59, 664 vacuum, 37, 62–63 wumpus, 238, 305 agent function, 35, 647 agent program, 35, 46, 59 Agerbeck, C., 228, 1064 Aggarwal, G., 682, 1064 aggregation, 403 Agichtein, E., 885, 1064 Agmon, S., 761, 1064 Agre, P. E., 434, 1064 agreement (in a sentence), 900 Aguirre, A., 278, 1068 Aho, A. V ., 1059, 1064 AI,seeartiﬁcial intelligence aircraft carrier scheduling, 434 airport, driving to, 480 airport siting, 622, 626 AISB (Society for Artiﬁcial Intelligence and Simulation of Behaviour),31 AI Winter, 24, 28 Aizerman, M., 760, 1064 Al-Chang, M., 28, 1064 1095

Token 23376:


Token 23377:
1096 Index al-Khowarazmi, 8 Alberti, L. B., 966Albus, J. S., 855, 1064 Aldiss, B., 1040 Aldous, D., 154, 1064 Alekhnovich, M., 277, 1064 Alexandria, 15 algorithm, 8 algorithmic complexity, 759Alhazen, 966alignment method, 956 Allais, M., 620, 638, 1064 Allais paradox, 620Alldiff constraint, 206 Allen, B., 432, 1072 Allen, C., 638, 1069 Allen, J. F., 396, 431, 448, 470, 1064 alliance (in multiplayer games), 166 Allis, L., 194, 1064 Almanac Game, 640 Almuallim, H., 799, 1064 Almulla, M., 111, 1085 ALPAC., 922, 1064 Alperin Resnick, L., 457, 471, 1066 α(normalization constant), 497 alpha–beta pruning, 167, 199 alpha–beta search, 167–171, 189, 191 A LPHA -BETA-SEARCH ,170 Alterman, R., 432, 1064 Altman, A., 195, 1064 altruism, 483 Alvey report, 24 AM, 800 Amarel, S., 109, 115, 156, 468, 1064 ambient illumination, 934 ambiguity, 287, 465, 861, 904–912, 919 lexical, 905 semantic, 905 syntactic, 905, 920 ambiguity aversion, 620 Amir, E., 195, 278, 556, 1064 ,1070 , 1086 Amit, D., 761, 1064 analogical reasoning, 799 ANALOGY , 19, 31 analysis of algorithms, 1053 Analytical Engine, 14analytical generalization, 799Anantharaman, T. S., 192, 1076 Anbulagan, 277, 1080 anchoring effect, 621 anchor text, 463 AND–OR graph, 257 And-Elimination, 250 A ND-OR-GRAPH -SEARCH ,136 AND –ORtree, 135AND-SEARCH ,136 Andersen, S. K., 552, 553, 1064 Anderson, C. R., 395, 433, 1091 Anderson, C. W., 855, 1065 Anderson, J.

Token 23378:
A., 761, 1075 Anderson, J. R., 13, 336, 555, 799, 1064 ,1085 AND node, 135 Andoni, A., 760, 1064 Andre, D., 156, 855, 856, 1064 ,1070 , 1079 ANGELIC -SEARCH ,414 angelic semantics, 431 answer literal, 350 answer set programming, 359 antecedent, 244 Anthony, M., 762, 1064 anytime algorithm, 1048 Aoki, M., 686, 1064 aortic coarctation, 634 apparent motion, 940appearance, 942 appearance model, 959 Appel, K., 227, 1064 Appelt, D., 884, 921, 1064 ,1075, 1076 A PPEND ,341 applicable, 67, 368, 375 apprenticeship learning, 857, 1037 approximate near-neighbors, 741 Apt, K. R., 228, 230, 1064 Apt´e, C., 884, 1064 Arbuthnot, J., 504, 1064 arc consistency, 208 Archibald, C., 195, 1064 architecture, 46 agent, 26, 1047 cognitive, 336 for speech recognition, 25 hybrid, 1003, 1047 parallel, 112pipeline, 1005 reﬂective, 1048 rule-based, 336 three-layer, 1004 arc reversal, 559 Arentoft, M. M., 432, 1064 argmax , 1059 argmax , 166 argument from disability, 1021–1022 from informality, 1024–1025 Ariely, D., 619, 638, 1064 Aristotle, 4–7, 10, 59, 60, 275, 313, 468, 469, 471, 758, 966, 1041 arity, 292, 332 Arkin, R., 1013, 1064Arlazarov, V .

Token 23379:
L., 192, 1064 Armando, A., 279, 1064 Arnauld, A., 7, 636, 1064 Arora, S., 110, 1064 ARPAbet, 914 artiﬁcial ﬂight, 3Artiﬁcial General Intelligence, 27 artiﬁcial intelligence, 1, 1–1052 applications of, 28–29 conferences, 31 foundations, 5–16, 845 future of, 1051–1052 goals of, 1049–1051 history of, 16–28 journals, 31 philosophy of, 1020–1043 possibility of, 1020–1025 programming language, 19 real-time, 1047 societies, 31 strong, 1020 , 1026–1033, 1040 subﬁelds, 1as universal ﬁeld, 1 weak, 1020 , 1040 artiﬁcial life, 155 artiﬁcial urea, 1027Arunachalam, R., 688, 1064 Asada, M., 195, 1014, 1078 asbestos removal, 615Ashby, W. R., 15, 1064 Asimov, I., 1011, 1038, 1064 ASKMSR, 872, 873, 885 assertion (logical), 301 assignment (in a CSP), 203 associative memory, 762 assumption, 462 Astrom, K. J., 156, 686, 1064 astronomer, 562asymptotic analysis, 1054 , 1053–1054 asymptotic bounded optimality, 1050 Atanasoff, J., 14 Atkeson, C. G., 854, 1083 Atkin, L. R., 110, 1089 atom, 295 atomic representation, 57,6 4 atomic sentence, 244, 295, 294–295, 299 attribute, 58 attribute-based extraction, 874 auction, 679 ascending-bid, 679 Dutch, 692English, 679 ﬁrst-price, 681 sealed-bid, 681 second-price, 681

Token 23380:


Token 23381:
Index 1097 truth-revealing, 680 Vickrey, 681 Audi, R., 1042, 1064 Auer, S., 439, 469, 1066 augmentation, 919augmented ﬁnite state machine (AFSM), 1003 augmented grammar, 897 A URA, 356, 360 Austin, G. A., 798, 1067 Australia, 203, 204, 216 authority, 872 AUTOCLASS , 826 automata, 1035, 1041 automated debugging, 800 automated taxi, 40, 56, 236, 480, 694, 695, 1047 automobile insurance, 621 Auton, L. D., 277, 1069 autonomic computing, 60 autonomous underwater vehicle (AUV), 972 autonomy, 39 average reward, 650 Axelrod, R., 687, 1064 axiom, 235, 302 action exclusion, 273, 428 of Chinese room, 1032 decomposability, 614domain-speciﬁc, 439effect axiom, 266 frame axiom, 267 Kolmogorov’s, 489 of number theory, 303of probability, 489 Peano, 303, 313, 333 precondition, 273of probability, 488–490, 1057of set theory, 304successor-state, 267, 279, 389 of utility theory, 613 wumpus world, 305 axon, 11 B b∗(branching factor), 103 B* search, 191Baader, F., 359, 471, 1064 Babbage, C., 14, 190Bacchus, F., 228, 230, 505, 555, 638, 1064, 1065 bachelor, 441Bachmann, P. G. H., 1059, 1065 B ACK-PROP-LEARNING ,734 back-propagation, 22, 24, 733–736, 761backgammon, 177–178, 186, 194, 846, 850 background knowledge, 235, 349, 777, 1024, 1025 background subtraction, 961 backing up (in a search tree), 99, 165 backjumping, 219, 229 backmarking, 229 backoff model, 863 BACKTRACK ,215 backtracking chronological, 218 dependency-directed, 229 dynamic, 229 intelligent, 218–220, 262 BACKTRACKING -SEARCH ,215 backtracking search, 87, 215, 218–220, 222, 227 Backus, J. W., 919, 1065 Backus–Naur form (BNF), 1060 backward chaining, 257, 259, 275, 337–345, 358 backward search for planning, 374–376 Bacon, F., 6bagging, 760 Bagnell, J.

Token 23382:
A., 852, 1013, 1065 bag of words, 866, 883 Baird, L. C. I., 685, 1092 Baker, J., 920, 922, 1065 Balashek, S., 922, 1070 Baldi, P., 604, 1065 Baldwin, J. M., 130, 1065 Ball, M., 396, 1091 Ballard, B. W., 191, 200, 1065 Baluja, S., 155, 968, 1065 ,1087 Bancilhon, F., 358, 1065 bandit problem, 840, 855 Banerji, R., 776, 799, 1082 bang-bang control, 851 Bangera, R., 688, 1092 Banko, M., 28, 439, 469, 756, 759, 872, 881, 885, 1065 ,1072 Bar-Hillel, Y ., 920, 922, 1065 Bar-Shalom, Y ., 604, 606, 1065 Barifaijo, E., 422, 1077 Barry, M., 553, 1076 Bartak, R., 230, 1065 Bartlett, F., 13 Bartlett, P., 762, 855, 1064, 1065 Barto, A. G., 157, 685, 854, 855, 857, 1065 ,1067 ,1090 Barwise, J., 280, 314, 1065 baseline, 950 basic groups, 875 Basin, D. A., 191, 1072 basis function, 845Basye, K., 1012, 1070 Bates, E., 921, 1071 Bates, M. A., 14, 192, 1090 Batman, 435 bats, 435Baum, E., 128, 191, 761, 762, 1065 Baum, L. E., 604, 826, 1065 Baumert, L., 228, 1074 Baxter, J., 855, 1065 Bayardo, R. J., 229, 230, 277, 1065 Bayer, K. M., 228, 1086 Bayerl, S., 359, 1080 Bayes’ rule, 9, 495, 495–497, 503, 508 Bayes, T., 495, 504, 1065 Bayes–Nash equilibrium, 678 Bayesian, 491Bayesian classiﬁer, 499 Bayesian learning, 752, 803, 803–804, 825 Bayesian network, 26, 510, 510–517, 551, 565, 827 dynamic, 590, 590–599 hybrid, 520, 552 inference in, 522–530 learning hidden variables in, 824 learning in, 813–814 multi-entity, 556 Bayes Net toolkit, 558 Beal, D. F., 191, 1065 Beal, J., 27, 1065 Beame, P., 277, 1064 beam search, 125, 174 Bear, J., 884, 1075 Beber, G., 30, 1071 Beckert, B., 359, 1065 beer factory scheduling, 434 Beeri, C., 229, 1065 beetle, dung, 39, 61, 424, 1004 behaviorism, 12, 15, 60 Bekey, G., 1014, 1065 belief, 450, 453 degree of, 482, 489 desires and, 610–611 belief function, 549 belief network, seeBayesian network belief propagation, 555 belief revision, 460 belief state, 138, 269, 415, 480 in game theory, 675 probabalistic, 566, 570 wiggly, 271 belief update, 460 Bell, C., 408, 431, 1065 Bell, D. A., 826, 1068 Bell, J. L., 314, 1065 Bell, T. C., 883, 884, 1092

Token 23383:


Token 23384:
1098 Index BELLE , 192 Bell Labs, 922Bellman, R. E., 2, 10, 109, 110, 194, 652, 685, 760, 1065 Bellman equation, 652 Bellman update, 652 Belongie, S., 755, 762, 1065 Ben-Tal, A., 155, 1065 benchmarking, 1053 Bendix, P. B., 359, 1078 Bengio, S., 604, 1089 Bengio, Y ., 760, 1047, 1065 B ENINQ, 472 Bennett, B., 473, 1069 Bennett, F. H., 156, 1079 Bennett, J., 360, 1074 Bentham, J., 637, 1065 Berger, H., 11 Berger, J. O., 827, 1065 Berkson, J., 554, 1065 Berlekamp, E. R., 113, 186, 1065 Berleur, J., 1034, 1065 Berliner, H. J., 191, 194, 198, 1065 Bernardo, J. M., 811, 1065 Berners-Lee, T., 469, 1065 Bernoulli, D., 617, 637, 1065 Bernoulli, J., 9, 504 Bernoulli, N., 641Bernstein, A., 192, 1065 Bernstein, P. L., 506, 691, 1065 Berrou, C., 555, 1065 Berry, C., 14Berry, D. A., 855, 1065 Bertele, U., 553, 1066 Bertoli, P., 433, 1066 Bertot, Y ., 359, 1066 Bertsekas, D., 60, 506, 685, 857, 1059, 1066 BESM, 192 Bessi` ere, C., 228, 1066 best-ﬁrst search, 92, 108 best possible prize, 615 beta distribution, 592, 811 Betlem, H., 422, 1077 Betlem, J., 422, 1077 betting game, 490 Bezzel, M., 109 BGB LITZ , 194 Bhar, R., 604, 1066 B i a l i k ,H .N .

Token 23385:
,9 0 8 bias, declarative, 787 Bibel, W., 359, 360, 1066 ,1080 Bickford, M., 356, 1089 biconditional, 244 Biddulph, R., 922, 1070 bidirectional search, 90–112Bidlack, C., 1013, 1069 Biere, A., 278, 1066 Bigelow, J., 15, 1087 Bigham, J., 885, 1085 bilingual corpus, 910 billiards, 195Billings, D., 678, 687, 1066 Bilmes, J., 604, 1080 ,1086 binary decision diagram, 395 binary resolution, 347 Binder, J., 604, 605, 826, 1066 ,1087 binding list, 301 Binford, T. O., 967, 1066 Binmore, K., 687, 1066 binocular stereopsis, 949, 949–964 binomial nomenclature, 469 bioinformatics, 884biological naturalism, 1031 Birbeck, M., 469, 1064 Bishop, C. M., 155, 554, 759, 762, 763, 827, 1066 Bishop, M., 1042, 1086 Bishop, R. H., 60, 1071 Bisson, T., 1042, 1066 Bistarelli, S., 228, 1066 Bitman, A. R., 192, 1064 Bitner, J. R., 228, 1066 Bizer, C., 439, 469, 1066 Bjornsson, Y ., 194, 1088 BKG (backgammon program), 194 B LACKBOX , 395 Blake, A., 605, 1077 Blakeslee, S., 1047, 1075 Blazewicz, J., 432, 1066 Blei, D. M., 883, 1066 Blinder, A. S., 691, 1066 blind search, seesearch, uninformed Bliss, C. I., 554, 1066 Block, H. D., 20, 1066 blocks world, 20, 23, 370, 370–371, 472 BLOG, 556 bluff, 184 Blum, A. L., 395, 752, 761, 885, 1066 Blumer, A., 759, 1066 BM25 scoring function, 868, 884 BNF (Backus–Naur form), 1060 BO, 1050 Bobick, A., 604, 1077 Bobrow, D. G., 19, 884, 1066 Boddy, M., 156, 433, 1048, 1070 ,1074 Boden, M. A., 275, 1042, 1066 body (of Horn clause), 256 boid, 429, 435 Bolognesi, A., 192, 1066 Boltzmann machine, 763 Bonaparte, N., 190Boneh, D., 128, 1065 Bonet, B., 156, 394, 395, 433, 686, 1066 ,1075 Bongard, J., 1041, 1085 Boole, G., 7, 8, 276, 1066 Boolean keyword model, 867boosting, 749, 760 Booth, J. W., 872 Booth, T. L., 919, 1066 bootstrap, 27, 760 Borel, E., 687, 1066 Borenstein, J., 1012, 1013, 1066 Borgida, A., 457, 471, 1066 Boroditsky, L., 287, 1066 Boser, B., 760, 762, 1066 ,1080 B OSS, 28, 1007, 1008, 1014 Bosse, M., 1012, 1066 Botea, A., 395, 1075 Bottou, L., 762, 967, 1080 boundary set, 774 bounded optimality (BO), 1050 bounded rationality, 1049 bounds consistent, 212 bounds propagation, 212 Bourlard, H., 604, 1089 Bourzutschky, M., 176, 1066 Boutilier, C., 434, 553, 686, 1066 Bouzy, B., 194, 1066 Bowden, B. V ., 14, 192, 1090 Bower, G. H., 854, 1075 Bowerman, M., 314, 1066 Bowling, M., 687, 1066 ,1091 ,1093 Box, G. E. P., 155, 604, 1066 BOXES , 851 Boyan, J.

Token 23386:
A., 154, 854, 1066 Boyd, S., 155, 1066 Boyden, E., 11, 1074 Boyen, X., 605, 1066 Boyen–Koller algorithm, 605Boyer, R. S., 356, 359, 360, 1066 Boyer–Moore theorem prover, 359, 360 Boyle, J., 360, 1092 Brachman, R. J., 457, 471, 473, 1066, 1067 ,1080 Bradshaw, G. L., 800, 1079 Bradtke, S. J., 157, 685, 854, 855, 1065 , 1067 Brady, J. M., 604, 1084 Brafman, O., 638, 1067 Brafman, R., 638, 1067 Brafman, R. I., 433, 434, 855, 1066, 1067 ,1076 Brahmagupta, 227 brain, 16 computational power, 12computer vs., 12

Token 23387:


Token 23388:
Index 1099 damage, optimal, 737 replacement, 1029–1031, 1043super, 9 in a vat, 1028 brains cause minds, 11Braitenberg, V ., 1013, 1067 branching factor, 80, 783 effective, 103, 111, 169 Bransford, J., 927, 1067 Brants, T., 29, 883, 921, 1067 ,1072 Bratko, I., 112, 359, 793, 1067 Bratman, M. E., 60, 1041, 1067 Braverman, E., 760, 1064 B READTH -FIRST-SEARCH ,82 breadth-ﬁrst search, 81, 81–83, 108, 408 Breese, J. S., 61, 553, 555, 639, 1048, 1067 ,1076 ,1091 Breiman, L., 758, 760, 1067 Brelaz, D., 228, 1067 Brent, R. P., 154, 1067 Bresina, J., 28, 1064 Bresnan, J., 920, 1067 Brewka, G., 472, 1067 Brey, R., 637, 1086 Brickley, D., 469, 1067 bridge (card game), 32, 186, 195 Bridge Baron, 189 Bridle, J. S., 761, 1067 Briggs, R., 468, 1067 brightness, 932 Brill, E., 28, 756, 759, 872, 885, 1065 Brin, D., 881, 885, 1036, 1067 Brin, S., 870, 880, 884, 1067 Bringsjord, S., 30, 1067 Brioschi, F., 553, 1066 Britain, 22, 24Broadbent, D. E., 13, 1067 Broadhead, M., 885, 1065 Broca, P., 10Brock, B., 360, 1076 Brokowski, M., 156, 1092 Brooks, M. J., 968, 1076 Brooks, R. A., 60, 275, 278, 434, 1003, 1012, 1013, 1041, 1067 ,1085 Brouwer, P. S., 854, 1065 Brown, C., 230, 1067 Brown, J. S., 472, 800, 1070 ,1080 Brown, K. C., 637, 1067 Brown, M., 604, 1079 Brown, P. F., 922, 1067 Brownston, L., 358, 1067 Bruce, V ., 968, 1067 Brunelleschi, F., 966 Bruner, J. S., 798, 1067 Brunnstein, K., 1034, 1065 Brunot, A., 762, 967, 1080Bryant, B. D., 435, 1067 Bryce, D., 157, 395, 433, 1067 Bryson, A. E., 22, 761, 1067 Buchanan, B. G., 22, 23, 61, 468, 557, 776, 799, 1067 ,1072 ,1080 Buckley, C., 870, 1089 Buehler, M., 1014, 1067 BUGS, 554, 555 BUILD , 472 Bulﬁn, R., 688, 1086 bunch, 442 Bundy, A., 799, 1091 Bunt, H. C., 470, 1067 Buntine, W., 800, 1083 Burch, N., 194, 678, 687, 1066 ,1088 Burgard, W., 606, 1012–1014, 1067, 1068 ,1072 ,1088 ,1090 Burges, C., 884, 1090 burglar alarm, 511–513Burkhard, H.-D., 1014, 1091 Burns, C., 553, 1083 Buro, M., 175, 186, 1067 Burstein, J., 1022, 1067 Burton, R., 638, 1067 Buss, D. M., 638, 1067 Butler, S., 1042, 1067 Bylander, T., 393, 395, 1067 Byrd, R. H., 760, 1067 C c(step cost), 68 Cabeza, R., 11, 1067 Cabral, J., 469, 1081 caching, 269 Cafarella, M. J., 885, 1065 ,1067 ,1072 Cajal, S., 10cake, eating and having, 380 calculus, 131 calculus of variations, 155Calvanese, D., 471, 1064 ,1067 Cambefort, Y ., 61, 1075 Cambridge, 13camera digital, 930, 943for robots, 973pinhole, 930 stereo, 949, 974 time of ﬂight, 974 video, 929, 963 Cameron-Jones, R. M., 793, 1086 Campbell, M. S., 192, 1067 ,1076 Campbell, W., 637, 1068 candidate elimination, 773 can machines think?, 1021 Canny, J., 967, 1013, 1068Canny edge detection, 755, 967 canonical distribution, 518 canonical form, 80 Cantor, C. R., 553, 1093 Cantu-Paz, E., 155, 1085 Capek, K., 1011, 1037 Capen, E., 637, 1068 Caprara, A., 395, 1068 Carbone, R., 279, 1064 Carbonell, J. G., 27, 432, 799, 1068 , 1075 ,1091 Carbonell, J. R., 799, 1068 Cardano, G., 9, 194, 503, 1068 card games, 183Carin, L., 686, 1077 Carlin, J.

Token 23389:
B., 827, 1073 Carlson, A., 288, 1082 C ARMEL , 1013 Carnap, R., 6, 490, 491, 504, 505, 555, 1068 Carnegie Mellon University, 17, 18 Carpenter, M., 432, 1070 Carreras, X., 920, 1079 Carroll, S., 155, 1068 Carson, D., 359, 1092 cart–pole problem, 851 Casati, R., 470, 1068 cascaded ﬁnite-state transducers, 875 case-based reasoning, 799case agreement, 900 case folding, 870 case statement (in condition plans), 136 Cash, S. S., 288, 1087 Cassandra, A. R., 686, 1068 ,1077 Cassandras, C. G., 60, 1068 Casteran, P., 359, 1066 Castro, R., 553, 1068 categorization, 865 category, 440, 440–445, 453 causal network, seeBayesian network causal probability, 496 causal rule, 317, 517 causation, 246, 498 caveman, 778Cazenave, T., 194, 1066 CCD (charge-coupled device), 930, 969 cell decomposition, 986, 989 exact, 990 cell layout, 74 center (in mechanism design), 679 central limit theorem, 1058 cerebral cortex, 11 certainty effect, 620 certainty equivalent, 618 certainty factor, 23, 548, 557 Cesa-Bianchi, N., 761, 1068

Token 23390:


Token 23391:
1100 Index Cesta, A., 28, 1068 CGP, 433C HAFF , 277 Chaﬁn, B., 28, 1064 chain rule (for differentiation), 726 chain rule (for probabilities), 514 Chakrabarti, P. P., 112, 157, 1068, 1069 Chambers, R. A., 851, 854, 1082 chance node (decision network), 626 chance node (game tree), 177 chance of winning, 172 Chandra, A. K., 358, 1068 Chang, C.-L., 360, 1068 Chang, K.-M., 288, 1082 Chang, K. C., 554, 1073 channel routing, 74Chapman, D., 394, 434, 1064 ,1068 Chapman, N., 109 characters, 861 Charest, L., 28, 1064 charge-coupled device, 930, 969 Charniak, E., 2, 23, 358, 556, 557, 604, 920, 921, 1068 chart parser, 893, 919 Chase, A., 28, 1064 chatbot, 1021 Chater, N., 638, 1068 ,1084 Chatﬁeld, C., 604, 1068 Chatila, R., 1012, 1083 Chauvin, Y ., 604, 1065 checkers, 18, 61, 186, 193, 850 checkmate accidental, 182 guaranteed, 181 probabilistic, 181 Cheeseman, P., 9, 26, 229, 277, 557, 826, 1012, 1068 ,1089 Chekaluk, R., 1012, 1070 chemistry, 22 Chen, R., 605, 1080 Chen, S. F., 883, 1068 Chen, X., 395, 1091 Cheng, J., 554, 826, 1068 Cheng, J.-F., 555, 1082 Chervonenkis, A. Y ., 759, 1091 chess, 172–173, 185–186 automaton, 190 history, 192prediction, 21 Chess, D. M., 60, 1078 C HESS 4.5, 110 χ2pruning, 706 Chickering, D. M., 191, 826, 1075 ,1079 Chien, S., 431, 1073 CHILD -NODE,79 CHILL , 902chimpanzee, 860 Chinese room, 1031–1033 CHINOOK , 186, 193, 194 Chklovski, T., 439, 1068 choice point, 340 Chomsky, C., 920, 1074 Chomsky, N., 13, 16, 883, 889, 919, 921, 923, 1068 Chomsky Normal Form, 893, 919 Chopra, S., 762, 1086 Choset, H., 1013, 1014, 1068 Choueiry, B. Y ., 228, 1086 Christmas, 1026chronicles, 470chronological backtracking, 218 cHUGIN, 554 Chung, K. L., 1059, 1068 Chung, S., 278, 1092 chunking, 799 Church, A., 8, 314, 325, 358, 1068 Church, K., 883, 894, 920, 923, 1068 Churchland, P. M., 1042, 1068 Churchland, P. S., 1030, 1042, 1068 Ciancarini, P., 60, 192, 1066 ,1068 C IGOL , 800 Cimatti, A., 396, 433, 1066 ,1068 circuit veriﬁcation, 312 circumscription, 459, 468, 471 prioritized, 459 city block distance, 103 Claessen, K., 360, 1090 clairvoyance, 184Clamp, S. E., 505, 1070 Clapp, R., 637, 1068 Clark, A., 1025, 1041, 1068 Clark, K. L., 472, 1068 Clark, P., 800, 1068 Clark, S., 920, 1012, 1068 ,1071 Clark completion, 472Clarke, A. C., 552, 1034, 1068 Clarke, E., 395, 1068 Clarke, M. R. B., 195, 1068 C LASSIC , 457, 458 classiﬁcation (in description logic), 456 classiﬁcation (in learning), 696 class probability, 764 clause, 253 Clearwater, S. H., 688, 1068 CLINT , 800 Clocksin, W. F., 359, 1068 closed-world assumption, 299, 344, 417, 468, 541 closed class, 890 closed list, seeexplored set CLP, 228, 345 CLP(R), 359clustering, 553, 694, 817, 818 clustering (in Bayesian networks), 529, 529–530 clutter (in data association), 602 CMAC, 855 CMU, 922 CN2, 800 CNF (Conjunctive Normal Form), 253 CNLP, 433 co-NP, 1055 co-NP-complete, 247, 276, 1055 Coarfa, C., 278, 1068 coarticulation, 913, 917 coastal navigation, 994 Coates, A., 857, 1068 Coates, M., 553, 1068 Cobham, A., 8, 1068 Cocke, J., 922, 1067 coercion, 416cognitive architecture, 336 cognitive architecture, 336 cognitive modeling, 3cognitive psychology, 13 cognitive science, 3 Cohen, B., 277, 1088 Cohen, C., 1013, 1069 Cohen, P. R., 25, 30, 434, 1069 Cohen, W. W., 800, 1069 Cohn, A. G., 473, 1069 coin ﬂip, 548, 549, 641 C OLBERT , 1013 Collin, Z., 230, 1069 Collins, A. M., 799, 1068 Collins, F. S., 27, 1069 Collins, M. , 760, 920, 921, 1069 ,1079 , 1093 collusion, 680 Colmerauer, A., 314, 358, 359, 919, 1069 Colombano, S. P., 155, 1080 color, 935color constancy, 935 combinatorial explosion, 22 commitment epistemological, 289, 290, 313, 482 ontological, 289, 313, 482, 547 common sense, 546common value, 679communication, 286, 429, 888 commutativity (in search problems), 214 Compagna, L., 279, 1064 competitive ratio, 148 compilation, 342, 1047 complementary literals, 252 complete-state formulation, 72

Token 23392:


Token 23393:
Index 1101 complete assignment, 203 complete data, 806 completeness of inference, 247of a proof procedure, 242, 274 of resolution, 350–353of a search algorithm, 80, 108 completing the square, 586 completion (of a data base), 344 complexity, 1053–1055 sample, 715 space, 80, 108 time, 80, 108 complexity analysis, 1054 complex phrases, 876complex sentence, 244, 295 complex words, 875compliant motion, 986, 995 component (of mixture distribution), 817 composite decision process, 111composite object, 442 compositionality, 286 compositional semantics, 901 compression, 846computability, 8 computational learning theory, 713, 714, 762 computational linguistics, 16 computer, 13–14 brain vs., 12 computer vision, 3, 12, 20, 228, 929–965 conclusion (of an implication), 244 concurrent action list, 428 condensation, 605Condie, T., 275, 1080 condition–action rule, 633 conditional distributions, 518 conditional effect, 419 conditional Gaussian, 521 conditional independence, 498, 502, 503, 517–523, 551, 574 conditional plan, 660conditional probability, 485, 503, 514 conditional probability table (CPT), 512 conditional random ﬁeld (CRF), 878 conditioning, 492 conditioning case, 512 Condon, J. H., 192, 1069 conﬁguration space, 986, 987 conﬁrmation theory, 6, 505 conﬂict-directed backjumping, 219, 227 conﬂict clause learning, 262conﬂict set, 219conformant planning, 415, 417–421, 431, 433, 994 Congdon, C. B., 1013, 1069 conjugate prior, 811 conjunct, 244 conjunction (logic), 244 conjunctive normal form, 253, 253–254, 275, 345–347 conjunct ordering, 333 Conlisk, J., 638, 1069 connected component, 222 Connect Four, 194 connectionism, 24, 727 connective, logical, 16, 244, 274, 295Connell, J., 1013, 1069 consciousness, 10, 1026, 1029, 1030, 1033, 1033 consequent, 244conservative approximation, 271, 419 consistency, 105, 456, 769 arc,208 of a CSP assignment, 203 of a heuristic, 95 path, 210, 228 consistency condition, 110 C ONSISTENT -DET?,786 consistent estimation, 531 Console, L., 60, 1074 Consortium, T. G. O., 469, 1069 conspiracy number, 191 constant symbol, 292, 294 constraint binary, 206 global, 206, 211 nonlinear, 205 preference constraint, 207 propagation, 208, 214, 217 resource constraint, 212 symmetry-breaking, 226 unary, 206 constraint-based generalization, 799 constraint graph, 203, 223 constraint hypergraph, 206 constraint language, 205 constraint learning, 220, 229 constraint logic programming, 344–345, 359 constraint logic programming (CLP), 228, 345 constraint optimization problem, 207 constraint satisfaction problem (CSP), 20,202, 202–207 constraint weighting, 222 constructive induction, 791 consumable resource, 402 context, 286context-free grammar, 889, 918, 919, 1060 context-sensitive grammar, 889 contingencies, 161 contingency planning, 133, 415, 421–422, 431 continuation, 341 continuity (of preferences), 612 continuous domains, 206 contour (in an image), 948, 953–954contour (of a state space), 97 contraction mapping, 654 contradiction, 250 controller, 59, 997 control theory, 15, 15, 60, 155, 393, 761, 851, 964, 998 adaptive, 833, 854 robust, 836 control uncertainty, 996convention, 429 conversion to normal form, 345–347 convexity, 133 convex optimization, 133, 153 C ONVINCE , 552 convolution, 938 Conway, J. H., 113, 1065 Cook, P. J., 1035, 1072 Cook, S. A., 8, 276, 278, 1059, 1069 Cooper, G., 554, 826, 1069 cooperation, 428 coordinate frame, 956 coordination, 426, 430 coordination game, 670 Copeland, J., 470, 1042, 1069 Copernicus., 1035, 1069 COQ, 227, 359 Cormen, T. H., 1059, 1069 corpus, 861 correlated sampling, 850 Cortellessa, G., 28, 1068 Cortes, C., 760, 762, 967, 1069 ,1080 cotraining, 881, 885count noun, 445 Cournot, A., 687, 1069 Cournot competition, 678 covariance, 1059 covariance matrix, 1058, 1059 Cover, T., 763, 1069 Cowan, J. D., 20, 761, 1069 ,1092 Coward, N., 1022 Cowell, R., 639, 826, 1069 ,1089 Cox, I., 606, 1012, 1069 Cox, R. T., 490, 504, 505, 1069 CPCS, 519, 552 CP LAN, 395 CPSC, ix

Token 23394:


Token 23395:
1102 Index CPT, 512 Craig, J., 1013, 1069 Craik, K. J., 13, 1069 Crammer, K., 761, 1071 Craswell, N., 884, 1069 Crato, N., 229, 1074 Crauser, A., 112, 1069 Craven, M., 885, 1069 Crawford, J. M., 277, 1069 creativity, 16Cremers, A.

Token 23396:
B., 606, 1012, 1067 ,1088 Cresswell, M. J., 470, 1076 CRF, 878 Crick, F. H. C., 130, 1091 Cristianini, N., 760, 1069 critic (in learning), 55 critical path, 403 Crocker, S. D., 192, 1074 Crockett, L., 279, 1069 Croft, B., 884, 1069 Croft, W. B., 884, 885, 1085 Cross, S. E., 29, 1069 C ROSS -VALIDA TION ,710 cross-validation, 708, 737, 759, 767 CROSS -VALIDA TION -WRAPPER ,710 crossover, 128, 153 crossword puzzle, 44, 231 Cruse, D. A., 870, 1069 cryptarithmetic, 206 Csorba, M., 1012, 1071 Cuellar, J., 279, 1064 Culberson, J., 107, 112, 1069 ,1092 culling, 128Cullingford, R. E., 23, 1069 cult of computationalism, 1020 Cummins, D., 638, 1069 cumulative distribution, 564, 623, 1058 cumulative learning, 791, 797cumulative probability density function, 1058 curiosity, 842 Curran, J. R., 920, 1068 current-best-hypothesis, 770, 798 C URRENT -BEST-LEARNING ,771 Currie, K. W., 432, 1073 curse of dimensionality, 739, 760, 989, 997 optimizer’s, 619, 637 winner’s, 637 Cushing, W., 432, 1069 cutoff test, 171 cutset conditioning, 225, 227, 554 cutset, cycle, 225 cutset conditioning, 225, 227, 554 Cybenko, G., 762, 1069CYBER LOVER , 1021 cybernetics, 15,1 5 CYC, 439, 469, 470 cyclic solution, 137 Cyganiak, R., 439, 469, 1066 CYK-P ARSE ,894 CYK algorithm, 893, 919 D D’Ambrosio, B., 553, 1088 d-separation, 517 DAG, 511, 552Daganzo, C., 554, 1069 Dagum, P., 554, 1069 Dahy, S. A., 723, 724, 1078 Dalal, N., 946, 968, 1069 D ALTON , 800 Damerau, F., 884, 1064 Daniels, C. J., 112, 1081 Danish, 907Dantzig, G. B., 155, 1069 D ARKTHOUGHT , 192 DARPA, 29, 922DARPA Grand Challenge, 1007, 1014 Dartmouth workshop, 17, 18 Darwiche, A., 277, 517, 554, 557, 558, 1069 ,1085 Darwin, C., 130, 1035, 1069 Dasgupta, P., 157, 1069 data-driven, 258 data association, 599, 982 database, 299 database semantics, 300, 343, 367, 540 data complexity, 334 data compression, 866 Datalog, 331, 357, 358 data matrix, 721 data mining, 26 data sparsity, 888 dative case, 899 Daun, B., 432, 1070 Davidson, A., 678, 687, 1066 Davidson, D., 470, 1069 Davies, T. R., 784, 799, 1069 Davis, E., 469–473, 1069, 1070 Davis, G., 432, 1070 Davis, K. H., 922, 1070 Davis, M., 260, 276, 350, 358, 1070 Davis, R., 800, 1070 Davis–Putnam algorithm, 260 Dawid, A. P., 553, 639, 826, 1069 , 1080 ,1089 Dayan, P., 763, 854, 855, 1070 ,1083 , 1088 da Vinci, L., 5, 966DBN, 566, 590, 590–599, 603, 604, 646, 664 DB PEDIA , 439, 469 DCG, 898, 919 DDN (dynamic decision network), 664, 685 Deacon, T. W., 25, 1070 dead end, 149 Deale, M., 432, 1070 Dean, J., 29, 921, 1067 Dean, M. E., 279, 1084 Dean, T., 431, 557, 604, 686, 1012, 1013, 1048, 1070 Dearden, R., 686, 855, 1066 ,1070 Debevec, P., 968, 1070 Debreu, G., 625, 1070 debugging, 308 Dechter, R., 110, 111, 228–230, 553, 1069, 1070 ,1076 ,1085 decision rational, 481, 610, 633 sequential, 629, 645 DECISION -LIST-LEARNING ,717 DECISION -TREE-LEARNING ,702 decision analysis, 633 decision boundary, 723 decision list, 715 decision maker, 633 decision network, 510, 610, 626, 626–628, 636, 639, 664 dynamic, 664, 685 evaluation of, 628 decision node, 626 decision stump, 750 decision theory, 9, 26, 483, 636 decision tree, 638, 697, 698 expressiveness, 698 pruning, 705 declarative, 286 declarative bias, 787 declarativism, 236, 275 decomposability (of lotteries), 613 DECOMPOSE ,414 decomposition, 378 DeCoste, D., 760, 762, 1070 Dedekind, R., 313, 1070 deduction, seelogical inference deduction theorem, 249 deductive database, 336, 357, 358 deductive learning, 694 deep belief networks, 1047 DEEPBLUE, ix, 29, 185, 192 DEEPFRITZ , 193 Deep Space One, 60, 392, 432 DEEPTHOUGHT , 192 Deerwester, S. C., 883, 1070

Token 23397:


Token 23398:
Index 1103 default logic, 459, 468, 471 default reasoning, 458–460, 547 default value, 456 de Finetti’s theorem, 490deﬁnite clause, 256, 330–331 deﬁnition (logical), 302 deformable template, 957 degree heuristic, 216, 228, 261 degree of belief, 482, 489 interval-valued, 547 degree of freedom, 975 degree of truth, 289 DeGroot, M. H., 506, 827, 1070 DeJong, G., 799, 884, 1070 delete list, 368 Delgrande, J., 471, 1070 deliberative layer, 1005 Dellaert, F., 195, 1012, 1072 ,1091 Della Pietra, S. A., 922, 1067 Della Pietra, V .

Token 23399:
J., 922, 1067 delta rule, 846 Del Favero, B.

Token 23400:
A., 553, 1088 Del Moral, P., 605, 1070 demodulation, 354, 359, 364 Demopoulos, D., 278, 1068 De Morgan’s rules, 298 De Morgan, A., 227, 313 Dempster, A. P., 557, 604, 826, 1070 Dempster–Shafer theory, 547, 549, 549–550, 557 D ENDRAL , 22, 23, 468 dendrite, 11 Deng, X., 157, 1070 Denis, F., 921, 1070 Denis, M., 28, 1068 Denker, J., 762, 967, 1080 Dennett, D. C., 1024, 1032, 1033, 1042, 1070 Denney, E., 360, 1071 density estimation, 806 nonparametric, 814 DeOliveira, J., 469, 1081 depth-ﬁrst search, 85, 85–87, 108, 408 DEPTH -LIMITED -SEARCH ,88 depth limit, 173 depth of ﬁeld, 932 derivational analogy, 799derived sentences, 242Descartes, R., 6, 966, 1027, 1041, 1071 descendant (in Bayesian networks), 517 Descotte, Y ., 432, 1071 description logic, 454, 456, 456–458, 468, 471 descriptive theory, 619 detachment, 547 detailed balance, 537detection failure (in data association), 602 determination, 784, 799, 801 minimal, 787 deterministic environment, 43 deterministic node, 518 Detwarasiti, A., 639, 1071 Deville, Y ., 228, 1091 D EVISER , 431 Devroye, L., 827, 1071 Dewey Decimal system, 440 de Bruin, A., 191, 1085 de Dombal, F. T., 505, 1070 de Finetti, B., 489, 504, 1070 de Freitas, J. F. G., 605, 1070 de Freitas, N., 605, 1071 de Kleer, J., 229, 358, 472, 1070 ,1072 , 1091 de Marcken, C., 921, 1070 De Morgan, A., 1070 De Raedt, L., 800, 921, 1070 ,1083 de Salvo Braz, R., 556, 1070 de Sarkar, S. C., 112, 157, 1068, 1069 Diaconis, P., 620diagnosis, 481, 496, 497, 909 dental, 481 medical, 23, 505, 517, 548, 629, 1036 diagnostic rule, 317, 517 dialysis, 616 diameter (of a graph), 88 Dias, W., 28, 1064 Dickmanns, E. D., 1014, 1071 dictionary, 21 Dietterich, T., 799, 856, 1064 ,1071 Difference Engine, 14 differential drive, 976 differential equation, 997 stochastic, 567 differential GPS, 975 differentiation, 780diffuse albedo, 934 diffuse reﬂection, 933 Digital Equipment Corporation (DEC), 24, 336 digit recognition, 753–755Dijkstra, E. W., 110, 1021, 1071 Dill, D. L., 279, 1084 Dillenburg, J. F., 111, 1071 Dimopoulos, Y ., 395, 1078 Dinh, H., 111, 1071 Diophantine equations, 227 Diophantus, 227Diorio, C., 604, 1086 DiPasquo, D., 885, 1069 Diplomacy, 166directed acyclic graph (DAG), 511, 552directed arc consistency, 223 direct utility estimation, 853Dirichlet distribution, 811 Dirichlet process, 827 disabilities, 1043disambiguation, 904–912, 919discontinuities, 936 discount factor, 649, 685, 833 discovery system, 800 discrete event, 447 discretization, 131, 519 discriminative model, 878 disjoint sets, 441 disjunct, 244disjunction, 244 disjunctive constraint, 205 disjunctive normal form, 283 disparity, 949 Dissanayake, G., 1012, 1071 distant point light source, 934 distortion, 910 distribute ∨over∧, 254, 347 distributed constraint satisfaction, 230 distribution beta, 592, 811 conditional, nonparametric, 520 cumulative, 564, 623, 1058 mixture, 817 divide-and-conquer, 606 Dix, J., 472, 1067 Dizdarevic, S., 158, 1080 DLV, 472 DNF (disjunctive normal form), 283Do, M. B., 390, 431, 1071 Doctorow, C., 470, 1071 DOF, 975 dolphin, 860domain, 486 continuous, 206 element of, 290 ﬁnite, 205, 344 inﬁnite, 205 in ﬁrst-order logic, 290 in knowledge representation, 300 domain closure, 299, 540 dominance stochastic, 622, 636 strict, 622 dominant strategy, 668, 680 dominant strategy equilibrium, 668 dominated plan (in POMDP), 662 domination (of heuristics), 104 Domingos, P., 505, 556, 826, 1071 Domshlak, C., 395, 434, 1067 ,1076 Donati, A., 28, 1068 Donninger, C., 193, 1071

Token 23401:


Token 23402:
1104 Index Doorenbos, R., 358, 1071 Doran, J., 110, 111, 1071 Dorf, R. C., 60, 1071 Doucet, A., 605, 1070, 1071 Dow, R. J. F., 762, 1088 Dowling, W. F., 277, 1071 Downey, D., 885, 1072 downward reﬁnement property, 410 Dowty, D., 920, 1071 Doyle, J., 60, 229, 471, 472, 638, 1071 , 1082 ,1092 DPLL, 261, 277, 494 DPLL-S AT I SFI AB L E ?,261 Drabble, B., 432, 1071 DRAGON , 922 Draper, D., 433, 1072 Drebbel, C., 15 Dredze, M., 761, 1071 Dreussi, J., 432, 1088 Dreyfus, H. L., 279, 1024, 1049, 1071 Dreyfus, S. E., 109, 110, 685, 1024, 1065 ,1071 Driessens, K., 857, 1090 drilling rights, 629 drone, 1009 dropping conditions, 772 Drucker, H., 762, 967, 1080 Druzdzel, M. J., 554, 1068 DT-A GENT ,484 dual graph, 206 dualism, 6, 1027, 1041 Dubois, D., 557, 1071 Dubois, O., 277, 1089 duck, mechanical, 1011 Duda, R. O., 505, 557, 763, 825, 827, 1071 Dudek, G., 1014, 1071 Duffy, D., 360, 1071 Duffy, K., 760, 1069 Dumais, S. T., 29, 872, 883, 885, 1065 , 1070 ,1087 dung beetle, 39, 61, 424, 1004 Dunham, B., 21, 1072 Dunham, C., 358, 1090 Dunn, H. L., 556, 1071 DuPont, 24duration, 402 D¨urer, A., 966 Durfee, E. H., 434, 1071 Durme, B. V ., 885, 1071 Durrant-Whyte, H., 1012, 1071 ,1080 Dyer, M., 23, 1071 dynamical systems, 603 dynamic backtracking, 229 dynamic Bayesian network (DBN), 566, 590, 590–599, 603, 604, 646,664 dynamic decision network, 664, 685 dynamic environment, 44 dynamic programming, 60, 106, 110, 111, 342, 575, 685 adaptive, 834, 834–835, 853, 858 nonserial, 553 dynamic state, 975 dynamic weighting, 111Dyson, G., 1042, 1071 dystopia, 1052Duzeroski, S., 796, 800, 1071 ,1078 , 1080 E E, 359 E0(English fragment), 890 Earley, J., 920, 1071 early stopping, 706 earthquake, 511Eastlake, D. E., 192, 1074 EBL, 432, 778, 780–784, 798, 799 Ecker, K., 432, 1066 Eckert, J., 14economics, 9–10, 59, 616Edelkamp, S., 111, 112, 395, 1071 , 1079 edge (in an image), 936 edge detection, 936–939Edinburgh, 800, 1012Edmonds, D., 16Edmonds, J., 8, 1071 Edwards, D. J., 191, 1075 Edwards, P., 1042, 1071 Edwards, W., 637, 1091 EEG, 11 Een, N., 277, 1071 effect, 367 missing, 423 negative, 398 effector, 971 efﬁcient auction, 680 Efros, A.

Token 23403:
A., 28, 955, 968, 1075, 1076 Ehrenfeucht, A., 759, 1066 8-puzzle, 70, 102, 105, 109, 113 8-queens problem, 71, 109Einstein, A., 1Eisner, J., 920, 1089 Eitelman, S., 358, 1090 Eiter, T., 472, 1071 Ekart, A., 155, 1086 electric motor, 977 electronic circuits domain, 309–312Elfes, A., 1012, 1083 E LIMINA TION -ASK,528 Elio, R., 638, 1071Elisseeff, A., 759, 1074 ELIZA , 1021, 1035 Elkan, C., 551, 826, 1071 Ellington, C., 1045, 1072 Elliot, G. L., 228, 1075 Elliott, P., 278, 1092 Ellsberg, D., 638, 1071 Ellsberg paradox, 620 Elman, J., 921, 1071 EM algorithm, 571, 816–824 structural, 824 embodied cognition, 1026 emergent behavior, 430, 1002 EMNLP, 923empirical gradient, 132, 849 empirical loss, 712 empiricism, 6, 923 Empson, W., 921, 1071 EMV (expected monetary value), 616Enderton, H. B., 314, 358, 1071 English, 21, 32 fragment, 890 ENIAC, 14ensemble learning, 748, 748–752 entailment, 240, 274 inverse, 795 entailment constraint, 777, 789, 798 entropy, 703 E NUMERATE -ALL,525 ENUMERATION -ASK,525 environment, 34, 40–46 artiﬁcial, 41 class, 45 competitive, 43 continuous, 44 cooperative, 43 deterministic, 43 discrete, 44 dynamic, 44 game-playing, 197, 858 generator, 46 history, 646known, 44 multiagent, 42, 425 nondeterministic, 43 observable, 42 one-shot, 43 partially observable, 42 properties, 42 semidynamic, 44 sequential, 43 single-agent, 42 static, 44 stochastic, 43 taxi, 40uncertain, 43

Token 23404:


Token 23405:
Index 1105 unknown, 44 unobservable, 42 EPAM (Elementary Perceiver And Memorizer), 758 Ephrati, E., 434, 1079 epiphenomenalism, 1030 episodic environment, 43 epistemological commitment, 289, 290, 313, 482 Epstein, R., 30, 1071 EQP, 360 equality, 353 equality (in logic), 299 equality symbol, 299 equilibrium, 183, 668 equivalence (logical), 249 Erdmann, M. A., 156, 1071 ergodic, 537 Ernst, G., 110, 1084 Ernst, H. A., 1012, 1071 Ernst, M., 395, 1071 Erol, K., 432, 1071, 1072 error (of a hypothesis), 708, 714 error function, 1058 error rate, 708 Essig, A., 505, 1074 Etchemendy, J., 280, 314, 1065 ethics, 1034–1040 Etzioni, A., 1036, 1072 Etzioni, O., 61, 433, 439, 469, 881, 885, 1036, 1050, 1065 ,1072 ,1079 , 1091 Euclid, 8, 966 EURISKO , 800 Europe, 24 European Space Agency, 432 evaluation function, 92, 108, 162, 171–173, 845 linear, 107 Evans, T. G., 19, 31, 1072 event, 446–447, 450 atomic, 506 discrete, 447 exogenous, 423 in probability, 484, 522 liquid, 447 event calculus, 446, 447, 470, 903 Everett, B., 1012, 1066 evidence, 485, 802 reversal, 605 evidence variable, 522 evolution, 130 machine, 21 evolutionary psychology, 621 evolution strategies, 155 exceptions, 438, 456exclusive or, 246, 766 execution, 66 execution monitoring, 422, 422–434 executive layer, 1004 exhaustive decomposition, 441 existence uncertainty, 541 existential graph, 454 Existential Instantiation, 323 Existential Introduction, 360 expansion (of states), 75 expectation, 1058 expected monetary value, 616 expected utility, 53, 61, 483, 610, 611, 616 expected value (in a game tree), 172, 178 expectiminimax, 178, 191 complexity of, 179 expert system, 468, 633, 636, 800, 1036 commercial, 336 decision-theoretic, 633–636ﬁrst, 23ﬁrst commercial, 24HPP (Heuristic Programming Project), 23 logical, 546medical, 557Prolog-based, 339 with uncertainty, 26 explaining away, 548 explanation, 462, 781 explanation-based generalization, 187 explanation-based learning (EBL), 432, 778, 780–784, 798, 799 explanatory gap, 1033 exploitation, 839 exploration, 39, 147–154, 831, 839, 855 safe, 149 exploration function, 842, 844 explored set, 77 expressiveness (of a representation scheme), 58 EXTEND -EXAMPLE ,793 extended Kalman ﬁlter (EKF), 589, 982 extension (of a concept), 769 extension (of default theory), 460 extensive form, 674 externalities, 683 extrinsic property, 445 eyes, 928, 932, 966 F fact, 256 factor (in variable elimination), 524 factored frontier, 605factored representation, 58, 64, 202, 367, 486, 664, 694 factoring, 253, 347 Fagin, R., 229, 470, 477, 1065 ,1072 Fahlman, S. E., 20, 472, 1072 failure model, 593 false alarm (in data association), 602 false negative, 770 false positive, 770 family tree, 788 Farrell, R., 358, 1067 FAST DIAGONALLY DOWNWARD , 387 FASTDOWNWARD , 395 FASTFORWARD , 379 FASTUS, 874, 875, 884 Faugeras, O., 968, 1072 Fearing, R. S., 1013, 1072 Featherstone, R., 1013, 1072 feature (in speech), 915 feature (of a state), 107, 172 feature extraction, 929 feature selection, 713, 866 feed-forward network, 729 feedback loop, 548 Feigenbaum, E. A., 22, 23, 468, 758, 1067 ,1072 ,1080 Feiten, W., 1012, 1066 Feldman, J., 639, 1072 Feldman, R., 799, 1089 Fellbaum, C., 921, 1072 Fellegi, I., 556, 1072 Felner, A., 107, 112, 395, 1072 ,1079 , 1092 Felzenszwalb, P., 156, 959, 1072 Feng, C., 800, 1083 Feng, L., 1012, 1066 Fergus, R., 741, 1090 Ferguson, T., 192, 827, 1072 Fermat, P., 9, 504 Ferraris, P., 433, 1072 Ferriss, T., 1035, 1072 FF, 379, 387, 392, 395 15-puzzle, 109 Fifth Generation project, 24ﬁgure of speech, 905, 906 Fikes, R. E., 60, 156, 314, 367, 393, 432, 434, 471, 799, 1012, 1067 , 1072 ﬁltering, 145, 460, 571–573, 603, 659, 823, 856, 978, 1045 assumed-density, 605 Fine, S., 604, 1072 ﬁnite-domain, 205, 344 ﬁnite-state automata, 874, 889 Finkelstein, L., 230, 1067 Finney, D. J., 554, 1072

Token 23406:


Token 23407:
1106 Index Firby, R. J., 431, 1070 ﬁrst-order logic, 285, 285–321 ﬁrst-order proba bilistic l ogic, 539–546 Firth, J., 923, 1072 Fischer, B., 360, 1071 Fischetti, M., 395, 1068 Fisher, R. A., 504, 1072 ﬁtness (in genetic algorithms), 127 ﬁtness landscape, 155Fix, E., 760, 1072 ﬁxation, 950 F IXED -LAG-SMOOTHING ,580 ﬁxed-lag smoothing, 576 ﬁxed point, 258, 331 Flannery, B. P., 155, 1086 ﬂaw, 390 Floreano, D., 1045, 1072 ﬂuent, 266, 275, 388, 449–450 ﬂy eyes, 948, 963FMP, seeplanning, ﬁne-motion fMRI, 11, 288 focal plane, 932 F OCUS , 799 focus of expansion, 948 Fogel, D. B., 156, 1072 Fogel, L. J., 156, 1072 FOIL,793 FOL-BC-A SK,338 FOL-FC-A SK,332 folk psychology, 473 Foo, N., 279, 1072 FOPC, seelogic, ﬁrst-order Forbes, J., 855, 1072 FORBIN , 431, 432 Forbus, K. D., 358, 472, 1072 force sensor, 975 Ford, K. M., 30, 1072 foreshortening, 952 Forestier, J.-P., 856, 1072 Forgy, C., 358, 1072 formulate, search, execute, 66Forrest, S., 155, 1082 Forsyth, D., 960, 968, 1072 ,1086 Fortmann, T. E., 604, 606, 1065 forward–backward, 575, 822 F ORWARD -BACKWARD ,576 forward chaining, 257, 257–259, 275, 277, 330–337, 358 forward checking, 217, 217–218 forward pruning, 174 forward search for planning, 373–374four-color map problem, 227, 1023 Fourier, J., 227, 1072 Fowlkes, C., 941, 967, 1081 Fox, C., 638, 1072Fox, D., 606, 1012, 1014, 1067 ,1072 , 1088 ,1090 Fox, M. S., 395, 432, 1072 frame in representation, 24, 471 in speech, 915 problem inferential, 267, 279 frame problem, 266, 279 inferential, 447 representational, 267 framing effect, 621 Franco, J., 277, 1072 Frank, E., 763, 1092 Frank, I., 191, 1072 Frank, M., 231, 1073 Frank, R. H., 1035, 1072 Frankenstein, 1037Franz, A., 883, 921, 1072 Fratini, S., 28, 1068 F REDDY , 74, 156, 1012 Fredkin Prize, 192 Freeman, W., 555, 1091, 1092 free space, 988 free will, 6 Frege, G., 8, 276, 313, 357, 1072 Freitag, D., 877, 885, 1069 ,1072 frequentism, 491 Freuder, E. C., 228–230, 1072 ,1087 Freund, Y ., 760, 1072 Friedberg, R. M., 21, 156, 1072 Friedgut, E., 278, 1073 Friedman, G. J., 155, 1073 Friedman, J., 758, 761, 763, 827, 1067 , 1073 ,1075 Friedman, N., 553, 558, 605, 826, 827, 855, 1066 ,1070 ,1073 ,1078 Friendly AI, 27, 1039 Fristedt, B., 855, 1065 frontier, 75 Frost, D., 230, 1070 Fruhwirth, T., 230, 1073 FRUMP , 884 Fuchs, J. J., 432, 1073 Fudenberg, D., 688, 1073 Fukunaga, A. S., 431, 1073 fully observable, 658 function, 288 total, 291 functional dependency, 784, 799 functionalism, 60, 1029 , 1030, 1041, 1042 function approximation, 845, 847 function symbol, 292, 294 Fung, R., 554, 1073 Furnas, G. W., 883, 1070Furst, M., 395, 1066 futility pruning, 185 fuzzy control, 550 fuzzy logic, 240, 289, 547, 550, 557fuzzy set, 550, 557 G g(path cost), 78 G-set, 774 G¨odel number, 352 Gabor, Z.

Token 23408:
Z., 640Gaddum, J. H., 554, 1073 Gaifman, H., 555, 1073 gain parameter, 998 gain ratio, 707, 765 gait, 1001 Gale, W. A., 883, 1068 Galileo, G., 1, 56, 796Gallaire, H., 358, 1073 Gallier, J. H., 277, 314, 1071 ,1073 Gamba, A., 761, 1073 Gamba perceptrons, 761Gamberini, L., 761, 1073 gambling, 9, 613game, 9, 161 of chance, 177–180dice, 183Go, 186, 194 of imperfect information, 162 inspection game, 666 multiplayer, 165–167Othello, 186partially observable, 180–184of perfect information, 161 poker, 507pursuit–evasion, 196 repeated, 669, 673 robot (with humans), 1019Scrabble, 187, 195 zero-sum, 161, 162, 199, 670 game playing, 161–162, 190 game programs, 185–187 G AMER , 387 game show, 616game theory, 9, 161, 645, 666, 666–678, 685 combinatorial, 186 game tree, 162 Gamma function, 828 Garding, J., 968, 1073 Gardner, M., 276, 1073 Garey, M. R., 1059, 1073 Garg, A., 604, 1084 G ARI, 432 Garofalakis, M., 275, 1080

Token 23409:


Token 23410:
Index 1107 Garrett, C., 128, 1065 Gaschnig’s heuristic, 119 Gaschnig, J., 111, 119, 228, 229, 557, 1071 ,1073 Gasquet, A., 432, 1073 Gasser, R., 112, 194, 1073 Gat, E., 1013, 1073 gate (logic), 309Gauss, C. F., 227, 603, 759, 1073 Gauss, K. F., 109 Gaussian distribution, 1058 multivariate, 584, 1058 Gaussian error model, 592 Gaussian ﬁlter, 938 Gaussian process, 827 Gawande, A., 1036, 1073 Gawron, J. M., 922, 1078 Gay, D. E., 275, 1080 Gearhart, C., 686, 1074 Gee, A. H., 605, 1070 Geffner, H., 156, 394, 395, 431, 433, 1066 ,1075 ,1084 Geiger, D., 553, 826, 1073 ,1075 Geisel, T., 864, 1073 Gelatt, C. D., 155, 229, 1078 Gelb, A., 604, 1073 Gelder, A. V ., 360, 1090 Gelernter, H., 18, 359, 1073 Gelfond, M., 359, 472, 1073 Gelly, S., 194, 1073 ,1091 Gelman, A., 827, 1073 Geman, D., 554, 967, 1073 Geman, S., 554, 967, 1073 generality, 783 generalization, 770, 772 generalization hierarchy, 776 generalization loss, 711 generalized arc consistent, 210 generalized cylinder, 967 general ontology, 453 General Problem Solver, 3, 7, 18, 393 generation (of states), 75 generative capacity, 889 generator, 337 Genesereth, M. R., 59, 60, 156, 195, 314, 345, 350, 359, 363, 1019, 1073 ,1080 ,1089 G ENETIC -ALGORITHM ,129 genetic algorithm, 21, 126–129, 153, 155–156, 841 genetic programming, 155 Gent, I., 230, 1073 Gentner, D., 314, 799, 1073 Geometry Theorem Prover, 18 Georgeson, M., 968, 1067 Gerbault, F., 826, 1074Gerevini, A., 394, 395, 1073 Gershwin, G., 917, 1073 Gestalt school, 966 Getoor, L., 556, 1073 Ghahramani, Z., 554, 605, 606, 827, 1073 ,1077 ,1087 Ghallab, M., 372, 386, 394–396, 431, 1073 Ghose, S., 112, 1068 GIB, 187, 195 Gibbs, R. W., 921, 1073 GIBBS -ASK,537 Gibbs sampling, 536, 538, 554 Gibson, J. J., 967, 968, 1073 Gil, Y ., 439, 1068 Gilks, W. R., 554, 555, 826, 1073 Gilmore, P. C., 358, 1073 Ginsberg, M. L., 187, 195, 229, 231, 359, 363, 557, 1069 ,1073 ,1089 Gionis, A., 760, 1073 Gittins, J. C., 841, 855, 1074 Gittins index, 841, 855 Giunchiglia, E., 433, 1072 Givan, R., 857, 1090 Glanc, A., 1011, 1074 Glass, J., 604, 1080 GLAUBER , 800 Glavieux, A., 555, 1065 GLIE, 840 global constraint, 206, 211 Global Positioning System (GPS), 974 Glover, F., 154, 1074 Glymour, C., 314, 826, 1074 ,1089 Go (game), 186, 194 goal, 52, 64, 65, 108, 369 based agent, 52–53, 59, 60 formulation of, 65 goal-based agent, 52–53, 59 goal-directed reasoning, 259 inferential, 301 serializable, 392 goal clauses, 256 goal monitoring, 423 goal predicate, 698 goal test, 67, 108 God, existence of, 504 G¨odel, K., 8, 276, 358, 1022, 1074 Goebel, J., 826, 1074 Goebel, R., 2, 59, 1085 Goel, A., 682, 1064 Goertzel, B., 27, 1074 GOFAI, 1024, 1041gold, 237 Gold, B., 922, 1074 Gold, E. M., 759, 921, 1074 Goldbach’s conjecture, 800Goldberg, A. V ., 111, 1074 Goldberg, D. E., 155, 1085 Goldberg, K., 156, 1092 Goldin-Meadow, S., 314, 1073 Goldman, R., 156, 433, 555, 556, 921, 1068 ,1074 ,1091 gold standard, 634 Goldszmidt, M., 553, 557, 686, 826, 1066 ,1073, 1074 G OLEM , 800 Golgi, C., 10 Golomb, S., 228, 1074 Golub, G., 759, 1074 Gomard, C. K., 799, 1077 Gomes, C., 154, 229, 277, 1074 Gonthier, G., 227, 1074 Good, I. J., 491, 552, 1037, 1042, 1074 Good–Turing smoothing, 883good and evil, 637 Gooday, J. M., 473, 1069 Goodman, D., 29, 1074 Goodman, J., 29, 883, 1068 ,1074 Goodman, N., 470, 798, 1074 ,1080 Goodnow, J. J., 798, 1067 good old-fashioned AI (GOFAI), 1024, 1041 Google, 870, 883, 889, 922 Google Translate, 907 Gopnik, A., 314, 1074 Gordon, D. M., 429, 1074 Gordon, G., 605, 686, 1013, 1085 ,1087 , 1091 Gordon, M. J., 314, 1074 Gordon, N., 187, 195, 605, 1071 ,1074 Gorry, G. A., 505, 1074 Gottlob, G., 230, 1074 Gotts, N., 473, 1069 GP-CSP, 390 GPS (General Problem Solver), 3, 7, 18, 393 GPS (Global Positioning System), 974 graceful degradation, 666 gradient, 131 empirical, 132, 849 gradient descent, 125, 719 batch, 720 stochastic, 720 Graham, S. L., 920, 1074 Grama, A., 112, 1074 grammar, 860, 890, 1060 attribute, 919 augmented, 897 categorial, 920context-free, 889, 918, 919, 1060 lexicalized, 897 probabilistic, 890, 888–897, 919

Token 23411:


Token 23412:
1108 Index context-sensitive, 889 deﬁnite clause (DCG), 898, 919 dependency, 920 English, 890–892 induction of, 921lexical-functional (LFG), 920 phrase structure, 918 probabilistic, 897recursively enumerable, 889regular, 889 grammatical formalism, 889 Grand Prix, 185graph, 67 coloring, 227 Eulerian, 157 G RAPH -SEARCH ,77 graphical model, 510, 558 GRAPHPLAN , 379, 383, 392, 394–396, 402, 433 grasping, 1013 Grassmann, H., 313, 1074 Gravano, L., 885, 1064 Grayson, C. J., 617, 1074 Greece, 275, 468, 470greedy search, 92Green, B., 920, 1074 Green, C., 19, 314, 356, 358, 1074 Green, P., 968, 1067 Greenbaum, S., 920, 1086 Greenblatt, R. D., 192, 1074 Greenspan, M., 195, 1079 Grefenstette, G., 27, 1078 Greiner, R., 799, 826, 1068 ,1074 Grenager, T., 857, 1088 grid, rectangular, 77 Grifﬁths, T., 314, 1090 Grinstead, C., 506, 1074 GRL, 1013 Grosof, B., 799, 1087 Grosz, B. J., 682, 688, 1076 grounding, 243 ground resolution theorem, 255, 350 ground term, 295, 323 Grove, A., 505, 638, 1064, 1065 Grove, W., 1022, 1074 Gruber, T., 439, 470, 1074 grue, 798 Grumberg, O., 395, 1068 GSAT, 277 Gu, J., 229, 277, 1074 ,1089 Guard, J., 360, 1074 Guestrin, C., 639, 686, 856, 857, 1074 , 1079 ,1081 Guha, R. V ., 439, 469, 1067 ,1080 Guibas, L. J., 1013, 1074 Gumperz, J., 314, 1074Gupta, A., 639, 1079 G US, 884 Gutfreund, H., 761, 1064 Guthrie, F., 227Guugu Yimithirr, 287Guy, R. K., 113, 1065 Guyon, I., 759, 760, 762, 967, 1066 , 1074 ,1080 H H (entropy), 704 h(heuristic function), 92 hMAP (MAP hypothesis), 804 hML(ML hypothesis), 805 HACKER , 394 Hacking, I., 506, 1074 Haghighi, A., 896, 920, 1074 Hahn, M., 760, 1069 H¨ahnel, D., 1012, 1067 Haimes, M., 556, 1082 Haken, W., 227, 1064 HAL 9000 computer, 552 Hald, A., 506, 1074 Halevy, A., 28, 358, 470, 759, 885, 1067 ,1074 Halgren, E., 288, 1087 Halpern, J. Y ., 314, 470, 477, 505, 555, 1065 ,1072 ,1074 Halpin, M. P., 231, 1073 halting problem, 325ham, 865Hamm, F., 470, 1091 Hamming, R. W., 506, 1074 Hamming distance, 738 Hammond, K., 432, 1074 Hamori, S., 604, 1066 ham sandwich, 906 Hamscher, W., 60, 1074 Han, X., 11, 1074 Hanan, S., 395, 1072 Hand, D., 763, 1074 hand–eye machine, 1012 Handschin, J. E., 605, 1075 handwritten digit recognition, 753–755Hanks, S., 433, 1072 Hanna, F. K., 800, 1087 Hansard, 911 Hansen, E., 112, 156, 422, 433, 686, 1075 ,1093 Hansen, M. O., 228, 1064 Hansen, P., 277, 1075 Hanski, I., 61, 1075 Hansson, O., 112, 119, 1075 happy graph, 703haptics, 1013 Harabagiu, S. M., 885, 1085Harada, D., 856, 1084 Haralick, R. M., 228, 1075 Hardin, G., 688, 1075 Hardy, G. H., 1035, 1075 Harel, D., 358, 1068 Harman, G. H., 1041, 1075 H ARPY , 154, 922 Harris, Z., 883, 1075 Harrison, J. R., 637, 1075 Harrison, M. A., 920, 1074 Harsanyi, J., 687, 1075 Harshman, R. A., 883, 1070 Hart, P. E., 110, 156, 191, 432, 434, 505, 557, 763, 799, 825, 827,1071, 1072 ,1075 Hart, T. P., 191, 1075 Hartley, H., 826, 1075 Hartley, R., 968, 1075 Harvard, 621 Haslum, P., 394, 395, 431, 1075 Hastie, T., 760, 761, 763, 827, 1073 , 1075 Haugeland, J., 2, 30, 1024, 1042, 1075 Hauk, T., 191, 1075 Haussler, D., 604, 759, 762, 800, 1065, 1066 ,1075 ,1079 Havelund, K., 356, 1075 Havenstein, H., 28, 1075 Hawkins, J., 1047, 1075 Hayes, P. J., 30, 279, 469–472, 1072 , 1075 ,1082 Haykin, S., 763, 1075 Hays, J., 28, 1075 head, 897 head (of Horn clause), 256 Hearst, M. A., 879, 881, 883, 884, 922, 1075 ,1084 ,1087 Heath, M., 759, 1074 Heath Robinson, 14 heavy-tailed distribution, 154 Heawood, P., 1023 Hebb, D. O., 16, 20, 854, 1075 Hebbian learning, 16 Hebert, M., 955, 968, 1076 Heckerman, D., 26, 29, 548, 552, 553, 557, 605, 634, 640, 826, 1067 , 1074–1076 ,1087–1089 hedonic calculus, 637 Heidegger, M., 1041, 1075 Heinz, E. A., 192, 1075 Held, M., 112, 1075 Hellerstein, J. M., 275, 1080 Helmert, M., 111, 395, 396, 1075 Helmholtz, H., 12 Hempel, C., 6 Henderson, T. C., 210, 228, 1082

Token 23413:


Token 23414:
Index 1109 Hendler, J., 27, 396, 432, 469, 1064, 1065 ,1071, 1072 ,1075 ,1089 Henrion, M., 61, 519, 552, 554, 639, 1075, 1076 ,1086 Henzinger, M., 884, 1088 Henzinger, T. A., 60, 1075 Hephaistos, 1011 Herbrand’s theorem, 351, 358 Herbrand, J., 276, 324, 351, 357, 358, 1075 Herbrand base, 351 Herbrand universe, 351, 358 Hernadvolgyi, I., 112, 1076 Herskovits, E., 826, 1069 Hessian, 132 Heule, M., 278, 1066 heuristic, 108 admissible, 94, 376 composite, 106 degree, 216, 228, 261 for planning, 376–379 function, 92, 102–107 least-constraining-value, 217 level sum, 382 Manhattan, 103 max-level, 382 min-conﬂicts, 220 minimum-remaining-values, 216, 228, 333, 405 minimum remaining values, 216, 228, 333, 405 null move, 185 search, 81, 110 set-level, 382 straight-line, 92 heuristic path algorithm, 118 Heuristic Programming Project (HPP), 23 Hewitt, C., 358, 1075 hexapod robot, 1001 hidden Markov model factorial, 605 hidden Markov model (HMM), 25, 566, 578, 578–583, 590, 603, 604, 822–823 hidden Markov model (HMM) (HMM), 578, 590, 876, 922 hidden unit, 729 hidden variable, 522, 816 HIERARCHICAL -SEARCH ,409 hierarchical decomposition, 406 hierarchical lookahead, 415 hierarchical reinforcement learning, 856, 1046 hierarchical structure, 1046hierarchical task network (HTN), 406, 431 Hierholzer, C., 157, 1075 higher-order logic, 289 high level action, 406 Hilgard, E. R., 854, 1075 HILL-CLIMBING ,122 hill climbing, 122, 153, 158 ﬁrst-choice, 124 random-restart, 124 stochastic, 124 Hingorani, S. L., 606, 1069 Hinrichs, T., 195, 1080 Hintikka, J., 470, 1075 Hinton, G. E., 155, 761, 763, 1047, 1075 ,1087 Hirsch, E. A., 277, 1064 Hirsh, H., 799, 1075 Hitachi, 408hit list, 869 HITS, 871, 872 HMM, 578, 590, 876, 922Ho, Y .-C., 22, 761, 1067 Hoane, A. J., 192, 1067 Hobbes, T., 5, 6 Hobbs, J. R., 473, 884, 921, 1075, 1076 Hodges, J. L., 760, 1072 Hoff, M. E., 20, 833, 854, 1092 Hoffmann, J., 378, 379, 395, 433, 1076 , 1078 Hogan, N., 1013, 1076 HOG feature, 947 Hoiem, D., 955, 968, 1076 holdout cross-validation, 708 holistic context, 1024 Holland, J. H., 155, 1076 ,1082 Hollerbach, J. M., 1013, 1072 holonomic, 976 Holte, R., 107, 112, 678, 687, 1066 , 1072 ,1076 ,1092 Holzmann, G. J., 356, 1076 homeostatic, 15 homophones, 913 Homo sapiens ,1 ,8 6 0 Hon, H., 922, 1076 Honavar, V ., 921, 1084 Hong, J., 799, 1082 Hood, A., 10, 1076 Hooker, J., 230, 1076 Hoos, H., 229, 1076 Hopcroft, J., 1012, 1059, 1064 ,1088 Hope, J., 886, 1076 Hopﬁeld, J. J., 762, 1076 Hopﬁeld network, 762 Hopkins Beast, 1011 horizon (in an image), 931horizon (in MDPs), 648 horizon effect, 174 Horn, A., 276, 1076 Horn, B. K. P., 968, 1076 Horn, K. V ., 505, 1076 Horn clause, 256, 791 Horn form, 275, 276 Horning, J. J., 1076 Horowitz, E., 110, 1076 Horowitz, M., 279, 1084 Horrocks, J. C., 505, 1070 horse, 1028Horswill, I., 1013, 1076 Horvitz, E. J., 26, 29, 61, 553, 604, 639, 1048, 1076 ,1084 ,1087 Hovel, D., 553, 1076 Howard, R. A., 626, 637–639, 685, 1076 ,1082 Howe, A., 394, 1073 Howe, D., 360, 1076 HSCP, 433 HSP, 387, 395HSP R, 395 Hsu, F.-H., 192, 1067 ,1076 Hsu, J., 28, 1064 HTML, 463, 875 HTN, 406, 431 HTN planning, 856 Hu, J., 687, 857, 1076 Huang, K.-C., 228, 1086 Huang, T., 556, 604, 1076 Huang, X. D., 922, 1076 hub, 872 Hubble Space Telescope, 206, 221, 432Hubel, D. H., 968, 1076 Huber, M., 1013, 1069 Hubs and Authorities, 872 Huddleston, R. D., 920, 1076 Huet, G., 359, 1066 Huffman, D. A., 20, 1076 Huffman, S., 1013, 1069 Hughes, B. D., 151, 1076 Hughes, G. E., 470, 1076 H UGIN , 553, 604 Huhns, M. N., 61, 1076 human-level AI, 27, 1034 human judgment, 546, 557, 619 humanoid robot, 972 human performance, 1 human preference, 649 Hume, D., 6, 1076 Humphrys, M., 1021, 1076 Hungarian algorithm, 601 Hunkapiller, T., 604, 1065 Hunsberger, L., 682, 688, 1076 Hunt, W., 360, 1076

Token 23415:


Token 23416:
1110 Index Hunter, L., 826, 1076 Hurst, M., 885, 1076 Hurwicz, L., 688, 1076 Husmeier, D., 605, 1076 Hussein, A. I., 723, 724, 1078 Hutchinson, S., 1013, 1014, 1068 Huth, M., 314, 1076 Huttenlocher, D., 959, 967, 1072 ,1076 Huygens, C., 504, 687, 1076 Huyn, N., 111, 1076 Hwa, R., 920, 1076 Hwang, C. H., 469, 1076 HYBRID -WUMPUS -AGENT ,270 hybrid A*, 991 hybrid architecture, 1003 , 1047 HYDRA , 185, 193 hyperparameter, 811 hypertree width, 230hypothesis, 695 approximately correct, 714consistent, 696 null, 705 prior, 803, 810 hypothesis prior, 803, 810 hypothesis space, 696, 769 Hyun, S., 1012, 1070 I i.i.d.

Token 23417:
(independent and identically distributed), 708, 803 Iagnemma, K., 1014, 1067 IBAL, 556 IBM, 18, 19, 29, 185, 193, 922IBM 704 computer, 193ice cream, 483 ID3, 800 IDA* search, 99, 111 identiﬁcation in the limit, 759identity matrix ( I),1056 identity uncertainty, 541, 876 idiot Bayes, 499IEEE, 469ignorance, 547, 549 practical, 481 theoretical, 481 ignore delete lists, 377 ignore preconditions heuristic, 376 Iida, H., 192, 1087 IJCAI (International Joint Conference on AI), 31 ILOG, 359 ILP, 779, 800 image, 929 formation, 929–935, 965processing, 965segmentation, 941–942imperfect information, 190, 666 implementation (of a high-level action), 407 implementation level, 236 implication, 244 implicative normal form, 282, 345 importance sampling, 532, 554 incentive, 426 incentive compatible, 680 inclusion–exclusion principle, 489 incompleteness, 342 theorem, 8, 352, 1022 inconsistent support, 381incremental formulation, 72 incremental learning, 773, 777 independence, 494, 494–495, 498, 503 absolute, 494 conditional, 498, 502, 503, 517–523, 551, 574 context-speciﬁc, 542, 563 marginal, 494 independent subproblems, 222 index, 869 indexical, 904 indexing, 328, 327–329 India, 16, 227, 468 indicator variable, 819 indifference, principle of, 491, 504 individual (in genetic algorithms), 127 individuation, 445 induced width, 229 induction, 6 constructive, 791 mathematical, 8 inductive learning, 694, 695–697 inductive logic programming (ILP), 779, 800 Indyk, P., 760, 1064 ,1073 inference, 208, 235 probabilistic, 490, 490–494, 510 inference procedure, 308 inference rule, 250, 275 inferential equivalence, 323 inferential frame problem, 267, 279 inﬁnite horizon problems, 685 inﬂuence diagram, 552, 610, 626 INFORMATION -GAT HE R I NG -AGENT , 632 information extraction, 873, 873–876, 883 information gain, 704, 705 information gathering, 39, 994 information retrieval (IR), 464, 867, 867–872, 883, 884 information sets, 675 information theory, 703–704, 758information value, 629, 639 informed search, 64, 81, 92, 92–102, 108 infuence diagram, 510, 610, 626, 626–628, 636, 639, 664 Ingerman, P. Z., 919, 1076 Ingham, M., 278, 1092 inheritance, 440, 454, 478 multiple, 455 initial state, 66, 108, 162, 369 Inoue, K., 795, 1076 input resolution, 356 inside–outside algorithm, 896 instance (of a schema), 128 instance-based learning, 737, 737–739, 855 insufﬁcient reason, principle of, 504 insurance premium, 618 intelligence, 1,3 4 intelligent back tracking, 218–220, 262 intentionality, 1026, 1042 intentional state, 1028 intercausal reasoning, 548 interior-point method, 155 interleaving, 147interleaving (actions), 394 interleaving (search and action), 136 interlingua, 908 internal state, 50 Internet search, 464 Internet shopping, 462–467 interpolation smoothing, 883interpretation, 292, 313 extended, 313 intended, 292 pragmatic, 904 interreﬂections, 934, 953 interval, 448Intille, S., 604, 1077 intractability, 21 intrinsic property, 445 introspection, 3, 12 intuition pump, 1032 inverse (of a matrix), 1056 inverse entailment, 795 inverse game theory, 679 inverse kinematics, 987 inverse reinforcement learning, 857 inverse resolution, 794, 794–797, 800 inverted pendulum, 851 inverted spectrum, 1033 Inza, I., 158, 1080 IPL, 17 IPP, 387, 395 IQ test, 19, 31IR, 464, 867, 867–872, 883, 884

Token 23418:


Token 23419:
Index 1111 irrationality, 2, 613 irreversible, 149 IS-A links, 471Isard, M., 605, 1077 ISBN, 374, 541 I SIS, 432 Israel, D., 884, 1075 ITEP, 192 ITEP chess program, 192 ITERA TIVE -DEEPENING -SEARCH ,89 iterative deepening search, 88, 88–90, 108, 110, 173, 408 iterative expansion, 111 iterative lengthening search, 117 ITOU, 800 Itsykson, D., 277, 1064 Iwama, K., 277, 1077 Iwasawa, S., 1041, 1085 IXTET, 395 J Jaakkola, T., 555, 606, 855, 1077 ,1088 JACK, 195 Jackel, L., 762, 967, 1080 Jackson, F., 1042, 1077 Jacobi, C. G., 606 Jacquard, J., 14 Jacquard loom, 14Jaffar, J., 359, 1077 Jaguar, 431Jain, A., 885, 1085 James, W., 13janitorial science, 37Japan, 24 Jasra, A., 605, 1070 Jaumard, B., 277, 1075 Jaynes, E. T., 490, 504, 505, 1077 Jeavons, P., 230, 1085 Jefferson, G., 1026, 1077 Jeffrey, R. C., 504, 637, 1077 Jeffreys, H., 883, 1077 Jelinek, F., 883, 922, 923, 1067 ,1077 Jenkin, M., 1014, 1071 Jenkins, G., 604, 1066 Jennings, H. S., 12, 1077 Jenniskens, P., 422, 1077 Jensen, F., 552, 553, 1064 Jensen, F. V ., 552, 553, 558, 1064 ,1077 Jevons, W. S., 276, 799, 1077 Ji, S., 686, 1077 Jimenez, P., 156, 433, 1077 Jitnah, N., 687, 1079 Joachims, T., 760, 884, 1077 job,402 job-shop scheduling problem, 402Johanson, M., 687, 1066 ,1093 Johnson, C. R., 61, 1067 Johnson, D. S., 1059, 1073 Johnson, M., 920, 921, 927, 1041, 1067, 1068 ,1071 ,1079 Johnson, W. W., 109, 1077 Johnston, M. D., 154, 229, 432, 1077 , 1082 joint action, 427 joint probability distribution, 487 full, 488, 503, 510, 513–517 join tree, 529 Jones, M., 968, 1025, 1091 Jones, N. D., 799, 1077 Jones, R., 358, 885, 1077 Jones, R. M., 358, 1092 Jones, T., 59, 1077 Jonsson, A., 28, 60, 431, 1064 ,1077 Jordan, M. I., 555, 605, 606, 686, 761, 827, 850, 852, 855, 857, 883,1013, 1066 ,1073 ,1077 ,1083, 1084 ,1088, 1089 ,1091 Jouannaud, J.-P., 359, 1077 Joule, J., 796Juang, B.-H., 604, 922, 1086 Judd, J. S., 762, 1077 Juels, A., 155, 1077 Junker, U., 359, 1077 Jurafsky, D., 885, 886, 920, 922, 1077 Just, M. A., 288, 1082 justiﬁcation (in a JTMS), 461 K k-consistency, 211 k-DL(decision list), 716 k-DT(decision tree), 716 k-d tree, 739 k-fold cross-validation, 708 Kadane, J.

Token 23420:
B., 639, 687, 1077 Kaelbling, L. P., 278, 556, 605, 686, 857, 1012, 1068 ,1070 ,1077 , 1082 ,1088 ,1090 Kager, R., 921, 1077 Kahn, H., 855, 1077 Kahneman, D., 2, 517, 620, 638, 1077 , 1090 Kaindl, H., 112, 1077 Kalman, R., 584, 604, 1077 Kalman ﬁlter, 566, 584, 584–591, 603, 604, 981 switching, 589, 608 Kalman gain matrix, 588 Kambhampati, S., 157, 390, 394, 395, 431–433, 1067 ,1069 ,1071 , 1077 ,1084 Kameya, Y ., 556, 1087Kameyama, M., 884, 1075 Kaminka, G., 688, 1089 Kan, A., 110, 405, 432, 1080 Kanade, T., 951, 968, 1087 ,1090 Kanal, L. N., 111, 112, 1077 ,1079 , 1083 Kanazawa, K., 604, 605, 686, 826, 1012, 1066 ,1070 ,1077 ,1087 Kanefsky, B., 9, 28, 229, 277, 1064 , 1068 Kanodia, N., 686, 1074 Kanoui, H., 314, 358, 1069 Kant, E., 358, 1067 Kantor, G., 1013, 1014, 1068 Kantorovich, L. V ., 155, 1077 Kaplan, D., 471, 1077 Kaplan, H., 111, 1074 Kaplan, R., 884, 920, 1066 ,1081 Karmarkar, N., 155, 1077 Karmiloff-Smith, A., 921, 1071 Karp, R. M., 8, 110, 112, 1059, 1075 , 1077 Kartam, N. A., 434, 1077 Kasami, T., 920, 1077 Kasif, S., 553, 1093 Kasparov, G., 29, 192, 193, 1077 Kassirer, J. P., 505, 1074 Katriel, I., 212, 228, 1091 Katz, S., 230, 1069 Kaufmann, M., 360, 1077 Kautz, D., 432, 1070 Kautz, H., 154, 229, 277, 279, 395, 1074 ,1077, 1078 ,1088 Kavraki, L., 1013, 1014, 1068 ,1078 Kay, A. R., 11, 1084 Kay, M., 884, 907, 922, 1066 ,1078 KB, 235, 274, 315 KB-A GENT ,236 Keane, M. A., 156, 1079 Kearns, M., 686, 759, 763, 764, 855, 1078 Kebeasy, R. M., 723, 724, 1078 Kedar-Cabelli, S., 799, 1082 Keene, R., 29, 1074 Keeney, R. L., 621, 625, 626, 638, 1078 Keil, F. C., 3, 1042, 1092 Keim, G. A., 231, 1080 Keller, R., 799, 1082 Kelly, J., 826, 1068 Kemp, M., 966, 1078 Kempe, A.

Token 23421:
B., 1023 Kenley, C. R., 553, 1088 Kephart, J. O., 60, 1078 Kepler, J., 966 kernel, 743 kernel function, 747, 816

Token 23422:


Token 23423:
1112 Index polynomial, 747 kernelization, 748 kernel machine, 744–748 kernel trick, 744, 748, 760 Kernighan, B. W., 110, 1080 Kersting, K., 556, 1078 ,1082 Kessler, B., 862, 883, 1078 Keynes, J. M., 504, 1078 Khare, R., 469, 1078 Khatib, O., 1013, 1078 Khmelev, D. V ., 886, 1078 Khorsand, A., 112, 1077 Kietz, J.-U., 800, 1078 Kilgarriff, A., 27, 1078 killer move, 170 Kim, H. J., 852, 857, 1013, 1084 Kim, J.-H., 1022, 1078 Kim, J. H., 552, 1078 Kim, M., 194kinematics, 987 kinematic state, 975 King, R. D., 797, 1078 ,1089 Kinsey, E., 109kinship domain, 301–303Kirchner, C., 359, 1077 Kirk, D. E., 60, 1078 Kirkpatrick, S., 155, 229, 1078 Kirman, J., 686, 1070 Kishimoto, A., 194, 1088 Kister, J., 192, 1078 Kisynski, J., 556, 1078 Kitano, H., 195, 1014, 1078 Kjaerulff, U., 604, 1078 KL-O NE, 471 Kleer, J. D., 60, 1074 Klein, D., 883, 896, 900, 920, 921, 1074 ,1078 ,1085 Kleinberg, J. M., 884, 1078 Klemperer, P., 688, 1078 Klempner, G., 553, 1083 Kneser, R., 883, 1078 Knight, B., 20, 1066 Knight, K., 2, 922, 927, 1078 ,1086 Knoblock, C. A., 394, 432, 1068 ,1073 KNOW ITALL, 885 knowledge acquisition, 860 and action, 7, 453 background, 235, 349, 777, 1024, 1025 base (KB), 235, 274, 315 commonsense, 19diagnostic, 497 engineering, 307, 307–312, 514 for decision-theoretic systems, 634 level, 236, 275model-based, 497 prior, 39, 768, 778, 787 knowledge-based agents, 234 knowledge-based system, 22–24, 845 knowledge acquisition, 23, 307, 860 knowledge compilation, 799 knowledge map, seeBayesian network knowledge representation, 2, 16, 19, 24, 234, 285–290, 437–479 analogical, 315 everything, 437 language, 235, 274, 285 uncertain, 510–513 Knuth, D. E., 73, 191, 359, 919, 1013, 1059, 1074 ,1078 Kobilarov, G., 439, 469, 1066 Kocsis, L., 194, 1078 Koditschek, D., 1013, 1078 Koehler, J., 395, 1078 Koehn, P., 922, 1078 Koenderink, J. J., 968, 1078 Koenig, S., 157, 395, 434, 685, 1012, 1075 ,1078 ,1088 Koller, D., 191, 505, 553, 556, 558, 604, 605, 639, 677, 686, 687, 826, 827, 884, 1012, 1065, 1066 , 1073, 1074 ,1076–1078 ,1083 , 1085 ,1087 ,1090 Kolmogorov’s axioms, 489 Kolmogorov, A. N., 504, 604, 759, 1078 Kolmogorov complexity, 759 Kolobov, A., 556, 1082 Kolodner, J., 24, 799, 1078 Kondrak, G., 229, 230, 1078 Konolige, K., 229, 434, 472, 1012, 1013, 1067 ,1078, 1079 Koo, T., 920, 1079 Koopmans, T. C., 685, 1079 Korb, K. B., 558, 687, 1079 Koren., Y ., 1013, 1066 Korf, R. E., 110–112, 157, 191, 394, 395, 1072 ,1079 ,1085 Kortenkamp, D., 1013, 1069 Koss, F., 1013, 1069 Kotok, A., 191, 192, 1079 Koutsoupias, E., 154, 277, 1079 Kowalski, R., 282, 314, 339, 345, 359, 470, 472, 1079 ,1087 ,1091 Kowalski form, 282, 345 Koza, J. R., 156, 1079 Kramer, S., 556, 1078 Kraus, S., 434, 1079 Kraus, W. F., 155, 1080 Krause, A., 639, 1079 Krauss, P., 555, 1088 Kriegspiel, 180Kripke, S. A., 470, 1079 Krishnan, T., 826, 1082 Krogh, A., 604, 1079 KRYPTON , 471 Ktesibios of Alexandria, 15K¨ubler, S., 920, 1079 Kuhn, H. W., 601, 606, 687, 1079 Kuhns, J.-L., 884, 1081 Kuijpers, C., 158, 1080 Kuipers, B. J., 472, 473, 1012, 1079 Kumar, P. R., 60, 1079 Kumar, V ., 111, 112, 230, 1074 ,1077 , 1079 ,1083 Kuniyoshi, Y ., 195, 1014, 1078 Kuppuswamy, N., 1022, 1078 Kurien, J., 157, 1079 Kurzweil, R., 2, 12, 28, 1038, 1079 Kwok, C., 885, 1079 Kyburg, H. E., 505, 1079 L L-BFGS, 760 label (in plans), 137, 158 Laborie, P., 432, 1079 Ladanyi, L., 112, 1086 Ladkin, P., 470, 1079 Lafferty, J., 884, 885, 1079 Lagoudakis, M. G., 854, 857, 1074 , 1079 Laguna, M., 154, 1074 Laird, J., 26, 336, 358, 432, 799, 1047, 1077 ,1079 ,1092 Laird, N., 604, 826, 1070 Laird, P., 154, 229, 1082 Lake, R., 194, 1088 Lakemeyer, G., 1012, 1067 Lakoff, G., 469, 921, 1041, 1079 Lam, J., 195, 1079 LAMA, 387, 395 Lamarck, J.

Token 23424:
B., 130, 1079 Lambert’s cosine law, 934 Lambertian surface, 969Landauer, T. K., 883, 1070 Landhuis, E., 620, 1079 landmark, 980 landscape (in state space), 121 Langdon, W., 156, 1079 ,1085 Langley, P., 800, 1079 Langlotz, C. P., 26, 1076 Langton, C., 155, 1079 language, 860, 888, 890 abhors synonyms, 870 formal, 860model, 860, 909, 913 in disambiguation, 906 natural, 4, 286, 861

Token 23425:


Token 23426:
Index 1113 processing, 16, 860 translation, 21, 784, 907–912 understanding, 20, 23 language generation, 899 language identiﬁcation, 862 Laplace, P., 9, 491, 504, 546, 883, 1079 Laplace smoothing, 863 Laptev, I., 961, 1080 large-scale learning, 712 Lari, K., 896, 920, 1080 Larkey, P. D., 687, 1077 Larra˜ naga, P., 158, 1080 Larsen, B., 553, 1080 Larson, G., 778Larson, S. C., 759, 1080 Laruelle, H., 395, 431, 1073 Laskey, K. B., 556, 1080 Lassez, J.-L., 359, 1077 Lassila, O., 469, 1065 latent Dirichlet allocation, 883 latent semantic indexing, 883latent variable, 816 Latham, D., 856, 1081 Latombe, J.-C., 432, 1012, 1013, 1071 , 1078 ,1080 ,1093 lattice theory, 360Laugherty, K., 920, 1074 Lauritzen, S., 553, 558, 639, 826, 1069 , 1080 ,1084 ,1089 LaValle, S., 396, 1013, 1014, 1080 Lave, R. E., 686, 1087 Lavra uc, N., 796, 799, 800, 1080 ,1082 L AWALY , 432 Lawler, E. L., 110, 111, 405, 432, 1080 laws of thought, 4 layers, 729 Lazanas, A., 1013, 1080 laziness, 481 La Mettrie, J. O., 1035, 1041, 1079 La Mura, P., 638, 1079 LCF, 314 Leacock, C., 1022, 1067 leaf node, 75 leak node, 519 Leaper, D. J., 505, 1070 leaping to conclusions, 778 learning, 39, 44, 59, 236, 243, 693, 1021, 1025 active, 831 apprenticeship, 857, 1037 assessing performance, 708–709 Bayesian, 752, 803, 803–804, 825 Bayesian network, 813–814 blocks-world, 20 cart–pole problem, 851checkers, 18computational theory, 713 decision lists, 715–717 decision trees, 697–703 determinations, 785element, 55 ensemble, 748, 748–752 explanation-based, 780–784game playing, 850–851 grammar, 921 heuristics, 107hidden Markov model, 822–823 hidden variables, 820 hidden variables, 822incremental, 773, 777inductive, 694, 695–697 knowledge-based, 779, 788, 798 instance-based, 737, 737–739, 855 knowledge in, 777–780 linearly separable functions, 731 logical, 768–776MAP, 804–805 maximum likelihood, 806–810 metalevel, 102 mixtures of Gaussians, 817–820 naive Bayes, 808–809 neural network, 16, 736–737new predicates, 790, 796 noise, 705–706 nonparametric, 737online, 752, 846 PAC, 714, 759, 784 parameter, 806, 810–813 passive, 831 Q,831, 843, 844, 848, 973 rate of, 719, 836 reinforcement, 685, 695, 830–859, 1025 inverse, 857 relational, 857 relevance-based, 784–787 restaurant problem, 698 statistical, 802–805 temporal difference, 836–838, 853, 854 top-down, 791–794 to search, 102 unsupervised, 694, 817–820, 1025 utility functions, 831 weak, 749 learning curve, 702 least-constraining-value heuristic, 217 least commitment, 391 leave-one-out cross-validation (LOOCV), 708 LeCun, Y ., 760, 762, 967, 1047, 1065 , 1080 ,1086Lederberg, J., 23, 468, 1072 ,1080 Lee, C.-H., 1022, 1078 Lee, K.-H., 1022, 1078 Lee, M. S., 826, 1083 Lee, R. C.-T., 360, 1068 Lee, T.-M., 11, 1084 Leech, G., 920, 921, 1080 ,1086 legal reasoning, 32 Legendre, A. M., 759, 1080 Lehmann, D., 434, 1079 Lehmann, J., 439, 469, 1066 Lehrer, J., 638, 1080 Leibniz, G. W., 6, 131, 276, 504, 687Leimer, H., 553, 1080 Leipzig, 12 Leiserson, C. E., 1059, 1069 Lempel-Ziv-Welch compression (LZW), 867 Lenat, D. B., 27, 439, 469, 474, 800, 1070 ,1075 ,1080 lens system, 931 Lenstra, J. K., 110, 405, 432, 1080 Lenzerini, M., 471, 1067 Leonard, H. S., 470, 1080 Leonard, J., 1012, 1066 ,1080 Leone, N., 230, 472, 1071 ,1074 Lesh, N., 433, 1072 Le´sniewski, S., 470, 1080 Lesser, V .

Token 23427:
R., 434, 1071 Lettvin, J. Y ., 963, 1080 Letz, R., 359, 1080 level (in planning graphs), 379 level cost, 382 leveled off (planning graph), 381 Levesque, H. J., 154, 277, 434, 471, 473, 1067 ,1069 ,1080 , 1088 Levin, D. A., 604, 1080 Levinson, S., 314, 1066 ,1074 Levitt, G. M., 190, 1080 Levitt, R. E., 434, 1077 Levitt, T. S., 1012, 1079 Levy, D., 195, 1022, 1080 Lewis, D. D., 884, 1080 Lewis, D. K., 60, 1042, 1080 LEX, 776, 799 lexical category, 888 lexicalized grammar, 897 lexicalized PCFG, 897, 919, 920 lexicon, 890, 920 Leyton-Brown, K., 230, 435, 688, 1080 , 1088 LFG, 920Li, C. M., 277, 1080 Li, H., 686, 1077 Li, M., 759, 1080 liability, 1036

Token 23428:


Token 23429:
1114 Index Liang, G., 553, 1068 Liang, L., 604, 1083 Liao, X., 686, 1077 Liberatore, P., 279, 1080 Lifchits, A., 885, 1085 life insurance, 621 Lifschitz, V ., 472, 473, 1073 ,1080 , 1091 lifting, 326, 325–329, 367 in probabilistic inference, 544 lifting lemma, 350, 353 light, 932 Lighthill, J., 22, 1080 Lighthill report, 22, 24 likelihood, 803 LIKELIHOOD -WEIGHTING ,534 likelihood weighting, 532, 552, 596 Lim, G., 439, 1089 limited rationality, 5 Lin, D., 885, 1085 Lin, J., 872, 885, 1065 Lin, S., 110, 688, 1080 ,1092 Lin, T., 439, 1089 Lincoln, A., 872 Lindley, D. V ., 639, 1080 Lindsay, R. K., 468, 1080 linear-chain conditional random ﬁeld, 878 linear algebra, 1055–1057linear constraint, 205 linear function, 717 linear Gaussian, 520, 553, 584, 809 linearization, 981 linear programming, 133, 153, 155, 206, 673 linear regression, 718, 810 linear resolution, 356, 795 linear separability, 723 linear separator, 746line search, 132 linguistics, 15–16 link, 870 link (in a neural network), 728 linkage constraints, 986 Linnaeus, 469 L INUS , 796 Lipkis, T. A., 471, 1088 liquid event, 447 liquids, 472 Lisp, 19, 294 lists, 305 literal (sentence), 244 literal, watched, 277Littman, M. L., 155, 231, 433, 686, 687, 857, 1064 ,1068 ,1077 ,1080, 1081Liu, J. S., 605, 1080 Liu, W., 826, 1068 Liu, X., 604, 1083 Livescu, K., 604, 1080 Livnat, A., 434, 1080 lizard toasting, 778 local beam search, 125, 126 local consistency, 208 locality, 267, 547 locality-sensitive hash (LSH), 740 localization, 145, 581, 979 Markov, 1012 locally structured system, 515 locally weighted regression, 742 local optimum, 669 local search, 120–129, 154, 229, 262–263, 275, 277 location sensors, 974 Locke, J., 6, 1042, 1080 Lodge, D., 1051, 1080 Loebner Prize, 1021Loftus, E., 287, 1080 Logemann, G., 260, 276, 1070 logic, 4, 7, 240–243 atoms, 294–295default, 459, 468, 471 equality in, 299 ﬁrst-order, 285, 285–321 inference, 322–325semantics, 290 syntax, 290 fuzzy, 240, 289, 547, 550, 557higher-order, 289 inductive, 491, 505 interpretations, 292–294model preference, 459 models, 290–292 nonmonotonic, 251, 458, 458–460, 471 notation, 4propositional, 235, 243–247, 274, 286 inference, 247–263 semantics, 245–246 syntax, 244–245 quantiﬁer, 295–298resolution, 252–256 sampling, 554 temporal, 289 terms, 294 variable in, 340 logical connective, 16, 244, 274, 295logical inference, 242, 322–365logical minimization, 442 logical omniscience, 453 logical piano, 276logical positivism, 6logical reasoning, 249–264, 284 logicism, 4 logic programming, 257, 314, 337, 339–345 constraint, 344–345, 359 inductive (ILP), 779, 788–794, 798 tabled, 343 Logic Theorist, 17, 276 L OGISTELLO , 175, 186 logistic function, 522, 760 logistic regression, 726 logit distribution, 522 log likelihood, 806 Lohn, J. D., 155, 1080 London, 14 Long, D., 394, 395, 1072, 1073 long-distance dependencies, 904long-term memory, 336 Longley, N., 692, 1080 Longuet-Higgins, H. C., 1080 Loo, B. T., 275, 1080 LOOCV , 708 Look ma, no hands, 18 lookup table, 736 Loomes, G., 637, 1086 loosely coupled system, 427 Lorenz, U., 193, 1071 loss function, 710 Lotem, A., 396, 1091 lottery, 612, 642 standard, 615 love, 1021Love, N., 195, 1080 Lovejoy, W. S., 686, 1080 Lovelace, A., 14 Loveland, D., 260, 276, 359, 1070 ,1080 low-dimensional embedding, 985 Lowe, D., 947, 967, 968, 1081 L¨owenheim, L., 314, 1081 Lowerre, B. T., 154, 922, 1081 Lowrance, J. D., 557, 1087 Lowry, M., 356, 360, 1075 ,1081 Loyd, S., 109, 1081 Lozano-Perez, T., 1012, 1013, 1067 , 1081 ,1092 LPG, 387, 395 LRTA*, 151, 157, 415 LRTA*-A GENT ,152 LRTA*-C OST,152 LSH (locality-sensitive hash), 740 LT, 17 Lu, F., 1012, 1081 Lu, P., 194, 760, 1067 ,1088 Luby, M., 124, 554, 1069 ,1081 Lucas, J. R., 1023, 1081 Lucas, P., 505, 634, 1081

Token 23430:


Token 23431:
Index 1115 Luce, D. R., 9, 687, 1081 Lucene, 868Ludlow, P., 1042, 1081 Luger, G. F., 31, 1081 Lugosi, G., 761, 1068 Lull, R., 5Luong, Q.-T., 968, 1072 Lusk, E., 360, 1092 Lygeros, J., 60, 1068 Lyman, P., 759, 1081 Lynch, K., 1013, 1014, 1068 LZW, 867 M MA* search, 101, 101–102, 112 MACHACK-6, 192 Machina, M., 638, 1081 machine evolution, 21 machine learning, 2,4 machine reading, 881 machine translation, 32, 907–912, 919 statistical, 909–912 Machover, M., 314, 1065 MacKay, D. J. C., 555, 761, 763, 1081, 1082 MacKenzie, D., 360, 1081 Mackworth, A. K., 2, 59, 209, 210, 228, 230, 1072 ,1081 ,1085 macrop (macro operator), 432, 799 madalines, 761Madigan, C. F., 277, 1083 magic sets, 336, 358 Mahalanobis distance, 739 Mahanti, A., 112, 1081 Mahaviracarya, 503Maheswaran, R., 230, 1085 Maier, D., 229, 358, 1065 Mailath, G., 688, 1081 Majercik, S. M., 433, 1081 majority function, 731makespan, 402 Makov, U. E., 826, 1090 Malave, V .

Token 23432:
L., 288, 1082 Maldague, P., 28, 1064 Mali, A. D., 432, 1077 Malik, J., 604, 755, 762, 941, 942, 953, 967, 968, 1065 ,1070 ,1076 , 1081 ,1088 Malik, S., 277, 1083 Manchak, D., 470, 1091 Maneva, E., 278, 1081 Maniatis, P., 275, 1080 manipulator, 971 Manna, Z., 314, 1081 Mannila, H., 763, 1074Manning, C., 883–885, 920, 921, 1078 , 1081 Mannion, M., 314, 1081 Manolios, P., 360, 1077 Mansour, Y ., 686, 764, 855, 856, 1078 , 1090 mantis shrimp, 935Manzini, G., 111, 1081 map, 65 MAP (maximum a posteriori), 804 MAPGEN, 28 Marais, H., 884, 1088 Marbach, P., 855, 1081 March, J. G., 637, 1075 Marcinkiewicz, M. A., 895, 921, 1081 Marcot, B., 553, 1086 Marcus, G., 638, 1081 Marcus, M. P., 895, 921, 1081 margin, 745 marginalization, 492 Markov assumption sensor, 568 process ﬁrst-order, 568 Markov, A.

Token 23433:
A., 603, 883, 1081 Markov assumption, 568, 603 Markov blanket, 517, 560 Markov chain, 537, 568, 861 Markov chain Monte Carlo (MCMC), 535, 535–538, 552, 554, 596 decayed, 605 Markov decision process (MDP), 10, 647, 684, 686, 830 factored, 686 partially observable (POMDP), 658, 658–666, 686 Markov games, 857 Markov network, 553 Markov process, 568 Markov property, 577, 603, 646 Maron, M. E., 505, 884, 1081 Marr, D., 968, 1081 Marriott, K., 228, 1081 Marshall, A. W., 855, 1077 Marsland, A. T., 195, 1081 Marsland, S., 763, 1081 Martelli, A., 110, 111, 156, 1081 Marthi, B., 432, 556, 605, 856, 1081, 1082 ,1085 Martin, D., 941, 967, 1081 Martin, J. H., 885, 886, 920–922, 1077 , 1081 Martin, N., 358, 1067 Martin, P., 921, 1076Mason, M., 156, 433, 1013, 1014, 1071 , 1081 Mason, R. A., 288, 1082 mass (in Dempster–Shafer theory), 549 mass noun, 445 mass spectrometer, 22 Mataric, M. J., 1013, 1081 Mateescu, R., 230, 1070 Mateis, C., 472, 1071 materialism, 6 material value, 172 Mates, B., 276, 1081 mathematical induction schema, 352 mathematics, 7–9, 18, 30 Matheson, J. E., 626, 638, 1076 ,1082 matrix, 1056 Matsubara, H., 191, 195, 1072 ,1078 Maturana, H. R., 963, 1080 Matuszek, C., 469, 1081 Mauchly, J., 14 Mausam., 432, 1069 MAV E N , 195 MAX-VALUE ,166, 170 maximin, 670 maximin equilibrium, 672 maximum global, 121 local, 122 maximum a posteriori, 804, 825 maximum expected utility, 483, 611 maximum likelihood, 805, 806–810, 825 maximum margin separator, 744, 745 max norm, 654 MAXPLAN, 387 Maxwell, J., 546, 920, 1081 Mayer, A., 112, 119, 1075 Mayne, D. Q., 605, 1075 Mazumder, P., 110, 1088 Mazurie, A., 605, 1085 MBP, 433 McAllester, D. A., 25, 156, 191, 198, 394, 395, 472, 855, 856, 1072 , 1077 ,1081 ,1090 MCC, 24 McCallum, A., 877, 884, 885, 1069 , 1072 ,1077 ,1079 ,1081 ,1084, 1085 ,1090 McCarthy, J., 17–19, 27, 59, 275, 279, 314, 395, 440, 471, 1020, 1031,1081, 1082 McCawley, J. D., 920, 1082 McClelland, J. L., 24, 1087 McClure, M., 604, 1065 McCorduck, P., 1042, 1082

Token 23434:


Token 23435:
1116 Index McCulloch, W. S., 15, 16, 20, 278, 727, 731, 761, 963, 1080 ,1082 McCune, W., 355, 360, 1082 McDermott, D., 2, 156, 358, 394, 433, 434, 454, 470, 471, 1068 ,1073 , 1082 McDermott, J., 24, 336, 358, 1082 McDonald, R., 288, 920, 1079 McEliece, R. J., 555, 1082 McGregor, J. J., 228, 1082 McGuinness, D., 457, 469, 471, 1064 , 1066 ,1089 McIlraith, S., 314, 1082 McLachlan, G. J., 826, 1082 McMahan, B., 639, 1079 MCMC, 535, 535–538, 552, 554, 596 McMillan, K. L., 395, 1082 McNealy, S., 1036 McPhee, N., 156, 1085 MDL, 713, 759, 805 MDP, 10, 647, 684, 686, 830 mean-ﬁeld approximation, 554 measure, 444 measurement, 444mechanism, 679 strategy-proof, 680 mechanism design, 679, 679–685 medical diagnosis, 23, 505, 517, 548, 629, 1036 Meehan, J., 358, 1068 Meehl, P., 1022, 1074 ,1082 Meek, C., 553, 1092 Meet (interval relation), 448 Megarian school, 275 megavariable, 578 Meggido, N., 677, 687, 1078 Mehlhorn, K., 112, 1069 mel frequency cepstral coefﬁcient (MFCC), 915 Mellish, C. S., 359, 1068 memoization, 343, 357, 780 memory requirements, 83, 88 MEMS, 1045 Mendel, G., 130, 1082 meningitis, 496–509 mental model, in disambiguation, 906 mental objects, 450–453 mental states, 1028 Mercer’s theorem, 747 Mercer, J., 747, 1082 Mercer, R. L., 883, 922, 1067 ,1077 mereology, 470 Merkhofer, M. M., 638, 1082 Merleau-Ponty, M., 1041, 1082 Meshulam, R., 112, 1072 Meta-D ENDRAL , 776, 798metadata, 870 metalevel, 1048 metalevel state space, 102 metaphor, 906, 921 metaphysics, 6metareasoning, 189 decision-theoretic, 1048 metarule, 345 meteorite, 422, 480 metonymy, 905, 921 Metropolis, N., 155, 554, 1082 Metropolis–Hastings, 564 Metropolis algorithm, 155, 554 Metzinger, T., 1042, 1082 Metzler, D., 884, 1069 MEXAR2, 28 Meyer, U., 112, 1069 M´ezard, M., 762, 1082 M GONZ , 1021 MGSS*, 191 MGU (most general uniﬁer), 327, 329, 353, 361 MHT (multiple hypothesis tracker), 606 Mian, I. S., 604, 605, 1079 ,1083 Michalski, R. S., 799, 1082 Michaylov, S., 359, 1077 Michie, D., 74, 110, 111, 156, 191, 763, 851, 854, 1012, 1071 ,1082 micro-electromechanical systems (MEMS), 1045 micromort, 616, 637, 642 Microsoft, 553, 874microworld, 19, 20, 21 Middleton, B., 519, 552, 1086 Miikkulainen, R., 435, 1067 Milch, B., 556, 639, 1078 ,1082 ,1085 Milgrom, P., 688, 1082 Milios, E., 1012, 1081 military uses of AI, 1035Mill, J. S., 7, 770, 798, 1082 Miller, A. C., 638, 1082 Miller, D., 431, 1070 million queens problem, 221, 229 Millstein, T., 395, 1071 Milner, A. J., 314, 1074 M IN-CONFLICTS ,221 min-conﬂicts heuristic, 220, 229 MIN-VALUE ,166, 170 mind, 2, 1041 dualistic view, 1041 and mysticism, 12 philosophy of, 1041as physical system, 6 theory of, 3 mind–body problem, 1027 minesweeper, 284M INIMAL -CONSISTENT -DET,786 minimal model, 459 MINIMAX -DECISION ,166 minimax algorithm, 165, 670 minimax decision, 165 minimax search, 165–168, 188, 189 minimax value, 164, 178 minimum global, 121 local, 122 minimum-remaining-values, 216, 333 minimum description length (MDL), 713, 759, 805 minimum slack, 405 minimum spanning tree (MST), 112, 119 MINISAT, 277 Minker, J., 358, 473, 1073 ,1082 Minkowski distance, 738 Minsky, M. L., 16, 18, 19, 22, 24, 27, 434, 471, 552, 761, 1020, 1039,1042, 1082 Minton, S., 154, 229, 432, 799, 1068 , 1082 Miranker, D. P., 229, 1065 Misak, C., 313, 1082 missing attribute values, 706 missionaries and cannibals, 109, 115, 468 MIT, 17–19, 1012 Mitchell, D., 154, 277, 278, 1069 ,1088 Mitchell, M., 155, 156, 1082 Mitchell, T. M., 61, 288, 763, 776, 798, 799, 884, 885, 1047, 1066, 1067 , 1069 ,1082 ,1084 Mitra, M., 870, 1089 mixed strategy, 667 mixing time, 573 mixture distribution, 817 mixture distribution, 817 mixture of Gaussians, 608, 817, 820 Mizoguchi, R., 27, 1075 ML, seemaximum likelihood modal logic, 451 model, 50, 240, 274, 289, 313, 451 causal, 517 (in representation), 13 sensor, 579, 586, 603 theory, 314 transition, 67, 108, 134, 162, 266, 566, 597, 603, 646, 684, 832,979 M ODEL -BASED -REFLEX -AGENT ,51 model-based reﬂex agents, 59 model checking, 242, 274

Token 23436:


Token 23437:
Index 1117 model selection, 709, 825 Modus Ponens, 250, 276, 356, 357, 361 Generalized, 325, 326 Moffat, A., 884, 1092 MOGO, 186, 194 Mohr, R., 210, 228, 968, 1082 ,1088 Mohri, M., 889, 1083 Molloy, M., 277, 1064 monism, 1028 monitoring, 145 monkey and bananas, 113, 396monotone condition, 110monotonicity of a heuristic, 95 of a logical system, 251, 458 of preferences, 613 Montague, P. R., 854, 1083 ,1088 Montague, R., 470, 471, 920, 1077 , 1083 Montanari, U., 111, 156, 228, 1066 , 1081 ,1083 M ONTE -CARLO -LOCALIZATION ,982 Monte Carlo (in games), 183 Monte Carlo, sequential, 605 Monte Carlo algorithm, 530 Monte Carlo localization, 981 Monte Carlo simulation, 180 Montemerlo, M., 1012, 1083 Mooney, R., 799, 902, 921, 1070 ,1083 , 1093 Moore’s Law, 1038 Moore, A., 826, 1083 Moore, A. W., 154, 826, 854, 857, 1066 , 1077 ,1083 Moore, E. F., 110, 1083 Moore, J. S., 356, 359, 360, 1066 ,1077 Moore, R. C., 470, 473, 922, 1076 ,1083 Moravec, H. P., 1012, 1029, 1038, 1083 More, T., 17 Morgan, J., 434, 1069 Morgan, M., 27, 1069 Morgan, N., 922, 1074 Morgenstern, L., 470, 472, 473, 1070 , 1083 Morgenstern, O., 9, 190, 613, 637, 1091 Moricz, M., 884, 1088 Morjaria, M. A., 553, 1083 Morris, A., 604, 1089 Morris, P., 28, 60, 431, 1064 ,1077 Morrison, E., 190, 1083 Morrison, P., 190, 1083 Moses, Y ., 470, 477, 1072 Moskewicz, M. W., 277, 1083 Mossel, E., 278, 1081 Mosteller, F., 886, 1083most general uniﬁer (MGU), 327, 329, 353, 361 most likely explanation, 553, 603 most likely state, 993 Mostow, J., 112, 119, 1083 motion, 948–951 compliant, 986, 995 guarded, 995 motion blur, 931 motion model, 979 motion parallax, 949, 966motion planning, 986 Motwani, R., 682, 760, 1064 ,1073 Motzkin, T. S., 761, 1083 Moutarlier, P., 1012, 1083 movies movies 2001: A Space Odyssey , 552 movies A.I., 1040 movies The Matrix , 1037 movies The Terminator , 1037 Mozetic, I., 799, 1082 MPI (mutual preferential independence), 625 MRS (metalevel reasoning system), 345 MST, 112, 119 Mueller, E. T., 439, 470, 1083 ,1089 Muggleton, S. H., 789, 795, 797, 800, 921, 1071 ,1083 ,1089, 1090 M¨uller, M., 186, 194, 1083 ,1088 Muller, U., 762, 967, 1080 multiagent environments, 161 multiagent planning, 425–430 multiagent systems, 60, 667 multiattribute utility theory, 622, 638 multibody planning, 425, 426–428 multiplexer, 543 multiply connected network, 528 multivariate linear regression, 720 Mumford, D., 967, 1083 MUNIN , 552 Murakami, T., 186 Murga, R., 158, 1080 Murphy, K., 555, 558, 604, 605, 1012, 1066 ,1071 ,1073 ,1083 ,1090 Murphy, R., 1014, 1083 Murray-Rust, P., 469, 1083 Murthy, C., 360, 1083 Muscettola, N., 28, 60, 431, 432, 1077 , 1083 music, 14 Muslea, I., 885, 1083 mutagenicity, 797mutation, 21, 128, 153 mutex, 380 mutual exclusion, 380 mutual preferential independence (MPI), 625 mutual utility independence (MUI), 626 MYCIN , 23, 548, 557 Myerson, R., 688, 1083 myopic policy, 632 mysticism, 12 N n-armed bandit, 841 n-gram model, 861 Nadal, J.-P., 762, 1082 Nagasawa, Y ., 1042, 1081 Nagel, T., 1042, 1083 Na¨ım, P., 553, 1086 naive Bayes, 499, 503, 505, 808–809, 820, 821, 825 naked, 214Nalwa, V .

Token 23438:
S., 12, 1083 Naor, A., 278, 1064 Nardi, D., 471, 1064 ,1067 narrow content, 1028 NASA, 28, 392, 432, 472, 553, 972 Nash, J., 1083 Nash equilibrium, 669, 685 N ASL, 434 NATACHATA , 1021 natural kind, 443 natural numbers, 303 natural stupidity, 454 Nau, D. S., 111, 187, 191, 192, 195, 372, 386, 395, 396, 432,1071–1073 ,1079 ,1083, 1084 , 1089 ,1091 navigation function, 994 Nayak, P., 60, 157, 432, 472, 1079 ,1083 Neal, R., 762, 1083 Nealy, R., 193nearest-neighbor ﬁlter, 601 nearest-neighbors, 738, 814 nearest-neighbors regression, 742 neat vs.scruffy, 25 Nebel, B., 394, 395, 1076 ,1078 ,1083 needle in a haystack, 242Neﬁan, A., 604, 1083 negation, 244 negative example, 698 negative literal, 244 negligence, 1036 Nelson, P. C., 111, 1071 Nemirovski, A., 155, 1065 ,1083 N ERO, 430, 435

Token 23439:


Token 23440:
1118 Index Nesterov, Y ., 155, 1083 Netto, E., 110, 1083 network tomography, 553 neural network, 16, 20, 24, 186, 727, 727–737 expressiveness, 16 feed-forward, 729 hardware, 16 learning, 16, 736–737 multilayer, 22, 731–736 perceptron, 729–731 radial basis function, 762single layer, seeperceptron neurobiology, 968 N EUROGAMMON , 851 neuron, 10, 16, 727, 1030 neuroscience, 10, 10–12, 728 computational, 728 Nevill-Manning, C. G., 921, 1083 NEW-CLAUSE ,793 Newborn, M., 111, 1085 Newell, A., 3, 17, 18, 26, 60, 109, 110, 191, 275, 276, 336, 358, 393,432, 799, 1047, 1079 ,1084 , 1089 Newman, P., 1012, 1066 ,1071 Newton, I., 1, 47, 131, 154, 570, 760, 1084 Newton–Raphson method, 132 Ney, H., 604, 883, 922, 1078 ,1084 Ng, A. Y ., 686, 759, 850, 852, 855–857, 883, 1013, 1066 ,1068 ,1078 , 1084 Nguyen, H., 883, 1078 Nguyen, X., 394, 395, 1084 Niblett, T., 800, 1068 Nicholson, A., 558, 604, 686, 687, 1070 ,1079 ,1084 Nielsen, P. E., 358, 1077 Niemel¨ a, I., 472, 1084 Nigam, K., 884, 885, 1069 ,1077 ,1084 Nigenda, R. S., 395, 1084 Niles, I., 469, 1084, 1085 Nilsson, D., 639, 1084 Nilsson, N. J., 2, 27, 31, 59, 60, 109–111, 119, 156, 191, 275,314, 350, 359, 367, 393, 432,434, 555, 761, 799, 1012, 1019,1034, 1072, 1073 ,1075 ,1084 , 1091 Nine-Men’s Morris, 194Niranjan, M., 605, 855, 1070 ,1087 Nisan, N., 688, 1084 NIST, 753nitroaromatic compounds, 797 Niv, Y ., 854, 1070Nivre, J., 920, 1079 Nixon, R., 459, 638, 906 Nixon diamond, 459 Niyogi, S., 314, 1090 NLP (natural language processing), 2, 860 no-good, 220, 385 no-regret learning, 753 N OAH, 394, 433 Nobel Prize, 10, 22 Nocedal, J., 760, 1067 Noda, I., 195, 1014, 1078 node child, 75 current, in local search, 121 parent, 75 node consistency, 208 Noe, A., 1041, 1084 noise, 701, 705–706, 712, 776, 787, 802 noisy-AND, 561 noisy-OR, 518 noisy channel model, 913 nominative case, 899 nondeterminism angelic, 411 demonic, 410 nondeterministic environment, 43 nonholonomic, 976 NONLIN , 394 NONLIN +, 431, 432 nonlinear, 589 nonlinear constraints, 205 nonmonotonicity, 458 nonmonotonic logic, 251, 458, 458–460, 471 Nono, 330 nonstationary, 857 nonterminal symbol, 889, 890, 1060 Normal–Wishart, 811 normal distribution, 1058 standard, 1058 normal form, 667 normalization (of a probability distribution), 493 normalization (of attribute ranges), 739 Norman, D. A., 884, 1066 normative theory, 619 North, O., 330 North, T., 21, 1072 Norvig, P., 28, 358, 444, 470, 604, 759, 883, 921, 922, 1074 ,1078 ,1084 , 1087 notation inﬁx, 303 logical, 4 preﬁx, 304noughts and crosses, 162, 190, 197 Nourbakhsh, I., 156, 1073 Nowak, R., 553, 1068 Nowatzyk, A., 192, 1076 Nowick, S. M., 279, 1084 Nowlan, S. J., 155, 1075 NP (hard problems), 1054–1055NP-complete, 8, 71, 109, 250, 276, 471, 529, 762, 787, 1055 N QTHM , 360 NSS chess program, 191nuclear power, 561 number theory, 800 Nunberg, G., 862, 883, 921, 1078 ,1084 N UPRL , 360 Nussbaum, M. C., 1041, 1084 Nyberg, L., 11, 1067 O O()notation, 1054 O’Malley, K., 688, 1092 O’Reilly, U.-M., 155, 1084 O-P LAN, 408, 431, 432 Oaksford, M., 638, 1068 ,1084 object, 288, 294 composite, 442 object-level state space, 102 object-oriented programming, 14, 455 objective case, 899 objective function, 15, 121 objectivism, 491 object model, 928 observable, 42observation model, 568observation prediction, 142observation sentences, 6 occupancy grid, 1012 occupied space, 988 occur check, 327, 340 Och, F. J., 29, 604, 921, 922, 1067 , 1084 ,1093 Ockham’s razor, 696, 757–759, 777, 793, 805 Ockham, W., 696, 758 Oddi, A., 28, 1068 odometry, 975 Odyssey, 1040Ofﬁce Assistant, 553 ofﬂine search, 147 Ogasawara, G., 604, 1076 Ogawa, S., 11, 1084 Oglesby, F., 360, 1074 Oh, S., 606, 1084 Ohashi, T., 195, 1091 Olalainty, B., 432, 1073

Token 23441:


Token 23442:
Index 1119 Olesen, K. G., 552–554, 1064 ,1084 Oliver, N., 604, 1084 Oliver, R. M., 639, 1084 Oliver, S. G., 797, 1078 Olshen, R. A., 758, 1067 omniscience, 38 Omohundro, S., 27, 920, 1039, 1084 , 1089 Ong, D., 556, 1082 ONLINE -DFS-A GENT ,150 online learning, 752, 846 online planning, 415 online replanning, 993 online search, 147, 147–154, 157 ontological commitment, 289, 313, 482, 547 ontological engineering, 437, 437–440 ontology, 308, 310 upper, 467 open-coding, 341 open-loop, 66 open-universe probability model (OUPM), 545, 552 open-world assumption, 417 open class, 890 OPENCYC, 469 open list, seefrontier OPENMIND, 439 operationality, 783 operations research, 10, 60, 110, 111 Oppacher, F., 155, 1084 OPS-5, 336, 358 optical ﬂow, 939, 964, 967 optimal brain damage, 737 optimal controllers, 997 optimal control theory, 155optimality, 121 optimality (of a search algorithm), 80, 108 optimality theory (Linguistics), 921optimally efﬁcient algorithm, 98 optimal solution, 68 optimism under uncertainty, 151 optimistic description (of an action), 412 optimistic prior, 842 optimization, 709 convex, 133, 153 optimizer’s curse, 619, 637 O PTIMUM -AIV, 432 OR-SEARCH ,136 orderability, 612 ordinal utility, 614 Organon, 275, 469 orientation, 938 origin function, 545 Ormoneit, D., 855, 1084ORnode, 135 Osawa, E., 195, 1014, 1078 Osborne, M. J., 688, 1084 Oscar, 435Osherson, D. N., 759, 1084 Osindero, S., 1047, 1075 Osman, I., 112, 1086 Ostland, M., 556, 606, 1085 Othello, 186 O TTER , 360, 364 OUPM, 545, 552 outcome, 482, 667 out of vocabulary, 864 Overbeek, R., 360, 1092 overﬁtting, 705, 705–706, 736, 802, 805 overgeneration, 892 overhypotheses, 798Overmars, M., 1013, 1078 overriding, 456 Owens, A. J., 156, 1072 OWL, 469 P P(probability vector), 487 P(s/prime|s,a)(transition model), 646, 832 PAC learning, 714, 716, 759 Padgham, L., 59, 1084 Page, C. D., 800, 1069 ,1084 Page, L., 870, 884, 1067 PageRank, 870 Palacios, H., 433, 1084 Palay, A. J., 191, 1084 Palmer, D. A., 922, 1084 Palmer, J., 287, 1080 Palmer, S., 968, 1084 Palmieri, G., 761, 1073 Panini, 16, 919 Papadimitriou, C. H., 154, 157, 277, 685, 686, 883, 1059, 1070 ,1079 , 1084 Papadopoulo, T., 968, 1072 Papavassiliou, V ., 855, 1084 Papert, S., 22, 761, 1082 PARADISE , 189 paradox, 471, 641 Allais, 620Ellsberg, 620St.

Token 23443:
Petersburg, 637 parallel distributed processing, see neural network parallelism AND-, 342 OR-, 342 parallel lines, 931parallel search, 112 parameter, 520, 806parameter independence, 812 parametric model, 737 paramodulation, 354, 359 Parekh, R., 921, 1084 Pareto dominated, 668 Pareto optimal, 668 Parisi, D., 921, 1071 Parisi, G., 555, 1084 Parisi, M. M. G., 278, 1084 Park, S., 356, 1075 Parker, A., 192, 1084 Parker, D. B., 761, 1084 Parker, L. E., 1013, 1084 Parr, R., 686, 854, 856, 857, 1050, 1074 , 1077–1079 ,1084 ,1087 Parrod, Y ., 432, 1064 parse tree, 890 parsing, 892, 892–897 Partee, B. H., 920, 1086 partial assignment, 203 partial evaluation, 799 partial observability, 180, 658partial program, 856 P ARTICLE -FILTERING ,598 particle ﬁltering, 597, 598, 603, 605 Rao-Blackwellized, 605, 1012 partition, 441 part of, 441 part of speech, 888 Parzen, E., 827, 1085 Parzen window, 827Pasca, M., 885, 1071 ,1085 Pascal’s wager, 504, 637 Pascal, B., 5, 9, 504 Pasero, R., 314, 358, 1069 Paskin, M., 920, 1085 P ASSIVE -ADP-A GENT ,834 PASSIVE -TD-A GENT ,837 passive learning, 831 Pasula, H., 556, 605, 606, 1081 ,1085 Patashnik, O., 194, 1085 Patel-Schneider, P., 471, 1064 path, 67, 108, 403 loopy, 75 redundant, 76 path consistency, 210, 228 path cost, 68, 108 PAT HFI NDE R , 552 path planning, 986 Patil, R., 471, 894, 920, 1068 ,1071 Patrick, B. G., 111, 1085 Patrinos, A., 27, 1069 pattern database, 106, 112, 379 disjoint, 107 pattern matching, 333 Paul, R. P., 1013, 1085

Token 23444:


Token 23445:
1120 Index Paulin-Mohring, C., 359, 1066 Paull, M., 277, 1072 Pauls, A., 920, 1085 Pavlovic, V ., 553, 1093 Pax-6 gene, 966 payoff function, 162, 667 Pazzani, M., 505, 826, 1071 PCFG lexicalized, 897, 919, 920 P controller, 998 PD controller, 999 PDDL (Planing Domain Deﬁnition Language), 367 PDP (parallel distributed processing), 761 Peano, G., 313, 1085 Peano axioms, 303, 313, 333 Pearce, J., 230, 1085 Pearl, J., 26, 61, 92, 110–112, 154, 191, 229, 509, 511, 517, 549,552–555, 557, 558, 644, 826,827, 1070 ,1073, 1074 ,1076 , 1078 ,1085 Pearson, J., 230, 1085 PEAS description, 40,4 2 Pease, A., 469, 1084, 1085 Pecheur, C., 356, 1075 Pednault, E. P. D., 394, 434, 1085 peeking, 708, 737 P EGASUS , 850, 852, 859 Peirce, C. S., 228, 313, 454, 471, 920, 1085 Pelikan, M., 155, 1085 Pell, B., 60, 432, 1083 Pemberton, J. C., 157, 1085 penalty, 56Penberthy, J. S., 394, 1085 Peng, J., 855, 1085 P ENGI , 434 penguin, 435 Penix, J., 356, 1075 Pennachin, C., 27, 1074 Pennsylvania, Univ.

Token 23446:
of, 14 Penn Treebank, 881, 895 Penrose, R., 1023, 1085 Pentagon Papers, 638Peot, M., 433, 554, 1085 ,1088 percept, 34 perception, 34, 305, 928, 928–965 perception layer, 1005 perceptron, 20, 729, 729–731, 761 convergence theorem, 20 learning rule, 724 network, 729 representational power, 22 sigmoid, 729percept schema, 416 percept sequence, 34,3 7 Pereira, F., 28, 339, 341, 470, 759, 761, 884, 885, 889, 919, 1025, 1071 , 1074 ,1079 ,1083 ,1085 ,1088 , 1091 Pereira, L. M., 341, 1091 Peres, Y ., 278, 604, 605, 1064 ,1080, 1081 Perez, P., 961, 1080 perfect information, 666 perfect recall, 675 performance element, 55,5 6 performance measure, 37, 40, 59, 481, 611 Perkins, T., 439, 1089 Perlis, A., 1043, 1085 Perona, P., 967, 1081 perpetual punishment, 674 perplexity, 863 Perrin, B. E., 605, 1085 persistence action, 380 persistence arc, 594 persistent (variable), 1061 persistent failure model, 593 Person, C., 854, 1083 perspective, 966 perspective projection, 930 Pesch, E., 432, 1066 Peshkin, M., 156, 1092 pessimistic description (of an action), 412 Peters, S., 920, 1071 Peterson, C., 555, 1085 Petrie, K., 230, 1073 Petrie, T., 604, 826, 1065 Petrik, M., 434, 1085 Petrov, S., 896, 900, 920, 1085 Pfeffer, A., 191, 541, 556, 687, 1078 , 1085 Pfeifer, G., 472, 1071 Pfeifer, R., 1041, 1085 phase transition, 277 phenomenology, 1026 Philips, A.

Token 23447:
B., 154, 229, 1082 Philo of Megara, 275philosophy, 5–7, 59, 1020–1043 phone (speech sound), 914 phoneme, 915 phone model, 915 phonetic alphabet, 914 photometry, 932 photosensitive spot, 963 phrase structure, 888, 919 physicalism, 1028 , 1041 physical symbol system, 18Pi, X., 604, 1083 Piccione, C., 687, 1093 Pickwick, Mr., 1026 pictorial structure model, 958 PID controller, 999 Pieper, G., 360, 1092 pigeons, 13 Pijls, W., 191, 1085 pineal gland, 1027 Pineau, J., 686, 1013, 1085 Pinedo, M., 432, 1085 ping-pong, 32, 830 pinhole camera, 930 Pinkas, G., 229, 1085 Pinker, S., 287, 288, 314, 921, 1085 , 1087 Pinto, D., 885, 1085 Pipatsrisawat, K., 277, 1085 Pippenger, N., 434, 1080 Pisa, tower of, 56 Pistore, M., 275, 1088 pit, bottomless, 237Pitts, W., 15, 16, 20, 278, 727, 731, 761, 963, 1080 ,1082 pixel, 930 PL-FC-E NTAILS ?,258 PL-R ESOLUTION ,255 Plaat, A., 191, 1085 Place, U. T., 1041, 1085 PLAN-ERS1, 432 PLAN-ROUTE ,270 planetary rover, 971 PLANEX , 434 Plankalk¨ ul, 14 plan monitoring, 423 PLANNER , 24, 358 planning, 52, 366–436 and acting, 415–417 as constraint satisfaction, 390 as deduction, 388as reﬁnement, 390as satisﬁability, 387 blocks world, 20 case-based, 432 conformant, 415, 417–421, 431, 433, 994 contingency, 133, 415, 421–422, 431 decentralized, 426 ﬁne-motion, 994 graph, 379, 379–386, 393 serial, 382 hierarchical, 406–415, 431 hierarchical task network, 406 history of, 393 linear, 394 multibody, 425, 426–428

Token 23448:


Token 23449:
Index 1121 multieffector, 425 non-interleaved, 398 online, 415 reactive, 434 regression, 374, 394 route, 19search space, 373–379 sensorless, 415, 417–421 planning and control layer, 1006 plan recognition, 429 PlanSAT, 372 bounded, 372 plateau (in local search), 123 Plato, 275, 470, 1041 Platt, J., 760, 1085 player (in a game), 667 Plotkin, G., 359, 800, 1085 Plunkett, K., 921, 1071 ply,164 poetry, 1 Pohl, I., 110, 111, 118, 1085 point-to-point motion, 986 pointwise product, 526 poker, 507 Poland, 470 Poli, R., 156, 1079 ,1085 Policella, N., 28, 1068 policy, 176, 434, 647, 684, 994 evaluation, 656, 832 gradient, 849 improvement, 656 iteration, 656, 656–658, 685, 832 asynchronous, 658 modiﬁed, 657 loss, 655 optimal, 647 proper, 650, 858 search, 848, 848–852, 1002 stochastic, 848 value, 849 P OLICY -ITERA TION ,657 polite convention (Turing’s), 1026, 1027Pollack, M. E., 434, 1069 polytree, 528, 552, 575 POMDP-V ALUE -ITERA TION ,663 Pomerleau, D. A., 1014, 1085 Ponce, J., 968, 1072 Ponte, J., 884, 922, 1085 ,1093 Poole, D., 2, 59, 553, 556, 639, 1078 , 1085 ,1093 Popat, A. C., 29, 921, 1067 Popescu, A.-M., 885, 1072 Popper, K. R., 504, 759, 1086 population (in genetic algorithms), 127 Porphyry, 471 Port-Royal Logic, 636Porter, B., 473, 1091 Portner, P., 920, 1086 Portuguese, 778 pose, 956, 958, 975 Posegga, J., 359, 1065 positive example, 698 positive literal, 244 positivism, logical, 6 possibility axiom, 388 possibility theory, 557 possible world, 240, 274, 313, 451, 540 Post, E. L., 276, 1086 post-decision disappointment, 637 posterior probability, seeprobability, conditional potential ﬁeld, 991 potential ﬁeld control, 999Poultney, C., 762, 1086 Poundstone, W., 687, 1086 Pourret, O., 553, 1086 Powers, R., 857, 1088 Prade, H., 557, 1071 Prades, J. L. P., 637, 1086 Pradhan, M., 519, 552, 1086 pragmatic interpretation, 904pragmatics, 904 Prawitz, D., 358, 1086 precedence constraints, 204 precision, 869 precondition, 367 missing, 423 precondition axiom, 273 predecessor, 91 predicate, 902 predicate calculus, seelogic, ﬁrst-order predicate indexing, 328 predicate symbol, 292 prediction, 139, 142, 573, 603 preference, 482, 612 monotonic, 616 preference elicitation, 615 preference independence, 624 premise, 244 president, 449 Presley, E., 448 Press, W. H., 155, 1086 Preston, J., 1042, 1086 Price, B., 686, 1066 Price Waterhouse, 431 Prieditis, A. E., 105, 112, 119, 1083 , 1086 Princeton, 17Principia Mathematica ,1 8 Prinz, D. G., 192, 1086 P RIOR -SAMPLE ,531 prioritized sweeping, 838, 854priority queue, 80, 858 prior knowledge, 39, 768, 778, 787 prior probability, 485, 503 prismatic joint, 976 prisoner’s dilemma, 668 private value, 679 probabilistic network, seeBayesian network probabilistic roadmap, 993 probability, 9, 26, 480–565, 1057–1058 alternatives to, 546 axioms of, 488–490 conditional, 485, 503, 514 conjunctive, 514 density function, 487, 1057 distribution, 487, 522 history, 506judgments, 516marginal, 492 model, 484, 1057 open-universe, 545 prior, 485, 503 theory, 289, 482, 636 probably approximately correct (PAC), 714, 716, 759 P ROBCUT, 175 probit distribution, 522, 551, 554 problem, 66, 108 airport-siting, 643 assembly sequencing, 74 bandit, 840, 855 conformant, 138 constraint optimization, 207 8-queens, 71, 1098-puzzle, 102, 105 formulation, 65, 68–69 frame, 266, 279 generator, 56 halting, 325 inherently hard, 1054–1055million queens, 221, 229missionaries and cannibals, 115 monkey and bananas, 113, 396 nqueens, 263 optimization, 121 constrained, 132 piano movers, 1012 real-world, 69 relaxed, 105, 376 robot navigation, 74 sensorless, 138 solving, 22touring, 74 toy,69 traveling salesperson, 74underconstrained, 263

Token 23450:


Token 23451:
1122 Index VLSI layout, 74, 125 procedural approach, 236, 286 procedural attachment, 456, 466 process, 447, 447 PRODIGY , 432 production, 48 production system, 322, 336, 357, 358 product rule, 486, 495 PROGOL , 789, 795, 797, 800 programming language, 285 progression, 393 Prolog, 24, 339, 358, 394, 793, 899 parallel, 342 Prolog Technology Theorem Prover (PTTP), 359 pronunciation model, 917 proof, 250 proper policy, 650, 858 property (unary relation), 288 proposal distribution, 565 proposition probabilistic, 483symbol, 244 propositional attitude, 450 propositionalization, 324, 357, 368, 544 propositional logic, 235, 243–247, 274, 286 proprioceptive sensor, 975 P ROSPECTOR , 557 Prosser, P., 229, 1086 protein design, 75 prototypes, 896 Proust, M., 910 Provan, G. M., 519, 552, 1086 pruning, 98, 162, 167, 705 forward, 174 futility, 185 in contingency problems, 179 in EBL, 783 pseudocode, 1061 pseudoexperience, 837 pseudoreward, 856 PSPACE, 372, 1055 PSPACE-complete, 385, 393psychological reasoning, 473 psychology, 12–13 experimental, 3, 12 psychophysics, 968public key encryption, 356 Puget, J.-F., 230, 800, 1073 ,1087 Pullum, G. K., 889, 920, 921, 1076 , 1086 PUMA, 1011 Purdom, P., 230, 1067 pure strategy, 667 pure symbol, 260Puterman, M. L., 60, 685, 1086 Putnam, H., 60, 260, 276, 350, 358, 505, 1041, 1042, 1070 ,1086 Puzicha, J., 755, 762, 1065 Pylyshyn, Z. W., 1041, 1086 Q Q(s,a)(value of action in state), 843 Q-function, 627, 831 Q-learning, 831, 843, 844, 848, 973 Q-L EARNING -AGENT ,844 QA3, 314 QALY , 616, 637 Qi, R., 639, 1093 QUACKLE , 187 quadratic dynamical systems, 155 quadratic programming, 746 qualia, 1033 qualiﬁcation problem, 268, 481, 1024, 1025 qualitative physics, 444, 472 qualitative probabilistic network, 557, 624 quantiﬁcation, 903 quantiﬁer, 295, 313 existential, 297 in logic, 295–298 nested, 297–298 universal, 295–296, 322 quantization factor, 914 quasi-logical form, 904 Qubic, 194 query (logical), 301 query language, 867 query variable, 522 question answering, 872, 883 queue, 79 FIFO, 80,8 1 LIFO, 80,8 5 priority, 80, 858 Quevedo, T., 190 quiescence, 174 Quillian, M. R., 471, 1086 Quine, W. V ., 314, 443, 469, 470, 1086 Quinlan, J. R., 758, 764, 791, 793, 800, 1086 Quirk, R., 920, 1086 QX TRACT , 885 R R1, 24, 336, 358 Rabani, Y ., 155, 1086 Rabenau, E., 28, 1068 Rabideau, G., 431, 1073Rabiner, L. R., 604, 922, 1086 Rabinovich, Y ., 155, 1086 racing cars, 1050 radar, 10 radial basis function, 762 Radio Rex, 922 Raedt, L. D., 556, 1078 Raghavan, P., 883, 884, 1081 ,1084 Raiffa, H., 9, 621, 625, 638, 687, 1078 , 1081 Rajan, K., 28, 60, 431, 1064 ,1077 Ralaivola, L., 605, 1085 Ralphs, T. K., 112, 1086 Ramakrishnan, R., 275, 1080 Ramanan, D., 960, 1086 Ramsey, F. P., 9, 504, 637, 1086 RAND Corporation, 638 randomization, 35, 50 randomized weighted majority algorithm, 752 random restart, 158, 262random set, 551 random surfer model, 871 random variable, 486, 515 continuous, 487, 519, 553 indexed, 555 random walk, 150, 585 range ﬁnder, 973 laser, 974 range sensor array, 981Ranzato, M., 762, 1086 Rao, A., 61, 1092 Rao, B., 604, 1076 Rao, G., 678Raphael, B., 110, 191, 358, 1074, 1075 Raphson, J., 154, 760, 1086 rapid prototyping, 339Raschke, U., 1013, 1069 Rashevsky, N., 10, 761, 1086 Rasmussen, C. E., 827, 1086 Rassenti, S., 688, 1086 Ratio Club, 15 rational agent, 4, 4–5, 34, 36–38, 59, 60, 636, 1044 rationalism, 6, 923 rationality, 1, 36–38 calculative, 1049 limited, 5 perfect, 5, 1049 rational thought, 4 Ratner, D., 109, 1086 rats, 13Rauch, H. E., 604, 1086 Rayner, M., 784, 1087 Rayson, P., 921, 1080 Rayward-Smith, V ., 112, 1086

Token 23452:


Token 23453:
Index 1123 RBFS, 99–101, 109 RBL, 779, 784–787, 798 RDF, 469 reachable set, 411 reactive control, 1001 reactive layer, 1004 reactive planning, 434 real-world problem, 69 realizability, 697 reasoning, 4, 19, 234 default, 458–460, 547 intercausal, 548 logical, 249–264, 284 uncertain, 26 recall, 869 Rechenberg, I., 155, 1086 recognition, 929 recommendation, 539reconstruction, 929 recurrent network, 729, 762 R ECURSIVE -BEST-FIRST-SEARCH ,99 RECURSIVE -DLS, 88 recursive deﬁnition, 792 recursive estimation, 571 Reddy, R., 922, 1081 reduction, 1059Reeson, C. G., 228, 1086 Reeves, C., 112, 1086 Reeves, D., 688, 1092 reference class, 491, 505 reference controller, 997 reference path, 997 referential transparency, 451 reﬁnement (in hierarchical planning), 407 reﬂectance, 933, 952 R EFLEX -VACUUM -AGENT ,48 reﬂex agent, 48, 48–50, 59, 647, 831 refutation, 250 refutation completeness, 350 regex, 874 Regin, J., 228, 1086 regions, 941 regression, 393, 696, 760 linear, 718, 810 nonlinear, 732 tree, 707 regression to the mean, 638 regret, 620, 752 regular expression, 874 regularization, 713, 721 Reichenbach, H., 505, 1086 Reid, D. B., 606, 1086 Reid, M., 111, 1079 Reif, J., 1012, 1013, 1068 ,1086 reiﬁcation, 440REINFORCE , 849, 859 reinforcement, 830 reinforcement learning, 685, 695, 830–859, 1025 active, 839–845Bayesian, 835 distributed, 856 generalization in, 845–848 hierarchical, 856, 1046 multiagent, 856off-policy, 844 on-policy, 844 Reingold, E. M., 228, 1066 Reinsel, G., 604, 1066 Reiter, R., 279, 395, 471, 686, 1066 , 1086 R EJECTION -SAMPLING ,533 rejection sampling, 532 relation, 288 relational extraction, 874 relational probability model (RPM), 541, 552 relational reinforcement learning, 857 relative error, 98 relaxed problem, 105, 376 relevance, 246, 375, 779, 799 relevance (in information retrieval), 867 relevance-based learning (RBL), 779, 784–787, 798 relevant-states, 374 Remote Agent, 28, 60, 356, 392, 432 REMOTE AGENT ,2 8 renaming, 331 rendering model, 928 Renner, G., 155, 1086 R´enyi, A., 504, 1086 repeated game, 669, 673 replanning, 415, 422–434 REPOP, 394 representation, seeknowledge representation atomic, 57 factored, 58 structured, 58 representation theorem, 624 REPRODUCE ,129 reserve bid, 679 resolution, 19, 21, 253, 252–256, 275, 314, 345–357, 801 closure, 255, 351 completeness proof for, 350 input, 356 inverse, 794, 794–797, 800 linear, 356 strategies, 355–356 resolvent, 252, 347, 794resource constraints, 401 resources, 401–405, 430response, 13 restaurant hygiene inspector, 183 result, 368 result set, 867 rete, 335, 358 retrograde, 176 reusable resource, 402 revelation principle, 680 revenue equivalence theorem, 682 Reversi, 186 revolute joint, 976 reward, 56, 646, 684, 830 additive, 649 discounted, 649 shaping, 856 reward-to-go, 833 reward function, 832, 1046 rewrite rule, 364, 1060 Reynolds, C. W., 435, 1086 Riazanov, A., 359, 360, 1086 Ribeiro, F., 195, 1091 Rice, T. R., 638, 1082 Rich, E., 2, 1086 Richards, M., 195, 1086 Richardson, M., 556, 604, 1071 ,1086 Richardson, S., 554, 1073 Richter, S., 395, 1075 ,1086 ridge (in local search), 123 Ridley, M., 155, 1086 Rieger, C., 24, 1086 Riesbeck, C., 23, 358, 921, 1068 ,1088 right thing, doing the, 1, 5, 1049 Riley, J., 688, 1087 Riley, M., 889, 1083 Riloff, E., 885, 1077 ,1087 Rink, F. J., 553, 1083 Rintanen, J., 433, 1087 Ripley, B. D., 763, 1087 risk aversion, 617 risk neutrality, 618 risk seeking, 617 Rissanen, J., 759, 1087 Ritchie, G. D., 800, 1087 Ritov, Y ., 556, 606, 1085 Rivest, R., 759, 1059, 1069 ,1087 RMS (root mean square), 1059 Robbins algebra, 360 Roberts, G., 30, 1071 Roberts, L. G., 967, 1087 Roberts, M., 192, 1065 Robertson, N., 229, 1087 Robertson, S., 868 Robertson, S. E., 505, 884, 1069 ,1087 Robinson, A., 314, 358, 360, 1087

Token 23454:
1124 Index Robinson, G., 359, 1092 Robinson, J.

Token 23455:
A., 19, 276, 314, 350, 358, 1087 Robocup, 1014 robot, 971, 1011 game (with humans), 1019hexapod, 1001 mobile, 971 navigation, 74 soccer, 161, 434, 1009 robotics, 3, 592, 971–1019 robust control, 994 Roche, E., 884, 1087 Rochester, N., 17, 18, 1020, 1082 Rock, I., 968, 1087 Rockefeller Foundation, 922 R¨oger, G., 111, 1075 rollout, 180 Romania, 65, 203 Roomba, 1009 Roossin, P., 922, 1067 root mean square, 1059 Roscoe, T., 275, 1080 Rosenblatt, F., 20, 761, 1066 ,1087 Rosenblatt, M., 827, 1087 Rosenblitt, D., 394, 1081 Rosenbloom, P. S., 26, 27, 336, 358, 432, 799, 1047, 1075 ,1079 Rosenblueth, A., 15, 1087 Rosenbluth, A., 155, 554, 1082 Rosenbluth, M., 155, 554, 1082 Rosenholtz, R., 953, 968, 1081 Rosenschein, J. S., 688, 1087 ,1089 Rosenschein, S. J., 60, 278, 279, 1077 , 1087 Ross, P. E., 193, 1087 Ross, S. M., 1059, 1087 Rossi, F., 228, 230, 1066 ,1087 rotation, 956Roth, D., 556, 1070 Roughgarden, T., 688, 1084 Roussel, P., 314, 358, 359, 1069 ,1087 route ﬁnding, 73 Rouveirol, C., 800, 1087 Roveri, M., 396, 433, 1066 ,1068 Rowat, P. F., 1013, 1087 Roweis, S. T., 554, 605, 1087 Rowland, J., 797, 1078 Rowley, H., 968, 1087 Roy, N., 1013, 1087 Rozonoer, L., 760, 1064 RPM, 541, 552 RSA (Rivest, Shamir, and Adelman), 356 RSAT, 277 Rubik’s Cube, 105Rubin, D., 604, 605, 826, 827, 1070 , 1073 ,1087 Rubinstein, A., 688, 1084 rule, 244 causal, 317, 517 condition–action, 48 default, 459 diagnostic, 317, 517 if–then, 48, 244implication, 244situation–action, 48uncertain, 548 rule-based system, 547, 1024 with uncertainty, 547–549 Rumelhart, D. E., 24, 761, 1087 Rummery, G. A., 855, 1087 Ruspini, E. H., 557, 1087 Russell, A., 111, 1071 Russell, B., 6, 16, 18, 357, 1092 Russell, J. G. B., 637, 1087 Russell, J. R., 360, 1083 Russell, S. J., 111, 112, 157, 191, 192, 198, 278, 345, 432, 444, 556, 604–606, 686, 687, 799, 800,826, 855–857, 1012, 1048, 1050,1064 ,1066 ,1069–1071 ,1073 , 1076, 1077 ,1081–1085 ,1087 , 1090 ,1092, 1093 Russia, 21, 192, 489 Rustagi, J. S., 554, 1087 Ruzzo, W. L., 920, 1074 Ryan, M., 314, 1076 R YBKA , 186, 193 Rzepa, H. S., 469, 1083 S S-set, 774 Sabharwal, A., 277, 395, 1074 ,1076 Sabin, D., 228, 1087 Sacerdoti, E. D., 394, 432, 1087 Sackinger, E., 762, 967, 1080 Sadeh, N. M., 688, 1064 Sadri, F., 470, 1087 Sagiv, Y ., 358, 1065 Sahami, M., 29, 883, 884, 1078 ,1087 Sahin, N. T., 288, 1087 Sahni, S., 110, 1076 SAINT , 19, 156 St. Petersburg paradox, 637, 641 Sakuta, M., 192, 1087 Salisbury, J., 1013, 1081 Salmond, D. J., 605, 1074 Salomaa, A., 919, 1087 Salton, G., 884, 1087 Saltzman, M. J., 112, 1086SAM, 360 sample complexity, 715 sample space, 484 sampling, 530–535sampling rate, 914 Samuel, A. L., 17, 18, 61, 193, 850, 854, 855, 1087 Samuelson, L., 688, 1081 Samuelson, W., 688, 1087 Samuelsson, C., 784, 1087 Sanders, P., 112, 1069 Sankaran, S., 692, 1080 Sanna, R., 761, 1073 Sanskrit, 468, 919 Santorini, B., 895, 921, 1081 S APA, 431 Sapir–Whorf hypothesis, 287 Saraswat, V ., 228, 1091 Sarawagi, S., 885, 1087 SARSA, 844 Sastry, S., 60, 606, 852, 857, 1013, 1075 ,1084 SAT, 250 Satia, J. K., 686, 1087 satisfaction (in logic), 240 satisﬁability, 250, 277 satisﬁability threshold conjecture, 264, 278 satisﬁcing, 10, 1049 SATMC, 279 Sato, T., 359, 556, 1087 ,1090 SATP LAN, 387, 392, 396, 402, 420, 433 SAT PLAN ,272 saturation, 351 SAT Z, 277 Saul, L. K., 555, 606, 1077 ,1088 Saund, E., 883, 1087 Savage, L. J., 489, 504, 637, 1088 Sayre, K., 1020, 1088 scaled orthographic projection, 932 scanning lidars, 974 Scarcello, F., 230, 472, 1071 ,1074 scene, 929 Schabes, Y ., 884, 1087 Schaeffer, J., 112, 186, 191, 194, 195, 678, 687, 1066 ,1069 ,1081 , 1085 ,1088 Schank, R. C., 23, 921, 1088 Schapire, R. E., 760, 761, 884, 1072 , 1088 Scharir, M., 1012, 1088 Schaub, T., 471, 1070 Schauenberg, T., 678, 687, 1066 scheduling, 403, 401–405 Scheines, R., 826, 1089 schema (in a genetic algorithm), 128

Token 23456:


Token 23457:
Index 1125 schema acquisition, 799 Schervish, M. J., 506, 1070 Schickard, W., 5 Schmid, C., 968, 1088 Schmidt, G., 432, 1066 Schmolze, J. G., 471, 1088 Schneider, J., 852, 1013, 1065 Schnitzius, D., 432, 1070 Schnizlein, D., 687, 1091 Schoenberg, I. J., 761, 1083 Sch¨olkopf, B., 760, 762, 1069, 1070 , 1088 Schomer, D., 288, 1087 Sch¨oning, T., 277, 1088 Schoppers, M. J., 434, 1088 Schrag, R. C., 230, 277, 1065 Schr¨oder, E., 276, 1088 Schubert, L. K., 469, 1076 Schulster, J., 28, 1068 Schultz, W., 854, 1088 Schultze, P., 112, 1079 Schulz, D., 606, 1012, 1067 ,1088 Schulz, S., 360, 1088 ,1090 Schumann, J., 359, 360, 1071 ,1080 Sch¨utze, H., 883–885, 920, 921, 1081 , 1088 Sch¨utze, H., 862, 883, 1078 Schwartz, J. T., 1012, 1088 Schwartz, S. P., 469, 1088 Schwartz, W. B., 505, 1074 scientiﬁc discovery, 759Scott, D., 555, 1088 Scrabble, 187, 195 scruffy vs.neat, 25 search, 22, 52, 66, 108 A*, 93–99 alpha–beta, 167–171, 189, 191 B*, 191 backtracking, 87, 215, 218–220, 222, 227 beam, 125, 174 best-ﬁrst, 92, 108 bidirectional, 90–112 breadth-ﬁrst, 81, 81–83, 108, 408 conformant, 138–142continuous space, 129–133, 155 current-best-hypothesis, 770 cutting off, 173–175depth-ﬁrst, 85 , 85–87, 108, 408 depth-limited, 87, 87–88 general, 108 greedy best-ﬁrst, 92,9 2 heuristic, 81, 110 hill-climbing, 122–125, 150 in a CSP, 214–222incremental belief-state, 141informed, 64, 81, 92, 92–102, 108 Internet, 464 iterative deepening, 88, 88–90, 108, 110, 173, 408 iterative deepening A*, 99, 111 learning to, 102 local, 120–129, 154, 229, 262–263, 275, 277 greedy, 122 local, for CSPs, 220–222 local beam, 125, 126 memory-bounded, 99–102, 111 memory-bounded A*, 101, 101–102, 112 minimax, 165–168, 188, 189 nondeterministic, 133–138 online, 147, 147–154, 157 parallel, 112 partially observable, 138–146 policy, 848, 848–852, 1002 quiescence, 174 real-time, 157, 171–175 recursive best-ﬁrst (RBFS), 99–101, 111 simulated annealing, 125 stochastic beam, 126 strategy, 75 tabu, 154, 222 tree, 163 uniform-cost, 83, 83–85, 108 uninformed, 64, 81, 81–91, 108, 110 search cost, 80 search tree, 75, 163 Searle, J. R., 11, 1027, 1029–1033, 1042, 1088 Sebastiani, F., 884, 1088 Segaran, T., 688, 763, 1088 segmentation (of an image), 941 segmentation (of words), 886, 913 Sejnowski, T., 763, 850, 854, 1075 , 1083 ,1090 Self, M., 826, 1068 Selfridge, O. G., 17 Selman, B., 154, 229, 277, 279, 395, 471, 1074 ,1077, 1078 , 1088 semantic interpretation, 900–904, 920semantic networks, 453–456, 468, 471 semantics, 240, 860 database, 300, 343, 367, 540 logical, 274 Semantic Web, 469 semi-supervised learning, 695 semidecidable, 325, 357semidynamic environment, 44 Sen, S., 855, 1084 sensitivity analysis, 635sensor, 34, 41, 928 active, 973 failure, 592, 593 model, 579, 586, 603passive, 973 sensor interface layer, 1005 sensorless planning, 415, 417–421 sensor model, 566, 579, 586, 603, 658, 928, 979 sentence atomic, 244, 294–295, 299 complex, 244, 295 in a KB, 235, 274 as physical conﬁguration, 243 separator (in Bayes net), 499 sequence form, 677 sequential environment, 43 sequential decision problem, 645–651, 685 sequential environment, 43 sequential importance-sampling resampling, 605 serendipity, 424 Sergot, M., 470, 1079 serializable subgoals, 392 Serina, I., 395, 1073 Sestoft, P., 799, 1077 set (in ﬁrst-order logic), 304 set-cover problem, 376 SETHEO, 359 set of support, 355 set semantics, 367 Settle, L., 360, 1074 Seymour, P. D., 229, 1087 SGP, 395, 433 SGPLAN, 387 Sha, F., 1025, 1088 Shachter, R. D., 517, 553, 554, 559, 615, 634, 639, 687, 1071 ,1088 ,1090 shading, 933, 948, 952–953 shadow, 934 Shafer, G., 557, 1088 shaft decoder, 975 Shah, J., 967, 1083 Shahookar, K., 110, 1088 Shaked, T., 885, 1072 Shakey, 19, 60, 156, 393, 397, 434, 1011 Shalla, L., 359, 1092 Shanahan, M., 470, 1088 Shankar, N., 360, 1088 Shannon, C. E., 17, 18, 171, 192, 703, 758, 763, 883, 913, 1020, 1082 , 1088 Shaparau, D., 275, 1088 shape, 957

Token 23458:


Token 23459:
1126 Index from shading, 968 Shapiro, E., 800, 1088 Shapiro, S. C., 31, 1088 Shapley, S., 687, 1088 Sharir, M., 1013, 1074 Sharp, D. H., 761, 1069 Shatkay, H., 1012, 1088 Shaw, J. C., 109, 191, 276, 1084 Shawe-Taylor, J., 760, 1069 Shazeer, N. M., 231, 1080 Shelley, M., 1037, 1088 Sheppard, B., 195, 1088 Shewchuk, J., 1012, 1070 Shi, J., 942, 967, 1088 Shieber, S., 30, 919, 1085 ,1088 Shimelevich, L. I., 605, 1093 Shin, M. C., 685, 1086 Shinkareva, S. V ., 288, 1082 Shmoys, D. B., 110, 405, 432, 1080 Shoham, Y ., 60, 195, 230, 359, 435, 638, 688, 857, 1064 ,1079, 1080 , 1088 short-term memory, 336 shortest path, 114Shortliffe, E. H., 23, 557, 1067 ,1088 shoulder (in state space), 123 Shpitser, I., 556, 1085 S HRDLU , 20, 23, 370 Shreve, S. E., 60, 1066 sibyl attack, 541 sideways move (in state space), 123 Sietsma, J., 762, 1088 SIGART, 31 sigmoid function, 726 sigmoid perceptron, 729 signal processing, 915 signiﬁcance test, 705 signs, 888 Siklossy, L., 432, 1088 Silver, D., 194, 1073 Silverstein, C., 884, 1088 Simard, P., 762, 967, 1080 Simmons, R., 605, 1012, 1088 ,1091 Simon’s predictions, 20 Simon, D., 60, 1088 Simon, H. A., 3, 10, 17, 18, 30, 60, 109, 110, 191, 276, 356, 393, 639,800, 1049, 1077 ,1079 ,1084 , 1088, 1089 Simon, J. C., 277, 1089 Simonis, H., 228, 1089 Simons, P., 472, 1084 S IMPLE -REFLEX -AGENT ,49 simplex algorithm, 155 SIMULA TED -ANNEALING ,126simulated annealing, 120, 125, 153, 155, 158, 536 simulation of world, 1028 simultaneous localization and mapping (SLAM), 982 Sinclair, A., 124, 155, 1081 ,1086 Singer, P. W., 1035, 1089 Singer, Y ., 604, 884, 1072 ,1088 Singh, M. P., 61, 1076 Singh, P., 27, 439, 1082 ,1089 Singh, S., 1014, 1067 Singh, S. P., 157, 685, 855, 856, 1065 , 1077, 1078 ,1090 Singhal, A., 870, 1089 singly connected network, 528 singular, 1056 singular extension, 174 singularity, 12 technological, 1038 sins, seven deadly, 122 SIPE, 431, 432, 434 SIR, 605 Sittler, R .

Token 23460:
W., 556, 606, 1089 situated agent, 1025 situation, 388 situation calculus, 279, 388, 447 Sjolander, K., 604, 1079 skeletonization, 986, 991 Skinner, B. F., 15, 60, 1089 Skolem, T., 314, 358, 1089 Skolem constant, 323, 357 Skolem function, 346, 358 skolemization, 323, 346 slack, 403 Slagle, J. R., 19, 1089 SLAM, 982 slant, 957 Slate, D. J., 110, 1089 Slater, E., 192, 1089 Slattery, S., 885, 1069 Sleator, D., 920, 1089 sliding-block puzzle, 71, 376 sliding window, 943 Slocum, J., 109, 1089 Sloman, A., 27, 1041, 1082 ,1089 Slovic, P., 2, 638, 1077 small-scale learning, 712 Smallwood, R. D., 686, 1089 Smarr, J., 883, 1078 Smart, J. J. C., 1041, 1089 SMA∗, 109 Smith, A., 9 Smith, A. F. M., 605, 811, 826, 1065 , 1074 ,1090 Smith, B., 28, 60, 431, 470, 1077 ,1089 Smith, D. A., 920, 1089Smith, D. E., 156, 157, 345, 359, 363, 395, 433, 1067 ,1073 ,1079 , 1085 ,1089 ,1091 Smith, G., 112, 1086 Smith, J. E., 619, 637, 1089 Smith, J. M., 155, 688, 1089 Smith, J. Q., 638, 639, 1084 ,1089 Smith, M. K., 469, 1089 Smith, R. C., 1012, 1089 Smith, R. G., 61, 1067 Smith, S. J. J., 187, 195, 1089 Smith, V ., 688, 1086 Smith, W. D., 191, 553, 1065 ,1083 SMODELS , 472 Smola, A. J., 760, 1088 Smolensky, P., 24, 1089 smoothing, 574–576, 603, 822, 862, 863, 938 linear interpolation, 863 online, 580 Smullyan, R. M., 314, 1089 Smyth, P., 605, 763, 1074 ,1089 SNARC ,1 6 Snell, J., 506, 1074 Snell, M. B., 1032, 1089 SNLP, 394 Snyder, W., 359, 1064 SOAR, 26, 336, 358, 432, 799, 1047 soccer, 195social laws, 429 society of mind, 434 Socrates, 4 Soderland, S., 394, 469, 885, 1065 , 1072 ,1089 softbot, 41,6 1 soft margin, 748 softmax function, 848 soft threshold, 521 software agent, 41 software architecture, 1003 Soika, M., 1012, 1066 Solomonoff, R. J., 17, 27, 759, 1089 solution, 66, 68, 108, 134, 203, 668 optimal, 68 solving games, 163–167soma, 11 Sompolinsky, H., 761, 1064 sonar sensors, 973 Sondik, E. J., 686, 1089 sonnet, 1026 Sonneveld, D., 109, 1089 Sontag, D., 556, 1082 S¨orensson, N., 277, 1071 Sosic, R., 229, 1089 soul, 1041

Token 23461:


Token 23462:
Index 1127 soundness (of inference), 242, 247, 258, 274, 331 sour grapes, 37 Sowa, J., 473, 1089 Spaan, M. T. J., 686, 1089 space complexity, 80, 108 spacecraft assembly, 432 spam detection, 865 spam email, 886 Sparck Jones, K., 505, 868, 884, 1087 sparse model, 721 sparse system, 515 SPASS , 359 spatial reasoning, 473 spatial substance, 447 specialization, 771, 772 species, 25, 130, 439–441, 469, 817, 860, 888, 948, 1035, 1042 spectrophotometry, 935specularities, 933 specular reﬂection, 933 speech act, 904 speech recognition, 25, 912, 912–919, 922 sphex wasp, 39, 425 SPI (Symbolic Probabilistic Inference), 553 Spiegelhalter, D. J., 553–555, 639, 763, 826, 1069 ,1073 ,1080 ,1082 , 1089 Spielberg, S., 1040, 1089 S PIKE , 432 SPIN, 356 spin glass, 761 Spirtes, P., 826, 1089 split point, 707 Sproull, R. F., 639, 1072 Sputnik, 21 square roots, 47 SRI, 19, 314, 393, 638 Srinivasan, A., 797, 800, 1084 ,1089 Srinivasan, M. V ., 1045, 1072 Srivas, M., 356, 1089 Srivastava, B., 432, 1077 SSD (sum of squared differences), 940SSS* algorithm, 191 Staab, S., 469, 1089 stability of a controller, 998 static vs. dynamic, 977 strict, 998 stack, 80 Stader, J., 432, 1064 S TAGE , 154 STAHL , 800 Stallman, R. M., 229, 1089STAN, 395 standardizing apart, 327, 363, 375 Stanﬁll, C., 760, 1089 Stanford University, 18, 19, 22, 23, 314Stanhope Demonstrator, 276Staniland, J. R., 505, 1070 S TANLEY , 28, 1007, 1008, 1014, 1025 start symbol, 1060 state, 367 repeated, 75 world, 69 State-Action-Reward-State-Action (SARSA), 844 state abstraction, 377 state estimation, 145, 181, 269, 275, 570, 978 recursive, 145, 571 States, D. J., 826, 1076 state space, 67, 108 metalevel, 102 state variable missing, 423 static environment, 44 stationarity (for preferences), 649 stationarity assumption, 708 stationary distribution, 537, 573 stationary process, 568, 568–570, 603 statistical mechanics, 761 Steﬁk, M., 473, 557, 1089 Stein, J., 553, 1083 Stein, L. A., 1051, 1089 Stein, P., 192, 1078 Steiner, W., 1012, 1067 stemming, 870 Stensrud, B., 358, 1090 step cost, 68 Stephenson, T., 604, 1089 step size, 132 stereopsis, binocular, 948 stereo vision, 974 Stergiou, K., 228, 1089 Stern, H. S., 827, 1073 Sternberg, M. J. E., 797, 1089, 1090 Stickel, M. E., 277, 359, 884, 921, 1075, 1076 ,1089 ,1093 stiff neck, 496 Stiller, L., 176, 1089 stimulus, 13Stob, M., 759, 1084 stochastic beam search, 126 stochastic dominance, 622, 636 stochastic environment, 43 stochastic games, 177 stochastic gradient descent, 720 Stockman, G., 191, 1089 Stoffel, K., 469, 1089Stoica, I., 275, 1080 Stoic school, 275 Stokes, I., 432, 1064 Stolcke, A., 920, 1089 Stoljar, D., 1042, 1081 Stone, C. J., 758, 1067 Stone, M., 759, 1089 Stone, P., 434, 688, 1089 Stork, D. G., 763, 827, 966, 1071 ,1089 Story, W. E., 109, 1077 Strachey, C., 14, 192, 193, 1089, 1090 straight-line distance, 92 Strat, T. M., 557, 1087 strategic form, 667 strategy, 133, 163, 181, 667 strategy proﬁle, 667 Stratonovich, R. L., 604, 639, 1089 strawberries, enjoy, 1021 Striebel, C. T., 604, 1086 string (in logic), 471 STRIPS , 367, 393, 394, 397, 432, 434, 799 Stroham, T., 884, 1069 Strohm, G., 432, 1072 strong AI, 1020 , 1026–1033, 1040 strong domination, 668 structured representation, 58,6 4 Stuckey, P. J., 228, 359, 1077 ,1081 STUDENT ,1 9 stuff, 445 stupid pet tricks, 39 Stutz, J., 826, 1068 stylometry, 886 Su, Y ., 111, 1071 subcategory, 440 subgoal independence, 378 subjective case, 899 subjectivism, 491 submodularity, 644 subproblem, 106 Subrahmanian, V .

Token 23463:
S., 192, 1084 Subramanian, D., 278, 472, 799, 1050, 1068 ,1087 ,1089, 1090 substance, 445 spatial, 447 temporal, 447 substitutability (of lotteries), 612 substitution, 301, 323 subsumption in description logic, 456 in resolution, 356 subsumption architecture, 1003 subsumption lattice, 329 successor-state axiom, 267, 279, 389 successor function, 67Sudoku, 212

Token 23464:


Token 23465:
1128 Index Sulawesi, 223 SUMMATION ,1053 summer’s day, 1026 summing out, 492, 527 sum of squared differences, 940 Sun Microsystems, 1036Sunstein, C., 638, 1090 Sunter, A., 556, 1072 Superman, 286superpixels, 942 supervised learning, 695, 846, 1025 support vector machine, 744, 744–748, 754 sure thing, 617 surveillance, 1036survey propagation, 278 survival of the ﬁttest, 605 Sussman, G. J., 229, 394, 1089, 1090 Sussman anomaly, 394, 398 Sutcliffe, G., 360, 1090 Sutherland, G. L., 22, 1067 Sutherland, I., 228, 1090 Sutphen, S., 194, 1088 Suttner, C., 360, 1090 Sutton, C., 885, 1090 Sutton, R. S., 685, 854–857, 1065 ,1090 Svartvik, J., 920, 1086 Svestka, P., 1013, 1078 Svetnik, V .

Token 23466:
B., 605, 1093 Svore, K., 884, 1090 Swade, D., 14, 1090 Swartz, R., 1022, 1067 Swedish, 32 Swerling, P., 604, 1090 Swift, T., 359, 1090 switching Kalman ﬁlter, 589, 608 syllogism, 4, 275 symbolic differentiation, 364 symbolic integration, 776symmetry breaking (in CSPs), 226 synapse, 11 synchro drive, 976 synchronization, 427 synonymy, 465, 870syntactic ambiguity, 905, 920 syntactic categories, 888 syntactic sugar, 304 syntactic theory (of knowledge), 470 syntax, 23, 240, 244 of logic, 274 of natural language, 888of probability, 488 synthesis, 356 deductive, 356 synthesis of algorithms, 356Syrj¨anen, T., 472, 1084 ,1090systems reply, 1031 Szafron, D., 678, 687, 1066 , 1091 Szathm´ ary, E., 155, 1089 Szepesvari, C., 194, 1078 T T(ﬂuent holds), 446 T-S CHED , 432 T4, 431T ABLE -DRIVEN -AGENT ,47 table lookup, 737 table tennis, 32tabu search, 154, 222 tactile sensors, 974 Tadepalli, P., 799, 857, 1090 Tait, P. G., 109, 1090 Takusagawa, K. T., 556, 1085 Talos, 1011 T ALPLANNER , 387 Tamaki, H., 359, 883, 1084 ,1090 Tamaki, S., 277, 1077 Tambe, M., 230, 1085 Tank, D. W., 11, 1084 Tardos, E., 688, 1084 Tarjan, R. E., 1059, 1090 Tarski, A., 8, 314, 920, 1090 Tash, J. K., 686, 1090 Taskar, B., 556, 1073 ,1090 task environment, 40,5 9 task network, 394 Tasmania, 222Tate, A., 394, 396, 408, 431, 432, 1064, 1065 ,1090 Tatman, J.

Token 23467:
A., 687, 1090 Tattersall, C., 176, 1090 taxi, 40, 694 in Athens, 509automated, 56, 236, 480, 695, 1047 taxonomic hierarchy, 24, 440taxonomy, 440, 465, 469 Taylor, C., 763, 968, 1070 ,1082 Taylor, G., 358, 1090 Taylor, M., 469, 1089 Taylor, R., 1013, 1081 Taylor, W., 9, 229, 277, 1068 Taylor expansion, 982 TD-G AMMON , 186, 194, 850, 851 Teh, Y .

Token 23468:
W., 1047, 1075 telescope, 562television, 860Teller, A., 155, 554, 1082 Teller, E., 155, 554, 1082 Teller, S., 1012, 1066 Temperley, D., 920, 1089 template, 874temporal difference learning, 836–838, 853, 854 temporal inference, 570–578temporal logic, 289 temporal projection, 278 temporal reasoning, 566–609temporal substance, 447 Tenenbaum, J., 314, 1090 Teng, C.-M., 505, 1079 Tennenholtz, M., 855, 1067 tennis, 426 tense, 902 term (in logic), 294, 294 ter Meulen, A., 314, 1091 terminal states, 162 terminal symbol, 890, 1060 terminal test, 162 termination condition, 995term rewriting, 359 Tesauro, G., 180, 186, 194, 846, 850, 855, 1090 test set, 695 T ETRAD , 826 Teukolsky, S. A., 155, 1086 texel, 951 text classiﬁcation, 865, 882 TEXTRUNNER , 439, 881, 882, 885 texture, 939, 948, 951 texture gradient, 967 Teyssier, M., 826, 1090 Thaler, R., 637, 638, 1090 thee and thou, 890 THEO, 1047 Theocharous, G., 605, 1090 theorem, 302 incompleteness, 8, 352, 1022 theorem prover, 2, 356theorem proving, 249, 393 mathematical, 21, 32 Theseus, 758Thiele, T., 604, 1090 Thielscher, M., 279, 470, 1090 thingiﬁcation, 440 thinking humanly, 3thinking rationally, 4Thitimajshima, P., 555, 1065 Thomas, A., 554, 555, 826, 1073 Thomas, J., 763, 1069 Thompson, H., 884, 1066 Thompson, K., 176, 192, 1069 ,1090 thought, 4, 19, 234 laws of, 4 thrashing, 102 3-SAT, 277, 334, 362 threshold function, 724 Throop, T. A., 187, 195, 1089

Token 23469:


Token 23470:
Index 1129 Thrun, S., 28, 605, 686, 884, 1012–1014, 1067, 1068 ,1072 , 1083–1085 ,1087 ,1090, 1091 Tibshirani, R., 760, 761, 763, 827, 1073 , 1075 tic-tac-toe, 162, 190, 197 Tikhonov, A. N., 759, 1090 tiling, 737 tilt,957 time (in grammar), 902 time complexity, 80, 108 time expressions, 925 time interval, 470 time of ﬂight camera, 974 time slice (in DBNs), 567 Tinsley, M., 193 Tirole, J., 688, 1073 Tishby, N., 604, 1072 tit for tat, 674 Titterington, D. M., 826, 1090 TLPLAN, 387 TMS, 229, 461, 460–462, 472, 1041 Tobarra, L., 279, 1064 Tofﬂer, A., 1034, 1090 tokenization, 875Tomasi, C., 951, 968, 1090 toothache, 481 topological sort, 223 torque sensor, 975 Torralba, A., 741, 1090 Torrance, M. C., 231, 1073 Torras, C., 156, 433, 1077 total cost, 80, 102 Toth, P., 395, 1068 touring problem, 74 toy problem, 69 TPTP, 360 trace, 904 tractability of inference, 8, 457 trading, 477 tragedy of the commons, 683 trail, 340 training curve, 724 set,695 replicated, 749 weighted, 749 transfer model (in MT), 908 transhumanism, 1038 transient failure, 592 transient failure model, 593 transition matrix, 564 transition model, 67, 108, 134, 162, 266, 566, 597, 603, 646, 684, 832,979 transition probability, 536transitivity (of preferences), 612 translation model, 909 transpose, 1056 transposition (in a game), 170 transposition table, 170 traveling salesperson problem, 74 traveling salesperson problem (TSP), 74, 110, 112, 119 Traverso, P., 275, 372, 386, 395, 396, 433, 1066 ,1068 ,1073 ,1088 tree, 223 T REE-CSP-S OLVER ,224 TREE-SEARCH ,77 treebank, 895, 919 Penn, 881, 895 tree decomposition, 225, 227 tree width, 225, 227, 229, 434, 529 trial, 832 triangle inequality, 95 trichromacy, 935 Triggs, B., 946, 968, 1069 Troyanskii, P., 922 Trucco, E., 968, 1090 truth, 240, 295 functionality, 547, 552 preserving inference, 242 table, 245, 276 truth maintenance system (TMS), 229, 461, 460–462, 472, 1041 assumption-based, 462 justiﬁcation-based, 461 truth value, 245 Tsang, E., 229, 1076 Tsitsiklis, J. N., 506, 685, 686, 847, 855, 857, 1059, 1066 ,1081 ,1084 , 1090 TSP, 74, 110, 112, 119 TT-C HECK -ALL,248 TT-E NTAILS ?,248 Tumer, K., 688, 1090 Tung, F., 604, 1086 tuple, 291 turbo decoding, 555 Turcotte, M., 797, 1090 Turing, A., 2, 8, 14, 16, 17, 19, 30, 31, 54, 192, 325, 358, 552, 761, 854,1021, 1022, 1024, 1026, 1030,1043, 1052, 1090 Turing award, 1059 Turing machine, 8, 759 Turing Test, 2, 2–4, 30, 31, 860, 1021 total, 3 Turk, 190 Tversky, A., 2, 517, 620, 638, 1072 , 1077 ,1090 T WEAK , 394Tweedie, F. J., 886, 1078 twin earths, 1041 two-ﬁnger Morra, 6662001: A Space Odyssey, 552type signature, 542 typical instance, 443Tyson, M., 884, 1075 U U(utility), 611 u/latticetop(best prize), 615 u⊥(worst catastrophe), 615 UCPOP, 394 UCT (upper conﬁdence bounds on trees), 194 UI (Universal Instantiation), 323Ulam, S., 192, 1078 Ullman, J. D., 358, 1059, 1064, 1065 , 1090 Ullman, S., 967, 968, 1076 ,1090 ultraintelligent machine, 1037 Ulysses, 1040 unbiased (estimator), 618 uncertain environment, 43 uncertainty, 23, 26, 438, 480–509, 549, 1025 existence, 541 identity, 541, 876 relational, 543 rule-based approach to, 547summarizing, 482and time, 566–570 unconditional probability, see probability, prior undecidability, 8undergeneration, 892 unicorn, 280 uniﬁcation, 326, 326–327, 329, 357 and equality, 353equational, 355 uniﬁer, 326 most general (MGU), 327, 329, 353, 361 U NIFORM -COST-SEARCH ,84 uniform-cost search, 83, 83–85, 108 uniform convergence theory, 759 uniform prior, 805uniform probability distribution, 487uniform resource locator (URL), 463 U NIFY ,328 UNIFY -VAR,328 Unimate, 1011 uninformed search, 64, 81, 81–91, 108, 110 unique action axioms, 389 unique names assumption, 299, 540

Token 23471:


Token 23472:
1130 Index unit (in a neural network), 728 unit clause, 253, 260, 355 United States, 13, 629, 640, 753, 755, 922, 1034, 1036 unit preference, 355unit preference strategy, 355 unit propagation, 261 unit resolution, 252, 355 units function, 444 universal grammar, 921 Universal Instantiation, 323 universal plan, 434 unmanned air vehicle (UA V), 971 unmanned ground vehicle (UGV), 971 U NPOP, 394 unrolling, 544, 595 unsatisﬁability, 274 unsupervised learning, 694, 817–820, 1025 UOSAT -II, 432 update, 142upper ontology, 467URL, 463Urmson, C., 1014, 1091 urn-and-ball, 803 URP, 638 Uskov, A. V ., 192, 1064 Utgoff, P. E., 776, 799, 1082 utilitarianism, 7utility, 9, 53, 162, 482 axioms of, 613 estimation, 833 expected, 53, 61, 483, 610, 611, 616 function, 53, 54, 162, 611, 615–621, 846 independence, 626 maximum expected, 483, 611 of money, 616–618multiattribute, 622–626, 636, 648multiplicative, 626 node, 627 normalized, 615 ordinal, 614 theory, 482, 611–615, 636 utility-based agent, 1044utopia, 1052 UWL, 433 V vacuum tube, 16vacuum world, 35, 37, 62, 159 erratic, 134 slippery, 137 vagueness, 547Valiant, L., 759, 1091validation cross, 737, 759, 767 validation, cross, 708 validation set, 709 validity, 249, 274 value, 58 V ALUE -ITERA TION ,653 value determination, 691 value function, 614 additive, 625 value iteration, 652, 652–656, 684 point-based, 686 value node, seeutility node value of computation, 1048value of information, 628–633, 636, 644, 659, 839, 1025, 1048 value of perfect information, 630 value symmetry, 226 V AMPIRE , 359, 360 van Beek, P., 228–230, 395, 470, 1065 , 1078 ,1087 ,1091 van Bentham, J., 314, 1091 Vandenberghe, L., 155, 1066 van Harmelen, F., 473, 799, 1091 van Heijenoort, J., 360, 1091 van Hoeve, W.-J., 212, 228, 1091 vanishing point, 931 van Lambalgen, M., 470, 1091 van Maaren, H., 278, 1066 van Nunen, J.

Token 23473:
A. E. E., 685, 1091 van Run, P., 230, 1065 van der Gaag, L., 505, 1081 Van Emden, M. H., 472, 1091 Van Hentenryck, P., 228, 1091 Van Roy, B., 847, 855, 1090, 1091 Van Roy, P. L., 339, 342, 359, 1091 Vapnik, V .

Token 23474:
N., 759, 760, 762, 763, 967, 1066 ,1069 ,1080 ,1091 Varaiya, P., 60, 856, 1072 ,1079 Vardi, M. Y ., 470, 477, 1072 variabilization (in EBL), 781 variable, 58 atemporal, 266 elimination, 524, 524–528, 552, 553, 596 in continuous state space, 131 indicator, 819 logic, 340 in logic, 295 ordering, 216, 527 random, 486, 515 Boolean, 486continuous, 487, 519, 553 relevance, 528 Varian, H. R., 688, 759, 1081 ,1091 variational approximation, 554variational parameter, 554 Varzi, A., 470, 1068 Vaucanson, J., 1011 Vauquois, B., 909, 1091 Vazirani, U., 154, 763, 1064 ,1078 Vazirani, V ., 688, 1084 VC dimension, 759 VCG, 683 Vecchi, M. P., 155, 229, 1078 vector, 1055 vector ﬁeld histograms, 1013 vector space model, 884vehicle interface layer, 1006 Veloso, M., 799, 1091 Vempala, S., 883, 1084 Venkataraman, S., 686, 1074 Venugopal, A., 922, 1093 Vere, S. A., 431, 1091 veriﬁcation, 356 hardware, 312 Verma, T., 553, 826, 1073 ,1085 Verma, V ., 605, 1091 Verri, A., 968, 1090 V ERSION -SPACE -LEARNING ,773 VERSION -SPACE -UPDATE ,773 version space, 773, 774, 798 version space collapse, 776Vetterling, W. T., 155, 1086 Vickrey, W., 681 Vickrey-Clarke-Groves, 683 Vienna, 1028 views, multiple, 948 Vinge, V ., 12, 1038, 1091 Viola, P., 968, 1025, 1091 virtual counts, 812 visibility graph, 1013 vision, 3, 12, 20, 228, 929–965 Visser, U., 195, 1014, 1091 Visser, W., 356, 1075 Vitali set, 489 Vitanyi, P. M. B., 759, 1080 Viterbi, A. J., 604, 1091 Viterbi algorithm, 578 Vlassis, N., 435, 686, 1089 ,1091 VLSI layout, 74, 110, 125 vocabulary, 864 V olk, K., 826, 1074 von Mises, R., 504, 1091 von Neumann, J., 9, 15, 17, 190, 613, 637, 687, 1091 von Stengel, B., 677, 687, 1078 von Winterfeldt, D., 637, 1091 von Kempelen, W., 190 von Linne, C., 469V oronkov, A., 314, 359, 360, 1086, 1087

Token 23475:


Token 23476:
Index 1131 V oronoi graph, 991 V ossen, T., 396, 1091 voted perceptron, 760VPI (value of perfect information), 630 W Wadsworth, C. P., 314, 1074 Wahba, G., 759, 1074 Wainwright, M. J., 278, 555, 1081 ,1091 Walden, W., 192, 1078 Waldinger, R., 314, 394, 1081 ,1091 Walker, E., 29, 1069 Walker, H., 826, 1074 WALKSAT, 263, 395 Wall, R., 920, 1071 Wallace, A. R., 130, 1091 Wallace, D. L., 886, 1083 Walras, L., 9Walsh, M. J., 156, 1072 Walsh, T., 228, 230, 278, 1066 ,1087 , 1089 Walsh, W., 688, 1092 Walter, G., 1011 Waltz, D., 20, 228, 760, 1089 ,1091 WAM, 341, 359Wang, D. Z., 885, 1067 Wang, E., 472, 1090 Wang, Y ., 194, 1091 Wanner, E., 287, 1091 Warmuth, M., 109, 759, 1066 ,1086 W ARPLAN , 394 Warren, D. H. D., 339, 341, 359, 394, 889, 1085 ,1091 Warren, D. S., 359, 1090 Warren Abstract Machine (WAM), 341, 359 washing clothes, 927 Washington, G., 450wasp, sphex, 39, 425Wasserman, L., 763, 1091 Watkins, C. J., 685, 855, 1091 Watson, J., 12Watson, J. D., 130, 1091 Watt, J., 15Wattenberg, M., 155, 1077 Waugh, K., 687, 1091 W BRIDGE 5, 195 weak AI, 1020 , 1040 weak domination, 668 weak method, 22 Weaver, W., 703, 758, 763, 883, 907, 908, 922, 1088 ,1091 Webber, B. L., 31, 1091 Weber, J., 604, 1076 Wefald, E. H., 112, 191, 198, 1048, 1087Wegbreit, B., 1012, 1083 Weglarz, J., 432, 1066 Wei, X., 885, 1085 Weibull, J., 688, 1091 Weidenbach, C., 359, 1091 weight, 718 weight (in a neural network), 728 WEIGHTED -SAMPLE ,534 weighted linear function, 172 weight space, 719 Weinstein, S., 759, 1084 Weiss, G., 61, 435, 1091 Weiss, S., 884, 1064 Weiss, Y ., 555, 605, 741, 1083 , 1090–1092 Weissman, V ., 314, 1074 Weizenbaum, J., 1035, 1041, 1091 Weld, D. S., 61, 156, 394–396, 432, 433, 469, 472, 885, 1036, 1069 , 1071, 1072 ,1079 ,1085 ,1089 , 1091, 1092 Wellman, M. P., 10, 555, 557, 604, 638, 685–688, 857, 1013, 1070 ,1076 , 1091, 1092 Wells, H. G., 1037, 1092 Wells, M., 192, 1078 Welty, C., 469, 1089 Werbos, P., 685, 761, 854, 1092 Wermuth, N., 553, 1080 Werneck, R. F., 111, 1074 Wertheimer, M., 966 Wesley, M. A., 1013, 1092 West, Col., 330 Westinghouse, 432 Westphal, M., 395, 1086 Wexler, Y ., 553, 1092 Weymouth, T., 1013, 1069 White, J. L., 356, 1075 Whitehead, A. N., 16, 357, 781, 1092 Whiter, A. M., 431, 1090 Whittaker, W., 1014, 1091 Whorf, B., 287, 314, 1092 wide content, 1028 Widrow, B., 20, 761, 833, 854, 1092 Widrow–Hoff rule, 846 Wiedijk, F., 360, 1092 Wiegley, J., 156, 1092 Wiener, N., 15, 192, 604, 761, 922, 1087 ,1092 wiggly belief state, 271Wilczek, F., 761, 1065 Wilensky, R., 23, 24, 1031, 1092 Wilfong, G. T., 1012, 1069 Wilkins, D. E., 189, 431, 434, 1092 Williams, B., 60, 278, 432, 472, 1083 , 1092Williams, C. K. I., 827, 1086 Williams, R., 640 Williams, R. J., 685, 761, 849, 855, 1085 ,1087 ,1092 Williamson, J., 469, 1083 Williamson, M., 433, 1072 Willighagen, E. L., 469, 1083 Wilmer, E. L., 604, 1080 Wilson, A., 921, 1080 Wilson, R., 227, 1092 Wilson, R. A., 3, 1042, 1092 Windows, 553 Winikoff, M., 59, 1084 Winker, S., 360, 1092 Winkler, R. L., 619, 637, 1089 winner’s curse, 637 Winograd, S., 20, 1092 Winograd, T., 20, 23, 884, 1066 ,1092 Winston, P. H., 2, 20, 27, 773, 798, 1065 ,1092 Wintermute, S., 358, 1092 Witbrock, M., 469, 1081 Witten, I. H., 763, 883, 884, 921, 1083 , 1092 Wittgenstein, L., 6, 243, 276, 279, 443, 469, 1092 Wizard, 553W¨ohler, F., 1027 Wojciechowski, W. S., 356, 1092 Wojcik, A. S., 356, 1092 Wolf, A., 920, 1074 Wolfe, D., 186, 1065 Wolfe, J., 157, 192, 432, 1081 ,1087 , 1092 Wolpert, D., 688, 1090 Wong, A., 884, 1087 Wong, W.-K., 826, 1083 Wood, D. E., 111, 1080 Woods, W. A., 471, 921, 1092 Wooldridge, M., 60, 61, 1068 ,1092 Woolsey, K., 851workspace representation, 986 world model, in disambiguation, 906 world state, 69World War II, 10, 552, 604 World Wide Web (WWW), 27, 462, 867, 869 worst possible catastrophe, 615 Wos, L., 359, 360, 1092 wrapper (for Internet site), 466 wrapper (for learning), 709 Wray, R. E., 358, 1092 W r i g h t ,O .a n dW .

Token 23477:
,3Wright, R. N., 884, 1085 Wright, S., 155, 552, 1092 Wu, D., 921, 1092

Token 23478:


Token 23479:
1132 Index Wu, E., 885, 1067 Wu, F., 469, 1092 wumpus world, 236, 236–240, 246–247, 279, 305–307, 439, 499–503,509 Wundt, W., 12Wurman, P., 688, 1092 WWW, 27, 462, 867, 869 X XCON, 336 XML, 875xor, 246, 766Xu, J., 358, 1092 Xu, P., 29, 921, 1067 Y Yakimovsky, Y ., 639, 1072 Yale, 23Yan, D., 431, 1073 Yang, C. S., 884, 1087 Yang, F., 107, 1092 Yang, Q., 432, 1092 Yannakakis, M., 157, 229, 1065 ,1084 Yap, R. H. C., 359, 1077 Yardi, M., 278, 1068 Yarowsky, D., 27, 885, 1092 Yates, A., 885, 1072Yedidia, J., 555, 1092 Yglesias, J., 28, 1064 Yip, K. M.-K., 472, 1092 Yngve, V ., 920, 1092 Yob, G., 279, 1092 Yoshikawa, T., 1013, 1092 Young, H. P., 435, 1092 Young, M., 797, 1078 Young, S. J., 896, 920, 1080 Younger, D. H., 920, 1092 Yu, B., 553, 1068 Yudkowsky, E., 27, 1039, 1093 Yung, M., 110, 119, 1064 ,1075 Yvanovich, M., 432, 1070 Z Z-3, 14 Zadeh, L. A., 557, 1093 Zahavi, U., 107, 1092 Zapp, A., 1014, 1071 Zaragoza, H., 884, 1069 Zaritskii, V .

Token 23480:
S., 605, 1093 zebra puzzle, 231 Zecchina, R., 278, 1084 Zeldner, M., 908 Zelle, J., 902, 921, 1093 Zeng, H., 314, 1082 Zermelo, E., 687, 1093zero-sum game, 161, 162, 199, 670 Zettlemoyer, L. S., 556, 921, 1082 ,1093 Zhai, C., 884, 1079 Zhang, H., 277, 1093 Zhang, L., 277, 553, 1083 ,1093 Zhang, N. L., 553, 639, 1093 Zhang, W., 112, 1079 Zhang, Y ., 885, 1067 Zhao, Y ., 277, 1083 Zhivotovsky, A.

Token 23481:
A., 192, 1064 Zhou, R., 112, 1093 Zhu, C., 760, 1067 Zhu, D. J., 1012, 1093 Zhu, W. L., 439, 1089 Zilberstein, S., 156, 422, 433, 434, 1075 ,1085 Zimdars, A., 857, 1087 Zimmermann, H.-J., 557, 1093 Zinkevich, M., 687, 1093 Zisserman, A., 960, 968, 1075 ,1086 Zlotkin, G., 688, 1087 Zog, 778Zollmann, A., 922, 1093 Zuckerman, D., 124, 1081 Zufferey, J. C., 1045, 1072 Zuse, K., 14, 192 Zweben, M., 432, 1070 Zweig, G., 604, 1093 Zytkow, J. M., 800, 1079

