{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens saved to the knowledge_base folder in the file: AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def read_pdf_from_local(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += f\"Page {page_num + 1}:\\n{pdf_reader.pages[page_num].extract_text()}\\n\\n\"\n",
    "    return text\n",
    "\n",
    "def read_pdf_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    text = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += f\"Page {page_num + 1}:\\n{pdf_reader.pages[page_num].extract_text()}\\n\\n\"\n",
    "    return text\n",
    "\n",
    "def create_tokens(text):\n",
    "    # Split the text into paragraphs of 300 characters or at the end of a sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = []\n",
    "    current_token = \"\"\n",
    "    current_page = None\n",
    "    for sentence in sentences:\n",
    "        # Check for the \"Page\" marker\n",
    "        match = re.match(r'Page (\\d+):', sentence)\n",
    "        if match:\n",
    "            current_page = int(match.group(1))\n",
    "        elif len(current_token) + len(sentence) <= 300:\n",
    "            current_token += sentence + ' '\n",
    "        else:\n",
    "            tokens.append((current_page, current_token.strip()))\n",
    "            current_token = sentence + ' '\n",
    "    if current_token:\n",
    "        tokens.append((current_page, current_token.strip()))\n",
    "    return tokens\n",
    "\n",
    "def save_tokens_to_file(tokens, file_path):\n",
    "    # Create the folder and parent directory if they don't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save all tokens to a single file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for page, token in tokens:\n",
    "            file.write(f\"Page {page}:\\n{token}\\n\\n\")\n",
    "\n",
    "def main():\n",
    "    # Ask the user for the PDF file path or URL\n",
    "    pdf_source = input(\"Enter the path to the local PDF file or the URL of the PDF: \")\n",
    "\n",
    "    # Check if the input is a local file path or a URL\n",
    "    if pdf_source.startswith(('http://', 'https://')):\n",
    "        # Read PDF from URL\n",
    "        pdf_text = read_pdf_from_url(pdf_source)\n",
    "    else:\n",
    "        # Read PDF from local file\n",
    "        pdf_text = read_pdf_from_local(pdf_source)\n",
    "\n",
    "    # Tokenize the text into paragraphs with page numbers\n",
    "    tokens = create_tokens(pdf_text)\n",
    "\n",
    "    # Prompt the user for the file name\n",
    "    file_name = input(\"Enter the desired file name (including extension, e.g., all_tokens.txt): \")\n",
    "\n",
    "    # Create a folder named \"knowledge_base\" and save all tokens to the specified file\n",
    "    knowledge_base_folder = \"knowledge_base\"\n",
    "    knowledge_base_file = os.path.join(knowledge_base_folder, file_name)\n",
    "    save_tokens_to_file(tokens, knowledge_base_file)\n",
    "\n",
    "    print(f\"Tokens saved to the knowledge_base folder in the file: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens saved to the knowledge_base folder in the file: AI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def read_pdf_from_local(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += f\"Page {page_num + 1}: {pdf_reader.pages[page_num].extract_text()}\\n\"\n",
    "    return text\n",
    "\n",
    "def read_pdf_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    text = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        text += f\"Page {page_num + 1}: {pdf_reader.pages[page_num].extract_text()}\\n\"\n",
    "    return text\n",
    "\n",
    "def create_tokens(text):\n",
    "    # Split the text into paragraphs of 300 characters or at the end of a sentence\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "    tokens = []\n",
    "    current_token = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_token) + len(sentence) <= 300:\n",
    "            current_token += sentence + ' '\n",
    "        else:\n",
    "            tokens.append(current_token.strip())\n",
    "            current_token = sentence + ' '\n",
    "    if current_token:\n",
    "        tokens.append(current_token.strip())\n",
    "    return tokens\n",
    "\n",
    "def save_tokens_to_file(tokens, file_path):\n",
    "    # Create the folder and parent directory if they don't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    \n",
    "    # Save all tokens to a single file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(tokens))\n",
    "\n",
    "def main():\n",
    "    # Ask the user for the PDF file path or URL\n",
    "    pdf_source = input(\"Enter the path to the local PDF file or the URL of the PDF: \")\n",
    "\n",
    "    # Check if the input is a local file path or a URL\n",
    "    if pdf_source.startswith(('http://', 'https://')):\n",
    "        # Read PDF from URL\n",
    "        pdf_text = read_pdf_from_url(pdf_source)\n",
    "    else:\n",
    "        # Read PDF from local file\n",
    "        pdf_text = read_pdf_from_local(pdf_source)\n",
    "\n",
    "    # Tokenize the text into paragraphs\n",
    "    tokens = create_tokens(pdf_text)\n",
    "\n",
    "    # Prompt the user for the file name\n",
    "    file_name = input(\"Enter the desired file name (including extension, e.g., all_tokens.txt): \")\n",
    "\n",
    "    # Create a folder named \"knowledge_base\" and save all tokens to the specified file\n",
    "    knowledge_base_folder = \"knowledge_base\"\n",
    "    knowledge_base_file = os.path.join(knowledge_base_folder, file_name)\n",
    "    save_tokens_to_file(tokens, knowledge_base_file)\n",
    "\n",
    "    print(f\"Tokens saved to the knowledge_base folder in the file: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
